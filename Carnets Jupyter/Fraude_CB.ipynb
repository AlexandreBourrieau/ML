{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Fraude_CB.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/Fraude_CB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xohAxkytgvbx"
      },
      "source": [
        "# Détection de fraudes à la carte bancaire\n",
        "\n",
        "Cette base de données contient des transactions effectuées par CB en septembre 2013 par des utilisateurs européens. Les données ont été enregistrées sur deux jours, pendant lesquels 492 utilisations frauduleuses sur un total de 284 807 transactions ont été enregistrées.  \n",
        "\n",
        "Ce ratio est fortement déséquilibré car les cas de fraudes ne représentent que 0.172% des transactions. Un apprentissage supervisé n'est donc pas possible ici.  \n",
        "\n",
        "Nous allons utiliser un auto-encodeur pour effectuer un apprentissage non supervisé afin de détecter des irrégularités dans les données, indices de fraudes.  \n",
        "\n",
        "Les données sont disponibles sur le site de [Kaggle](https://www.kaggle.com/mlg-ulb/creditcardfraud)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPIVi6aGgvb0"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqbDAvEbgvb7"
      },
      "source": [
        "## Lecture des données\n",
        "\n",
        "Le nombre de données est très grand. Pour réduire le temps de calcul, nous n'utiliserons qu'une partie d'entre elles.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmDvv8e0NIfT"
      },
      "source": [
        "!wget --no-check-certificate --content-disposition \"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/Donn%C3%A9es/creditcard.zip?raw=true\"\n",
        "!unzip creditcard.zip -x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyfM9Ccc4nLc"
      },
      "source": [
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSEi4bFBgvb8"
      },
      "source": [
        "data = pd.read_csv(\"creditcard.csv\")\n",
        "data = data.head(30000)\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QqUPf5N5tAn"
      },
      "source": [
        "Les colonnes qui ne sont pas anonymisées sont :\r\n",
        "- Time : Temps écoulé depuis la première transaction\r\n",
        "- Amount : Montant de la transaction\r\n",
        "- Class : 0 = Achat non frauduleux / 1 = achat frauduleux\r\n",
        "  \r\n",
        "Les autres données anonymes sont par exemple les coordonnées de l'acheteur, son adresse, ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zh-HZs3Gv5c2"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgMLc0626hzL"
      },
      "source": [
        "Regardons combien nous avons de données frauduleuses :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDDQ22kt81nt"
      },
      "source": [
        "data.groupby(['Class']).count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLM0ajP88zQO"
      },
      "source": [
        "100*(94/29906)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxjaflpVgvcD"
      },
      "source": [
        "## Préparation des données\n",
        "\n",
        "La colonne Time tout d'abord ignorée"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JB_FNz-4gvcE"
      },
      "source": [
        "data = data.drop(['Time'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbwh5kGRtcqI"
      },
      "source": [
        "Ensuite, les montants des transactions sont normalisés en supprimant la valeur moyenne et en faisant en sorte que la variance soit égale à 1. Les autres colonnes sont inchangées."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RArB6Uqst6Ng"
      },
      "source": [
        "# Histogramme des montants avant normalisation\r\n",
        "\r\n",
        "df = data['Amount']\r\n",
        "print(df.describe())\r\n",
        "df.hist(bins=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TcSpH40vE5j"
      },
      "source": [
        "# Normalisation des montants\r\n",
        "\r\n",
        "scaler = StandardScaler()\r\n",
        "data['Amount'] = scaler.fit_transform(data['Amount'].values.reshape(-1, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1t2FxySvRkh"
      },
      "source": [
        "# Histogramme des montants après normalisation\r\n",
        "\r\n",
        "df = data['Amount']\r\n",
        "print(df.describe())\r\n",
        "df.hist(bins=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VhZZRvwgvcG"
      },
      "source": [
        "# Séparation des données d'entrainement et de test\n",
        "\n",
        "x_entrainement, x_test = train_test_split(data, test_size=0.2, random_state=0)\n",
        "x_entrainement = x_entrainement.drop(['Class'], axis=1)\n",
        "y_test = x_test['Class']\n",
        "x_test = x_test.drop(['Class'], axis=1)\n",
        "x_entrainement = x_entrainement.values\n",
        "x_test = x_test.values\n",
        "x_entrainement.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idV-blvXgvcJ"
      },
      "source": [
        "## Construction du modèle\n",
        "\n",
        "Ce modèle est un modèle standard d'auto-encodeur à quatre couches :\n",
        "- Une couche d'entrée à 14 neurones, avec une fonction d'activation de type tanh\n",
        "- Une première couche cachées de 14/2 = 7 neurones, avec une fonction d'activation de type relu\n",
        "- Une deuxième couche cachée de 14/2 = 7 neurones, avec une fonction d'activation de type tanh\n",
        "- Une couche de sortie à 29 neurones (même dimension que les données d'entrées), avec une fonction d'activation de type relu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bJv2G4ENkkR"
      },
      "source": [
        "La structure de notre modèle peut être visualisée comme ci-dessous :  \r\n",
        "![picture](https://github.com/AlexandreBourrieau/ML-F1/blob/master/Carnets%20Jupyter/Images/Schema_Non_Supervis%C3%A932.png?raw=true \"ReseauNeurone\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDTwKbXqln-T"
      },
      "source": [
        "dimension_entrees = x_entrainement.shape[1]\n",
        "dimension_encodeur = 14\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(dimension_encodeur, activation=\"tanh\", input_shape=(dimension_entrees,)))\n",
        "model.add(Dense(int(dimension_encodeur /2), activation=\"relu\"))\n",
        "model.add(Dense(int(dimension_encodeur /2), activation='tanh'))\n",
        "model.add(Dense(dimension_entrees, activation='relu'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvOSXfO2yRAF"
      },
      "source": [
        "# Entrainement du modèle\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sw64M6v7gvcN"
      },
      "source": [
        "nb_iterations = 50\n",
        "batch_size = 30\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['acc'])\n",
        "\n",
        "history = model.fit(x_entrainement, x_entrainement,\n",
        "                    epochs=nb_iterations,\n",
        "                    batch_size=batch_size,\n",
        "                    validation_data=(x_test, x_test),\n",
        "                    verbose=1)\n",
        "\n",
        "autoencoder = model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOreexr_gvcP"
      },
      "source": [
        "## Performances du modèle\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KU1Y1-12gvcQ"
      },
      "source": [
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Précision')\n",
        "plt.xlabel('Itérations')\n",
        "plt.legend(['Entrainement', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX3kJxBEWYPj"
      },
      "source": [
        "## Structure du modèle et calcul de l'erreur de prédiction\r\n",
        "La structure de notre modèle peut être visualisée comme ci-dessous :  \r\n",
        "![picture](https://github.com/AlexandreBourrieau/ML-F1/blob/master/Carnets%20Jupyter/Images/Schema_Non_Supervis%C3%A932.png?raw=true \"ReseauNeurone\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wolqQ57_bgxY"
      },
      "source": [
        "L'erreur de prédiction est l'erreur quadratique moyenne entre les entrées et les sorties (écart type). Pour chaque couple d'entrée / sortie, on calcule donc la valeur suivante :\r\n",
        "\r\n",
        " $Erreur =  \\sqrt {{{({V_{1{\\rm{\\_sortie}}}} - {V_{1{\\rm{\\_entr}}e }})}^2} + {{({V_{2{\\rm{\\_sortie}}}} - {V_{2{\\rm{\\_entr}}e }})}^2} + ... + {{(Amoun{t_{{\\rm{\\_sortie}}}} - Amoun{t_{{\\rm{\\_entr}}e }})}^2}} $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npC653kSONK5"
      },
      "source": [
        "x_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EK8OLAVegvcT"
      },
      "source": [
        "## Prédictions\n",
        "\n",
        "Les prédictions sont effectuées sur les données de tests. L'erreur quadratique moyenne (mean-squared error - MSE) est calculés entre les tests et les prédictions. Si cette erreur est grande, c'est une potentielle anomalie et donc une fraude potentielle. Bien sûr, cela n'est pas parfait et il y a des faux-positifs (et des faux négatifs) !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pedvVXtngvcU"
      },
      "source": [
        "predictions = autoencoder.predict(x_test)\n",
        "mse = np.mean(np.power(x_test - predictions, 2), axis=1)\n",
        "error_df = pd.DataFrame({'erreur_reconstruction': mse, 'class': y_test})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaYsHkoIUb53"
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WN6s2FeOjSl"
      },
      "source": [
        "predictions\r\n",
        "mse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVXgySCoNb-O"
      },
      "source": [
        "error_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXPep-KbgvcX"
      },
      "source": [
        "## Affichage de l'erreur de reconstruction\n",
        "\n",
        "L'erreur de reconstruction est affichée pour chaque échantillon. Seuls 6000 échantillons sont affichés, mais les données affichées sont tirées au hasard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3TXPKJIgvcX"
      },
      "source": [
        "niveau_detection = 7\n",
        "\n",
        "groups = error_df.groupby('class')\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "for name, group in groups:\n",
        "    ax.plot(group.index, group.erreur_reconstruction, marker='o', ms=2.0, linestyle='',\n",
        "            label = \"Fraude\" if name == 1 else \"Normal\",\n",
        "            color = \"red\" if name == 1 else \"blue\")\n",
        "ax.hlines(niveau_detection, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"green\", zorder=100, label='Niveau de détection')\n",
        "ax.legend()\n",
        "plt.title(\"Erreur de reconstruction pour différents échantillons\")\n",
        "plt.ylabel(\"Erreur de reconstruction\")\n",
        "plt.xlabel(\"Index de l'échantillon\")\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4JxXgvyxJku"
      },
      "source": [
        "# Analyse\n",
        "\n",
        "Comme nous connaissons les transactions frauduleuses, nous pouvons afficher et calculer le nombre de faux positifs. Dans l'idéal, il faudrait que leur nombre soit très faible, mais notre modèle est imparfait... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcYrqCvrgvce"
      },
      "source": [
        "transactions_normales = error_df[error_df['class'] == 0]\n",
        "transactions_frauduleuses = error_df[error_df['class'] == 1]\n",
        "\n",
        "print('Transactions normales : %d, Transactions frauduleuse : %d' % (len(transactions_normales), len(transactions_frauduleuses)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQXyjh6Wb54u"
      },
      "source": [
        "transactions_frauduleuses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ySewU_gsZBZ"
      },
      "source": [
        "vraies_transactions_frauduleuses = len(transactions_frauduleuses[transactions_frauduleuses['erreur_reconstruction'] >= niveau_detection])\n",
        "vraies_transactions_normales = len(transactions_normales[transactions_normales['erreur_reconstruction'] < niveau_detection])\n",
        "\n",
        "faux_positifs = len(transactions_normales[transactions_normales['erreur_reconstruction'] >= niveau_detection])\n",
        "faux_negatifs = len(transactions_frauduleuses[transactions_frauduleuses['erreur_reconstruction'] < niveau_detection])\n",
        "\n",
        "print('Faux positifs: %d, Vraies transactions frauduleuses: %d' % (faux_positifs, vraies_transactions_frauduleuses))\n",
        "print('Faux négatifs : %d, Vraies transactions normales: %d' % (faux_negatifs, vraies_transactions_normales))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbU9T7gMgvca"
      },
      "source": [
        "## Matrice de synthèse\n",
        "\n",
        "La matrice de synthèse ci-dessous permet d'avoir une vue d'ensemble des résultats obtenus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zb_Azs0Xgvcb"
      },
      "source": [
        "labels = [\"Normale\", \"Fraude\"]\n",
        "\n",
        "y_pred = [1 if e > niveau_detection else 0 for e in error_df.erreur_reconstruction.values]\n",
        "conf_matrix = confusion_matrix(error_df['class'], y_pred)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(conf_matrix, xticklabels=labels, yticklabels=labels, annot=True, fmt=\"d\");\n",
        "plt.title(\"Matrice de synthèse\")\n",
        "plt.ylabel('Cas réels')\n",
        "plt.xlabel('Prédictions')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}