{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "GloVe Sentiment_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygBHZ2O-MFK1"
      },
      "source": [
        "# Analyse des ressentis avec GloVe Vectors\n",
        "\n",
        "L'analyse des ressentis a pour objectif de classifier les opinions exprimées sous forme de texte dans un langage naturel. Différentes méthodes peuvent être utlisées pour réaliser cette analyse. Dans cet exemple, des commentaires recueillis surle site [allociné](http://www.allocine.fr/) vont être utlisés pour entrainer un modèle de classification supervisé. Le modèle utilise un réseau de neurones à convolution 1D.\n",
        "\n",
        "Les mots contenus dans les phrases doivent être encodés dans des vecteurs qui seront utilisés pour l'entrainement et les tests. Une manière simple serait d'assigner à chaque mot une valeur numérique. Le problème de cette méthode est que le contexte dans lequel les mots sont utilisés n'est pas pris en compte. Une méthode permettant de combler ce manque est d'utiliser un algorithme de prolongation lexicale (Word embedding). Parmi ce genre d'algorithme, on trouve [GloVe (Global Vectors for Word Representation)](https://nlp.stanford.edu/projects/glove/) qui se base sur les probabilités de co-occurence de mots. Une base de données pré-entrainée (mais en Anglais) est déjà disponnible sur leur site web.  \n",
        "\n",
        "Une base de donnée pré-entrainée en Français peut-être néanmoins téléchargée à cette [adresse](http://www.cs.cmu.edu/~afm/projects/multilingual_embeddings.html). C'est celle-ci que nous utiliserons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVAIpfO0MFK7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN_eQNboMFLA"
      },
      "source": [
        "# Chargement des données d'entrainement\n",
        "\n",
        "Les données d'entrainement sont chargées dans une dataframe Panda depuis des fichiers textes. Ces données contiennent deux colonnes : Un commentaire et une valeur binaire définissant un sentiment négatif ou positif sur ce commentaire. \n",
        "\n",
        "Ces données sont disponnibles sur le github de [TheophileBlard](https://github.com/TheophileBlard/french-sentiment-analysis-with-bert/tree/master/allocine_dataset). Ces données sont issues de commentaires récupérés sur Allociné. Les utilisateurs votent avec des notes allant de 0.5 à 5.0 :    \n",
        "<img src=\"https://github.com/AlexandreBourrieau/ML-F1/blob/master/Carnets%20Jupyter/Images/rating_counts.png?raw=1\" width=\"600\"/>\n",
        "\n",
        "Afin de récupérer une note binaire (négative ou positive) à partir de cet intervalle de valeurs, les votes <= 2 sont classé comme négatifs et ceux >=4 sont classés comme positifs. Les autres sont classés comme neutres :    \n",
        "<img src=\"https://github.com/AlexandreBourrieau/ML-F1/blob/master/Carnets%20Jupyter/Images/polarity_frequency.png?raw=1\" width=\"600\"/>  \n",
        "  \n",
        "\n",
        "Enfin, pour construire les données, 100 000 avis négatifs et 100 000 avis positifs sont extraits aléatoirement, puis décomposés en deux catégories : Les données d'entrainement (80%), les données de test (10%) et les données de validation (10%) :  \n",
        "<img src=\"https://github.com/AlexandreBourrieau/ML-F1/blob/master/Carnets%20Jupyter/Images/splits_polarity.png?raw=1\" width=\"600\"/>  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFyrqXX4M5ek"
      },
      "source": [
        "# Téléchargement des données depuis le repot github \"https://github.com/AlexandreBourrieau/ML/raw/master/Carnets%20Jupyter/Donn%C3%A9es/data.tar.bz2\"\n",
        "\n",
        "!mkdir data\n",
        "!wget get --no-check-certificate --content-disposition \"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/Donn%C3%A9es/data.tar.bz2?raw=true\"\n",
        "!tar -xjvf data.tar.bz2 data\n",
        "!ls -l data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je5Lkv_aMFLA",
        "outputId": "437c8f75-0dce-4e86-c56f-9bfe59906b58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "source": [
        "DataEntrainement = pd.read_json(\"/content/data/test.jsonl\", lines=True)\n",
        "DataEntrainement.head(10)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>film-url</th>\n",
              "      <th>review</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>http://www.allocine.fr/film/fichefilm-25385/cr...</td>\n",
              "      <td>Magnifique épopée, une belle histoire, touchan...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>http://www.allocine.fr/film/fichefilm-1954/cri...</td>\n",
              "      <td>Je n'ai pas aimé mais pourtant je lui mets 2 é...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>http://www.allocine.fr/film/fichefilm-135523/c...</td>\n",
              "      <td>Un dessin animé qui brille par sa féerie et se...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>http://www.allocine.fr/film/fichefilm-61514/cr...</td>\n",
              "      <td>Si c'est là le renouveau du cinéma français, c...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>http://www.allocine.fr/film/fichefilm-260395/c...</td>\n",
              "      <td>Et pourtant on s’en Doutait !Second volet très...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>http://www.allocine.fr/film/fichefilm-220641/c...</td>\n",
              "      <td>Vous reprendrez bien un peu d'été ? Ce film je...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>http://www.allocine.fr/film/fichefilm-120103/c...</td>\n",
              "      <td>Bon c'est pas un grand film mais on passe un b...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>http://www.allocine.fr/film/fichefilm-190956/c...</td>\n",
              "      <td>Terrible histoire que ces êtres sans amour, ce...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>http://www.allocine.fr/film/fichefilm-186185/c...</td>\n",
              "      <td>Un très joli film, qui ressemble à un téléfilm...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>http://www.allocine.fr/film/fichefilm-17327/cr...</td>\n",
              "      <td>Mais comment certaines personnes ont pus lui m...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            film-url  ... polarity\n",
              "0  http://www.allocine.fr/film/fichefilm-25385/cr...  ...        1\n",
              "1  http://www.allocine.fr/film/fichefilm-1954/cri...  ...        0\n",
              "2  http://www.allocine.fr/film/fichefilm-135523/c...  ...        1\n",
              "3  http://www.allocine.fr/film/fichefilm-61514/cr...  ...        0\n",
              "4  http://www.allocine.fr/film/fichefilm-260395/c...  ...        0\n",
              "5  http://www.allocine.fr/film/fichefilm-220641/c...  ...        1\n",
              "6  http://www.allocine.fr/film/fichefilm-120103/c...  ...        1\n",
              "7  http://www.allocine.fr/film/fichefilm-190956/c...  ...        1\n",
              "8  http://www.allocine.fr/film/fichefilm-186185/c...  ...        1\n",
              "9  http://www.allocine.fr/film/fichefilm-17327/cr...  ...        0\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq6cMhkEMFLG"
      },
      "source": [
        "# Chargement des vecteurs GloVe et préparation des données\n",
        "\n",
        "**Commençons par charger les vecteurs GloVe**  \n",
        "\n",
        "Les fichiers des vecteurs GloVe sont téléchargés à partir du site de [GloVe](https://nlp.stanford.edu/projects/glove/) mais ils sont en Anglais. Nous allons utiliser une [version française](http://www.cs.cmu.edu/~afm/projects/multilingual_embeddings.html) équivalente.  \n",
        "  \n",
        "Le fichier que nous utilisons contient plus de 40000 vecteurs de dimension 300. Cela signifie que pour chaque mot contenu dans ce fichier, un vecteur de 300 valeurs permet de définir les relations lexicales de ce mots avec les autres mots du fichier.  \n",
        "\n",
        "<img src=\"https://github.com/AlexandreBourrieau/ML-F1/blob/master/Carnets%20Jupyter/Images/EmbeddedVectors.png?raw=1\"/>  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGcwMGR-R3tR",
        "outputId": "decce3ac-818e-4717-fd8a-dc47f738671c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Téléchargement des vecteurs\n",
        "!wget get --no-check-certificate --content-disposition \"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/Donn%C3%A9es/multilingual_embeddings.rar?raw=true\"\n",
        "!mv multilingual_embeddings.rar /content/data/multilingual_embeddings.rar\n",
        "!unrar x /content/data/multilingual_embeddings.rar /content/data"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-10 12:17:45--  http://get/\n",
            "Resolving get (get)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘get’\n",
            "--2021-03-10 12:17:45--  https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/Donn%C3%A9es/multilingual_embeddings.rar?raw=true\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/AlexandreBourrieau/ML/raw/main/Carnets%20Jupyter/Donn%C3%A9es/multilingual_embeddings.rar [following]\n",
            "--2021-03-10 12:17:45--  https://github.com/AlexandreBourrieau/ML/raw/main/Carnets%20Jupyter/Donn%C3%A9es/multilingual_embeddings.rar\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/AlexandreBourrieau/ML/main/Carnets%20Jupyter/Donn%C3%A9es/multilingual_embeddings.rar [following]\n",
            "--2021-03-10 12:17:45--  https://raw.githubusercontent.com/AlexandreBourrieau/ML/main/Carnets%20Jupyter/Donn%C3%A9es/multilingual_embeddings.rar\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 80894821 (77M) [application/octet-stream]\n",
            "Saving to: ‘multilingual_embeddings.rar’\n",
            "\n",
            "multilingual_embedd 100%[===================>]  77.15M   187MB/s    in 0.4s    \n",
            "\n",
            "2021-03-10 12:17:49 (187 MB/s) - ‘multilingual_embeddings.rar’ saved [80894821/80894821]\n",
            "\n",
            "FINISHED --2021-03-10 12:17:49--\n",
            "Total wall clock time: 4.1s\n",
            "Downloaded: 1 files, 77M in 0.4s (187 MB/s)\n",
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from /content/data/multilingual_embeddings.rar\n",
            "\n",
            "Extracting  /content/data/multilingual_embeddings.fr                     \b\b\b\b  5%\b\b\b\b 10%\b\b\b\b 15%\b\b\b\b 20%\b\b\b\b 25%\b\b\b\b 31%\b\b\b\b 36%\b\b\b\b 41%\b\b\b\b 46%\b\b\b\b 51%\b\b\b\b 57%\b\b\b\b 62%\b\b\b\b 67%\b\b\b\b 72%\b\b\b\b 77%\b\b\b\b 82%\b\b\b\b 88%\b\b\b\b 93%\b\b\b\b 98%\b\b\b\b 99%\b\b\b\b\b  OK \n",
            "All OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNI60PfJkhp8"
      },
      "source": [
        "**Encodage des commentaires**  \n",
        "Pour utiliser notre réseau de neurones, nous devons encoder le texte des commentaires. L'encodage suit le principe suivant :  \n",
        "* Chaque mot contenu dans l'ensemble des commentaires va se voir attribuer une valeur entière unique\n",
        "* Chaque commentaire va ensuite être transformé en un vecteur de nombre entiers, dont les nombres correspondent aux valeurs entières attribuées aux mots précédemment  \n",
        "* Les commentaires sont ensuite redimensionnés afin d'avoir tous la même dimension (avec bourrage de 0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MobDmxmfzg69",
        "outputId": "543f33a6-9b44-453f-cc50-2cf2a2d077d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(DataEntrainement['review'][0])\n",
        "print(DataEntrainement['review'][1])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Magnifique épopée, une belle histoire, touchante avec des acteurs qui interprètent très bien leur rôles (Mel Gibson, Heath Ledger, Jason Isaacs...), le genre de film qui se savoure en famille! :)\n",
            "Je n'ai pas aimé mais pourtant je lui mets 2 étoiles car l'expérience est louable. Rien de conventionnel ici. Une visite E.T. mais jonchée d'idées /- originales. Le soucis, tout ceci avait-il vraiment sa place dans un film de S.F. tirant sur l'horreur ? Voici un film qui, à l'inverse de tant d'autres qui y ont droit, mériterait peut-être un remake.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvcLUToHlN2N"
      },
      "source": [
        "Le code suivant réalise ces opérations avec les données d'entrainements de notre projet :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqI5FJ8OlRWN"
      },
      "source": [
        "MAX_NB_MOTS = 1000000\n",
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "\n",
        "# Chargement des commentaires et des ressentis\n",
        "commentaires = DataEntrainement['review'].astype(str).tolist()      # Récupère tous les commentaires dans une liste python\n",
        "ressentis = DataEntrainement['polarity'].tolist()                   # Récupère tous les ressentis dans une liste python\n",
        "labels = np.asarray(ressentis)                                      # Créé un tableau de type numpy avec les ressentis\n",
        "\n",
        "# Encodage des commentaires\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_MOTS)                              # Initialise la fonction Tokenizer de Keras\n",
        "tokenizer.fit_on_texts(commentaires)                                      # Création des index des mots\n",
        "sequences = tokenizer.texts_to_sequences(commentaires)                    # Transformation des phrases en séquences d'index de mots \n",
        "padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)   # Bourrage des vecteurs"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvHU4F21lWHh"
      },
      "source": [
        "**Création des données d'entrainement et de tests**  \n",
        "On utilise la fonction `train_test_split` de [ScikitLearn](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) afin de créer les données d'entrainement et de tests à partir des séquences :\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBFoHU9RlkOD"
      },
      "source": [
        "index_des_mots = tokenizer.word_index\n",
        "nbr_mots = min(MAX_NB_MOTS, len(index_des_mots)) + 1\n",
        "\n",
        "x_entrainement, x_test, y_entrainement, y_test = train_test_split(padded_sequences, labels, test_size=0.2)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fg3t3ahhxJ9p"
      },
      "source": [
        "x_entrainement.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdcJUpqilsoC"
      },
      "source": [
        "**Création de la matrice embarquant les données numériques des vecteurs des mots contenus dans les commentaires**  \n",
        "L'objectif est ici de créer une matrice dont chaque ligne contient le vecteur du mot issu de l'algorithme GolVe. La matrice est donc de taille n x m avec :  \n",
        "* n : Nombre de mots (uniques) pris en compte dans l'ensemble des commentiares  \n",
        "* m : Nombre de valeurs contenues dans les vecteurs GolvE (300 dans notre exemple)  \n",
        "  \n",
        "$$\\begin{array}{*{20}{c}}\n",
        "{glace}\\\\\n",
        "{soda}\\\\\n",
        "{...}\\\\\n",
        "{film}\n",
        "\\end{array}\\left( \\begin{array}{l}\n",
        "\\begin{array}{*{20}{c}}\n",
        "{ - 0.3}&{0.2}&{...}&{0.32}&{ - 0.24}\n",
        "\\end{array}\\\\\n",
        "\\begin{array}{*{20}{c}}\n",
        "{ - 0.1}&{0.3}&{...}&{0.52}&{ - 0.94}\n",
        "\\end{array}\\\\\n",
        "\\begin{array}{*{20}{c}}\n",
        "{...}&{...}&{...}&{...}&{...}\n",
        "\\end{array}\\\\\n",
        "\\begin{array}{*{20}{c}}\n",
        "{ - 0.5}&{0.9}&{...}&{0.72}&{ - 0.24}\n",
        "\\end{array}\n",
        "\\end{array} \\right)$$    \n",
        "  \n",
        "    \n",
        "\n",
        "Les lignes ne sont pas arrangées dans n'importe quel ordre : Elles suivent l'ordre des séquences créées par la fonction `tokenizer.texts_to_sequences()`\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qp7oQXgtVz8x"
      },
      "source": [
        "Pour cela définit tout d'abord la fonction `Chargement_Vecteurs()` qui va créer un tableau de type numpy, avec pour chaque mot son vecteur correspondant :  \n",
        "\n",
        "\n",
        "```\n",
        "  ...\n",
        "  'embêtant': array([-2.26152748e-01,  3.20324749e-01, -1.10406213e-01, -6.05279326e-01,\n",
        "        -4.68072683e-01,  1.29561171e-01,  5.62916815e-01, -1.16834176e+00,\n",
        "       ...\n",
        "        -7.50736117e-01, -2.48611599e-01, -2.42264550e-02, -9.54209745e-01],\n",
        "       dtype=float32),\n",
        " 'lockheed': array([ 3.6074609e-01, -8.0667698e-01,  8.7549436e-01,  6.2351477e-01,\n",
        "        -9.2155904e-01,  7.3180795e-01, -2.8121206e-01,  2.9078028e-01,\n",
        "        ...\n",
        "         1.5100185e+00,  8.1941241e-01, -1.6970781e+00,  1.9289741e-01],\n",
        "       dtype=float32),\n",
        " 'séparez': array([-0.5703459 , -0.8884122 , -0.4579496 ,  0.55588883, -0.8727098 ,\n",
        "         0.56783265, -0.10067926,  0.14027229, -0.89301944, -0.42706665,\n",
        "        ...\n",
        "        -0.36254016, -0.40695533,  1.087127  , -0.641696  ,  0.10919298,\n",
        "        ...\n",
        "```  \n",
        "\n",
        "Puis la fonction `Creation_Matrice()` qui va créer notre matrice. Pour chaque mot contenu dans les séquences créées par la fonction `tokenizer.texts_to_sequences` on récupère le vecteur correspondant à l'aide du tableau numpy créé précédemment. Ainsi chaque ligne de la matrice indexe chaque mot des séquences.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVeUb4AsMFLG"
      },
      "source": [
        "MAX_NB_VECTORS = 400000\n",
        "EMBEDDING_DIM = 300\n",
        "\n",
        "def Chargement_Vecteurs():\n",
        "    print('Chargement des vecteurs GloVe...')\n",
        "    glove_dict = {}\n",
        "    Max_Nb_Vect = 0\n",
        "    with open('/content/data/multilingual_embeddings.fr', encoding='utf8') as fichier:\n",
        "        for ligne in fichier:\n",
        "            Max_Nb_Vect = Max_Nb_Vect + 1\n",
        "            if Max_Nb_Vect > MAX_NB_VECTORS:\n",
        "              break\n",
        "            valeur = ligne.split()\n",
        "            mot = valeur[0]\n",
        "            glove_dict[mot] = np.asarray(valeur[1:], dtype='float32')\n",
        "    return glove_dict\n",
        "\n",
        "def Creation_Matrice(index_mot, nbr_mots):\n",
        "    glove_dict = Chargement_Vecteurs()\n",
        "    matrice = np.zeros((nbr_mots, EMBEDDING_DIM))\n",
        "    for mot, i in index_mot.items():\n",
        "        if i > nbr_mots:\n",
        "            continue\n",
        "        vector = glove_dict.get(mot)\n",
        "        if vector is not None:\n",
        "            matrice[i] = vector\n",
        "    print('Matrice créée...')\n",
        "    return matrice"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQJoxfCTf1Hd"
      },
      "source": [
        "On créé donc maintenant la matrice pour notre projet :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZlja3d1MFLL",
        "outputId": "590d3d01-4f7d-4b85-803c-fb826ccf7caf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "matrice = Creation_Matrice(index_des_mots, nbr_mots)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chargement des vecteurs GloVe...\n",
            "Matrice créée...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d03F2eT2gbYG",
        "outputId": "cce483bb-6a5c-41ca-8670-b1cfa55f133b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "matrice.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(66079, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSCKoWPYMFLN"
      },
      "source": [
        "# Définition du Modèle\n",
        "\n",
        "Nous utilisons un réseau de neurones récurrent de type LSTM avec Keras pour effectuer l'encodage des vecteurs GloVe afin d'en extraire une représentation interne sous forme de vecteurs d'états capturant les informations à long terme et à court terme. \n",
        "\n",
        "La structure du réseau est la suivante :  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9G1oAXnVJJP",
        "outputId": "ebfc5787-3d71-4d80-ea47-d0a7963c32ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        }
      },
      "source": [
        "from IPython.display import Image\n",
        "Image(url='https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/Images/Conv1D2.png?raw=true', width=1180)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/Images/Conv1D2.png?raw=true\" width=\"1180\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0IUw1VDMFLO",
        "outputId": "cb624914-a2fb-4fdd-c243-90aa8d4edaf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dropout = 0.4\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(nbr_mots, EMBEDDING_DIM, weights=[matrice],\n",
        "                    input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
        "model.add(Dropout(dropout))\n",
        "\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256)))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 1000, 300)         19823700  \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 1000, 300)         0         \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 512)               1140736   \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 2)                 1026      \n",
            "=================================================================\n",
            "Total params: 20,965,462\n",
            "Trainable params: 1,141,762\n",
            "Non-trainable params: 19,823,700\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWFUoRi6XZYU"
      },
      "source": [
        "# Entrainement du modèle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUMaXNjMMFLQ"
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics=['acc'])\n",
        "\n",
        "# Entraine le modèle sur un certain nombre d'itérations\n",
        "historique = model.fit(x_entrainement, y_entrainement, batch_size=128, epochs=40, verbose=1, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evalue le modèle avec les échantillons de tests\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Fonction d\\'objectif des tests :', score[0])\n",
        "print('Précision des tests :', score[1])\n",
        "\n",
        "# Synthèse\n",
        "plt.plot(historique.history['acc'])\n",
        "plt.plot(historique.history['val_acc'])\n",
        "plt.title('Précision du modèle')\n",
        "plt.ylabel('Précision')\n",
        "plt.xlabel('Itérations')\n",
        "plt.legend(['Entrainements', 'Tests'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LVwi0vLMFLS"
      },
      "source": [
        "# Prédictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i423WNQMFLS"
      },
      "source": [
        "predictions = model.predict(padded_sequences)\n",
        "plus_probable = predictions.argmax(1)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHSJKeRT6Rmn",
        "outputId": "672434af-72e6-4469-a750-48aafa04dc78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(predictions)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ve2a-roMFLV",
        "outputId": "6a891ab0-4d4f-4aef-8b81-69327721ba2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "index = random.randrange(len(predictions))\n",
        "print(commentaires[index])\n",
        "print('Prédiction: %d, label: %d' % (plus_probable[index], ressentis[index]))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sur un scénario concocté avec Diastème, qui avait déjà réuni Jeanne Rosa et Emma de Caunes dans \"Le bruit des gens autour\", Olivier Jahan réalise un film sensible et lucide qui, partant d’une base triste (le deuil d’un père), nous offre in fine une romance douce amère, classique sur le fond mais très originale dans sa forme. L’incrustation ponctuelle d’une voix-off monocorde qui raconte des instants passés du vécu des personnages et permet de mieux appréhender leur complexité, de quelques flash-back et surtout de photographies en noir et blanc font de ces \"châteaux de sable\" une œuvre unique et exclusive proche du roman-photo cinématographique. La musique envoûtante, signée Patrick Watson, contribue à amplifier les émotions et éveiller nos sens. Enfin, la grâce et l’authenticité des comédiens fait à elle seule la différence. Emma de Caunes et Yannick Rénier sont étonnants, Alain Chamfort surprend dans son rôle de père attentif et mention spéciale à la piquante Jeanne Rosa, agent immobilier, bretonne pour le moins décalée, qui observe les gens et, quelquefois, met sans le vouloir son petit grain de sel à leur histoire. Une ode à l’Amour, filial ou conjugal, à la photographie d'art, à la Bretagne et son climat capricieux, qui donne envie de croquer la vie à pleines dents.\n",
            "Prédiction: 1, label: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcR3Qu6FMFLX"
      },
      "source": [
        "# Analyse des erreurs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7aMVD5lMFLX"
      },
      "source": [
        "for i in range(10000):\n",
        "    index = random.randrange(len(predictions))\n",
        "    if plus_probable[index] != ressentis[index]:\n",
        "        break\n",
        "\n",
        "print(commentaires[index])\n",
        "print('Prédiction: %d, label: %d' % (plus_probable[index], ressentis[index]))\n",
        "\n",
        "plt.bar(range(2), predictions[index], tick_label=range(2))\n",
        "plt.title('Valeur prédite ')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}