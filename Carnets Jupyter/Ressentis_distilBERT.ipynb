{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOrZGYpqVk6yk7Kg8I2LkNI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0fe140f132ef4edfb9a0a3935f14f5cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_00f3d6a33d114f3d8eaeffdfb0a9ab6d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a708176850534d31bcd174f0eb615fd1",
              "IPY_MODEL_d2da2e6436574185be955980e59a1c53"
            ]
          }
        },
        "00f3d6a33d114f3d8eaeffdfb0a9ab6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a708176850534d31bcd174f0eb615fd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cca4bbc0d32e43899df1d7a34d026e88",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 363423424,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 363423424,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ec8b1323667b45c68311a816e9528105"
          }
        },
        "d2da2e6436574185be955980e59a1c53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_025e782692044c6298d19578604da801",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 363M/363M [00:11&lt;00:00, 30.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0414961a49cc4bf4b211d30776fb18ea"
          }
        },
        "cca4bbc0d32e43899df1d7a34d026e88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ec8b1323667b45c68311a816e9528105": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "025e782692044c6298d19578604da801": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0414961a49cc4bf4b211d30776fb18ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/Ressentis_distilBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJih03NWtfi0"
      },
      "source": [
        "# **Classification de ressentis avec distilBERT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6Xo1x4rtto8"
      },
      "source": [
        "L'objectif est de créer un modèle qui prend en entrée des commentaires (en Anglais) et attribue à chacun un ressenti positif ou négatif.  \n",
        "Le modèle global est composé de deux parties :  \n",
        "* [DistilBERT](https://huggingface.co/transformers/model_doc/distilbert.html) (une version allégée de BERT) va encoder le commentaire et en extraire des informations qui seront passées ensuite au réseau de neurones.  \n",
        "* Le modèle suivant est un réseau de neurones qui sera créé avec l'API [Keras](https://www.tensorflow.org/guide/keras?hl=fr) de [Tensorflow](https://www.tensorflow.org/?hl=fr).  \n",
        "\n",
        "Nous testerons deux modèles de réseaux de neurones pour traiter les sorties de DistilBERT :  \n",
        "* Un modèle très simple, composé de 2 neurones avec une fonction d'activation Softmax\n",
        "* Un modèle plus complexe, qui sera en fait le même que celui utilisé lors de l'activité avec GloVe\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/AlexandreBourrieau/ML-F1/master/Carnets%20Jupyter/Images/StructureBERT.png\" />  \n",
        "  \n",
        "  Les données qui s'échangent entre les deux modèles sont des vecteurs de dimension 768. On peut voir ces vecteurs comme l'équivalent de l'application d'un algorithme de prolongation lexicale sur les mots qui composent le commentaire."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gePh7FotwPTH"
      },
      "source": [
        "# **Installation et importation des librairies**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5WrrT_pzIHu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f80c0b47-4845-46f0-9b9c-df35c1c984aa"
      },
      "source": [
        "!pip install transformers sentencepiece --quiet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.8MB 10.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 44.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 43.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 39.8MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5n4uHtutPlJ"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input, Dropout, Lambda, Conv1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from transformers import DistilBertConfig\n",
        "from transformers import TFDistilBertModel\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWZEaivhxTd2"
      },
      "source": [
        "# **Importation des données**\n",
        "\n",
        "On utilise la librairie pandas pour lire les données depuis le fichier csv disponible sur le site de [standford](https://nlp.stanford.edu/sentiment/index.html) qui contient des commentaires sur des films, chacun d'eux avec une note positive (1) ou négative (0)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOrjCZGyyH4P"
      },
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/AlexandreBourrieau/ML/master/Carnets%20Jupyter/Donn%C3%A9es/train.csv', delimiter='\\t', header=None)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiO-lNQ6zYpU"
      },
      "source": [
        "Affiche quelques informations :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOMoEtbysA5q"
      },
      "source": [
        "def LongueurMax(df):\n",
        "  Lmax = 0\n",
        "  for com in df[0]:\n",
        "    Longueur = len(com)\n",
        "    if Lmax < Longueur:\n",
        "      Lmax = Longueur\n",
        "  return Lmax"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQT1SwyBzPvP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faff82f7-15ab-4ce8-e672-dca0f2484483"
      },
      "source": [
        "print(df[0:10])\n",
        "print(\"Total des données : \", str(len(df)))\n",
        "print(\"Nombre d'avis positifs et négatifs : \",df[1].value_counts())\n",
        "print(\"Longueur maximale d'un commentaire : \",LongueurMax(df))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                   0  1\n",
            "0  a stirring , funny and finally transporting re...  1\n",
            "1  apparently reassembled from the cutting room f...  0\n",
            "2  they presume their audience wo n't sit still f...  0\n",
            "3  this is a visually stunning rumination on love...  1\n",
            "4  jonathan parker 's bartleby should have been t...  1\n",
            "5  campanella gets the tone just right funny in t...  1\n",
            "6  a fan film that for the uninitiated plays bett...  0\n",
            "7  b art and berling are both superb , while hupp...  1\n",
            "8  a little less extreme than in the past , with ...  0\n",
            "9                       the film is strictly routine  0\n",
            "Total des données :  6920\n",
            "Nombre d'avis positifs et négatifs :  1    3610\n",
            "0    3310\n",
            "Name: 1, dtype: int64\n",
            "Longueur maximale d'un commentaire :  271\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4Vcm5K3Dej4"
      },
      "source": [
        "# **Préparation des données**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XefRRLg2DiQs"
      },
      "source": [
        "MAX = 3000\n",
        "\n",
        "# Chargement des commentaires et des ressentis\n",
        "commentaires = df[0].astype(str).tolist()    # Récupère tous les commentaires dans une liste python\n",
        "ressentis = df[1].tolist()                   # Récupère tous les ressentis dans une liste python\n",
        "labels = np.asarray(ressentis)               # Créé un tableau de type numpy avec les ressentis\n",
        "\n",
        "x_entrainement, x_test, y_entrainement, y_test = train_test_split(commentaires[0:MAX], labels[0:MAX], test_size=0.25)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tajm9VKu1pg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4760eade-8fb1-45f9-a050-585ba8980093"
      },
      "source": [
        "print (\"Nombre de commentaires pour l'entrainement : \", len(x_entrainement))\n",
        "print (\"Nombre de commentaires pour les tests : \", len(x_test))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nombre de commentaires pour l'entrainement :  2250\n",
            "Nombre de commentaires pour les tests :  750\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDDdUWj83wuz"
      },
      "source": [
        "# Tokénisation  \n",
        "La première étape est de tokéniser les commentaires : les mots sont décomposés en index numériques au format BERT.  \n",
        "\n",
        "<img src=\"https://github.com/AlexandreBourrieau/ML-F1/blob/master/Carnets%20Jupyter/Images/TokenizeBERT.png?raw=true\" />\n",
        "\n",
        "Après tokénisation, on obtient une liste de séquences et chaque séquence représente une liste d'index. On souhaite que BERT analyse toutes les séquences en une seule fois (ce qui est plus rapide). Il faut donc que toutes les séquences aient la même taille. On va donc ajouter du bourrage pour égaliser la longueur des séquences. Cela est indiqué avec le paramètre `padding='True'`.  \n",
        "Lorsque un bourrage est ajouté, il faut que BERT ne prenne pas en compte les mots à cette position (car il n'y en a pas !). Cette restriction est réalisé grace à l'`attention_mask`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHhqUPeiqzam"
      },
      "source": [
        "LONGUEUR_MAX_COMMENTAIRE = LongueurMax(df) + 2\n",
        "\n",
        "# Instanciation du tokeniseur\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased',use_fast=False)\n",
        "\n",
        "# Préparation des données d'entrainement\n",
        "output_tokenizer_entrainement = tokenizer(x_entrainement,max_length=LONGUEUR_MAX_COMMENTAIRE, padding='max_length', truncation=False, return_tensors='tf',add_special_tokens=True)\n",
        "\n",
        "# Préparation des données de tests\n",
        "output_tokenizer_tests = tokenizer(x_test,max_length=LONGUEUR_MAX_COMMENTAIRE, padding='max_length', truncation=False, return_tensors='tf',add_special_tokens=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kwmm53NCihe"
      },
      "source": [
        "Les commentaires sont maintenant tous tokénisés :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WVYvpcRChKd"
      },
      "source": [
        "\n",
        "<img src=\"https://raw.githubusercontent.com/AlexandreBourrieau/ML-F1/master/Carnets%20Jupyter/Images/SeparationData2.png\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzboG3fX5LkP"
      },
      "source": [
        "Regardons un peu comment sont formatées les données en sortie du tokéniseur :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0BL0iiKzVS7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0edcd51e-6b2e-48a3-ff74-2dd7b8ca3ae6"
      },
      "source": [
        "output_tokenizer_entrainement"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': <tf.Tensor: shape=(2250, 273), dtype=int32, numpy=\n",
              "array([[  101,  2348,  2643, ...,     0,     0,     0],\n",
              "       [  101,  1037, 10036, ...,     0,     0,     0],\n",
              "       [  101,  2019,  3178, ...,     0,     0,     0],\n",
              "       ...,\n",
              "       [  101,  6187, 13469, ...,     0,     0,     0],\n",
              "       [  101,  2071,  1045, ...,     0,     0,     0],\n",
              "       [  101,  1996,  3185, ...,     0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2250, 273), dtype=int32, numpy=\n",
              "array([[1, 1, 1, ..., 0, 0, 0],\n",
              "       [1, 1, 1, ..., 0, 0, 0],\n",
              "       [1, 1, 1, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [1, 1, 1, ..., 0, 0, 0],\n",
              "       [1, 1, 1, ..., 0, 0, 0],\n",
              "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fejN7ZrzzaOu"
      },
      "source": [
        "Regardons comment le premier commentaire a été encodé :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-E6oJcG5Qlj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2adcdf1-fc81-4211-929e-6a9277ecfa2e"
      },
      "source": [
        "print(\"Commentaire original :\", x_entrainement[0])\n",
        "print(\"input_ids: \", output_tokenizer_entrainement['input_ids'][0])\n",
        "print(\"attention_mask: \", output_tokenizer_entrainement['attention_mask'][0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Commentaire original : although god is great addresses interesting matters of identity and heritage , it 's hard to shake the feeling that it was intended to be a different kind of film\n",
            "input_ids:  tf.Tensor(\n",
            "[  101  2348  2643  2003  2307 11596  5875  5609  1997  4767  1998  4348\n",
            "  1010  2009  1005  1055  2524  2000  6073  1996  3110  2008  2009  2001\n",
            "  3832  2000  2022  1037  2367  2785  1997  2143   102     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0], shape=(273,), dtype=int32)\n",
            "attention_mask:  tf.Tensor(\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0], shape=(273,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAmGZYP86Hx7"
      },
      "source": [
        "Regardons les 3 premiers résultats de la tokénisation : On peut identifier les mot-clés **[CLS]** (valeur : 101) et **[SEP]** (valeur : 102)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWVaSBqo59qW"
      },
      "source": [
        "for i in range (0,3):\n",
        "  print(output_tokenizer_entrainement['input_ids'][i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vakqzatl3le0"
      },
      "source": [
        "# **Définition et utilisation du modèle distilBERT avec Keras**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uE_TEftF1OCf"
      },
      "source": [
        "Les données d'entrées étant maintenant correctement préparées, commençons par définir le modèle distilBERT pour ensuite l'appliquer aux données afin de réaliser l'opération de prolongation lexicale.  \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/AlexandreBourrieau/ML-F1/master/Carnets%20Jupyter/Images/distilBERT_process.png\" style=\"width: 600px;\"/>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38a0wigo18Zg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698,
          "referenced_widgets": [
            "0fe140f132ef4edfb9a0a3935f14f5cd",
            "00f3d6a33d114f3d8eaeffdfb0a9ab6d",
            "a708176850534d31bcd174f0eb615fd1",
            "d2da2e6436574185be955980e59a1c53",
            "cca4bbc0d32e43899df1d7a34d026e88",
            "ec8b1323667b45c68311a816e9528105",
            "025e782692044c6298d19578604da801",
            "0414961a49cc4bf4b211d30776fb18ea"
          ]
        },
        "outputId": "c4455991-4895-49fa-919a-9f3871bef1f0"
      },
      "source": [
        "# Configuration du modèle distilBERT\n",
        "config = DistilBertConfig(num_labels=2)             # 2 labels en sortie\n",
        "\n",
        "# Instanciation du modèle distilBERT\n",
        "transformer_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased', config = config)\n",
        "\n",
        "# Défintion du format des entrées du modèle\n",
        "entrees_ids = tf.keras.layers.Input(shape=(LONGUEUR_MAX_COMMENTAIRE,), name='input_token', dtype='int32')\n",
        "entrees_masks = tf.keras.layers.Input(shape=(LONGUEUR_MAX_COMMENTAIRE,), name='masked_token', dtype='int32') \n",
        "\n",
        "# Création de la sortie du modèle\n",
        "sortie_distilBERT = transformer_model([entrees_ids,entrees_masks])\n",
        "\n",
        "# Instanciation du modèle avec Keras\n",
        "model_distilBERT = tf.keras.Model(inputs=[entrees_ids, entrees_masks], outputs = sortie_distilBERT,trainable=False)\n",
        "model_distilBERT.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0fe140f132ef4edfb9a0a3935f14f5cd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=363423424.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_transform', 'vocab_projector', 'vocab_layer_norm', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f2da5c166c8>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: <cyfunction Socket.send at 0x7f2dc30f62a0> is not a module, class, method, function, traceback, frame, or code object\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f2da5c166c8>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: <cyfunction Socket.send at 0x7f2dc30f62a0> is not a module, class, method, function, traceback, frame, or code object\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "\n",
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7f2dc0ed0c80> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function wrap at 0x7f2dc0ed0c80> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_token (InputLayer)        [(None, 273)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "masked_token (InputLayer)       [(None, 273)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_distil_bert_model (TFDistilB TFBaseModelOutput(la 66362880    input_token[0][0]                \n",
            "                                                                 masked_token[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 66,362,880\n",
            "Trainable params: 0\n",
            "Non-trainable params: 66,362,880\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9vwylPK8B2d"
      },
      "source": [
        "Pour chaque commentaire en entrée, la sortie du modèle distilBERT est un vecteur de dimension MAX_SEQUENCE_LENGTH :\n",
        "* Il y a au maximum MAX_SEQUENCE_LENGTH mots dans chaque commentaire\n",
        "* Il y a un vecteur en sortie du modèle par mot dans chaque commentaire\n",
        "* Le vecteur qui code chaque mot est de dimension 768"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84jTmbNS9och"
      },
      "source": [
        "Vérifions cela en regardant le format de la sortie du modèle :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRyx0t5z5_zY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b24b0de9-49a1-4d81-ac02-56cece2a47de"
      },
      "source": [
        "sortie_distilBERT"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TFBaseModelOutput([('last_hidden_state',\n",
              "                    <KerasTensor: shape=(None, 273, 768) dtype=float32 (created by layer 'tf_distil_bert_model')>)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDYf-Owpwk4Q"
      },
      "source": [
        "La fonction `predict()` permet d'exécuter le modèle sur les séquences d'entrées"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-NlEgWUxehg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfe13456-bfad-408f-f152-d67d5dd9f1f8"
      },
      "source": [
        "sortie_vecteurs_distilBERT = model_distilBERT.predict(\n",
        "    [output_tokenizer_entrainement['input_ids'][0:2],\n",
        "     output_tokenizer_entrainement['attention_mask'][0:2]]\n",
        "     ,verbose=1)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "1/1 [==============================] - 2s 2s/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nr1pLKgsCwMP"
      },
      "source": [
        "Regardons à quoi ressemble la sortie de distilBERT :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2EaziuPDIsO"
      },
      "source": [
        "\n",
        "<img src=\"https://raw.githubusercontent.com/AlexandreBourrieau/ML-F1/master/Carnets%20Jupyter/Images/distilBERT_process.png\" style=\"width: 600px;\"/>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aRQgN8AxmZ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f49d02d8-3ff5-4ab2-ebaa-00fa201a844b"
      },
      "source": [
        "sortie_vecteurs_distilBERT"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TFBaseModelOutput([('last_hidden_state',\n",
              "                    array([[[ 0.12697634,  0.07267125, -0.20222826, ..., -0.03190329,\n",
              "                              0.5445315 ,  0.4806475 ],\n",
              "                            [-0.26458287,  0.567291  , -0.16895247, ..., -0.09016019,\n",
              "                              0.3716725 ,  0.69785666],\n",
              "                            [ 0.19972987,  0.40689173, -0.1078022 , ...,  0.2005468 ,\n",
              "                              0.5142505 ,  0.34432217],\n",
              "                            ...,\n",
              "                            [ 0.02515908, -0.08468235, -0.10614525, ...,  0.23914665,\n",
              "                              0.20634145,  0.17430647],\n",
              "                            [ 0.03855621, -0.09520001, -0.08530612, ...,  0.22041687,\n",
              "                              0.1828146 ,  0.1669421 ],\n",
              "                            [ 0.04170456, -0.05432995, -0.10127669, ...,  0.22181287,\n",
              "                              0.17507976,  0.17163457]],\n",
              "                    \n",
              "                           [[ 0.04641276, -0.11166245,  0.01020955, ..., -0.16457352,\n",
              "                              0.5422787 ,  0.27313694],\n",
              "                            [ 0.3205078 , -0.10081042, -0.21449469, ..., -0.29576427,\n",
              "                              0.48745048,  0.37449846],\n",
              "                            [ 0.6077557 , -0.1921138 ,  0.12686606, ..., -0.3007386 ,\n",
              "                              0.08294331,  0.0673712 ],\n",
              "                            ...,\n",
              "                            [ 0.10001718, -0.0686582 ,  0.23733512, ..., -0.00075583,\n",
              "                              0.07199472,  0.10733491],\n",
              "                            [ 0.24421608, -0.20425083,  0.2629675 , ...,  0.19599424,\n",
              "                              0.07235032,  0.09708712],\n",
              "                            [ 0.23602489, -0.08787413,  0.2769128 , ...,  0.015169  ,\n",
              "                              0.11440535,  0.11303414]]], dtype=float32))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ti2mW-uFC1rV"
      },
      "source": [
        "Regardons par exemple le vecteur de dimension 768, résultant de l'encodage du [CLS] du premier commentaire : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ps4Rp9g5x7sU"
      },
      "source": [
        "sortie_vecteurs_distilBERT[0][0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0RqMRY1o4Gl"
      },
      "source": [
        "Exécutons maintenant distilBERT sur les 10 premiers commentaires afin de regarder le format des sorties obtenues :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_nAKA85o_Fm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce183213-2fb0-45c1-958c-da5384e8175d"
      },
      "source": [
        "sortie_vecteurs_distilBERT = model_distilBERT.predict(\n",
        "    [output_tokenizer_entrainement['input_ids'][0:10],\n",
        "     output_tokenizer_entrainement['attention_mask'][0:10]]\n",
        "     ,verbose=1)\n",
        "print(\"Commentaire :\", commentaires[1])\n",
        "print(\"input_ids\", output_tokenizer_entrainement['input_ids'][1])\n",
        "print(\"Sortie BERT\", sortie_vecteurs_distilBERT[0][:,0,:])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 259ms/step\n",
            "Commentaire : apparently reassembled from the cutting room floor of any given daytime soap\n",
            "input_ids tf.Tensor(\n",
            "[  101  1037 10036  8040  3286  2404  2362  2011  2070 26881 19815  2015\n",
            "  2012  4329  4835  1998  5674  4024  2000  2191  1996 26476  2015  2041\n",
            "  2045  7806  1023  1998  6109  2781  1997  4895  2890  3597 26061  3468\n",
            "  2166   102     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0], shape=(273,), dtype=int32)\n",
            "Sortie BERT [[ 0.12697634  0.07267125 -0.20222826 ... -0.03190329  0.5445315\n",
            "   0.4806475 ]\n",
            " [ 0.04641276 -0.11166245  0.01020955 ... -0.16457352  0.5422787\n",
            "   0.27313694]\n",
            " [-0.24519645 -0.30189425  0.09894709 ... -0.03840479  0.2554473\n",
            "   0.02322881]\n",
            " ...\n",
            " [-0.05158187 -0.1222697   0.03411561 ... -0.23046741  0.34121433\n",
            "   0.20497505]\n",
            " [-0.15886362 -0.14034292 -0.16950333 ... -0.08450413  0.45880303\n",
            "   0.1100563 ]\n",
            " [-0.07015765 -0.20217258 -0.07110689 ... -0.17074366  0.51881236\n",
            "   0.42326713]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bjWNVb4KdZG"
      },
      "source": [
        "# **Ajout du réseau de neurones simple en sortie du modèle distilBERT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xd2hHeGJGtVB"
      },
      "source": [
        "**Extraction des vecteurs [CLS]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkqicdHNDSpu"
      },
      "source": [
        "Parmi les MAX_SEQUENCE_LENGTH vecteurs en sortie, il ne nous faut que le premier (celui qui correspond au mot clé [CLS]). On doit donc récupérer, pour chaque commentaire, le premier vecteur de dimension 768 parmi les MAX_SEQUENCE_LENGTH en sortie :  \n",
        "  \n",
        "  \n",
        "<img src=\"https://github.com/AlexandreBourrieau/ML-F1/blob/master/Carnets%20Jupyter/Images/Slice_SortieBERT.png?raw=true\"/>  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_sA6CqIIiXR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3098aca1-be67-4ad3-8173-c55541fb0e84"
      },
      "source": [
        "sortie_distilBERT[0]"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 273, 768) dtype=float32 (created by layer 'tf_distil_bert_model')>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaDy1bm7gmKU",
        "outputId": "dc138404-0ad9-422a-d034-8188e63fd4a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sortie_distilBERT[0][:,0,:]"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf.__operators__.getitem_31')>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H26bHc7GxhP"
      },
      "source": [
        "**Construction du modèle global**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEv4gm-wEEP2"
      },
      "source": [
        "Les vecteurs de dimension 768 correspondants aux sorties [CLS] de chaque commentaire sont envoyés dans un réseau de neurones à 2 neurones avec une fonction d'activation Softmax :\n",
        "<img src=\"https://raw.githubusercontent.com/AlexandreBourrieau/ML-F1/master/Carnets%20Jupyter/Images/ReseauDistilBERT1.png\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEQc1zx5P25h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7e74364-5e8c-4706-99bb-32c54259394d"
      },
      "source": [
        "# Configuration du modèle distilBERT\n",
        "config = DistilBertConfig(num_labels=2)           # 1 label\n",
        "config.output_hidden_states = True               # Ne récupère pas la totalité des couches mais uniquement la dernière\n",
        "config.output_attentions = False\n",
        "\n",
        "# Instanciation du modèle distilBERT\n",
        "transformer_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased', config = config)\n",
        "\n",
        "# Défintion du format des entrées du modèle\n",
        "entrees_ids = tf.keras.layers.Input(shape=(LONGUEUR_MAX_COMMENTAIRE,), name='input_token', dtype='int32')\n",
        "entrees_masks = tf.keras.layers.Input(shape=(LONGUEUR_MAX_COMMENTAIRE,), name='masked_token', dtype='int32') \n",
        "\n",
        "# Création de la sortie du modèle\n",
        "sortie_distilBERT = transformer_model([entrees_ids,entrees_masks])[0]\n",
        "\n",
        "\n",
        "l1 = Lambda(lambda seq: seq[:, 0, :])(sortie_distilBERT)        # On ne récupère que les vecteurs [CLS]\n",
        "output = Dense(2, activation='softmax')(l1)\n",
        "\n",
        "model = tf.keras.Model(inputs=[entrees_ids, entrees_masks], outputs = output)\n",
        "model.layers[2].trainable = False         # Désactive d'entrainement de distilBERT\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_transform', 'vocab_projector', 'vocab_layer_norm', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_token (InputLayer)        [(None, 273)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "masked_token (InputLayer)       [(None, 273)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_distil_bert_model_1 (TFDisti TFBaseModelOutput(la 66362880    input_token[0][0]                \n",
            "                                                                 masked_token[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 768)          0           tf_distil_bert_model_1[0][7]     \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 2)            1538        lambda[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 66,364,418\n",
            "Trainable params: 1,538\n",
            "Non-trainable params: 66,362,880\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0NXqJTWDroW"
      },
      "source": [
        "On lance maintenant l'entrainement du modèle :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XOX7Ve54k0p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f4d9b77-6b2b-4970-f60d-5e5c4b2ef844"
      },
      "source": [
        "history = model.fit([output_tokenizer_entrainement['input_ids'],output_tokenizer_entrainement['attention_mask']],y_entrainement,\n",
        "                    epochs=5, verbose=1, batch_size = 3,\n",
        "                    validation_data=([output_tokenizer_tests['input_ids'],output_tokenizer_tests['attention_mask']],y_test))\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "750/750 [==============================] - ETA: 0s - loss: 0.6055 - accuracy: 0.6680WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "750/750 [==============================] - 71s 88ms/step - loss: 0.6055 - accuracy: 0.6681 - val_loss: 0.4129 - val_accuracy: 0.8333\n",
            "Epoch 2/5\n",
            "750/750 [==============================] - 65s 86ms/step - loss: 0.4473 - accuracy: 0.8005 - val_loss: 0.3823 - val_accuracy: 0.8347\n",
            "Epoch 3/5\n",
            "750/750 [==============================] - 65s 86ms/step - loss: 0.4048 - accuracy: 0.8205 - val_loss: 0.3751 - val_accuracy: 0.8400\n",
            "Epoch 4/5\n",
            "750/750 [==============================] - 65s 86ms/step - loss: 0.3918 - accuracy: 0.8269 - val_loss: 0.3654 - val_accuracy: 0.8333\n",
            "Epoch 5/5\n",
            "750/750 [==============================] - 65s 86ms/step - loss: 0.4070 - accuracy: 0.8210 - val_loss: 0.3765 - val_accuracy: 0.8293\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZhKM9mfQ0qi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "6dff9484-c836-4b59-9a1d-dd8ec4c4507c"
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Précision du modèle')\n",
        "plt.ylabel('Précision')\n",
        "plt.xlabel('Itération')\n",
        "plt.legend(['Entrainement', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU1f3/8dcnOySBAAkIBAgIKCCKGnFB61YRa1W6Y7XF1lZtq7Z2pdZd29rdarXuxbZ+XWqrotWfO+6toFJlkUVECAqEsISwJCT5/P64N2EYJguQyZ0k7+fjMY/c5dy5nxmY85l7zpl7zN0RERGJlxZ1ACIikpqUIEREJCElCBERSUgJQkREElKCEBGRhJQgREQkISUI6VDMbIKZzTKz3q0oe5aZPd2Kcrea2eVtE2Gz5znOzMqSfZ7dZWYlZuZmlhGup5nZ38zs+wnKnmNmr7R/lBIFJQiJhJktM7OtZlZlZqvNbLqZ5bVwzCDgF8Cp7r6upXO4+73uPrEV5S5w92tbH33n5u71wDnAeDObFHE4EiElCInSae6eBxwClAKXxRdo+FYL4O4r3P1Yd1/TjjF2Se5e5+5T3P3/RR2LREcJQiLn7iuBJ4EDAMLmju+Y2WJgcbjt02Y2x8w2mNlrZnZgw/FmNsjM/mVm5WZWYWZ/Crc3NodY4A9mtsbMKs3sXTNrON90M7su5vm+aWZLzGydmc0wswEx+9zMLjCzxWEsN5uZJXpdZtYtfO71ZjYfOCxuv5vZ8Jj1neKIK3uOmb0avoYNZrbUzI4Kt68IX9fUmPI9zeyv4XvyoZldZmZp4b50M/utma01s6XAqXHn6mlmd5nZx2a20sx+YWbpTcS1v5k9E75XC83si4nKScekBCGRC5uOPgW8HbN5MnA4MNrMDgbuBs4H+gC3ATPMLDusuB4HPgRKgIHA/QlOMxH4BDAS6Al8EahIEMsJwC/D/f3D541/vk8TVPYHhuVObuKlXQnsGz5OBqY2Ua61DgfeIXgP/i+M6zBgOHA28KeYZrqbCF7nMOBY4KvA18J93wxfw8EEV26fjzvPdMCBEQRXdxOBC+KDMbNc4Jkwlr7AFOAWMxu9l69TUoW766FHuz+AZUAVsIGgEr4F6Bbuc+CEmLJ/Bq6NO34hQcV3JFAOZCQ4xznAK+HyCcAi4AggLa7cdOC6cPku4Ncx+/KA7UBJTGxHx+x/EJjWxGtcCkyKWT8PKItZd2B4ojiaeC2LY9bHhsf3i9lWAYwD0oEaYHTMvvOBmeHy88AFMfsmhs+VAfQLj+0es//LMcfGvqdfAl6Oi/M24Mqo/3/p0TaPxvZdkQhMdvdnm9i3ImZ5CDDVzC6K2ZYFDADqgA/dvba5E7n782HT083AEDP7F/BDd6+MKzoAeCvmuCozqyC4MlkWbl4VU34LQRJJZEDc6/iwuRhbYXXM8tYwvvhteUAhkBl3vg8JXkNLcQ0BDHgrpuUsE1ifIJ4hwOFmtiFmWwbwt1a8FukA1MQkqSr2NsMrgJ+7e0HMo7u73xfuGxzbmd3kE7rf6O6HAqMJmpp+lKDYRwQVH9DYjNIHWLkHr+FjYFDM+uC4/VuA7jHr++zBORJZS3DVMyRm22B2vIbm4lpBkHTHuvv+4WNfdy9NcJ4VwItx/y557v6tNnodEjElCOkI7gAuMLPDw87mXDM71czygTcIKrzrw+05ZjYh/gnM7LDw+ExgM7ANqE9wrvuAr5nZODPLJhhW+193X7YHcT8I/NTMeplZMXBR3P45wJfDTuNJBE1me83d68Jz/9zM8s1sCPB94O8xcV1sZsVm1guYFnPsx8BTwA1hZ3Wame1rZoliexwYaWZfMbPM8HGYmY1qi9ch0VOCkJTn7rMJOlb/RNDUsYSgLbyhMjyNoKN2OVBG0DYerwdBollP0KRSAfwmwbmeBS4H/kmQePYl6HzdE1eH5/oAeJpdm16+G8a+ATgLeGQPz5PIRQSJcCnwCkFH8t3hvjsIksD/CJrT/hV37FcJ6oZ5BO/XQwQd9jtx900E/RdTCK68VgG/ArLb8HVIhMxdEwaJiMiudAUhIiIJKUGIiEhCShAiIpKQEoSIiCTUaX4oV1hY6CUlJVGHISLSobz55ptr3b0o0b5OkyBKSkqYPXt21GGIiHQoZtbkL/zVxCQiIgkpQYiISEJKECIiklCn6YNIZPv27ZSVlbFt27aoQ5EEcnJyKC4uJjMzM+pQRCSBTp0gysrKyM/Pp6SkhCYm/ZKIuDsVFRWUlZUxdOjQqMMRkQSS2sRkZpPCaQiXmNm0BPsHm9kLZva2mb1jZp9KsL/KzH64J+fftm0bffr0UXJIQWZGnz59dHUnksKSliDCqSBvBk4huP/+mQmmIrwMeNDdDyacrjBu/+8J5iremzj25nBJIv3biKS2ZDYxjQeWuPtSADO7HzgDmB9TxgluwwzB/LkfNewws8kEt0nenMQYRdqeO2wuhw3LYf0y2LgCsvKgaD8oHAl5/UDJUTqAZCaIgew8rWEZwaTrsa4Cng6nkswFPgkQTrz+E+AkoMnmJTM7j2CeXwYPjp+sKzWkp6czduzYxvUpU6YwbdourW2NZs6cSVZWFkcdddRunWf27Nn89a9/5cYbb9zjWJPhhhtu4LzzzqN79+4tF+4o3GHretjwYZgEPoxbXg61W5s+PrsnFI0MkkXhyB2Jo1cJpKW328sQaUnUndRnAtPd/XdmdiTwNzM7gCBx/CGcD7jJg939duB2gNLS0pSc2KJbt27MmTOn1eVnzpxJXl5ewgRRW1tLRkbif7LS0lJKSxPNChmtG264gbPPPrvjJYhtlUFF31QSqNm0c/mcnlAwBApHwIiToGBwsN5rCPQcBNWVsHYRlC+CtQuhfCEseQ7m3LvjOdKzoM/wXRNH4QjI7Na+r1+E5CaIlew8720xu87rey4wCcDdXzezHIIJ1w8HPm9mvwYKgHoz2+buf0pivO2qpKSEqVOn8thjj7F9+3b+8Y9/kJOTw6233kp6ejp///vfuemmm7jrrrvIycnh7bffZsKECUyZMoXvfve7bNu2jW7duvGXv/yF/fbbj5kzZ/Lb3/6Wxx9/nKuuuorly5ezdOlSli9fzve+9z0uvvhiAP7+979z4403UlNTw+GHH84tt9xCeno6eXl5fOtb3+KJJ56gf//+/OIXv+DHP/4xy5cv54YbbuD000+nrq6OadOmMXPmTKqrq/nOd77D+eefz8yZM7nqqqsoLCxk7ty5HHrooY3xf/TRRxx//PEUFhbywgsvRPyux6jZEiaAhiTw4c5JYOv6nctn5gaVfcEQGDJhx3LB4ODRraD582XnQY8BMOy4nbdv3QBrFwdJoyGBrHoHFswAb5gR1aBgEBTuFyaNETuWu/duozdEZFfJTBCzgBFmNpQgMUwBvhxXZjlwIjA9nMc2Byh392MaCpjZVUDV3iaHqx+bx/yPKvfmKXYxekAPrjxtTLNltm7dyrhx4xrXf/rTn/KlLwUzYhYWFvLWW29xyy238Nvf/pY777yTCy64gLy8PH74w6Bl7a677qKsrIzXXnuN9PR0Kisrefnll8nIyODZZ5/l0ksv5Z///Ocu533vvfd44YUX2LRpE/vttx/f+ta3WLJkCQ888ACvvvoqmZmZfPvb3+bee+/lq1/9Kps3b+aEE07gN7/5DZ/5zGe47LLLeOaZZ5g/fz5Tp07l9NNP56677qJnz57MmjWL6upqJkyYwMSJEwF4++23mTdvHgMGDGDChAm8+uqrXHzxxfz+97/nhRdeoLCwsK3e9taprYaNZUEfQEMSaGj+2fBh0EcQKyNnR2U/sDT425gEhgQVcTL6DboVwKDDgkes7dtg3dLwamNRkDzWLoRlr+zcfNW9MLzaGBkkjYblHsWQpt/Byt5JWoJw91ozu5Bg7tt04G53n2dm1wCz3X0G8APgDjO7hKDD+hzvZHOgNtfE9NnPfhaAQw89lH/9K35a4B2+8IUvkJ4etE1v3LiRqVOnsnjxYsyM7du3Jzzm1FNPJTs7m+zsbPr27cvq1at57rnnePPNNznssKAy2rp1K3379gUgKyuLSZMmATB27Fiys7PJzMxk7NixLFu2DICnn36ad955h4ceeqgxlsWLF5OVlcX48eMpLi4GYNy4cSxbtoyjjz56d96q3VNXC5VlO7f7xyaBTR8T/JcKpWVCz+Kg0t/vlB0Vf0MSyC1KrQo1Mwf6jQ4eserrg07vtYuCZqq1C4MrkPkzYOu6mOO7h1caYeJo6PPovS9kZLXva5EOK6l9EO7+BPBE3LYrYpbnAxNaeI6r2iKWlr7pRyE7O5jbPT09ndra2ibL5ebmNi5ffvnlHH/88Tz88MMsW7aM4447rtnnjn1+d2fq1Kn88pe/3KV8ZmZm47DTtLS0xuPT0tIaY3N3brrpJk4++eSdjp05c2bC8+2V+jrYtGrXzt+GJFC5ErxuR3lLC741FwyGfY/f0QfQcCWQ379zdACnpQWvp9eQoK8j1ua1YdJYtCOBLP8vvPuPHWUsHXoPDa82RoRNVuFyTg9EYkXdSS1x8vPzqaxsuils48aNDBw4EIDp06fv1nOfeOKJnHHGGVxyySX07duXdevWsWnTJoYMGdKq408++WT+/Oc/c8IJJ5CZmcmiRYsaY2lKfn4+mzZt2rWJyR3qa4OmoHcf2rUJaMMKqI+7OsrvH1T6g48Iv/nHJIGexZDexW/ZkVsYPErivnPVbA77OWISx9pFsPjpnd/j/P5xnePhsobldllKEEkW3wcxadIkrr/++ibLn3baaXz+85/n0Ucf5aabbtpl/49//GOmTp3Kddddx6mnnrpbsYwePZrrrruOiRMnUl9fT2ZmJjfffHOrE8Q3vvENli1bxiGHHIK7U1RUxCOPPJK4sDvU1XLe16cy6eSJDNinLy/MuB/qaoJHbQ1QD1Vr4Klzg2O6FwYVf/+DYNTpMUmgJEgAmTm79XollJULA8YFj1h1tUEfTcOoqobO8jn37TxKK7tnzNVGTOIoGALpqkI6M+ssTf6lpaUeP2HQggULGDVqVEQRdQH1tTsq+9iKv646WG4chROy9GAoZ0YWpGdDehYLlq5g1D7dgkSQlZv4PNK+3IPmvcYO8pgRVlWrdpRLzwr6NBo6yBtGWPUZAVkdbFhzF2Zmb7p7wjHySv+diTvgwV+vDx8O1Mdsi9lH/LamyoZ/Y/fVbd+5DwCCfoD0rOCRnb9jOSP8m5bgv1vmWuirJJ5SzKBH/+Ax7Lid9zUOy120I4GsmgsLHkswLDe2gzy8+sjt084vRvaGEkRba6my3aVSbqKi3ml7Kytx6puLrJUsqOgt/Bu/npYGlhncOqKh4k8PrwjS0tVW3dk1NSy3thoq3t8xqqphhNWyV+OG5fbZtYNcw3JTlhJEfV3wo6iWvm23tsKPHVq5pxJVzGZAw3pmM/vSmtgWs6/Z51YFL3sgI7uVw3LDjvIFj8Fb9+wol9k9+BV57G1H8vpC3j7B32699H8zAkoQ7sF/4J209C06nSYr36Yq5oTbElXUqqSlE2lpWG584ogfltsgPSsYTRWbNPLDv3n9dmzL66ffebQhJYi0dOg3hl0qcBFJroZhuUPi7jtWsxkqP4Kq1UFnedWaYLnhsX4ZrPgvbFmb+Hm79Wo+iTRsyynQZ70FShBmwbcTEUkNWbnhr8BHNF+ubntwy5Sq1bBp9c5JpCGxLH89+FubYGKq9OwdVyWNiSQ+sewT/Mq+i16VKEEkUUVFBSeeeCIAq1atIj09naKiIgDeeOMNsrKa/0+3p7f+FukS0jODGyD2GNB8OffgbrpNJZGq1cF9r5a/DlsqEj9Ht95NJJG4Zq+cnp3qqkQJIon69OnTeB+mq666aqeb8LVGc7f+FpFWMgsq7pxwHo7m1NaEVyUxySM+sXz4WvC3rnrX4zNydk4Yef0SJ5bcog7xy38liHb25ptv8v3vf5+qqioKCwuZPn06/fv358Ybb+TWW28lIyOD0aNHc/311+9y6+9jjjmm5ROIyJ7LyIKeA4NHc9xh24YESSRMLJtWBcN+P3xt55soNrJgyG+zTVzhvuwekV2VdJ0E8eQ0WPVu2z7nPmPhlKZvmxHP3bnooot49NFHKSoq4oEHHuBnP/sZd999N9dffz0ffPAB2dnZbNiwgYKCgl1u/S0iKcIs6Azv1isYmtuc2urgqiQ+icQmlool4VVJza7HZ3RLkETiEkv+AMjv1+Yvs+skiBRQXV3N3LlzOemkYLhfXV0d/fv3B+DAAw/krLPOYvLkyUyePDnKMEWkLWVkB/cS61ncfLmGqWyr1sQlkVU7tq1dDB+8HFy9xOo/Ds5/se1Db/NnTFW78U0/WdydMWPG8Prrr++y79///jcvvfQSjz32GD//+c959902vtoRkdRmFkxM1b039N2/+bK11TsP/03SSEz9tr0dZWdnU15e3pggtm/fzrx586ivr2fFihUcf/zx/OpXv2Ljxo1UVVU13ipbRGQnGdnB/a6KS2H/U3f9EWIbUYJoR2lpaTz00EP85Cc/4aCDDmLcuHG89tpr1NXVcfbZZzN27FgOPvhgLr74YgoKCjjttNN4+OGHGTduHC+//HLU4YtIF6PbfUuk9G8ksvtq6+pZt6WGiqoa1m2uIT3NOGLYnt0pV7f7FhFJYfX1zsat26nYXM3asNKvqAqWKzZXs25zTbBcFSyv37LzbIsHFffk0Qvbfg54JQgRkTbm7myqrmVdWMHvWunXsG5zNRVVQcW/fksNdfWJW3N6dc+kd24WffKy2W+f/GA5N5vCvCx652bTJy+Lfj2SM9tip08Q7o51op++dyadpXlTuoatNXWsDb/Bx1f6FVU1rI2p9CuqaqipSzw/S352Bn3ysuidm8Wg3t05eHABfXKzwySQRWHejuVe3bPITI+uq7hTJ4icnBwqKiro06ePkkSKcXcqKirIydE80xKNmtr6sOlmR6Xf8I2+8dt9TKW/paYu4fPkZKY1fqPvm5/D/vv0CCr6uEq/ocLPyUxv51e65zp1giguLqasrIzy8vKoQ5EEcnJyKC5u4cdDIq1UV++sDztuK6qqg8q9qpqKzTtX+hVhUti0rTbh82Sm207f6IcV5tInN4veYaXf8O2/odLvntV5q9HO+8qAzMxMhg4dGnUYIrIH3Bs6buMr/R3f9itiKv31W2pI1GqZZjS22/fJy2LMgB5B5R5W+g3f/vuETTs9cjLU4hDq1AlCRFLP5upa1myqpjx8rNm0Lfy7Y1tDs09tEx23BWHHbWFuNsP75nF42GFbGFb4wTf8oNLv2S2T9DRV+HtCCUJE9lpdvVOxuXqXij5REkjUlp+RZhTlZ1OUn82AghzGDuxJYf7OlX6fvCz65GbRKzfajtuuRAlCRJq0paaWNZXVlFeFFX3lNsqrqhu3NfytqKom0Zf9/JwM+oYV/4HFBY1JoG/j3xyK8rMp6JZJmr7lp5ykJggzmwT8EUgH7nT36+P2DwbuAQrCMtPc/QkzOwm4HsgCaoAfufvzyYxVpKuoq3fWba5J3LxTVU15Y+W/jc1NfNsvzAsq+P49cziwuGdjhV8UVvgN6x1pxI7sKmkJwszSgZuBk4AyYJaZzXD3+THFLgMedPc/m9lo4AmgBFgLnObuH5nZAcBTQAszeIh0bVtr6hor/NhKPz4JVGxO/KOs/OwMinpkU5SXzZgBPTh+v747f9sP9/XqnqVv+11EMq8gxgNL3H0pgJndD5wBxCYIB3qEyz2BjwDc/e2YMvOAbmaW7e4J5vgT6bzq6511W2oSVvgN29aGf6uqdx22mZ5mFOZlUZSfTb8eORwwoGdQ0ecHlX1Q6Qff+rtl6du+7CyZCWIgsCJmvQw4PK7MVcDTZnYRkAt8MsHzfA54S8lBOpNt2+vC9vudv903tu2HSWBtVeJv+3nZGY3t+aMG9ODYuDb9hm/9vbpnaQSP7LGoO6nPBKa7++/M7Ejgb2Z2gLvXA5jZGOBXwMREB5vZecB5AIMHD26nkEVab93mGu57YznvrdrU2MFbXlnNpgTf9tOMxrb9vvnZjO7fI2GlX5Sf3al/nCWpI5n/y1YCg2LWi8Ntsc4FJgG4++tmlgMUAmvMrBh4GPiqu7+f6ATufjtwOwS3+27b8EX2XNn6Ldz58gfcP2s527bXU9KnO33zcxi1Tw8+MSI74Wie3rn6ti+pJZkJYhYwwsyGEiSGKcCX48osB04EppvZKCAHKDezAuDfBKOaXk1ijCJt6r1Vldw6830ee+djDJh88EDO/8QwRvTLjzo0kd2WtATh7rVmdiHBCKR04G53n2dm1wCz3X0G8APgDjO7hKDD+hx39/C44cAVZnZF+JQT3X1NsuIV2VPuzqxl6/nzzCW8sLCc7lnpnHNUCecePZQBBd2iDk9kj3XqGeVEkqm+3nnuvTX8eeYS3lq+gd65WXztqBK+cuQQCronZxJ5kbamGeVE2lBNbT0z/vcRt734PovXVFHcqxtXnz6GL5YO0lBR6VSUIERaaXN1Lfe9sZy7XvmAjzduY/998vnjlHGcOrY/Gbo3kHRCShAiLaioquae15Zxz+sfsnHrdsYP7c0vPjuW40YW6bbQ0qkpQYg0YcW6Ldz58lIemL2CbdvrOWl0Py44dl8OHdIr6tBE2oUShEicBR9XctuLwVDVNIPJ4wZy/rHDGN5XQ1Wla1GCECHxUNWvHVXCuccMpX9PDVWVrkkJQrq0+nrn2QWrufXF9xuHqv7gpJEaqiqCEoR0UTW19Tw6ZyW3vbSUJeFQ1WvOGMMXDtVQVZEGShDSpWioqkjrKUFIlxA/VPVwDVUVaZEShHRq8UNVJ47uxwXH7cshgzVUVaQlShDSKWmoqsjeU4KQTsPdeeODddz64vsaqirSBpQgpMNrGKr65xff5+3lG+ijoaoibUIJQjosDVUVSS4lCOlwqqpruV9DVUWSTglCOoyKqmqmv7aMv2qoqki7UIKQlLdi3RbueHkpD2qoqki7UoKQlLXg40puffF9HtdQVZFIKEFISmkYqvrnF99n5sJycjVUVSQyShCSEurrnWfCu6o2DFX94cSRfOWIEnp2z4w6PJEuSQlCIlVTW88jc1Zy24vv8375Zop7dePaM8bwhdJB5GRqqKpIlJQgJBINQ1XvfPkDVlVqqKpIKlKCkHYVP1T1iGG9uf5zYzlWQ1VFUo4ShLSLhqGqD8xaQU1dOFT12H05WENVRVKWEoQk1fyPKrntpR1DVT9z8EDO+8S+DO+bF3VoItICJQhpc+7Of8O7qjYMVf36hBK+frSGqop0JEoQ0mY0VFWkc0lqgjCzScAfgXTgTne/Pm7/YOAeoCAsM83dnwj3/RQ4F6gDLnb3p5IZq+y5+KGqg3prqKpIZ5C0BGFm6cDNwElAGTDLzGa4+/yYYpcBD7r7n81sNPAEUBIuTwHGAAOAZ81spLvXJSte2X3xQ1VH9e+hoaoinUgyryDGA0vcfSmAmd0PnAHEJggHeoTLPYGPwuUzgPvdvRr4wMyWhM/3ehLjlVZqGKp6z2vLqNxWq6GqIp1UMhPEQGBFzHoZcHhcmauAp83sIiAX+GTMsf+JO3Zg/AnM7DzgPIDBgwe3SdDSvPJN1Uy64SXWbanRUFWRTi7qTuozgenu/jszOxL4m5kd0NqD3f124HaA0tJST1KMEuOXTyygctt2Hv3OBA4sLog6HBFJomQmiJXAoJj14nBbrHOBSQDu/rqZ5QCFrTxW2tnr71fwr7dXcuHxw5UcRLqAZPYkzgJGmNlQM8si6HSeEVdmOXAigJmNAnKA8rDcFDPLNrOhwAjgjSTGKi2oqa3n8kfnMqh3Ny48YXjU4YhIO0jaFYS715rZhcBTBENY73b3eWZ2DTDb3WcAPwDuMLNLCDqsz3F3B+aZ2YMEHdq1wHc0gilad7y8lCVrqvjLOYdp6KpIF2FBfdzxlZaW+uzZs6MOo1NasW4LJ/3hRY4dWcRtXymNOhwRaUNm9qa7J/xga7C6tOjqx+aRZsaVp42JOhQRaUetamIyswkEQ1KHhMcY4O4+LHmhSSp4et4qnl2whks/tT8DCnQfJZGupLV9EHcBlwBvEtz6QrqALTW1XP3YfPbrl8/XJgyNOhwRaWetTRAb3f3JpEYiKefG55awcsNW/nHBkWTq1hkiXU5rE8QLZvYb4F9AdcNGd38rKVFJ5Bat3sSdLy/lC4cWc1hJ76jDEZEItDZBNNwiI7an24ET2jYcSQXuzmWPzCUvJ4OffmpU1OGISERalSDc/fhkByKp459vreSND9Zx/WfH0js3K+pwRCQirWpYNrOeZvZ7M5sdPn5nZj2THZy0vw1bavjFEws4ZHABXywd1PIBItJptbbn8W5gE/DF8FEJ/CVZQUl0fv3UQjZu3c51k8eSlqZbd4t0Za3tg9jX3T8Xs361mc1JRkASnbeXr+e+N5bz9QlDGT2gR8sHiEin1toriK1mdnTDSvjDua3JCUmiUFtXz88enku//BwuOWlk1OGISApo7RXEt4B7wn4HA9YB5yQrKGl/f339Q+Z/XMktZx1CXnbU04SISCpo7SimOcBBZtYjXK9MalTSrlZXbuP3zyziEyOLOOWAfaIOR0RSRLMJwszOdve/m9n347YD4O6/T2Js0k6ufXw+NXX1XHP6GM0pLSKNWrqCyA3/5ic7EInGy4vLefydj7nkkyMpKcxt+QAR6TKaTRDuflv49+r2CUfa07btdVz+yFyGFuZy/rG6Ma+I7Ky1P5T7tZn1MLNMM3vOzMrN7OxkByfJdeuL77OsYgvXnDFGs8SJyC5aO8x1Ytgx/WlgGTAc+FGygpLkW7Z2M7fMfJ9PH9ifY0YURR2OiKSg1iaIhqaoU4F/uPvGJMUj7cDduWLGPLLS07j806OjDkdEUlRrE8TjZvYecCjwnJkVAduSF5Yk0xPvruKlReX8YOJI+vXIiTocEUlRrUoQ7j4NOAoodfftwGbgjGQGJsmxadt2rnl8HmMG9OArRwyJOhwRSWEt/Q7iBHd/3sw+G7Mttsi/khWYJMcNzy5mzaZqbj37UDI0S5yINKOl30EcCzwPnJZgn6ME0XRmAE0AABQ0SURBVKHM/6iS6a8t48zxgzl4cK+owxGRFNfS7yCuDP9+rX3CkWSpr3cue+RdCrpl8pOT9486HBHpAFr7O4hfmFlBzHovM7sueWFJW3tg9greWr6BSz81ip7dM6MOR0Q6gNY2Qp/i7hsaVtx9PfCp5IQkba2iqprrn3yP8UN789lDBkYdjoh0EK1NEOlmlt2wYmbdgOxmyksKuf7J99hcXct1kw/QzfhEpNVamyDuJfj9w7lmdi7wDHBPSweZ2SQzW2hmS8xsWoL9fzCzOeFjkZltiNn3azObZ2YLzOxGU822R2YtW8c/3izjG8cMY2Q/3XNRRFqvtfNB/MrM/gd8Mtx0rbs/1dwxZpYO3AycBJQBs8xshrvPj3neS2LKXwQcHC4fBUwADgx3v0Iwompma+KVwPa6ei57eC4DC7px8YnDow5HRDqY3Zk6bAFQ6+7Pmll3M8t3903NlB8PLHH3pQBmdj/Bj+vmN1H+TODKcNmBHCCLYAa7TGD1bsQqwN2vfMDC1Zu4/SuH0j1Ls8SJyO5psonJzIpjlr8JPATcFm4aCDzSwnMPBFbErJeF2xKdawgwlOA3F7j768ALwMfh4yl3X5DguPPMbLaZzS4vL28hnK7low1bueHZxXxyVF8mjtEscSKy+5rrgzjGzC4Ol79D0ORTCeDui4G+bRjHFOAhd68DMLPhwCigmCCpnGBmx8Qf5O63u3upu5cWFemOpLGufmwejnPlaWOiDkVEOqgmE4S73wdUhas17l7TsM/MMgiagZqzEhgUs14cbktkCnBfzPpngP+4e5W7VwFPAke2cD4JPf/eap6at5qLTxzBoN7dow5HRDqoZkcxufvd4eJMM7sU6GZmJwH/AB5r4blnASPMbKiZZREkgRnxhcxsf6AX8HrM5uXAsWaWYWaZBB3UuzQxya621tRxxaPzGN43j28crVniRGTPtXaY60+AcuBd4HzgCeCy5g5w91rgQuApgsr9QXefZ2bXmNnpMUWnAPe7e+wVyUPA++H5/gf8z91bSkgC3PzCEsrWb+XaMw4gK0M34xORPWc718sJCgTDVee5e0rfwKe0tNRnz54ddRiRWrKmilP++BKnHTiA339pXNThiEgHYGZvuntpon0tfsUMO44XmtngNo9M2oy7c/kjc+mWmc6lp46KOhwR6QRaOzi+FzDPzN4gmCwIAHc/velDpD09OucjXl9awXWTD6AwT3dBEZG919oEcXlSo5C9snHrdq7793wOKu7JmeN1oScibaOlGeVygAuA4QQdxneFnc+SQn739ELWba5h+tfGk56mW1aJSNtoqQ/iHqCUIDmcAvwu6RHJbnmnbAN/+8+HfPXIEg4Y2DPqcESkE2mpiWm0u48FMLO7gDeSH5K0Vl2987OH51KYl833J46MOhwR6WRauoLY3rCgpqXUc+9/P+TdlRu57NRR9MjRLHEi0rZauoI4yMwqw2Uj+CV1Zbjs7t4jqdFJk9Zs2sZvnlrIhOF9OP2gAVGHIyKdULMJwt3T2ysQ2T2/+PcCqrfXc+0ZmiVORJJD92LogF5bspZH5nzEBccOY1hRXtThiEgnpQTRwVTX1nHZo3MZ3Ls73z5es8SJSPJomrEO5o6XlrK0fDN/+dph5GSqBVBEkkdXEB3IinVbuOn5JZxywD4cv19bztckIrIrJYgOwt25csY8MtKMK04bHXU4ItIFKEF0EE/NW83z763hkpNG0r9nt6jDEZEuQAmiA9hcXcvVj81j/33ymXpUSdThiEgXoU7qDuDG5xbz8cZt3HTmwWSmK6eLSPtQbZPiFq7axF2vfMCXSgdRWtI76nBEpAtRgkhh9fXOZY+8S35OBtNOSekZX0WkE1KCSGEPvVXGrGXrmXbK/vTKzYo6HBHpYpQgUtT6zTX88okFHDqkF184dFDU4YhIF6QEkaJ+/dR7VG6r5brJB5CmWeJEJAJKECnozQ/Xc98bK/j6hBJG9dcd1UUkGkoQKaa2rp7LHpnLPj1y+N4nNUuciERHCSLFTH9tGQs+ruTK00aTm62fqYhIdJQgUsiqjdv4wzOLOG6/IiYdsE/U4YhIF6cEkUKufXw+tfXONadrljgRiV5SE4SZTTKzhWa2xMymJdj/BzObEz4WmdmGmH2DzexpM1tgZvPNrCSZsUbtxUXl/Pvdj7nw+OEM7tM96nBERJJ3LyYzSwduBk4CyoBZZjbD3ec3lHH3S2LKXwQcHPMUfwV+7u7PmFkeUJ+sWKO2bXsdVzw6l2GFuZx37LCowxERAZJ7BTEeWOLuS929BrgfOKOZ8mcC9wGY2Wggw92fAXD3KnffksRYI3XLzPf5sGIL104+gOwMzRInIqkhmQliILAiZr0s3LYLMxsCDAWeDzeNBDaY2b/M7G0z+014RRJ/3HlmNtvMZpeXl7dx+O3jg7WbuXXm+5x+0AAmDC+MOhwRkUap0kk9BXjI3evC9QzgGOCHwGHAMOCc+IPc/XZ3L3X30qKiovaKtc24O1c8OpfsjDQuO3VU1OGIiOwkmQliJRB7E6HicFsiUwibl0JlwJyweaoWeAQ4JClRRujxdz7m5cVr+cHEkfTtkRN1OCIiO0lmgpgFjDCzoWaWRZAEZsQXMrP9gV7A63HHFphZw2XBCcD8+GM7sk3btnPt4/M5YGAPvnJkSdThiIjsImkJIvzmfyHwFLAAeNDd55nZNWZ2ekzRKcD97u4xx9YRNC89Z2bvAgbckaxYo/D7ZxZRXlXNzyePJV034xORFJTUezm4+xPAE3Hbrohbv6qJY58BDkxacBGau3Ij97y2jLMOH8xBgwqiDkdEJKFU6aTuMoJZ4ubSOzeLH03ULHEikrqUINrZfbOWM2fFBi791Ch6ds+MOhwRkSYpQbSjtVXV/OrJ9zhiWG8+c3DCn4SIiKQMJYh29Msn3mPr9jqum6yb8YlI6lOCaCf/XVrBP98q45vHDGN43/yowxERaZESRDuoqQ1miRtY0I2LThgRdTgiIq2iKcvawV2vfMDiNVXc+dVSumXpZnwi0jHoCiLJytZv4cbnFnPS6H58cnS/qMMREWk1JYgku/qx4A4hV542OuJIRER2jxJEEj07fzXPzF/NxSeOoLiXZokTkY5FCSJJttTUcuWMeYzom8e5Rw+NOhwRkd2mTuokuen5JazcsJUHzjuCrAzlYRHpeFRzJcHi1Zu446WlfO6QYg4f1ifqcERE9ogSRBtzdy5/dC652Rlc+indjE9EOi4liDb28Nsr+c/Sdfx40n70ycuOOhwRkT2mBNGGNm7Zzs//vYBxgwo487DBUYcjIrJX1Endhn7z9Hus31LDPV8fT5pmiRORDk5XEG1kzooN3Pvf5Uw9qoQDBvaMOhwRkb2mBNEG6uqdyx55l6K8bL5/0siowxERaRNKEG3gb68vY+7KSi7/9GjyczRLnIh0DkoQe2lN5TZ+9/QijhlRyKcP7B91OCIibUYJYi9d9+8FVNfWc80ZmiVORDoXJYi98Mritcz430dccNy+DC3MjTocEZE2pQSxh6pr67ji0bkM6dOdbx+3b9ThiIi0Of0OYg/d9uJSlq7dzD1fH09OpmaJE5HOR1cQe+DDis386YUlnDq2P8eOLIo6HBGRpFCC2E3uzpUz5pGZZlz+ac0SJyKdlxLEbvp/c1cxc2E5l5w0kn165kQdjohI0iQ1QZjZJDNbaGZLzGxagv1/MLM54WORmW2I29/DzMrM7E/JjLO1qqprufqx+Yzq34NzjiqJOhwRkaRKWie1maUDNwMnAWXALDOb4e7zG8q4+yUx5S8CDo57mmuBl5IV4+664ZlFrKrcxs1nHUJGui6+RKRzS2YtNx5Y4u5L3b0GuB84o5nyZwL3NayY2aFAP+DpJMbYags+ruQvry3jzPGDOHRIr6jDERFJumQmiIHAipj1snDbLsxsCDAUeD5cTwN+B/ywuROY2XlmNtvMZpeXl7dJ0InU1zuXPTKXnt0y+fHJmiVORLqGVGknmQI85O514fq3gSfcvay5g9z9dncvdffSoqLkDTf9x5srePPD9Uw7ZX965WYl7TwiIqkkmT+UWwkMilkvDrclMgX4Tsz6kcAxZvZtIA/IMrMqd9+lozvZ1m2u4ZdPvsdhJb34/CHF7X16EZHIJDNBzAJGmNlQgsQwBfhyfCEz2x/oBbzesM3dz4rZfw5QGkVyAPjVk++xaVst100eq1niRKRLSVoTk7vXAhcCTwELgAfdfZ6ZXWNmp8cUnQLc7+6erFj21Oxl63hg9grOPXoo++2TH3U4IiLtylKwXt4jpaWlPnv27DZ7vu119Zx20ytUbt3OM98/ltxs3bZKRDofM3vT3UsT7UuVTuqUM/3VZby3ahNXnDZGyUFEuiQliAQ+2rCVPzy7iBP278vJY/pFHY6ISCSUIBK49vH51NU7V58+RrPEiUiXpQQR54WFa3hy7iouOmE4g3p3jzocEZHIKEHE2La9jisfncewoly++YlhUYcjIhIp9b7GuPmFJSxft4X/+8bhZGdoljgR6dp0BRF6v7yK215cyuRxAzhqeGHU4YiIRE4JgmCWuCsenUt2ZhqXnjoq6nBERFKCEgQw438f8eqSCn508n70zdcscSIioARB5bbtXPfvBRxY3JOzDh8SdTgiIimjy3dSb9tex7hBBVx0wnDSdTM+EZFGXT5B9M3P4Y6vJrwNiYhIl9blm5hERCQxJQgREUlICUJERBJSghARkYSUIEREJCElCBERSUgJQkREElKCEBGRhMzdo46hTZhZOfDhXjxFIbC2jcJpS4pr9yiu3aO4dk9njGuIuxcl2tFpEsTeMrPZ7p5yP6lWXLtHce0exbV7ulpcamISEZGElCBERCQhJYgdbo86gCYort2juHaP4to9XSou9UGIiEhCuoIQEZGElCBERCShLpUgzGySmS00syVmNi3B/mwzeyDc/18zK0mRuM4xs3IzmxM+vtFOcd1tZmvMbG4T+83MbgzjfsfMDkmRuI4zs40x79cV7RTXIDN7wczmm9k8M/tugjLt/p61Mq52f8/MLMfM3jCz/4VxXZ2gTLt/JlsZVySfyfDc6Wb2tpk9nmBf275f7t4lHkA68D4wDMgC/geMjivzbeDWcHkK8ECKxHUO8KcI3rNPAIcAc5vY/yngScCAI4D/pkhcxwGPR/B+9QcOCZfzgUUJ/i3b/T1rZVzt/p6F70FeuJwJ/Bc4Iq5MFJ/J1sQVyWcyPPf3gf9L9O/V1u9XV7qCGA8scfel7l4D3A+cEVfmDOCecPkh4EQzS/ZE1a2JKxLu/hKwrpkiZwB/9cB/gAIz658CcUXC3T9297fC5U3AAmBgXLF2f89aGVe7C9+DqnA1M3zEj5pp989kK+OKhJkVA6cCdzZRpE3fr66UIAYCK2LWy9j1Q9JYxt1rgY1AnxSIC+BzYZPEQ2Y2KMkxtVZrY4/CkWETwZNmNqa9Tx5e2h9M8O0zVqTvWTNxQQTvWdhcMgdYAzzj7k2+X+34mWxNXBDNZ/IG4MdAfRP72/T96koJoiN7DChx9wOBZ9jxDUESe4vg/jIHATcBj7Tnyc0sD/gn8D13r2zPczenhbgiec/cvc7dxwHFwHgzO6A9ztuSVsTV7p9JM/s0sMbd30z2uRp0pQSxEojN8sXhtoRlzCwD6AlURB2Xu1e4e3W4eidwaJJjaq3WvKftzt0rG5oI3P0JINPMCtvj3GaWSVAJ3+vu/0pQJJL3rKW4onzPwnNuAF4AJsXtiuIz2WJcEX0mJwCnm9kygqboE8zs73Fl2vT96koJYhYwwsyGmlkWQQfOjLgyM4Cp4fLngec97O2JMq64NurTCdqQU8EM4KvhyJwjgI3u/nHUQZnZPg3trmY2nuD/edIrlfCcdwEL3P33TRRr9/esNXFF8Z6ZWZGZFYTL3YCTgPfiirX7Z7I1cUXxmXT3n7p7sbuXENQTz7v72XHF2vT9ytjTAzsad681swuBpwhGDt3t7vPM7BpgtrvPIPgQ/c3MlhB0gk5JkbguNrPTgdowrnOSHReAmd1HMLql0MzKgCsJOuxw91uBJwhG5SwBtgBfS5G4Pg98y8xqga3AlHZI9BB8w/sK8G7Yfg1wKTA4JrYo3rPWxBXFe9YfuMfM0gkS0oPu/njUn8lWxhXJZzKRZL5futWGiIgk1JWamEREZDcoQYiISEJKECIikpAShIiIJKQEIbKHzOzLZjY46jhEkkUJQiQBM6sK/5aY2ZcT7D8X6Ovuy/fguS+NW39tjwMVSSINcxVJwMyq3D3PzI4Dfujun96NYzPC++A0+9xtEadIMukKQqR51wPHhPf8vyS8idtvzGxWeKO286FxPoWXzWwGMD/c9oiZvWnBnALnhduuB7qFz3dvuK3hasXC555rZu+a2ZdinntmeFO498zs3oZfPYskU5f5JbXIHppGzBVEWNFvdPfDzCwbeNXMng7LHgIc4O4fhOtfd/d14e0aZpnZP919mpldGN4ILt5ngXHAQUBheMxL4b6DgTHAR8CrBL+OfqXtX67IDrqCENk9EwnupTSH4JbZfYAR4b43YpIDBLdj+B/wH4IbqI2geUcD94V3El0NvAgcFvPcZe5eD8wBStrk1Yg0Q1cQIrvHgIvc/amdNgZ9FZvj1j8JHOnuW8xsJpCzF+etjlmuQ59daQe6ghBp3iaCaTobPEVwU7tMADMbaWa5CY7rCawPk8P+BNOLNtjecHycl4Evhf0cRQRTq77RJq9CZA/oW4hI894B6sKmounAHwmad94KO4rLgckJjvt/wAVmtgBYSNDM1OB24B0ze8vdz4rZ/jBwJMG85A782N1XhQlGpN1pmKuIiCSkJiYREUlICUJERBJSghARkYSUIEREJCElCBERSUgJQkREElKCEBGRhP4/XQtpi2+PHqYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5ThHOoGRGIa"
      },
      "source": [
        "# **Fine Tuning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhYtyQjuIIAZ"
      },
      "source": [
        "Afin d'obtenir une meilleur précision, on va également entrainer distilBERT : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eui3V0PRI6U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abe13f09-51af-4cea-c0fc-35da6ab738eb"
      },
      "source": [
        "model.layers[2].trainable = True\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(1e-5), metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_token (InputLayer)        [(None, 273)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "masked_token (InputLayer)       [(None, 273)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_distil_bert_model_1 (TFDisti TFBaseModelOutput(la 66362880    input_token[0][0]                \n",
            "                                                                 masked_token[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 768)          0           tf_distil_bert_model_1[0][7]     \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 2)            1538        lambda[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 66,364,418\n",
            "Trainable params: 66,364,418\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO3HSM6dRYl_"
      },
      "source": [
        "history = model.fit([output_tokenizer_entrainement['input_ids'],output_tokenizer_entrainement['attention_mask']],y_entrainement,\n",
        "                    epochs=5, verbose=1, batch_size = 3,\n",
        "                    validation_data=([output_tokenizer_tests['input_ids'],output_tokenizer_tests['attention_mask']],y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGsA4TSdRvnu"
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Précision du modèle')\n",
        "plt.ylabel('Précision')\n",
        "plt.xlabel('Itération')\n",
        "plt.legend(['Entrainement', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUAkgLQy2mSo"
      },
      "source": [
        "# **Un modèle plus complexe**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiWq_-8HJqAp"
      },
      "source": [
        "Cette fois, les vecteurs de dimension 768 correspondants aux sorties [CLS] de chaque commentaire sont envoyés au même modèle que celui utilisé lors de l'activité avec GloVe :\n",
        "<img src=\"https://raw.githubusercontent.com/AlexandreBourrieau/ML-F1/master/Carnets%20Jupyter/Images/ReseauDistilBERT3.png\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu0eaqewLTVB"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/AlexandreBourrieau/ML-F1/master/Carnets%20Jupyter/Images/ReseauDistilBERT4.png\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JS4_ExKz9Beu"
      },
      "source": [
        "dropout = 0.4\n",
        "\n",
        "# Configuration du modèle distilBERT\n",
        "config = DistilBertConfig(num_labels=2)           # 1 label\n",
        "config.output_hidden_states = True               # Ne récupère pas la totalité des couches mais uniquement la dernière\n",
        "config.output_attentions = False\n",
        "\n",
        "# Instanciation du modèle distilBERT\n",
        "transformer_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased', config = config)\n",
        "\n",
        "# Défintion du format des entrées du modèle\n",
        "entrees_ids = tf.keras.layers.Input(shape=(LONGUEUR_MAX_COMMENTAIRE,), name='input_token', dtype='int32')\n",
        "entrees_masks = tf.keras.layers.Input(shape=(LONGUEUR_MAX_COMMENTAIRE,), name='masked_token', dtype='int32') \n",
        "\n",
        "# Création de la sortie du modèle\n",
        "sortie_distilBERT = transformer_model([entrees_ids,entrees_masks])[0]\n",
        "\n",
        "l1 = Lambda(lambda seq: seq[:, 0, :])(sortie_distilBERT)\n",
        "l2 = Conv1D(128, 5, activation='relu', padding='same', strides=2)(tf.expand_dims(l1,axis=1))\n",
        "l3 = GlobalMaxPooling1D()(l2)\n",
        "l4 = Dropout(dropout)(l3)\n",
        "l5 = Dense(128, activation='relu')(l4)\n",
        "l6 = Dropout(dropout)(l5)\n",
        "l7 = Dense(2, activation='softmax')(l6)\n",
        "\n",
        "model = tf.keras.Model(inputs=[entrees_ids, entrees_masks], outputs = l7)\n",
        "model.layers[2].trainable = False\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJ1zFZr3_iKX"
      },
      "source": [
        "history = model.fit([output_tokenizer_entrainement['input_ids'],output_tokenizer_entrainement['attention_mask']],y_entrainement,\n",
        "                    epochs=5, verbose=1, batch_size = 3,\n",
        "                    validation_data=([output_tokenizer_tests['input_ids'],output_tokenizer_tests['attention_mask']],y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xbfmmKuB__B"
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Précision du modèle')\n",
        "plt.ylabel('Précision')\n",
        "plt.xlabel('Itération')\n",
        "plt.legend(['Entrainement', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlBsWHRNQPrP"
      },
      "source": [
        "**Fine-tuning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKn5K5VYQMtS"
      },
      "source": [
        "Afin de tenter d'obtenir une meilleur précision, on va également entrainer distilBERT : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsfwWC_sB9-k"
      },
      "source": [
        "model.layers[2].trainable = True\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(1e-5), metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh_1ALcKCFSk"
      },
      "source": [
        "history = model.fit([output_tokenizer_entrainement['input_ids'],output_tokenizer_entrainement['attention_mask']],y_entrainement,\n",
        "                    epochs=5, verbose=1, batch_size = 3,\n",
        "                    validation_data=([output_tokenizer_tests['input_ids'],output_tokenizer_tests['attention_mask']],y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2EGMAnCHXVf"
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Précision du modèle')\n",
        "plt.ylabel('Précision')\n",
        "plt.xlabel('Itération')\n",
        "plt.legend(['Entrainement', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}