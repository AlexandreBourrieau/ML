{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reseau_GRU_Avec_Attention_Hierarchique.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNxDEfIhHSnPZyHw8bFNbY0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/Reseau_GRU_Avec_Attention_Hierarchique.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubCeIvtF6R4W"
      },
      "source": [
        "Dans ce carnet nous allons mettre en place un modèle à réseau de neurones récurrent de type GRU associé à une **couche d'attention** comprenant un **vecteur contexte** pour réaliser des prédictions sur notre série temporelle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRhtHsNn5fc3"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFeah3y_6kif"
      },
      "source": [
        "# Création de la série temporelle et du dataset pour l'entrainement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJfSLtub6sdc"
      },
      "source": [
        "# Fonction permettant d'afficher une série temporelle\r\n",
        "def affiche_serie(temps, serie, format=\"-\", debut=0, fin=None, label=None):\r\n",
        "    plt.plot(temps[debut:fin], serie[debut:fin], format, label=label)\r\n",
        "    plt.xlabel(\"Temps\")\r\n",
        "    plt.ylabel(\"Valeur\")\r\n",
        "    if label:\r\n",
        "        plt.legend(fontsize=14)\r\n",
        "    plt.grid(True)\r\n",
        "\r\n",
        "# Fonction permettant de créer une tendance\r\n",
        "def tendance(temps, pente=0):\r\n",
        "    return pente * temps\r\n",
        "\r\n",
        "# Fonction permettant de créer un motif\r\n",
        "def motif_periodique(instants):\r\n",
        "    return (np.where(instants < 0.4,                            # Si les instants sont < 0.4\r\n",
        "                    np.cos(instants * 2 * np.pi),               # Alors on retourne la fonction cos(2*pi*t)\r\n",
        "                    1 / np.exp(3 * instants)))                  # Sinon, on retourne la fonction exp(-3t)\r\n",
        "\r\n",
        "# Fonction permettant de créer une saisonnalité avec un motif\r\n",
        "def saisonnalite(temps, periode, amplitude=1, phase=0):\r\n",
        "    \"\"\"Répétition du motif sur la même période\"\"\"\r\n",
        "    instants = ((temps + phase) % periode) / periode            # Mapping du temps =[0 1 2 ... 1460] => instants = [0.0 ... 1.0]\r\n",
        "    return amplitude * motif_periodique(instants)\r\n",
        "\r\n",
        "# Fonction permettant de générer du bruit gaussien N(0,1)\r\n",
        "def bruit_blanc(temps, niveau_bruit=1, graine=None):\r\n",
        "    rnd = np.random.RandomState(graine)\r\n",
        "    return rnd.randn(len(temps)) * niveau_bruit\r\n",
        "\r\n",
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\r\n",
        "# au format X(X1,X2,...Xn) / Y(Y1,Y2,...,Yn)\r\n",
        "# X sont les données d'entrées du réseau\r\n",
        "# Y sont les labels\r\n",
        "\r\n",
        "def prepare_dataset_XY(serie, taille_fenetre, nbr_sequences):\r\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie)\r\n",
        "  dataset = dataset.window(taille_fenetre+1, shift=1, drop_remainder=True)\r\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre + 1))\r\n",
        "  dataset = dataset.map(lambda x: (x[:-1], x[-1:]))\r\n",
        "  dataset = dataset.batch(nbr_sequences,drop_remainder=True).prefetch(1)\r\n",
        "  return dataset\r\n",
        "\r\n",
        "\r\n",
        "# Création de la série temporelle\r\n",
        "temps = np.arange(4 * 365)                # temps = [0 1 2 .... 4*365] = [0 1 2 .... 1460]\r\n",
        "amplitude = 40                            # Amplitude de la la saisonnalité\r\n",
        "niveau_bruit = 5                          # Niveau du bruit\r\n",
        "offset = 10                               # Offset de la série\r\n",
        "\r\n",
        "serie = offset + tendance(temps, 0.1) + saisonnalite(temps, periode=365, amplitude=amplitude) + bruit_blanc(temps,niveau_bruit)\r\n",
        "\r\n",
        "temps_separation = 1000\r\n",
        "\r\n",
        "# Extraction des temps et des données d'entrainement\r\n",
        "temps_entrainement = temps[:temps_separation]\r\n",
        "x_entrainement = serie[:temps_separation]\r\n",
        "\r\n",
        "# Exctraction des temps et des données de validation\r\n",
        "temps_validation = temps[temps_separation:]\r\n",
        "x_validation = serie[temps_separation:]\r\n",
        "\r\n",
        "# Définition des caractéristiques du dataset que l'on souhaite créer\r\n",
        "Nbr_Sequences = 32\r\n",
        "taille_fenetre = 20\r\n",
        "buffer_melange = 1000\r\n",
        "\r\n",
        "# Création du dataset X,Y\r\n",
        "dataset = prepare_dataset_XY(serie,taille_fenetre,Nbr_Sequences)\r\n",
        "\r\n",
        "# Création du dataset X,Y de validation\r\n",
        "dataset_Val = prepare_dataset_XY(x_validation,taille_fenetre,Nbr_Sequences)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dXVSTlBBMrk",
        "outputId": "8153bc27-d075-4a97-8551-58b817b4bacb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(len(list(dataset.as_numpy_iterator())))\r\n",
        "for element in dataset.take(1):\r\n",
        "  print(element)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45\n",
            "(<tf.Tensor: shape=(32, 20), dtype=float64, numpy=\n",
            "array([[53.5853445 , 51.27333417, 45.2659627 , 26.50977929, 51.34697088,\n",
            "        49.56667122, 44.99512005, 52.68258034, 47.16017807, 53.55019711,\n",
            "        45.39088389, 36.21144627, 47.39997337, 50.31730324, 57.67018325,\n",
            "        52.8015919 , 36.96866074, 52.15969337, 47.9971449 , 52.83492933],\n",
            "       [51.27333417, 45.2659627 , 26.50977929, 51.34697088, 49.56667122,\n",
            "        44.99512005, 52.68258034, 47.16017807, 53.55019711, 45.39088389,\n",
            "        36.21144627, 47.39997337, 50.31730324, 57.67018325, 52.8015919 ,\n",
            "        36.96866074, 52.15969337, 47.9971449 , 52.83492933, 59.36701011],\n",
            "       [45.2659627 , 26.50977929, 51.34697088, 49.56667122, 44.99512005,\n",
            "        52.68258034, 47.16017807, 53.55019711, 45.39088389, 36.21144627,\n",
            "        47.39997337, 50.31730324, 57.67018325, 52.8015919 , 36.96866074,\n",
            "        52.15969337, 47.9971449 , 52.83492933, 59.36701011, 50.68375856],\n",
            "       [26.50977929, 51.34697088, 49.56667122, 44.99512005, 52.68258034,\n",
            "        47.16017807, 53.55019711, 45.39088389, 36.21144627, 47.39997337,\n",
            "        50.31730324, 57.67018325, 52.8015919 , 36.96866074, 52.15969337,\n",
            "        47.9971449 , 52.83492933, 59.36701011, 50.68375856, 46.64416968],\n",
            "       [51.34697088, 49.56667122, 44.99512005, 52.68258034, 47.16017807,\n",
            "        53.55019711, 45.39088389, 36.21144627, 47.39997337, 50.31730324,\n",
            "        57.67018325, 52.8015919 , 36.96866074, 52.15969337, 47.9971449 ,\n",
            "        52.83492933, 59.36701011, 50.68375856, 46.64416968, 50.37447167],\n",
            "       [49.56667122, 44.99512005, 52.68258034, 47.16017807, 53.55019711,\n",
            "        45.39088389, 36.21144627, 47.39997337, 50.31730324, 57.67018325,\n",
            "        52.8015919 , 36.96866074, 52.15969337, 47.9971449 , 52.83492933,\n",
            "        59.36701011, 50.68375856, 46.64416968, 50.37447167, 42.44169982],\n",
            "       [44.99512005, 52.68258034, 47.16017807, 53.55019711, 45.39088389,\n",
            "        36.21144627, 47.39997337, 50.31730324, 57.67018325, 52.8015919 ,\n",
            "        36.96866074, 52.15969337, 47.9971449 , 52.83492933, 59.36701011,\n",
            "        50.68375856, 46.64416968, 50.37447167, 42.44169982, 51.50914896],\n",
            "       [52.68258034, 47.16017807, 53.55019711, 45.39088389, 36.21144627,\n",
            "        47.39997337, 50.31730324, 57.67018325, 52.8015919 , 36.96866074,\n",
            "        52.15969337, 47.9971449 , 52.83492933, 59.36701011, 50.68375856,\n",
            "        46.64416968, 50.37447167, 42.44169982, 51.50914896, 39.03577097],\n",
            "       [47.16017807, 53.55019711, 45.39088389, 36.21144627, 47.39997337,\n",
            "        50.31730324, 57.67018325, 52.8015919 , 36.96866074, 52.15969337,\n",
            "        47.9971449 , 52.83492933, 59.36701011, 50.68375856, 46.64416968,\n",
            "        50.37447167, 42.44169982, 51.50914896, 39.03577097, 50.29449529],\n",
            "       [53.55019711, 45.39088389, 36.21144627, 47.39997337, 50.31730324,\n",
            "        57.67018325, 52.8015919 , 36.96866074, 52.15969337, 47.9971449 ,\n",
            "        52.83492933, 59.36701011, 50.68375856, 46.64416968, 50.37447167,\n",
            "        42.44169982, 51.50914896, 39.03577097, 50.29449529, 51.75495943],\n",
            "       [45.39088389, 36.21144627, 47.39997337, 50.31730324, 57.67018325,\n",
            "        52.8015919 , 36.96866074, 52.15969337, 47.9971449 , 52.83492933,\n",
            "        59.36701011, 50.68375856, 46.64416968, 50.37447167, 42.44169982,\n",
            "        51.50914896, 39.03577097, 50.29449529, 51.75495943, 47.95302316],\n",
            "       [36.21144627, 47.39997337, 50.31730324, 57.67018325, 52.8015919 ,\n",
            "        36.96866074, 52.15969337, 47.9971449 , 52.83492933, 59.36701011,\n",
            "        50.68375856, 46.64416968, 50.37447167, 42.44169982, 51.50914896,\n",
            "        39.03577097, 50.29449529, 51.75495943, 47.95302316, 56.2546826 ],\n",
            "       [47.39997337, 50.31730324, 57.67018325, 52.8015919 , 36.96866074,\n",
            "        52.15969337, 47.9971449 , 52.83492933, 59.36701011, 50.68375856,\n",
            "        46.64416968, 50.37447167, 42.44169982, 51.50914896, 39.03577097,\n",
            "        50.29449529, 51.75495943, 47.95302316, 56.2546826 , 50.26983079],\n",
            "       [50.31730324, 57.67018325, 52.8015919 , 36.96866074, 52.15969337,\n",
            "        47.9971449 , 52.83492933, 59.36701011, 50.68375856, 46.64416968,\n",
            "        50.37447167, 42.44169982, 51.50914896, 39.03577097, 50.29449529,\n",
            "        51.75495943, 47.95302316, 56.2546826 , 50.26983079, 42.78862933],\n",
            "       [57.67018325, 52.8015919 , 36.96866074, 52.15969337, 47.9971449 ,\n",
            "        52.83492933, 59.36701011, 50.68375856, 46.64416968, 50.37447167,\n",
            "        42.44169982, 51.50914896, 39.03577097, 50.29449529, 51.75495943,\n",
            "        47.95302316, 56.2546826 , 50.26983079, 42.78862933, 45.59409325],\n",
            "       [52.8015919 , 36.96866074, 52.15969337, 47.9971449 , 52.83492933,\n",
            "        59.36701011, 50.68375856, 46.64416968, 50.37447167, 42.44169982,\n",
            "        51.50914896, 39.03577097, 50.29449529, 51.75495943, 47.95302316,\n",
            "        56.2546826 , 50.26983079, 42.78862933, 45.59409325, 53.86429318],\n",
            "       [36.96866074, 52.15969337, 47.9971449 , 52.83492933, 59.36701011,\n",
            "        50.68375856, 46.64416968, 50.37447167, 42.44169982, 51.50914896,\n",
            "        39.03577097, 50.29449529, 51.75495943, 47.95302316, 56.2546826 ,\n",
            "        50.26983079, 42.78862933, 45.59409325, 53.86429318, 46.91388376],\n",
            "       [52.15969337, 47.9971449 , 52.83492933, 59.36701011, 50.68375856,\n",
            "        46.64416968, 50.37447167, 42.44169982, 51.50914896, 39.03577097,\n",
            "        50.29449529, 51.75495943, 47.95302316, 56.2546826 , 50.26983079,\n",
            "        42.78862933, 45.59409325, 53.86429318, 46.91388376, 39.92631579],\n",
            "       [47.9971449 , 52.83492933, 59.36701011, 50.68375856, 46.64416968,\n",
            "        50.37447167, 42.44169982, 51.50914896, 39.03577097, 50.29449529,\n",
            "        51.75495943, 47.95302316, 56.2546826 , 50.26983079, 42.78862933,\n",
            "        45.59409325, 53.86429318, 46.91388376, 39.92631579, 40.78138659],\n",
            "       [52.83492933, 59.36701011, 50.68375856, 46.64416968, 50.37447167,\n",
            "        42.44169982, 51.50914896, 39.03577097, 50.29449529, 51.75495943,\n",
            "        47.95302316, 56.2546826 , 50.26983079, 42.78862933, 45.59409325,\n",
            "        53.86429318, 46.91388376, 39.92631579, 40.78138659, 47.28147182],\n",
            "       [59.36701011, 50.68375856, 46.64416968, 50.37447167, 42.44169982,\n",
            "        51.50914896, 39.03577097, 50.29449529, 51.75495943, 47.95302316,\n",
            "        56.2546826 , 50.26983079, 42.78862933, 45.59409325, 53.86429318,\n",
            "        46.91388376, 39.92631579, 40.78138659, 47.28147182, 41.41395271],\n",
            "       [50.68375856, 46.64416968, 50.37447167, 42.44169982, 51.50914896,\n",
            "        39.03577097, 50.29449529, 51.75495943, 47.95302316, 56.2546826 ,\n",
            "        50.26983079, 42.78862933, 45.59409325, 53.86429318, 46.91388376,\n",
            "        39.92631579, 40.78138659, 47.28147182, 41.41395271, 43.88569711],\n",
            "       [46.64416968, 50.37447167, 42.44169982, 51.50914896, 39.03577097,\n",
            "        50.29449529, 51.75495943, 47.95302316, 56.2546826 , 50.26983079,\n",
            "        42.78862933, 45.59409325, 53.86429318, 46.91388376, 39.92631579,\n",
            "        40.78138659, 47.28147182, 41.41395271, 43.88569711, 47.48865986],\n",
            "       [50.37447167, 42.44169982, 51.50914896, 39.03577097, 50.29449529,\n",
            "        51.75495943, 47.95302316, 56.2546826 , 50.26983079, 42.78862933,\n",
            "        45.59409325, 53.86429318, 46.91388376, 39.92631579, 40.78138659,\n",
            "        47.28147182, 41.41395271, 43.88569711, 47.48865986, 47.63743637],\n",
            "       [42.44169982, 51.50914896, 39.03577097, 50.29449529, 51.75495943,\n",
            "        47.95302316, 56.2546826 , 50.26983079, 42.78862933, 45.59409325,\n",
            "        53.86429318, 46.91388376, 39.92631579, 40.78138659, 47.28147182,\n",
            "        41.41395271, 43.88569711, 47.48865986, 47.63743637, 39.40880133],\n",
            "       [51.50914896, 39.03577097, 50.29449529, 51.75495943, 47.95302316,\n",
            "        56.2546826 , 50.26983079, 42.78862933, 45.59409325, 53.86429318,\n",
            "        46.91388376, 39.92631579, 40.78138659, 47.28147182, 41.41395271,\n",
            "        43.88569711, 47.48865986, 47.63743637, 39.40880133, 43.58450474],\n",
            "       [39.03577097, 50.29449529, 51.75495943, 47.95302316, 56.2546826 ,\n",
            "        50.26983079, 42.78862933, 45.59409325, 53.86429318, 46.91388376,\n",
            "        39.92631579, 40.78138659, 47.28147182, 41.41395271, 43.88569711,\n",
            "        47.48865986, 47.63743637, 39.40880133, 43.58450474, 32.94010941],\n",
            "       [50.29449529, 51.75495943, 47.95302316, 56.2546826 , 50.26983079,\n",
            "        42.78862933, 45.59409325, 53.86429318, 46.91388376, 39.92631579,\n",
            "        40.78138659, 47.28147182, 41.41395271, 43.88569711, 47.48865986,\n",
            "        47.63743637, 39.40880133, 43.58450474, 32.94010941, 43.74206893],\n",
            "       [51.75495943, 47.95302316, 56.2546826 , 50.26983079, 42.78862933,\n",
            "        45.59409325, 53.86429318, 46.91388376, 39.92631579, 40.78138659,\n",
            "        47.28147182, 41.41395271, 43.88569711, 47.48865986, 47.63743637,\n",
            "        39.40880133, 43.58450474, 32.94010941, 43.74206893, 39.94685898],\n",
            "       [47.95302316, 56.2546826 , 50.26983079, 42.78862933, 45.59409325,\n",
            "        53.86429318, 46.91388376, 39.92631579, 40.78138659, 47.28147182,\n",
            "        41.41395271, 43.88569711, 47.48865986, 47.63743637, 39.40880133,\n",
            "        43.58450474, 32.94010941, 43.74206893, 39.94685898, 41.76993051],\n",
            "       [56.2546826 , 50.26983079, 42.78862933, 45.59409325, 53.86429318,\n",
            "        46.91388376, 39.92631579, 40.78138659, 47.28147182, 41.41395271,\n",
            "        43.88569711, 47.48865986, 47.63743637, 39.40880133, 43.58450474,\n",
            "        32.94010941, 43.74206893, 39.94685898, 41.76993051, 34.87964497],\n",
            "       [50.26983079, 42.78862933, 45.59409325, 53.86429318, 46.91388376,\n",
            "        39.92631579, 40.78138659, 47.28147182, 41.41395271, 43.88569711,\n",
            "        47.48865986, 47.63743637, 39.40880133, 43.58450474, 32.94010941,\n",
            "        43.74206893, 39.94685898, 41.76993051, 34.87964497, 41.6213442 ]])>, <tf.Tensor: shape=(32, 1), dtype=float64, numpy=\n",
            "array([[59.36701011],\n",
            "       [50.68375856],\n",
            "       [46.64416968],\n",
            "       [50.37447167],\n",
            "       [42.44169982],\n",
            "       [51.50914896],\n",
            "       [39.03577097],\n",
            "       [50.29449529],\n",
            "       [51.75495943],\n",
            "       [47.95302316],\n",
            "       [56.2546826 ],\n",
            "       [50.26983079],\n",
            "       [42.78862933],\n",
            "       [45.59409325],\n",
            "       [53.86429318],\n",
            "       [46.91388376],\n",
            "       [39.92631579],\n",
            "       [40.78138659],\n",
            "       [47.28147182],\n",
            "       [41.41395271],\n",
            "       [43.88569711],\n",
            "       [47.48865986],\n",
            "       [47.63743637],\n",
            "       [39.40880133],\n",
            "       [43.58450474],\n",
            "       [32.94010941],\n",
            "       [43.74206893],\n",
            "       [39.94685898],\n",
            "       [41.76993051],\n",
            "       [34.87964497],\n",
            "       [41.6213442 ],\n",
            "       [43.29356881]])>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Yt-EgZ3sgPY"
      },
      "source": [
        "# Création du modèle GRU avec couche d'attention possédant un vecteur de contexte"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyrcfKcgCsZ7"
      },
      "source": [
        "**1. Création du réseau et adaptation des formats d'entrée et de sortie**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeJyix8HK7Kt"
      },
      "source": [
        "Sous forme de shéma, notre réseau est donc le suivant :\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OZkfsmnBNHY"
      },
      "source": [
        "<img src=\"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/images/Attention_VecteurContexte1.png?raw=true\" width=\"1200\"> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kgTrJOQ5DUo"
      },
      "source": [
        "# Remise à zéro de tous les états générés par Keras\r\n",
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLNIAGDlBizT"
      },
      "source": [
        "On créé une classe dérivée de la classe [Layer](https://keras.io/api/layers/base_layer/#layer-class) de Keras. Les méthodes utilisées sont les suivantes :  \r\n",
        " - [build](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#build) : Permet de créer les variables utilisées par la couche (commes les poids et les offsets)\r\n",
        " - [call](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#call) : Permet d'implanter la logique de la couche"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3COaR59t5WzJ"
      },
      "source": [
        "<img src=\"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/images/Attention_VecteurContexte2.png?raw=true\" width=\"1200\"> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyoh5UpQXCqm"
      },
      "source": [
        "Parmi les nouvelles fonctions de Tensorflow et de Keras utilisées, on trouve :\r\n",
        "- [transpose](https://www.tensorflow.org/api_docs/python/tf/transpose) : Permet de transposer un tenseur et éventuellement de reconstituer l'ordre des axes avec l'argument `perm`\r\n",
        "- [add_weight](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#add_weight) : Méthode de la classe Layers de Keras, qui permet d'ajouter un paramètre (poids et offset ou autre) qui sera une variable mémoire pour la couche construite. \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hhk0kmPSgqva"
      },
      "source": [
        "# Classe d'attention simple\r\n",
        "# Applique les poids d'attention sur les vecteurs de la couche récurrente\r\n",
        "\r\n",
        "# Importe le Backend de Keras\r\n",
        "from keras import backend as K\r\n",
        "\r\n",
        "# Définit une nouvelle classe Couche_Attention\r\n",
        "# Héritée de la classe Layer de Keras\r\n",
        "\r\n",
        "class Couche_Attention(tf.keras.layers.Layer):\r\n",
        "  # Fonction d'initialisation de la classe d'attention\r\n",
        "  def __init__(self,dim_att):\r\n",
        "    self.dim_att = dim_att          # Dimension du vecteur d'attention\r\n",
        "    super().__init__()              # Appel du __init__() de la classe Layer\r\n",
        "  \r\n",
        "  def build(self,input_shape):\r\n",
        "    self.W = self.add_weight(shape=(self.dim_att,input_shape[2]),initializer=\"normal\",name=\"W\")\r\n",
        "    self.b = self.add_weight(shape=(self.dim_att,1),initializer=\"zeros\",name=\"b\")\r\n",
        "    self.u = self.add_weight(shape=(self.dim_att,1),initializer=\"normal\",name=\"u\")\r\n",
        "    super().build(input_shape)        # Appel de la méthode build()\r\n",
        "\r\n",
        "  # Définit la logique de la couche d'attention\r\n",
        "  # Arguments :   x : Tenseur d'entrée de dimension (None, nbr_v,dim)\r\n",
        "  def call(self,x):\r\n",
        "    # Calcul de la matrice XH contenant les\r\n",
        "    # représentations cachées des vecteurs\r\n",
        "    # issus de la couche GRU\r\n",
        "    x = tf.transpose(x,perm=[0,2,1])          # x = (None, dim,20)\r\n",
        "    Xh = K.dot(self.W,x)                      # Xh = (dim_att,None,20)\r\n",
        "    Xh = tf.transpose(Xh,perm=[1,0,2])        # Xh = (None, dim_att,20)\r\n",
        "    Xh = Xh + tf.expand_dims(self.b,axis=0)   # Xh = (None, dim_att,20) + (None, dim_att,1)\r\n",
        "    Xh = K.tanh(Xh)                           # Xh = (None, dim_att,20)\r\n",
        "\r\n",
        "    # Calcul des poids d'attention normalisés\r\n",
        "    Xh = tf.transpose(Xh,perm=[0,2,1])        # Xh = (None,20,dim_att)\r\n",
        "    a = K.dot(Xh,self.u)                      # a = (None,20,1)\r\n",
        "    a = tf.keras.activations.softmax(a,axis=1)\r\n",
        "\r\n",
        "    # Calcul du vecteur d'attention\r\n",
        "    xa = tf.multiply(Xh,a)                    # xa = (None,20,dim)\r\n",
        "    sortie = K.sum(xa,axis=1)                 # sortie = (None,40)\r\n",
        "    return sortie\r\n",
        "\r\n",
        "  def compute_output_shape(self,input_shape):\r\n",
        "    return(input_shape[0],input_shape[2])\r\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTqdYAsF_ici",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "909d6ebe-d7b3-4470-ee9f-ae72dc094340"
      },
      "source": [
        "dim_GRU = 40\r\n",
        "batch_size = 1\r\n",
        "\r\n",
        "# Fonction de la couche lambda d'entrée\r\n",
        "def Traitement_Entrees(x):\r\n",
        "  return tf.expand_dims(x,axis=-1)\r\n",
        "\r\n",
        "# Fonction dela couche lambda de sortie\r\n",
        "def Traitement_Sorties(x):\r\n",
        "  return(x*100.0)\r\n",
        "\r\n",
        "# Construction du modèle d'encodage d'une séquence unique de 20 valeurs\r\n",
        "# avec couche d'attention et vecteur contexte\r\n",
        "# Model_EncodeurSequence\r\n",
        "# Entrée : Une séquence de 20 valeurs\r\n",
        "# Sortie : Vecteur attention de la séquence\r\n",
        "\r\n",
        "Entrees_Sequence_Unique = tf.keras.layers.Input(shape=(taille_fenetre,), batch_size=1)\r\n",
        "Sorties_TraitementEntrees = tf.keras.layers.Lambda(Traitement_Entrees)(Entrees_Sequence_Unique)\r\n",
        "Sortie_GRU = tf.keras.layers.GRU(dim_GRU,return_sequences=True)(Sorties_TraitementEntrees)\r\n",
        "Attention_Valeur = Couche_Attention(dim_GRU)(Sortie_GRU)\r\n",
        "Model_EncodeurSequence_Unique = tf.keras.Model(Entrees_Sequence_Unique,Attention_Valeur)\r\n",
        "\r\n",
        "Model_EncodeurSequence_Unique.summary()\r\n",
        "\r\n",
        "# Construction du modèle \r\n",
        "# Entrée : Séquences extraites de la séries\r\n",
        "# Sorties : Valeurs prédites de chaque séquence\r\n",
        "\r\n",
        "Entrees_Sequences = tf.keras.layers.Input(shape=(Nbr_sequences,taille_fenetre), batch_size=3)\r\n",
        "Sequences_Encodees = tf.keras.layers.TimeDistributed(Model_EncodeurSequence_Unique)(Entrees_Sequences)\r\n",
        "Sorties_GRU_Sequences = tf.keras.layers.GRU(dim_GRU,return_sequences=True)(Sequences_Encodees)\r\n",
        "Attention_sequences = Couche_Attention(dim_GRU)(Sorties_GRU_Sequences)\r\n",
        "predictions = tf.keras.layers.Dense(1)(Attention_sequences)\r\n",
        "predictions = tf.keras.layers.Lambda(Traitement_Sorties)(predictions)\r\n",
        "\r\n",
        "Model_Hierarchique = tf.keras.Model(Entrees_Sequences,predictions)\r\n",
        "\r\n",
        "Model_Hierarchique.summary()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_44\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_59 (InputLayer)        [(1, 20)]                 0         \n",
            "_________________________________________________________________\n",
            "lambda_53 (Lambda)           (1, 20, 1)                0         \n",
            "_________________________________________________________________\n",
            "gru_47 (GRU)                 (1, 20, 40)               5160      \n",
            "_________________________________________________________________\n",
            "couche__attention_46 (Couche (1, 40)                   1680      \n",
            "=================================================================\n",
            "Total params: 6,840\n",
            "Trainable params: 6,840\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:Model was constructed with shape (1, 20) for input KerasTensor(type_spec=TensorSpec(shape=(1, 20), dtype=tf.float32, name='input_59'), name='input_59', description=\"created by layer 'input_59'\"), but it was called on an input with incompatible shape (30, 20).\n",
            "WARNING:tensorflow:Model was constructed with shape (1, 20) for input KerasTensor(type_spec=TensorSpec(shape=(1, 20), dtype=tf.float32, name='input_59'), name='input_59', description=\"created by layer 'input_59'\"), but it was called on an input with incompatible shape (30, 20).\n",
            "Model: \"model_45\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_60 (InputLayer)        [(3, 10, 20)]             0         \n",
            "_________________________________________________________________\n",
            "time_distributed_27 (TimeDis (3, 10, 40)               6840      \n",
            "_________________________________________________________________\n",
            "gru_48 (GRU)                 (3, 10, 40)               9840      \n",
            "_________________________________________________________________\n",
            "couche__attention_47 (Couche (3, 40)                   1680      \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (3, 1)                    41        \n",
            "_________________________________________________________________\n",
            "lambda_54 (Lambda)           (3, 1)                    0         \n",
            "=================================================================\n",
            "Total params: 18,401\n",
            "Trainable params: 18,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUM0-SSXGLIQ"
      },
      "source": [
        "**2. Optimisation du taux d'apprentissage**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jejCBhXVuNQ4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 869
        },
        "outputId": "55354272-b471-442f-b378-217397984571"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\r\n",
        "def RegulationTauxApprentissage(periode, taux):\r\n",
        "  return 1e-8*10**(periode/10)\r\n",
        "\r\n",
        "# Définition de l'optimiseur à utiliser\r\n",
        "optimiseur=tf.keras.optimizers.Adam()\r\n",
        "\r\n",
        "# Utilisation de la méthode ModelCheckPoint\r\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\r\n",
        "\r\n",
        "# Compile le modèle\r\n",
        "Model_Hierarchique.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur, metrics=\"mae\")\r\n",
        "\r\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\r\n",
        "historique = Model_Hierarchique.fit(dataset,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint])"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "WARNING:tensorflow:Model was constructed with shape (3, 10, 20) for input KerasTensor(type_spec=TensorSpec(shape=(3, 10, 20), dtype=tf.float32, name='input_60'), name='input_60', description=\"created by layer 'input_60'\"), but it was called on an input with incompatible shape (32, None).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-c056273cf05b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mhistorique\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel_Hierarchique\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLearningRateScheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRegulationTauxApprentissage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCheckPoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 726\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:754 train_step\n        y_pred = self(x, training=True)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:1012 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py:425 call\n        inputs, training=training, mask=mask)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py:560 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/input_spec.py:223 assert_input_compatibility\n        str(tuple(shape)))\n\n    ValueError: Input 0 of layer time_distributed_27 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (32, None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1_WMNlzu2B4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "dceff4a0-3027-4bde-962c-6730d80307e8"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \r\n",
        "taux = 1e-8*(10**(np.arange(100)/10))\r\n",
        "\r\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\r\n",
        "plt.figure(figsize=(10, 6))\r\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\r\n",
        "plt.axis([ taux[0], taux[99], 0, 100])\r\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, \"Evolution de l'erreur en fonction du taux d'apprentissage\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XdXh_b0GP_F"
      },
      "source": [
        "**3. Entrainement du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80pEtJ10wIfY"
      },
      "source": [
        "# Charge les meilleurs poids\r\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16ujUiELwR33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b43a2aaf-80a5-423c-b9e0-ebe128240149"
      },
      "source": [
        "# Définition de l'optimiseur à utiliser\r\n",
        "optimiseur=tf.keras.optimizers.Adam(lr=1e-3)\r\n",
        "\r\n",
        "\r\n",
        "# Utilisation de la méthode ModelCheckPoint\r\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\r\n",
        "\r\n",
        "# Compile le modèle\r\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur,metrics=\"mae\")\r\n",
        "\r\n",
        "# Entraine le modèle\r\n",
        "historique = model.fit(dataset,validation_data=dataset_Val, epochs=500,verbose=1, callbacks=[CheckPoint])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "45/45 [==============================] - 3s 22ms/step - loss: 40.7588 - mae: 41.2551 - val_loss: 11.7995 - val_mae: 12.2920\n",
            "\n",
            "Epoch 00001: loss improved from inf to 27.44150, saving model to poids.hdf5\n",
            "Epoch 2/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 7.8237 - mae: 8.3113 - val_loss: 7.5679 - val_mae: 8.0504\n",
            "\n",
            "Epoch 00002: loss improved from 27.44150 to 7.45122, saving model to poids.hdf5\n",
            "Epoch 3/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 7.0212 - mae: 7.5054 - val_loss: 6.5059 - val_mae: 6.9805\n",
            "\n",
            "Epoch 00003: loss improved from 7.45122 to 6.99158, saving model to poids.hdf5\n",
            "Epoch 4/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 6.8402 - mae: 7.3250 - val_loss: 7.3562 - val_mae: 7.8422\n",
            "\n",
            "Epoch 00004: loss improved from 6.99158 to 6.94526, saving model to poids.hdf5\n",
            "Epoch 5/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 7.3999 - mae: 7.8814 - val_loss: 6.5930 - val_mae: 7.0687\n",
            "\n",
            "Epoch 00005: loss did not improve from 6.94526\n",
            "Epoch 6/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 6.6553 - mae: 7.1332 - val_loss: 7.1435 - val_mae: 7.6262\n",
            "\n",
            "Epoch 00006: loss improved from 6.94526 to 6.56361, saving model to poids.hdf5\n",
            "Epoch 7/500\n",
            "45/45 [==============================] - 1s 11ms/step - loss: 6.6534 - mae: 7.1374 - val_loss: 6.7346 - val_mae: 7.2153\n",
            "\n",
            "Epoch 00007: loss improved from 6.56361 to 6.54430, saving model to poids.hdf5\n",
            "Epoch 8/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 6.4577 - mae: 6.9379 - val_loss: 6.4449 - val_mae: 6.9245\n",
            "\n",
            "Epoch 00008: loss improved from 6.54430 to 6.52002, saving model to poids.hdf5\n",
            "Epoch 9/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 6.0496 - mae: 6.5344 - val_loss: 6.4535 - val_mae: 6.9335\n",
            "\n",
            "Epoch 00009: loss did not improve from 6.52002\n",
            "Epoch 10/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 6.3694 - mae: 6.8502 - val_loss: 6.2358 - val_mae: 6.7206\n",
            "\n",
            "Epoch 00010: loss did not improve from 6.52002\n",
            "Epoch 11/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 6.0505 - mae: 6.5242 - val_loss: 7.6865 - val_mae: 8.1739\n",
            "\n",
            "Epoch 00011: loss improved from 6.52002 to 6.14937, saving model to poids.hdf5\n",
            "Epoch 12/500\n",
            "45/45 [==============================] - 1s 20ms/step - loss: 6.3376 - mae: 6.8176 - val_loss: 6.2652 - val_mae: 6.7414\n",
            "\n",
            "Epoch 00012: loss did not improve from 6.14937\n",
            "Epoch 13/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 6.3459 - mae: 6.8216 - val_loss: 6.4012 - val_mae: 6.8778\n",
            "\n",
            "Epoch 00013: loss did not improve from 6.14937\n",
            "Epoch 14/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 6.6729 - mae: 7.1571 - val_loss: 6.3747 - val_mae: 6.8538\n",
            "\n",
            "Epoch 00014: loss did not improve from 6.14937\n",
            "Epoch 15/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 5.9107 - mae: 6.3864 - val_loss: 6.6722 - val_mae: 7.1515\n",
            "\n",
            "Epoch 00015: loss improved from 6.14937 to 5.85624, saving model to poids.hdf5\n",
            "Epoch 16/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 6.1089 - mae: 6.5929 - val_loss: 7.7918 - val_mae: 8.2784\n",
            "\n",
            "Epoch 00016: loss did not improve from 5.85624\n",
            "Epoch 17/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 6.1115 - mae: 6.5903 - val_loss: 7.6148 - val_mae: 8.1026\n",
            "\n",
            "Epoch 00017: loss did not improve from 5.85624\n",
            "Epoch 18/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 6.9439 - mae: 7.4292 - val_loss: 5.8332 - val_mae: 6.3144\n",
            "\n",
            "Epoch 00018: loss did not improve from 5.85624\n",
            "Epoch 19/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 5.8845 - mae: 6.3621 - val_loss: 5.7483 - val_mae: 6.2294\n",
            "\n",
            "Epoch 00019: loss improved from 5.85624 to 5.70679, saving model to poids.hdf5\n",
            "Epoch 20/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 5.7211 - mae: 6.1998 - val_loss: 5.7695 - val_mae: 6.2458\n",
            "\n",
            "Epoch 00020: loss improved from 5.70679 to 5.68429, saving model to poids.hdf5\n",
            "Epoch 21/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 5.5271 - mae: 6.0091 - val_loss: 5.7097 - val_mae: 6.1942\n",
            "\n",
            "Epoch 00021: loss did not improve from 5.68429\n",
            "Epoch 22/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 5.4419 - mae: 5.9217 - val_loss: 7.3352 - val_mae: 7.8219\n",
            "\n",
            "Epoch 00022: loss did not improve from 5.68429\n",
            "Epoch 23/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 5.7167 - mae: 6.1983 - val_loss: 6.4264 - val_mae: 6.9084\n",
            "\n",
            "Epoch 00023: loss improved from 5.68429 to 5.66745, saving model to poids.hdf5\n",
            "Epoch 24/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 5.5113 - mae: 5.9939 - val_loss: 7.0036 - val_mae: 7.4881\n",
            "\n",
            "Epoch 00024: loss improved from 5.66745 to 5.62688, saving model to poids.hdf5\n",
            "Epoch 25/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 5.4294 - mae: 5.9117 - val_loss: 6.2854 - val_mae: 6.7638\n",
            "\n",
            "Epoch 00025: loss improved from 5.62688 to 5.41357, saving model to poids.hdf5\n",
            "Epoch 26/500\n",
            "45/45 [==============================] - 1s 11ms/step - loss: 5.6250 - mae: 6.1087 - val_loss: 5.2667 - val_mae: 5.7557\n",
            "\n",
            "Epoch 00026: loss did not improve from 5.41357\n",
            "Epoch 27/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 5.6544 - mae: 6.1383 - val_loss: 5.3595 - val_mae: 5.8434\n",
            "\n",
            "Epoch 00027: loss did not improve from 5.41357\n",
            "Epoch 28/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 5.6137 - mae: 6.0943 - val_loss: 5.5642 - val_mae: 6.0389\n",
            "\n",
            "Epoch 00028: loss did not improve from 5.41357\n",
            "Epoch 29/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 5.4414 - mae: 5.9215 - val_loss: 5.3692 - val_mae: 5.8445\n",
            "\n",
            "Epoch 00029: loss improved from 5.41357 to 5.18805, saving model to poids.hdf5\n",
            "Epoch 30/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 5.1774 - mae: 5.6588 - val_loss: 6.2938 - val_mae: 6.7777\n",
            "\n",
            "Epoch 00030: loss did not improve from 5.18805\n",
            "Epoch 31/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 5.6267 - mae: 6.1101 - val_loss: 6.8611 - val_mae: 7.3477\n",
            "\n",
            "Epoch 00031: loss did not improve from 5.18805\n",
            "Epoch 32/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 5.1519 - mae: 5.6280 - val_loss: 5.4314 - val_mae: 5.9091\n",
            "\n",
            "Epoch 00032: loss improved from 5.18805 to 5.12133, saving model to poids.hdf5\n",
            "Epoch 33/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 5.4503 - mae: 5.9311 - val_loss: 5.7957 - val_mae: 6.2755\n",
            "\n",
            "Epoch 00033: loss did not improve from 5.12133\n",
            "Epoch 34/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 5.6355 - mae: 6.1164 - val_loss: 5.4049 - val_mae: 5.8829\n",
            "\n",
            "Epoch 00034: loss did not improve from 5.12133\n",
            "Epoch 35/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 5.6497 - mae: 6.1300 - val_loss: 5.1380 - val_mae: 5.6196\n",
            "\n",
            "Epoch 00035: loss did not improve from 5.12133\n",
            "Epoch 36/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.7673 - mae: 5.2488 - val_loss: 4.7869 - val_mae: 5.2640\n",
            "\n",
            "Epoch 00036: loss improved from 5.12133 to 4.83799, saving model to poids.hdf5\n",
            "Epoch 37/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.6533 - mae: 5.1322 - val_loss: 4.5428 - val_mae: 5.0182\n",
            "\n",
            "Epoch 00037: loss improved from 4.83799 to 4.58954, saving model to poids.hdf5\n",
            "Epoch 38/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 5.0198 - mae: 5.4986 - val_loss: 5.3206 - val_mae: 5.7957\n",
            "\n",
            "Epoch 00038: loss did not improve from 4.58954\n",
            "Epoch 39/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.5480 - mae: 5.0247 - val_loss: 4.4950 - val_mae: 4.9773\n",
            "\n",
            "Epoch 00039: loss did not improve from 4.58954\n",
            "Epoch 40/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 5.1977 - mae: 5.6764 - val_loss: 7.1212 - val_mae: 7.6097\n",
            "\n",
            "Epoch 00040: loss did not improve from 4.58954\n",
            "Epoch 41/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.9480 - mae: 5.4286 - val_loss: 4.4737 - val_mae: 4.9523\n",
            "\n",
            "Epoch 00041: loss did not improve from 4.58954\n",
            "Epoch 42/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.8222 - mae: 5.3010 - val_loss: 4.4964 - val_mae: 4.9686\n",
            "\n",
            "Epoch 00042: loss did not improve from 4.58954\n",
            "Epoch 43/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.4398 - mae: 4.9227 - val_loss: 4.6395 - val_mae: 5.1171\n",
            "\n",
            "Epoch 00043: loss improved from 4.58954 to 4.52527, saving model to poids.hdf5\n",
            "Epoch 44/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.5270 - mae: 5.0038 - val_loss: 4.3619 - val_mae: 4.8389\n",
            "\n",
            "Epoch 00044: loss did not improve from 4.52527\n",
            "Epoch 45/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.4648 - mae: 4.9414 - val_loss: 5.1057 - val_mae: 5.5825\n",
            "\n",
            "Epoch 00045: loss did not improve from 4.52527\n",
            "Epoch 46/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 5.0626 - mae: 5.5384 - val_loss: 5.9984 - val_mae: 6.4860\n",
            "\n",
            "Epoch 00046: loss did not improve from 4.52527\n",
            "Epoch 47/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 5.1506 - mae: 5.6311 - val_loss: 4.2999 - val_mae: 4.7795\n",
            "\n",
            "Epoch 00047: loss did not improve from 4.52527\n",
            "Epoch 48/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.8186 - mae: 5.2988 - val_loss: 4.2159 - val_mae: 4.6933\n",
            "\n",
            "Epoch 00048: loss did not improve from 4.52527\n",
            "Epoch 49/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.3702 - mae: 4.8482 - val_loss: 4.3677 - val_mae: 4.8485\n",
            "\n",
            "Epoch 00049: loss improved from 4.52527 to 4.44399, saving model to poids.hdf5\n",
            "Epoch 50/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.3469 - mae: 4.8214 - val_loss: 4.3254 - val_mae: 4.7968\n",
            "\n",
            "Epoch 00050: loss improved from 4.44399 to 4.39453, saving model to poids.hdf5\n",
            "Epoch 51/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.4947 - mae: 4.9764 - val_loss: 4.3085 - val_mae: 4.7856\n",
            "\n",
            "Epoch 00051: loss did not improve from 4.39453\n",
            "Epoch 52/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.4071 - mae: 4.8875 - val_loss: 4.2304 - val_mae: 4.7048\n",
            "\n",
            "Epoch 00052: loss improved from 4.39453 to 4.34958, saving model to poids.hdf5\n",
            "Epoch 53/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.3280 - mae: 4.8049 - val_loss: 4.7979 - val_mae: 5.2699\n",
            "\n",
            "Epoch 00053: loss did not improve from 4.34958\n",
            "Epoch 54/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.9732 - mae: 5.4509 - val_loss: 4.4540 - val_mae: 4.9266\n",
            "\n",
            "Epoch 00054: loss did not improve from 4.34958\n",
            "Epoch 55/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.6654 - mae: 5.1472 - val_loss: 4.4740 - val_mae: 4.9534\n",
            "\n",
            "Epoch 00055: loss did not improve from 4.34958\n",
            "Epoch 56/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.4102 - mae: 4.8880 - val_loss: 4.1053 - val_mae: 4.5837\n",
            "\n",
            "Epoch 00056: loss improved from 4.34958 to 4.34629, saving model to poids.hdf5\n",
            "Epoch 57/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.5978 - mae: 5.0755 - val_loss: 4.0794 - val_mae: 4.5525\n",
            "\n",
            "Epoch 00057: loss did not improve from 4.34629\n",
            "Epoch 58/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.2845 - mae: 4.7654 - val_loss: 5.7243 - val_mae: 6.2104\n",
            "\n",
            "Epoch 00058: loss did not improve from 4.34629\n",
            "Epoch 59/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.5889 - mae: 5.0666 - val_loss: 4.4207 - val_mae: 4.9024\n",
            "\n",
            "Epoch 00059: loss did not improve from 4.34629\n",
            "Epoch 60/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.6137 - mae: 5.0899 - val_loss: 4.0168 - val_mae: 4.4917\n",
            "\n",
            "Epoch 00060: loss did not improve from 4.34629\n",
            "Epoch 61/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.3285 - mae: 4.8093 - val_loss: 4.0785 - val_mae: 4.5556\n",
            "\n",
            "Epoch 00061: loss improved from 4.34629 to 4.29590, saving model to poids.hdf5\n",
            "Epoch 62/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.3989 - mae: 4.8722 - val_loss: 4.0599 - val_mae: 4.5390\n",
            "\n",
            "Epoch 00062: loss did not improve from 4.29590\n",
            "Epoch 63/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.5980 - mae: 5.0744 - val_loss: 3.9778 - val_mae: 4.4576\n",
            "\n",
            "Epoch 00063: loss did not improve from 4.29590\n",
            "Epoch 64/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.5461 - mae: 5.0260 - val_loss: 4.0719 - val_mae: 4.5497\n",
            "\n",
            "Epoch 00064: loss did not improve from 4.29590\n",
            "Epoch 65/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.3039 - mae: 4.7827 - val_loss: 4.2733 - val_mae: 4.7484\n",
            "\n",
            "Epoch 00065: loss did not improve from 4.29590\n",
            "Epoch 66/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.1234 - mae: 4.5951 - val_loss: 4.1894 - val_mae: 4.6685\n",
            "\n",
            "Epoch 00066: loss improved from 4.29590 to 4.12382, saving model to poids.hdf5\n",
            "Epoch 67/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.6763 - mae: 5.1528 - val_loss: 5.1710 - val_mae: 5.6529\n",
            "\n",
            "Epoch 00067: loss did not improve from 4.12382\n",
            "Epoch 68/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.8778 - mae: 5.3604 - val_loss: 4.1569 - val_mae: 4.6366\n",
            "\n",
            "Epoch 00068: loss did not improve from 4.12382\n",
            "Epoch 69/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.5584 - mae: 5.0381 - val_loss: 4.2178 - val_mae: 4.6968\n",
            "\n",
            "Epoch 00069: loss did not improve from 4.12382\n",
            "Epoch 70/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.6606 - mae: 5.1418 - val_loss: 4.1664 - val_mae: 4.6461\n",
            "\n",
            "Epoch 00070: loss did not improve from 4.12382\n",
            "Epoch 71/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.5820 - mae: 5.0623 - val_loss: 4.1215 - val_mae: 4.5976\n",
            "\n",
            "Epoch 00071: loss did not improve from 4.12382\n",
            "Epoch 72/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.5656 - mae: 5.0433 - val_loss: 4.4371 - val_mae: 4.9173\n",
            "\n",
            "Epoch 00072: loss did not improve from 4.12382\n",
            "Epoch 73/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.4421 - mae: 4.9173 - val_loss: 4.0068 - val_mae: 4.4778\n",
            "\n",
            "Epoch 00073: loss did not improve from 4.12382\n",
            "Epoch 74/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.6889 - mae: 5.1699 - val_loss: 4.3370 - val_mae: 4.8137\n",
            "\n",
            "Epoch 00074: loss did not improve from 4.12382\n",
            "Epoch 75/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.3087 - mae: 4.7884 - val_loss: 5.3201 - val_mae: 5.8040\n",
            "\n",
            "Epoch 00075: loss did not improve from 4.12382\n",
            "Epoch 76/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.6271 - mae: 5.1067 - val_loss: 4.0559 - val_mae: 4.5288\n",
            "\n",
            "Epoch 00076: loss did not improve from 4.12382\n",
            "Epoch 77/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.5398 - mae: 5.0219 - val_loss: 4.5612 - val_mae: 5.0380\n",
            "\n",
            "Epoch 00077: loss did not improve from 4.12382\n",
            "Epoch 78/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.4636 - mae: 4.9430 - val_loss: 4.1880 - val_mae: 4.6685\n",
            "\n",
            "Epoch 00078: loss did not improve from 4.12382\n",
            "Epoch 79/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.4164 - mae: 4.8984 - val_loss: 6.3607 - val_mae: 6.8544\n",
            "\n",
            "Epoch 00079: loss did not improve from 4.12382\n",
            "Epoch 80/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.6795 - mae: 5.1554 - val_loss: 4.1913 - val_mae: 4.6665\n",
            "\n",
            "Epoch 00080: loss did not improve from 4.12382\n",
            "Epoch 81/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.0733 - mae: 4.5565 - val_loss: 3.9814 - val_mae: 4.4615\n",
            "\n",
            "Epoch 00081: loss did not improve from 4.12382\n",
            "Epoch 82/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.2589 - mae: 4.7348 - val_loss: 4.5232 - val_mae: 4.9979\n",
            "\n",
            "Epoch 00082: loss improved from 4.12382 to 4.05429, saving model to poids.hdf5\n",
            "Epoch 83/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.1213 - mae: 4.5941 - val_loss: 5.2200 - val_mae: 5.7018\n",
            "\n",
            "Epoch 00083: loss did not improve from 4.05429\n",
            "Epoch 84/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.1268 - mae: 4.6036 - val_loss: 3.9683 - val_mae: 4.4461\n",
            "\n",
            "Epoch 00084: loss did not improve from 4.05429\n",
            "Epoch 85/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.1601 - mae: 4.6322 - val_loss: 4.6529 - val_mae: 5.1281\n",
            "\n",
            "Epoch 00085: loss did not improve from 4.05429\n",
            "Epoch 86/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 3.9832 - mae: 4.4569 - val_loss: 4.1547 - val_mae: 4.6390\n",
            "\n",
            "Epoch 00086: loss did not improve from 4.05429\n",
            "Epoch 87/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.3823 - mae: 4.8588 - val_loss: 3.9516 - val_mae: 4.4196\n",
            "\n",
            "Epoch 00087: loss did not improve from 4.05429\n",
            "Epoch 88/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.3138 - mae: 4.7868 - val_loss: 4.4445 - val_mae: 4.9206\n",
            "\n",
            "Epoch 00088: loss did not improve from 4.05429\n",
            "Epoch 89/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.0685 - mae: 4.5460 - val_loss: 3.9011 - val_mae: 4.3705\n",
            "\n",
            "Epoch 00089: loss did not improve from 4.05429\n",
            "Epoch 90/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.4398 - mae: 4.9180 - val_loss: 4.2102 - val_mae: 4.6862\n",
            "\n",
            "Epoch 00090: loss did not improve from 4.05429\n",
            "Epoch 91/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.2457 - mae: 4.7227 - val_loss: 3.8524 - val_mae: 4.3250\n",
            "\n",
            "Epoch 00091: loss did not improve from 4.05429\n",
            "Epoch 92/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.1764 - mae: 4.6485 - val_loss: 4.0744 - val_mae: 4.5544\n",
            "\n",
            "Epoch 00092: loss did not improve from 4.05429\n",
            "Epoch 93/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.4526 - mae: 4.9270 - val_loss: 3.9578 - val_mae: 4.4295\n",
            "\n",
            "Epoch 00093: loss did not improve from 4.05429\n",
            "Epoch 94/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.2534 - mae: 4.7280 - val_loss: 6.1300 - val_mae: 6.6149\n",
            "\n",
            "Epoch 00094: loss did not improve from 4.05429\n",
            "Epoch 95/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.4354 - mae: 4.9149 - val_loss: 5.0479 - val_mae: 5.5291\n",
            "\n",
            "Epoch 00095: loss did not improve from 4.05429\n",
            "Epoch 96/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.3865 - mae: 4.8618 - val_loss: 4.0554 - val_mae: 4.5349\n",
            "\n",
            "Epoch 00096: loss did not improve from 4.05429\n",
            "Epoch 97/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2717 - mae: 4.7543 - val_loss: 4.7198 - val_mae: 5.1980\n",
            "\n",
            "Epoch 00097: loss did not improve from 4.05429\n",
            "Epoch 98/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.1540 - mae: 4.6322 - val_loss: 4.3007 - val_mae: 4.7785\n",
            "\n",
            "Epoch 00098: loss did not improve from 4.05429\n",
            "Epoch 99/500\n",
            "45/45 [==============================] - 1s 11ms/step - loss: 4.3651 - mae: 4.8372 - val_loss: 4.2322 - val_mae: 4.7062\n",
            "\n",
            "Epoch 00099: loss did not improve from 4.05429\n",
            "Epoch 100/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.6140 - mae: 5.0952 - val_loss: 4.0389 - val_mae: 4.5150\n",
            "\n",
            "Epoch 00100: loss did not improve from 4.05429\n",
            "Epoch 101/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.3080 - mae: 4.7837 - val_loss: 4.9735 - val_mae: 5.4461\n",
            "\n",
            "Epoch 00101: loss did not improve from 4.05429\n",
            "Epoch 102/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.1518 - mae: 4.6289 - val_loss: 4.2791 - val_mae: 4.7550\n",
            "\n",
            "Epoch 00102: loss did not improve from 4.05429\n",
            "Epoch 103/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.1491 - mae: 4.6286 - val_loss: 4.1108 - val_mae: 4.5912\n",
            "\n",
            "Epoch 00103: loss did not improve from 4.05429\n",
            "Epoch 104/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.0904 - mae: 4.5671 - val_loss: 4.0319 - val_mae: 4.5125\n",
            "\n",
            "Epoch 00104: loss did not improve from 4.05429\n",
            "Epoch 105/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.1804 - mae: 4.6572 - val_loss: 4.5125 - val_mae: 4.9913\n",
            "\n",
            "Epoch 00105: loss did not improve from 4.05429\n",
            "Epoch 106/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.3886 - mae: 4.8665 - val_loss: 4.2325 - val_mae: 4.7147\n",
            "\n",
            "Epoch 00106: loss did not improve from 4.05429\n",
            "Epoch 107/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.5259 - mae: 5.0019 - val_loss: 3.8757 - val_mae: 4.3472\n",
            "\n",
            "Epoch 00107: loss did not improve from 4.05429\n",
            "Epoch 108/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 3.9335 - mae: 4.4118 - val_loss: 4.3362 - val_mae: 4.8122\n",
            "\n",
            "Epoch 00108: loss did not improve from 4.05429\n",
            "Epoch 109/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.3202 - mae: 4.7895 - val_loss: 3.9143 - val_mae: 4.3933\n",
            "\n",
            "Epoch 00109: loss did not improve from 4.05429\n",
            "Epoch 110/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.2663 - mae: 4.7372 - val_loss: 3.9280 - val_mae: 4.3983\n",
            "\n",
            "Epoch 00110: loss did not improve from 4.05429\n",
            "Epoch 111/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.0648 - mae: 4.5392 - val_loss: 6.7002 - val_mae: 7.1921\n",
            "\n",
            "Epoch 00111: loss did not improve from 4.05429\n",
            "Epoch 112/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 5.5435 - mae: 6.0251 - val_loss: 4.2780 - val_mae: 4.7540\n",
            "\n",
            "Epoch 00112: loss did not improve from 4.05429\n",
            "Epoch 113/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2178 - mae: 4.6915 - val_loss: 4.9915 - val_mae: 5.4768\n",
            "\n",
            "Epoch 00113: loss did not improve from 4.05429\n",
            "Epoch 114/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.1576 - mae: 4.6373 - val_loss: 4.8224 - val_mae: 5.3032\n",
            "\n",
            "Epoch 00114: loss did not improve from 4.05429\n",
            "Epoch 115/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.2323 - mae: 4.7047 - val_loss: 4.2527 - val_mae: 4.7303\n",
            "\n",
            "Epoch 00115: loss did not improve from 4.05429\n",
            "Epoch 116/500\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 4.7446 - mae: 5.2208 - val_loss: 5.0123 - val_mae: 5.4858\n",
            "\n",
            "Epoch 00116: loss did not improve from 4.05429\n",
            "Epoch 117/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2224 - mae: 4.6981 - val_loss: 3.8884 - val_mae: 4.3645\n",
            "\n",
            "Epoch 00117: loss did not improve from 4.05429\n",
            "Epoch 118/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.2964 - mae: 4.7749 - val_loss: 4.1176 - val_mae: 4.5959\n",
            "\n",
            "Epoch 00118: loss did not improve from 4.05429\n",
            "Epoch 119/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 3.9581 - mae: 4.4322 - val_loss: 4.1661 - val_mae: 4.6476\n",
            "\n",
            "Epoch 00119: loss did not improve from 4.05429\n",
            "Epoch 120/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.4446 - mae: 4.9130 - val_loss: 4.7767 - val_mae: 5.2498\n",
            "\n",
            "Epoch 00120: loss did not improve from 4.05429\n",
            "Epoch 121/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.4380 - mae: 4.9154 - val_loss: 3.7837 - val_mae: 4.2594\n",
            "\n",
            "Epoch 00121: loss did not improve from 4.05429\n",
            "Epoch 122/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.1177 - mae: 4.5957 - val_loss: 3.8348 - val_mae: 4.3108\n",
            "\n",
            "Epoch 00122: loss did not improve from 4.05429\n",
            "Epoch 123/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.6121 - mae: 5.0859 - val_loss: 3.8875 - val_mae: 4.3632\n",
            "\n",
            "Epoch 00123: loss did not improve from 4.05429\n",
            "Epoch 124/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 3.9436 - mae: 4.4178 - val_loss: 4.1450 - val_mae: 4.6222\n",
            "\n",
            "Epoch 00124: loss did not improve from 4.05429\n",
            "Epoch 125/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.1230 - mae: 4.5980 - val_loss: 4.9620 - val_mae: 5.4356\n",
            "\n",
            "Epoch 00125: loss did not improve from 4.05429\n",
            "Epoch 126/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 3.8846 - mae: 4.3586 - val_loss: 4.0201 - val_mae: 4.4981\n",
            "\n",
            "Epoch 00126: loss improved from 4.05429 to 4.01078, saving model to poids.hdf5\n",
            "Epoch 127/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.4274 - mae: 4.9047 - val_loss: 3.8031 - val_mae: 4.2818\n",
            "\n",
            "Epoch 00127: loss did not improve from 4.01078\n",
            "Epoch 128/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.1904 - mae: 4.6677 - val_loss: 4.0865 - val_mae: 4.5629\n",
            "\n",
            "Epoch 00128: loss did not improve from 4.01078\n",
            "Epoch 129/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.6338 - mae: 5.1176 - val_loss: 3.8816 - val_mae: 4.3583\n",
            "\n",
            "Epoch 00129: loss did not improve from 4.01078\n",
            "Epoch 130/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.0571 - mae: 4.5345 - val_loss: 4.2735 - val_mae: 4.7453\n",
            "\n",
            "Epoch 00130: loss did not improve from 4.01078\n",
            "Epoch 131/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.0797 - mae: 4.5526 - val_loss: 4.9698 - val_mae: 5.4434\n",
            "\n",
            "Epoch 00131: loss did not improve from 4.01078\n",
            "Epoch 132/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.0310 - mae: 4.5029 - val_loss: 3.8375 - val_mae: 4.3073\n",
            "\n",
            "Epoch 00132: loss did not improve from 4.01078\n",
            "Epoch 133/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2129 - mae: 4.6862 - val_loss: 3.9443 - val_mae: 4.4247\n",
            "\n",
            "Epoch 00133: loss did not improve from 4.01078\n",
            "Epoch 134/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2323 - mae: 4.7110 - val_loss: 5.2058 - val_mae: 5.6856\n",
            "\n",
            "Epoch 00134: loss did not improve from 4.01078\n",
            "Epoch 135/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.4754 - mae: 4.9543 - val_loss: 4.8866 - val_mae: 5.3631\n",
            "\n",
            "Epoch 00135: loss did not improve from 4.01078\n",
            "Epoch 136/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2030 - mae: 4.6806 - val_loss: 4.0627 - val_mae: 4.5449\n",
            "\n",
            "Epoch 00136: loss did not improve from 4.01078\n",
            "Epoch 137/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 3.9223 - mae: 4.3938 - val_loss: 3.8858 - val_mae: 4.3576\n",
            "\n",
            "Epoch 00137: loss improved from 4.01078 to 3.98951, saving model to poids.hdf5\n",
            "Epoch 138/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 3.9215 - mae: 4.3914 - val_loss: 4.2955 - val_mae: 4.7724\n",
            "\n",
            "Epoch 00138: loss did not improve from 3.98951\n",
            "Epoch 139/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.3177 - mae: 4.7977 - val_loss: 3.8606 - val_mae: 4.3301\n",
            "\n",
            "Epoch 00139: loss did not improve from 3.98951\n",
            "Epoch 140/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.1550 - mae: 4.6311 - val_loss: 3.8784 - val_mae: 4.3477\n",
            "\n",
            "Epoch 00140: loss did not improve from 3.98951\n",
            "Epoch 141/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.3290 - mae: 4.8055 - val_loss: 4.0230 - val_mae: 4.5042\n",
            "\n",
            "Epoch 00141: loss did not improve from 3.98951\n",
            "Epoch 142/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 3.9357 - mae: 4.4098 - val_loss: 4.5811 - val_mae: 5.0591\n",
            "\n",
            "Epoch 00142: loss did not improve from 3.98951\n",
            "Epoch 143/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2680 - mae: 4.7465 - val_loss: 4.0672 - val_mae: 4.5473\n",
            "\n",
            "Epoch 00143: loss did not improve from 3.98951\n",
            "Epoch 144/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.1374 - mae: 4.6107 - val_loss: 4.5034 - val_mae: 4.9762\n",
            "\n",
            "Epoch 00144: loss did not improve from 3.98951\n",
            "Epoch 145/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.3107 - mae: 4.7844 - val_loss: 5.0890 - val_mae: 5.5668\n",
            "\n",
            "Epoch 00145: loss did not improve from 3.98951\n",
            "Epoch 146/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2963 - mae: 4.7756 - val_loss: 4.2537 - val_mae: 4.7304\n",
            "\n",
            "Epoch 00146: loss did not improve from 3.98951\n",
            "Epoch 147/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.1482 - mae: 4.6215 - val_loss: 4.3698 - val_mae: 4.8459\n",
            "\n",
            "Epoch 00147: loss did not improve from 3.98951\n",
            "Epoch 148/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.2799 - mae: 4.7588 - val_loss: 4.0570 - val_mae: 4.5405\n",
            "\n",
            "Epoch 00148: loss did not improve from 3.98951\n",
            "Epoch 149/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.2004 - mae: 4.6757 - val_loss: 3.9236 - val_mae: 4.3934\n",
            "\n",
            "Epoch 00149: loss did not improve from 3.98951\n",
            "Epoch 150/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.4120 - mae: 4.8894 - val_loss: 4.0365 - val_mae: 4.5174\n",
            "\n",
            "Epoch 00150: loss did not improve from 3.98951\n",
            "Epoch 151/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2244 - mae: 4.7048 - val_loss: 4.5727 - val_mae: 5.0473\n",
            "\n",
            "Epoch 00151: loss did not improve from 3.98951\n",
            "Epoch 152/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2460 - mae: 4.7188 - val_loss: 4.0265 - val_mae: 4.5089\n",
            "\n",
            "Epoch 00152: loss did not improve from 3.98951\n",
            "Epoch 153/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.1753 - mae: 4.6555 - val_loss: 5.5044 - val_mae: 5.9820\n",
            "\n",
            "Epoch 00153: loss did not improve from 3.98951\n",
            "Epoch 154/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.4407 - mae: 4.9170 - val_loss: 4.0787 - val_mae: 4.5628\n",
            "\n",
            "Epoch 00154: loss did not improve from 3.98951\n",
            "Epoch 155/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.3337 - mae: 4.8114 - val_loss: 4.5188 - val_mae: 4.9943\n",
            "\n",
            "Epoch 00155: loss did not improve from 3.98951\n",
            "Epoch 156/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.4548 - mae: 4.9310 - val_loss: 4.0568 - val_mae: 4.5355\n",
            "\n",
            "Epoch 00156: loss did not improve from 3.98951\n",
            "Epoch 157/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.8628 - mae: 5.3440 - val_loss: 4.0180 - val_mae: 4.4953\n",
            "\n",
            "Epoch 00157: loss did not improve from 3.98951\n",
            "Epoch 158/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2160 - mae: 4.6892 - val_loss: 4.9934 - val_mae: 5.4682\n",
            "\n",
            "Epoch 00158: loss did not improve from 3.98951\n",
            "Epoch 159/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.3308 - mae: 4.8074 - val_loss: 5.5619 - val_mae: 6.0469\n",
            "\n",
            "Epoch 00159: loss did not improve from 3.98951\n",
            "Epoch 160/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.3623 - mae: 4.8378 - val_loss: 3.9102 - val_mae: 4.3806\n",
            "\n",
            "Epoch 00160: loss did not improve from 3.98951\n",
            "Epoch 161/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2025 - mae: 4.6753 - val_loss: 4.0180 - val_mae: 4.4934\n",
            "\n",
            "Epoch 00161: loss did not improve from 3.98951\n",
            "Epoch 162/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.4779 - mae: 4.9563 - val_loss: 4.5369 - val_mae: 5.0106\n",
            "\n",
            "Epoch 00162: loss did not improve from 3.98951\n",
            "Epoch 163/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2177 - mae: 4.6965 - val_loss: 3.9129 - val_mae: 4.3913\n",
            "\n",
            "Epoch 00163: loss did not improve from 3.98951\n",
            "Epoch 164/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.0319 - mae: 4.5089 - val_loss: 3.8996 - val_mae: 4.3761\n",
            "\n",
            "Epoch 00164: loss did not improve from 3.98951\n",
            "Epoch 165/500\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 4.2404 - mae: 4.7157 - val_loss: 4.9670 - val_mae: 5.4418\n",
            "\n",
            "Epoch 00165: loss did not improve from 3.98951\n",
            "Epoch 166/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2542 - mae: 4.7336 - val_loss: 3.8877 - val_mae: 4.3656\n",
            "\n",
            "Epoch 00166: loss did not improve from 3.98951\n",
            "Epoch 167/500\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 4.2114 - mae: 4.6925 - val_loss: 3.9220 - val_mae: 4.4005\n",
            "\n",
            "Epoch 00167: loss did not improve from 3.98951\n",
            "Epoch 168/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.3139 - mae: 4.7913 - val_loss: 3.8834 - val_mae: 4.3523\n",
            "\n",
            "Epoch 00168: loss did not improve from 3.98951\n",
            "Epoch 169/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2365 - mae: 4.7145 - val_loss: 3.8936 - val_mae: 4.3716\n",
            "\n",
            "Epoch 00169: loss did not improve from 3.98951\n",
            "Epoch 170/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2839 - mae: 4.7612 - val_loss: 4.0874 - val_mae: 4.5681\n",
            "\n",
            "Epoch 00170: loss did not improve from 3.98951\n",
            "Epoch 171/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.1127 - mae: 4.5870 - val_loss: 5.2376 - val_mae: 5.7118\n",
            "\n",
            "Epoch 00171: loss did not improve from 3.98951\n",
            "Epoch 172/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.6598 - mae: 5.1440 - val_loss: 4.5179 - val_mae: 4.9939\n",
            "\n",
            "Epoch 00172: loss did not improve from 3.98951\n",
            "Epoch 173/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.3352 - mae: 4.8109 - val_loss: 4.2580 - val_mae: 4.7395\n",
            "\n",
            "Epoch 00173: loss did not improve from 3.98951\n",
            "Epoch 174/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.0640 - mae: 4.5396 - val_loss: 3.8394 - val_mae: 4.3095\n",
            "\n",
            "Epoch 00174: loss did not improve from 3.98951\n",
            "Epoch 175/500\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 3.9118 - mae: 4.3907 - val_loss: 4.0896 - val_mae: 4.5699\n",
            "\n",
            "Epoch 00175: loss did not improve from 3.98951\n",
            "Epoch 176/500\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 4.1334 - mae: 4.6090 - val_loss: 4.2194 - val_mae: 4.6971\n",
            "\n",
            "Epoch 00176: loss did not improve from 3.98951\n",
            "Epoch 177/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 3.9078 - mae: 4.3741 - val_loss: 4.3333 - val_mae: 4.8112\n",
            "\n",
            "Epoch 00177: loss improved from 3.98951 to 3.96949, saving model to poids.hdf5\n",
            "Epoch 178/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2491 - mae: 4.7288 - val_loss: 3.9423 - val_mae: 4.4150\n",
            "\n",
            "Epoch 00178: loss did not improve from 3.96949\n",
            "Epoch 179/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.0725 - mae: 4.5498 - val_loss: 4.5438 - val_mae: 5.0189\n",
            "\n",
            "Epoch 00179: loss did not improve from 3.96949\n",
            "Epoch 180/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.1138 - mae: 4.5894 - val_loss: 4.2614 - val_mae: 4.7420\n",
            "\n",
            "Epoch 00180: loss improved from 3.96949 to 3.96713, saving model to poids.hdf5\n",
            "Epoch 181/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 3.9157 - mae: 4.3953 - val_loss: 3.8857 - val_mae: 4.3625\n",
            "\n",
            "Epoch 00181: loss improved from 3.96713 to 3.96341, saving model to poids.hdf5\n",
            "Epoch 182/500\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 4.1279 - mae: 4.6012 - val_loss: 4.0566 - val_mae: 4.5390\n",
            "\n",
            "Epoch 00182: loss did not improve from 3.96341\n",
            "Epoch 183/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.0946 - mae: 4.5738 - val_loss: 3.9238 - val_mae: 4.4008\n",
            "\n",
            "Epoch 00183: loss did not improve from 3.96341\n",
            "Epoch 184/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.0276 - mae: 4.4995 - val_loss: 3.8979 - val_mae: 4.3655\n",
            "\n",
            "Epoch 00184: loss did not improve from 3.96341\n",
            "Epoch 185/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.0158 - mae: 4.4912 - val_loss: 3.9100 - val_mae: 4.3832\n",
            "\n",
            "Epoch 00185: loss did not improve from 3.96341\n",
            "Epoch 186/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2006 - mae: 4.6761 - val_loss: 4.0366 - val_mae: 4.5132\n",
            "\n",
            "Epoch 00186: loss did not improve from 3.96341\n",
            "Epoch 187/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.0028 - mae: 4.4754 - val_loss: 3.8613 - val_mae: 4.3386\n",
            "\n",
            "Epoch 00187: loss did not improve from 3.96341\n",
            "Epoch 188/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.0802 - mae: 4.5566 - val_loss: 4.0648 - val_mae: 4.5410\n",
            "\n",
            "Epoch 00188: loss did not improve from 3.96341\n",
            "Epoch 189/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2930 - mae: 4.7703 - val_loss: 3.8317 - val_mae: 4.3054\n",
            "\n",
            "Epoch 00189: loss did not improve from 3.96341\n",
            "Epoch 190/500\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 4.0946 - mae: 4.5683 - val_loss: 4.4806 - val_mae: 4.9569\n",
            "\n",
            "Epoch 00190: loss did not improve from 3.96341\n",
            "Epoch 191/500\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 4.1729 - mae: 4.6465 - val_loss: 4.0789 - val_mae: 4.5585\n",
            "\n",
            "Epoch 00191: loss did not improve from 3.96341\n",
            "Epoch 192/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.5722 - mae: 5.0523 - val_loss: 4.0027 - val_mae: 4.4800\n",
            "\n",
            "Epoch 00192: loss did not improve from 3.96341\n",
            "Epoch 193/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.0600 - mae: 4.5384 - val_loss: 4.1520 - val_mae: 4.6302\n",
            "\n",
            "Epoch 00193: loss did not improve from 3.96341\n",
            "Epoch 194/500\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 4.3039 - mae: 4.7814 - val_loss: 4.5070 - val_mae: 4.9811\n",
            "\n",
            "Epoch 00194: loss did not improve from 3.96341\n",
            "Epoch 195/500\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 4.0757 - mae: 4.5490 - val_loss: 4.0317 - val_mae: 4.5094\n",
            "\n",
            "Epoch 00195: loss did not improve from 3.96341\n",
            "Epoch 196/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 3.7920 - mae: 4.2610 - val_loss: 3.9179 - val_mae: 4.3991\n",
            "\n",
            "Epoch 00196: loss improved from 3.96341 to 3.93000, saving model to poids.hdf5\n",
            "Epoch 197/500\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 3.9928 - mae: 4.4677 - val_loss: 3.9129 - val_mae: 4.3889\n",
            "\n",
            "Epoch 00197: loss did not improve from 3.93000\n",
            "Epoch 198/500\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 4.0098 - mae: 4.4870 - val_loss: 4.6932 - val_mae: 5.1725\n",
            "\n",
            "Epoch 00198: loss did not improve from 3.93000\n",
            "Epoch 199/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.1811 - mae: 4.6542 - val_loss: 3.9050 - val_mae: 4.3860\n",
            "\n",
            "Epoch 00199: loss did not improve from 3.93000\n",
            "Epoch 200/500\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 4.2992 - mae: 4.7740 - val_loss: 4.1846 - val_mae: 4.6671\n",
            "\n",
            "Epoch 00200: loss did not improve from 3.93000\n",
            "Epoch 201/500\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 4.3178 - mae: 4.7949 - val_loss: 4.1164 - val_mae: 4.5967\n",
            "\n",
            "Epoch 00201: loss did not improve from 3.93000\n",
            "Epoch 202/500\n",
            "44/45 [============================>.] - ETA: 0s - loss: 4.1262 - mae: 4.6009"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COP9u4yitvmw"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\r\n",
        "erreur_validation = historique.history[\"val_loss\"]\r\n",
        "\r\n",
        "# Affiche l'erreur en fonction de la période\r\n",
        "plt.figure(figsize=(10, 6))\r\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\r\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\r\n",
        "plt.legend()\r\n",
        "\r\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o2zh6N9t1b7"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\r\n",
        "erreur_validation = historique.history[\"val_loss\"]\r\n",
        "\r\n",
        "# Affiche l'erreur en fonction de la période\r\n",
        "plt.figure(figsize=(10, 6))\r\n",
        "plt.plot(np.arange(0,len(erreur_entrainement[400:500])),erreur_entrainement[400:500], label=\"Erreurs sur les entrainements\")\r\n",
        "plt.plot(np.arange(0,len(erreur_entrainement[400:500])),erreur_validation[400:500], label =\"Erreurs sur les validations\")\r\n",
        "plt.legend()\r\n",
        "\r\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6Gq2CkeGR_1"
      },
      "source": [
        "**4. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcGnqie_GVNE"
      },
      "source": [
        "# Charge les meilleurs poids\r\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRxXtHRXGXfF"
      },
      "source": [
        "taille_fenetre = 20\r\n",
        "\r\n",
        "# Création d'une liste vide pour recevoir les prédictions\r\n",
        "predictions = []\r\n",
        "\r\n",
        "# Calcul des prédiction pour chaque groupe de 20 valeurs consécutives de la série\r\n",
        "# dans l'intervalle de validation\r\n",
        "for t in temps[temps_separation:-taille_fenetre]:\r\n",
        "    X = np.reshape(serie[t:t+taille_fenetre],(1,taille_fenetre))\r\n",
        "    predictions.append(model.predict(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd2nfDcgG8EO"
      },
      "source": [
        "# Affiche la série et les prédictions\r\n",
        "plt.figure(figsize=(10, 6))\r\n",
        "affiche_serie(temps,serie,label=\"Série temporelle\")\r\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions)[:,0,0],label=\"Prédictions\")\r\n",
        "plt.title('Prédictions avec le modèle de régression linéaire')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# Zoom sur l'intervalle de validation\r\n",
        "plt.figure(figsize=(10, 6))\r\n",
        "affiche_serie(temps[temps_separation:],serie[temps_separation:],label=\"Série temporelle\")\r\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions)[:,0,0],label=\"Prédictions\")\r\n",
        "plt.title(\"Prédictions avec le modèle de régression linéaire (zoom sur l'intervalle de validation)\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDS7BJvZG_e1"
      },
      "source": [
        "# Calcule de l'erreur quadratique moyenne et de l'erreur absolue moyenne \r\n",
        "\r\n",
        "mae = tf.keras.metrics.mean_absolute_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions)[:,0,0]).numpy()\r\n",
        "mse = tf.keras.metrics.mean_squared_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions)[:,0,0]).numpy()\r\n",
        "\r\n",
        "print(mae)\r\n",
        "print(mse)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}