{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reseau_GRU_Avec_Attention_VecteurContexte.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM3ryAdVKqr6UiaNLfC9Zko",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/Reseau_GRU_Avec_Attention_Hierarchique.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubCeIvtF6R4W"
      },
      "source": [
        "Dans ce carnet nous allons mettre en place un modèle à réseau de neurones récurrent de type GRU associé à une **couche d'attention** comprenant un **vecteur contexte** pour réaliser des prédictions sur notre série temporelle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRhtHsNn5fc3"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFeah3y_6kif"
      },
      "source": [
        "# Création de la série temporelle et du dataset pour l'entrainement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJfSLtub6sdc"
      },
      "source": [
        "# Fonction permettant d'afficher une série temporelle\r\n",
        "def affiche_serie(temps, serie, format=\"-\", debut=0, fin=None, label=None):\r\n",
        "    plt.plot(temps[debut:fin], serie[debut:fin], format, label=label)\r\n",
        "    plt.xlabel(\"Temps\")\r\n",
        "    plt.ylabel(\"Valeur\")\r\n",
        "    if label:\r\n",
        "        plt.legend(fontsize=14)\r\n",
        "    plt.grid(True)\r\n",
        "\r\n",
        "# Fonction permettant de créer une tendance\r\n",
        "def tendance(temps, pente=0):\r\n",
        "    return pente * temps\r\n",
        "\r\n",
        "# Fonction permettant de créer un motif\r\n",
        "def motif_periodique(instants):\r\n",
        "    return (np.where(instants < 0.4,                            # Si les instants sont < 0.4\r\n",
        "                    np.cos(instants * 2 * np.pi),               # Alors on retourne la fonction cos(2*pi*t)\r\n",
        "                    1 / np.exp(3 * instants)))                  # Sinon, on retourne la fonction exp(-3t)\r\n",
        "\r\n",
        "# Fonction permettant de créer une saisonnalité avec un motif\r\n",
        "def saisonnalite(temps, periode, amplitude=1, phase=0):\r\n",
        "    \"\"\"Répétition du motif sur la même période\"\"\"\r\n",
        "    instants = ((temps + phase) % periode) / periode            # Mapping du temps =[0 1 2 ... 1460] => instants = [0.0 ... 1.0]\r\n",
        "    return amplitude * motif_periodique(instants)\r\n",
        "\r\n",
        "# Fonction permettant de générer du bruit gaussien N(0,1)\r\n",
        "def bruit_blanc(temps, niveau_bruit=1, graine=None):\r\n",
        "    rnd = np.random.RandomState(graine)\r\n",
        "    return rnd.randn(len(temps)) * niveau_bruit\r\n",
        "\r\n",
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\r\n",
        "# au format X(X1,X2,...Xn) / Y(Y1,Y2,...,Yn)\r\n",
        "# X sont les données d'entrées du réseau\r\n",
        "# Y sont les labels\r\n",
        "\r\n",
        "def prepare_dataset_XY(serie, taille_fenetre, batch_size, buffer_melange):\r\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie)\r\n",
        "  dataset = dataset.window(taille_fenetre+1, shift=1, drop_remainder=True)\r\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre + 1))\r\n",
        "  dataset = dataset.shuffle(buffer_melange).map(lambda x: (x[:-1], x[-1:]))\r\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\r\n",
        "  return dataset\r\n",
        "\r\n",
        "\r\n",
        "# Création de la série temporelle\r\n",
        "temps = np.arange(4 * 365)                # temps = [0 1 2 .... 4*365] = [0 1 2 .... 1460]\r\n",
        "amplitude = 40                            # Amplitude de la la saisonnalité\r\n",
        "niveau_bruit = 5                          # Niveau du bruit\r\n",
        "offset = 10                               # Offset de la série\r\n",
        "\r\n",
        "serie = offset + tendance(temps, 0.1) + saisonnalite(temps, periode=365, amplitude=amplitude) + bruit_blanc(temps,niveau_bruit)\r\n",
        "\r\n",
        "temps_separation = 1000\r\n",
        "\r\n",
        "# Extraction des temps et des données d'entrainement\r\n",
        "temps_entrainement = temps[:temps_separation]\r\n",
        "x_entrainement = serie[:temps_separation]\r\n",
        "\r\n",
        "# Exctraction des temps et des données de valiadation\r\n",
        "temps_validation = temps[temps_separation:]\r\n",
        "x_validation = serie[temps_separation:]\r\n",
        "\r\n",
        "# Définition des caractéristiques du dataset que l'on souhaite créer\r\n",
        "taille_fenetre = 20\r\n",
        "batch_size = 32\r\n",
        "buffer_melange = 1000\r\n",
        "\r\n",
        "# Création du dataset X,Y\r\n",
        "dataset = prepare_dataset_XY(serie,taille_fenetre,batch_size,buffer_melange)\r\n",
        "\r\n",
        "# Création du dataset X,Y de validation\r\n",
        "dataset_Val = prepare_dataset_XY(x_validation,taille_fenetre,batch_size,buffer_melange)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Yt-EgZ3sgPY"
      },
      "source": [
        "# Création du modèle GRU avec couche d'attention possédant un vecteur de contexte"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyrcfKcgCsZ7"
      },
      "source": [
        "**1. Création du réseau et adaptation des formats d'entrée et de sortie**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeJyix8HK7Kt"
      },
      "source": [
        "Sous forme de shéma, notre réseau est donc le suivant :\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OZkfsmnBNHY"
      },
      "source": [
        "<img src=\"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/images/Attention_VecteurContexte1.png?raw=true\" width=\"1200\"> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kgTrJOQ5DUo"
      },
      "source": [
        "# Remise à zéro de tous les états générés par Keras\r\n",
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLNIAGDlBizT"
      },
      "source": [
        "On créé une classe dérivée de la classe [Layer](https://keras.io/api/layers/base_layer/#layer-class) de Keras. Les méthodes utilisées sont les suivantes :  \r\n",
        " - [build](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#build) : Permet de créer les variables utilisées par la couche (commes les poids et les offsets)\r\n",
        " - [call](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#call) : Permet d'implanter la logique de la couche"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3COaR59t5WzJ"
      },
      "source": [
        "<img src=\"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/images/Attention_VecteurContexte2.png?raw=true\" width=\"1200\"> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyoh5UpQXCqm"
      },
      "source": [
        "Parmi les nouvelles fonctions de Tensorflow et de Keras utilisées, on trouve :\r\n",
        "- [transpose](https://www.tensorflow.org/api_docs/python/tf/transpose) : Permet de transposer un tenseur et éventuellement de reconstituer l'ordre des axes avec l'argument `perm`\r\n",
        "- [add_weight](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#add_weight) : Méthode de la classe Layers de Keras, qui permet d'ajouter un paramètre (poids et offset ou autre) qui sera une variable mémoire pour la couche construite. \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hhk0kmPSgqva"
      },
      "source": [
        "# Classe d'attention simple\r\n",
        "# Applique les poids d'attention sur les vecteurs de la couche récurrente\r\n",
        "\r\n",
        "# Importe le Backend de Keras\r\n",
        "from keras import backend as K\r\n",
        "\r\n",
        "# Définit une nouvelle classe Couche_Attention\r\n",
        "# Héritée de la classe Layer de Keras\r\n",
        "\r\n",
        "class Couche_Attention(tf.keras.layers.Layer):\r\n",
        "  # Fonction d'initialisation de la classe d'attention\r\n",
        "  def __init__(self,dim_att):\r\n",
        "    self.dim_att = dim_att          # Dimension du vecteur d'attention\r\n",
        "    super().__init__()              # Appel du __init__() de la classe Layer\r\n",
        "  \r\n",
        "  def build(self,input_shape):\r\n",
        "    self.W = self.add_weight(shape=(self.dim_att,input_shape[2]),initializer=\"normal\",name=\"W\")\r\n",
        "    self.b = self.add_weight(shape=(self.dim_att,1),initializer=\"zeros\",name=\"b\")\r\n",
        "    self.u = self.add_weight(shape=(self.dim_att,1),initializer=\"normal\",name=\"u\")\r\n",
        "    super().build(input_shape)        # Appel de la méthode build()\r\n",
        "\r\n",
        "  # Définit la logique de la couche d'attention\r\n",
        "  # Arguments :   x : Tenseur d'entrée de dimension (None, nbr_v,dim)\r\n",
        "  def call(self,x):\r\n",
        "    # Calcul de la matrice XH contenant les\r\n",
        "    # représentations cachées des vecteurs\r\n",
        "    # issus de la couche GRU\r\n",
        "    x = tf.transpose(x,perm=[0,2,1])          # x = (None, dim,20)\r\n",
        "    Xh = K.dot(self.W,x)                      # Xh = (dim_att,None,20)\r\n",
        "    Xh = tf.transpose(Xh,perm=[1,0,2])        # Xh = (None, dim_att,20)\r\n",
        "    Xh = Xh + tf.expand_dims(self.b,axis=0)   # Xh = (None, dim_att,20) + (None, dim_att,1)\r\n",
        "    Xh = K.tanh(Xh)                           # Xh = (None, dim_att,20)\r\n",
        "\r\n",
        "    # Calcul des poids d'attention normalisés\r\n",
        "    Xh = tf.transpose(Xh,perm=[0,2,1])        # Xh = (None,20,dim_att)\r\n",
        "    a = K.dot(Xh,self.u)                      # a = (None,20,1)\r\n",
        "    a = tf.keras.activations.softmax(a,axis=1)\r\n",
        "\r\n",
        "    # Calcul du vecteur d'attention\r\n",
        "    xa = tf.multiply(Xh,a)                    # xa = (None,20,dim)\r\n",
        "    sortie = K.sum(xa,axis=1)                 # sortie = (None,40)\r\n",
        "    return sortie"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTqdYAsF_ici",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd25a4b8-92ab-4752-e713-d2acf86d2d7a"
      },
      "source": [
        "dim_GRU = 40\r\n",
        "\r\n",
        "# Fonction de la couche lambda d'entrée\r\n",
        "def Traitement_Entrees(x):\r\n",
        "  return tf.expand_dims(x,axis=-1)\r\n",
        "\r\n",
        "# Fonction dela couche lambda de sortie\r\n",
        "def Traitement_Sorties(x):\r\n",
        "  return(x*100.0)\r\n",
        "\r\n",
        "model = tf.keras.models.Sequential()\r\n",
        "model.add(tf.keras.Input(shape=(taille_fenetre,)))\r\n",
        "model.add(tf.keras.layers.Lambda(Traitement_Entrees))\r\n",
        "model.add(tf.keras.layers.GRU(dim_GRU,return_sequences=True))\r\n",
        "model.add(Couche_Attention(dim_GRU))\r\n",
        "model.add(tf.keras.layers.Dense(1))\r\n",
        "model.add(tf.keras.layers.Lambda(Traitement_Sorties))\r\n",
        "\r\n",
        "model.save_weights('model_initial.h5')\r\n",
        "\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda (Lambda)              (None, 20, 1)             0         \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, 20, 40)            5160      \n",
            "_________________________________________________________________\n",
            "couche__attention (Couche_At (None, 40)                1680      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 41        \n",
            "_________________________________________________________________\n",
            "lambda_1 (Lambda)            (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 6,881\n",
            "Trainable params: 6,881\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUM0-SSXGLIQ"
      },
      "source": [
        "**2. Optimisation du taux d'apprentissage**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jejCBhXVuNQ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b00c7b83-c56a-4e51-ab3d-8a23d6958c6a"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\r\n",
        "def RegulationTauxApprentissage(periode, taux):\r\n",
        "  return 1e-8*10**(periode/10)\r\n",
        "\r\n",
        "# Définition de l'optimiseur à utiliser\r\n",
        "optimiseur=tf.keras.optimizers.Adam()\r\n",
        "\r\n",
        "# Utilisation de la méthode ModelCheckPoint\r\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\r\n",
        "\r\n",
        "# Compile le modèle\r\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur, metrics=\"mae\")\r\n",
        "\r\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\r\n",
        "historique = model.fit(dataset,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "45/45 [==============================] - 2s 9ms/step - loss: 85.2918 - mae: 85.7917\n",
            "\n",
            "Epoch 00001: loss improved from inf to 91.99272, saving model to poids.hdf5\n",
            "Epoch 2/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 85.4956 - mae: 85.9953\n",
            "\n",
            "Epoch 00002: loss improved from 91.99272 to 91.98437, saving model to poids.hdf5\n",
            "Epoch 3/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 85.1618 - mae: 85.6617\n",
            "\n",
            "Epoch 00003: loss improved from 91.98437 to 91.97383, saving model to poids.hdf5\n",
            "Epoch 4/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 87.2952 - mae: 87.7946\n",
            "\n",
            "Epoch 00004: loss improved from 91.97383 to 91.96048, saving model to poids.hdf5\n",
            "Epoch 5/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 84.6971 - mae: 85.1971\n",
            "\n",
            "Epoch 00005: loss improved from 91.96048 to 91.94359, saving model to poids.hdf5\n",
            "Epoch 6/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 83.7884 - mae: 84.2875\n",
            "\n",
            "Epoch 00006: loss improved from 91.94359 to 91.92264, saving model to poids.hdf5\n",
            "Epoch 7/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 85.2588 - mae: 85.7587\n",
            "\n",
            "Epoch 00007: loss improved from 91.92264 to 91.89610, saving model to poids.hdf5\n",
            "Epoch 8/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 86.2449 - mae: 86.7447\n",
            "\n",
            "Epoch 00008: loss improved from 91.89610 to 91.86254, saving model to poids.hdf5\n",
            "Epoch 9/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 84.7086 - mae: 85.2072\n",
            "\n",
            "Epoch 00009: loss improved from 91.86254 to 91.82029, saving model to poids.hdf5\n",
            "Epoch 10/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 84.2664 - mae: 84.7663\n",
            "\n",
            "Epoch 00010: loss improved from 91.82029 to 91.76701, saving model to poids.hdf5\n",
            "Epoch 11/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 85.0381 - mae: 85.5370\n",
            "\n",
            "Epoch 00011: loss improved from 91.76701 to 91.70004, saving model to poids.hdf5\n",
            "Epoch 12/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 85.5517 - mae: 86.0516\n",
            "\n",
            "Epoch 00012: loss improved from 91.70004 to 91.61585, saving model to poids.hdf5\n",
            "Epoch 13/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 85.3652 - mae: 85.8650\n",
            "\n",
            "Epoch 00013: loss improved from 91.61585 to 91.50977, saving model to poids.hdf5\n",
            "Epoch 14/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 84.1545 - mae: 84.6543\n",
            "\n",
            "Epoch 00014: loss improved from 91.50977 to 91.37644, saving model to poids.hdf5\n",
            "Epoch 15/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 85.9042 - mae: 86.4037\n",
            "\n",
            "Epoch 00015: loss improved from 91.37644 to 91.20855, saving model to poids.hdf5\n",
            "Epoch 16/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 83.6826 - mae: 84.1821\n",
            "\n",
            "Epoch 00016: loss improved from 91.20855 to 90.99779, saving model to poids.hdf5\n",
            "Epoch 17/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 84.8534 - mae: 85.3530\n",
            "\n",
            "Epoch 00017: loss improved from 90.99779 to 90.73200, saving model to poids.hdf5\n",
            "Epoch 18/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 85.4248 - mae: 85.9248\n",
            "\n",
            "Epoch 00018: loss improved from 90.73200 to 90.39884, saving model to poids.hdf5\n",
            "Epoch 19/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 81.9329 - mae: 82.4327\n",
            "\n",
            "Epoch 00019: loss improved from 90.39884 to 89.97964, saving model to poids.hdf5\n",
            "Epoch 20/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 82.6292 - mae: 83.1288\n",
            "\n",
            "Epoch 00020: loss improved from 89.97964 to 89.45217, saving model to poids.hdf5\n",
            "Epoch 21/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 82.2479 - mae: 82.7468\n",
            "\n",
            "Epoch 00021: loss improved from 89.45217 to 88.79034, saving model to poids.hdf5\n",
            "Epoch 22/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 82.0392 - mae: 82.5385\n",
            "\n",
            "Epoch 00022: loss improved from 88.79034 to 87.96139, saving model to poids.hdf5\n",
            "Epoch 23/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 81.0463 - mae: 81.5460\n",
            "\n",
            "Epoch 00023: loss improved from 87.96139 to 86.92273, saving model to poids.hdf5\n",
            "Epoch 24/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 79.9703 - mae: 80.4695\n",
            "\n",
            "Epoch 00024: loss improved from 86.92273 to 85.62590, saving model to poids.hdf5\n",
            "Epoch 25/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 77.2760 - mae: 77.7759\n",
            "\n",
            "Epoch 00025: loss improved from 85.62590 to 84.00254, saving model to poids.hdf5\n",
            "Epoch 26/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 75.7593 - mae: 76.2593\n",
            "\n",
            "Epoch 00026: loss improved from 84.00254 to 81.97233, saving model to poids.hdf5\n",
            "Epoch 27/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 73.9964 - mae: 74.4961\n",
            "\n",
            "Epoch 00027: loss improved from 81.97233 to 79.44484, saving model to poids.hdf5\n",
            "Epoch 28/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 69.9800 - mae: 70.4795\n",
            "\n",
            "Epoch 00028: loss improved from 79.44484 to 76.34565, saving model to poids.hdf5\n",
            "Epoch 29/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 66.6162 - mae: 67.1154\n",
            "\n",
            "Epoch 00029: loss improved from 76.34565 to 72.58852, saving model to poids.hdf5\n",
            "Epoch 30/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 64.8127 - mae: 65.3083\n",
            "\n",
            "Epoch 00030: loss improved from 72.58852 to 68.20993, saving model to poids.hdf5\n",
            "Epoch 31/100\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 56.9651 - mae: 57.4629\n",
            "\n",
            "Epoch 00031: loss improved from 68.20993 to 63.80959, saving model to poids.hdf5\n",
            "Epoch 32/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 54.0541 - mae: 54.5500\n",
            "\n",
            "Epoch 00032: loss improved from 63.80959 to 59.53421, saving model to poids.hdf5\n",
            "Epoch 33/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 50.4275 - mae: 50.9265\n",
            "\n",
            "Epoch 00033: loss improved from 59.53421 to 55.42286, saving model to poids.hdf5\n",
            "Epoch 34/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 45.7399 - mae: 46.2396\n",
            "\n",
            "Epoch 00034: loss improved from 55.42286 to 51.02010, saving model to poids.hdf5\n",
            "Epoch 35/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 42.2917 - mae: 42.7917\n",
            "\n",
            "Epoch 00035: loss improved from 51.02010 to 45.89888, saving model to poids.hdf5\n",
            "Epoch 36/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 37.0221 - mae: 37.5195\n",
            "\n",
            "Epoch 00036: loss improved from 45.89888 to 39.99009, saving model to poids.hdf5\n",
            "Epoch 37/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 31.6042 - mae: 32.1002\n",
            "\n",
            "Epoch 00037: loss improved from 39.99009 to 34.54066, saving model to poids.hdf5\n",
            "Epoch 38/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 28.5593 - mae: 29.0573\n",
            "\n",
            "Epoch 00038: loss improved from 34.54066 to 30.25077, saving model to poids.hdf5\n",
            "Epoch 39/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 23.0393 - mae: 23.5363\n",
            "\n",
            "Epoch 00039: loss improved from 30.25077 to 24.86856, saving model to poids.hdf5\n",
            "Epoch 40/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 17.5095 - mae: 18.0059\n",
            "\n",
            "Epoch 00040: loss improved from 24.86856 to 19.13984, saving model to poids.hdf5\n",
            "Epoch 41/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 13.2421 - mae: 13.7329\n",
            "\n",
            "Epoch 00041: loss improved from 19.13984 to 14.06044, saving model to poids.hdf5\n",
            "Epoch 42/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 9.8728 - mae: 10.3608\n",
            "\n",
            "Epoch 00042: loss improved from 14.06044 to 10.37168, saving model to poids.hdf5\n",
            "Epoch 43/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 8.3338 - mae: 8.8218\n",
            "\n",
            "Epoch 00043: loss improved from 10.37168 to 8.18328, saving model to poids.hdf5\n",
            "Epoch 44/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 7.0765 - mae: 7.5516\n",
            "\n",
            "Epoch 00044: loss improved from 8.18328 to 7.13108, saving model to poids.hdf5\n",
            "Epoch 45/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 6.9225 - mae: 7.4012\n",
            "\n",
            "Epoch 00045: loss improved from 7.13108 to 6.74766, saving model to poids.hdf5\n",
            "Epoch 46/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 6.6232 - mae: 7.1038\n",
            "\n",
            "Epoch 00046: loss improved from 6.74766 to 6.70203, saving model to poids.hdf5\n",
            "Epoch 47/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 6.7154 - mae: 7.1974\n",
            "\n",
            "Epoch 00047: loss improved from 6.70203 to 6.63667, saving model to poids.hdf5\n",
            "Epoch 48/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 6.6243 - mae: 7.1043\n",
            "\n",
            "Epoch 00048: loss did not improve from 6.63667\n",
            "Epoch 49/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 6.6255 - mae: 7.1017\n",
            "\n",
            "Epoch 00049: loss improved from 6.63667 to 6.55749, saving model to poids.hdf5\n",
            "Epoch 50/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 6.4685 - mae: 6.9520\n",
            "\n",
            "Epoch 00050: loss improved from 6.55749 to 6.31587, saving model to poids.hdf5\n",
            "Epoch 51/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 6.7580 - mae: 7.2367\n",
            "\n",
            "Epoch 00051: loss did not improve from 6.31587\n",
            "Epoch 52/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 6.6785 - mae: 7.1646\n",
            "\n",
            "Epoch 00052: loss did not improve from 6.31587\n",
            "Epoch 53/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 6.8550 - mae: 7.3387\n",
            "\n",
            "Epoch 00053: loss did not improve from 6.31587\n",
            "Epoch 54/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 7.0575 - mae: 7.5425\n",
            "\n",
            "Epoch 00054: loss did not improve from 6.31587\n",
            "Epoch 55/100\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 7.9304 - mae: 8.4171\n",
            "\n",
            "Epoch 00055: loss did not improve from 6.31587\n",
            "Epoch 56/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 7.3621 - mae: 7.8441\n",
            "\n",
            "Epoch 00056: loss did not improve from 6.31587\n",
            "Epoch 57/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 11.7791 - mae: 12.2691\n",
            "\n",
            "Epoch 00057: loss did not improve from 6.31587\n",
            "Epoch 58/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 8.6410 - mae: 9.1294\n",
            "\n",
            "Epoch 00058: loss did not improve from 6.31587\n",
            "Epoch 59/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 7.7806 - mae: 8.2657\n",
            "\n",
            "Epoch 00059: loss did not improve from 6.31587\n",
            "Epoch 60/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 10.5220 - mae: 11.0136\n",
            "\n",
            "Epoch 00060: loss did not improve from 6.31587\n",
            "Epoch 61/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 9.5521 - mae: 10.0404\n",
            "\n",
            "Epoch 00061: loss did not improve from 6.31587\n",
            "Epoch 62/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 8.1110 - mae: 8.5990\n",
            "\n",
            "Epoch 00062: loss did not improve from 6.31587\n",
            "Epoch 63/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 9.2184 - mae: 9.7092\n",
            "\n",
            "Epoch 00063: loss did not improve from 6.31587\n",
            "Epoch 64/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 13.6024 - mae: 14.0946\n",
            "\n",
            "Epoch 00064: loss did not improve from 6.31587\n",
            "Epoch 65/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 11.2999 - mae: 11.7898\n",
            "\n",
            "Epoch 00065: loss did not improve from 6.31587\n",
            "Epoch 66/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 10.5133 - mae: 11.0037\n",
            "\n",
            "Epoch 00066: loss did not improve from 6.31587\n",
            "Epoch 67/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 17.5500 - mae: 18.0468\n",
            "\n",
            "Epoch 00067: loss did not improve from 6.31587\n",
            "Epoch 68/100\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 13.3448 - mae: 13.8339\n",
            "\n",
            "Epoch 00068: loss did not improve from 6.31587\n",
            "Epoch 69/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 16.2407 - mae: 16.7347\n",
            "\n",
            "Epoch 00069: loss did not improve from 6.31587\n",
            "Epoch 70/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 14.8592 - mae: 15.3518\n",
            "\n",
            "Epoch 00070: loss did not improve from 6.31587\n",
            "Epoch 71/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 20.7323 - mae: 21.2282\n",
            "\n",
            "Epoch 00071: loss did not improve from 6.31587\n",
            "Epoch 72/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 22.3000 - mae: 22.7971\n",
            "\n",
            "Epoch 00072: loss did not improve from 6.31587\n",
            "Epoch 73/100\n",
            "45/45 [==============================] - 1s 9ms/step - loss: 47.4611 - mae: 47.9583\n",
            "\n",
            "Epoch 00073: loss did not improve from 6.31587\n",
            "Epoch 74/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 78.3584 - mae: 78.8570\n",
            "\n",
            "Epoch 00074: loss did not improve from 6.31587\n",
            "Epoch 75/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 324.7840 - mae: 325.2840\n",
            "\n",
            "Epoch 00075: loss did not improve from 6.31587\n",
            "Epoch 76/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 167.5329 - mae: 168.0326\n",
            "\n",
            "Epoch 00076: loss did not improve from 6.31587\n",
            "Epoch 77/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 308.5719 - mae: 309.0716\n",
            "\n",
            "Epoch 00077: loss did not improve from 6.31587\n",
            "Epoch 78/100\n",
            "45/45 [==============================] - 1s 9ms/step - loss: 584.6825 - mae: 585.1825\n",
            "\n",
            "Epoch 00078: loss did not improve from 6.31587\n",
            "Epoch 79/100\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 520.0794 - mae: 520.5794\n",
            "\n",
            "Epoch 00079: loss did not improve from 6.31587\n",
            "Epoch 80/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 476.1846 - mae: 476.6846\n",
            "\n",
            "Epoch 00080: loss did not improve from 6.31587\n",
            "Epoch 81/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 732.9598 - mae: 733.4597\n",
            "\n",
            "Epoch 00081: loss did not improve from 6.31587\n",
            "Epoch 82/100\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 450.7467 - mae: 451.2467\n",
            "\n",
            "Epoch 00082: loss did not improve from 6.31587\n",
            "Epoch 83/100\n",
            "45/45 [==============================] - 1s 9ms/step - loss: 443.2090 - mae: 443.7089\n",
            "\n",
            "Epoch 00083: loss did not improve from 6.31587\n",
            "Epoch 84/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 892.6758 - mae: 893.1758\n",
            "\n",
            "Epoch 00084: loss did not improve from 6.31587\n",
            "Epoch 85/100\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 1672.7794 - mae: 1673.2794\n",
            "\n",
            "Epoch 00085: loss did not improve from 6.31587\n",
            "Epoch 86/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 1863.7326 - mae: 1864.2324\n",
            "\n",
            "Epoch 00086: loss did not improve from 6.31587\n",
            "Epoch 87/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 3050.9702 - mae: 3051.4702\n",
            "\n",
            "Epoch 00087: loss did not improve from 6.31587\n",
            "Epoch 88/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 2287.7483 - mae: 2288.2483\n",
            "\n",
            "Epoch 00088: loss did not improve from 6.31587\n",
            "Epoch 89/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 3477.0055 - mae: 3477.5055\n",
            "\n",
            "Epoch 00089: loss did not improve from 6.31587\n",
            "Epoch 90/100\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 4755.7567 - mae: 4756.2567\n",
            "\n",
            "Epoch 00090: loss did not improve from 6.31587\n",
            "Epoch 91/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 6121.8878 - mae: 6122.3878\n",
            "\n",
            "Epoch 00091: loss did not improve from 6.31587\n",
            "Epoch 92/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 4396.3571 - mae: 4396.8571\n",
            "\n",
            "Epoch 00092: loss did not improve from 6.31587\n",
            "Epoch 93/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 9252.6279 - mae: 9253.1279\n",
            "\n",
            "Epoch 00093: loss did not improve from 6.31587\n",
            "Epoch 94/100\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 5393.3591 - mae: 5393.8591\n",
            "\n",
            "Epoch 00094: loss did not improve from 6.31587\n",
            "Epoch 95/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 9487.6470 - mae: 9488.1470\n",
            "\n",
            "Epoch 00095: loss did not improve from 6.31587\n",
            "Epoch 96/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 8898.8757 - mae: 8899.3757\n",
            "\n",
            "Epoch 00096: loss did not improve from 6.31587\n",
            "Epoch 97/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 18493.9243 - mae: 18494.4243\n",
            "\n",
            "Epoch 00097: loss did not improve from 6.31587\n",
            "Epoch 98/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 23291.0123 - mae: 23291.5123\n",
            "\n",
            "Epoch 00098: loss did not improve from 6.31587\n",
            "Epoch 99/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 32076.2789 - mae: 32076.7789\n",
            "\n",
            "Epoch 00099: loss did not improve from 6.31587\n",
            "Epoch 100/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 83457.9352 - mae: 83458.4352\n",
            "\n",
            "Epoch 00100: loss did not improve from 6.31587\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1_WMNlzu2B4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "dceff4a0-3027-4bde-962c-6730d80307e8"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \r\n",
        "taux = 1e-8*(10**(np.arange(100)/10))\r\n",
        "\r\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\r\n",
        "plt.figure(figsize=(10, 6))\r\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\r\n",
        "plt.axis([ taux[0], taux[99], 0, 100])\r\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, \"Evolution de l'erreur en fonction du taux d'apprentissage\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XdXh_b0GP_F"
      },
      "source": [
        "**3. Entrainement du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80pEtJ10wIfY"
      },
      "source": [
        "# Charge les meilleurs poids\r\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16ujUiELwR33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b43a2aaf-80a5-423c-b9e0-ebe128240149"
      },
      "source": [
        "# Définition de l'optimiseur à utiliser\r\n",
        "optimiseur=tf.keras.optimizers.Adam(lr=1e-3)\r\n",
        "\r\n",
        "\r\n",
        "# Utilisation de la méthode ModelCheckPoint\r\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\r\n",
        "\r\n",
        "# Compile le modèle\r\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur,metrics=\"mae\")\r\n",
        "\r\n",
        "# Entraine le modèle\r\n",
        "historique = model.fit(dataset,validation_data=dataset_Val, epochs=500,verbose=1, callbacks=[CheckPoint])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "45/45 [==============================] - 3s 22ms/step - loss: 40.7588 - mae: 41.2551 - val_loss: 11.7995 - val_mae: 12.2920\n",
            "\n",
            "Epoch 00001: loss improved from inf to 27.44150, saving model to poids.hdf5\n",
            "Epoch 2/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 7.8237 - mae: 8.3113 - val_loss: 7.5679 - val_mae: 8.0504\n",
            "\n",
            "Epoch 00002: loss improved from 27.44150 to 7.45122, saving model to poids.hdf5\n",
            "Epoch 3/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 7.0212 - mae: 7.5054 - val_loss: 6.5059 - val_mae: 6.9805\n",
            "\n",
            "Epoch 00003: loss improved from 7.45122 to 6.99158, saving model to poids.hdf5\n",
            "Epoch 4/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 6.8402 - mae: 7.3250 - val_loss: 7.3562 - val_mae: 7.8422\n",
            "\n",
            "Epoch 00004: loss improved from 6.99158 to 6.94526, saving model to poids.hdf5\n",
            "Epoch 5/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 7.3999 - mae: 7.8814 - val_loss: 6.5930 - val_mae: 7.0687\n",
            "\n",
            "Epoch 00005: loss did not improve from 6.94526\n",
            "Epoch 6/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 6.6553 - mae: 7.1332 - val_loss: 7.1435 - val_mae: 7.6262\n",
            "\n",
            "Epoch 00006: loss improved from 6.94526 to 6.56361, saving model to poids.hdf5\n",
            "Epoch 7/500\n",
            "45/45 [==============================] - 1s 11ms/step - loss: 6.6534 - mae: 7.1374 - val_loss: 6.7346 - val_mae: 7.2153\n",
            "\n",
            "Epoch 00007: loss improved from 6.56361 to 6.54430, saving model to poids.hdf5\n",
            "Epoch 8/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 6.4577 - mae: 6.9379 - val_loss: 6.4449 - val_mae: 6.9245\n",
            "\n",
            "Epoch 00008: loss improved from 6.54430 to 6.52002, saving model to poids.hdf5\n",
            "Epoch 9/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 6.0496 - mae: 6.5344 - val_loss: 6.4535 - val_mae: 6.9335\n",
            "\n",
            "Epoch 00009: loss did not improve from 6.52002\n",
            "Epoch 10/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 6.3694 - mae: 6.8502 - val_loss: 6.2358 - val_mae: 6.7206\n",
            "\n",
            "Epoch 00010: loss did not improve from 6.52002\n",
            "Epoch 11/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 6.0505 - mae: 6.5242 - val_loss: 7.6865 - val_mae: 8.1739\n",
            "\n",
            "Epoch 00011: loss improved from 6.52002 to 6.14937, saving model to poids.hdf5\n",
            "Epoch 12/500\n",
            "45/45 [==============================] - 1s 20ms/step - loss: 6.3376 - mae: 6.8176 - val_loss: 6.2652 - val_mae: 6.7414\n",
            "\n",
            "Epoch 00012: loss did not improve from 6.14937\n",
            "Epoch 13/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 6.3459 - mae: 6.8216 - val_loss: 6.4012 - val_mae: 6.8778\n",
            "\n",
            "Epoch 00013: loss did not improve from 6.14937\n",
            "Epoch 14/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 6.6729 - mae: 7.1571 - val_loss: 6.3747 - val_mae: 6.8538\n",
            "\n",
            "Epoch 00014: loss did not improve from 6.14937\n",
            "Epoch 15/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 5.9107 - mae: 6.3864 - val_loss: 6.6722 - val_mae: 7.1515\n",
            "\n",
            "Epoch 00015: loss improved from 6.14937 to 5.85624, saving model to poids.hdf5\n",
            "Epoch 16/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 6.1089 - mae: 6.5929 - val_loss: 7.7918 - val_mae: 8.2784\n",
            "\n",
            "Epoch 00016: loss did not improve from 5.85624\n",
            "Epoch 17/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 6.1115 - mae: 6.5903 - val_loss: 7.6148 - val_mae: 8.1026\n",
            "\n",
            "Epoch 00017: loss did not improve from 5.85624\n",
            "Epoch 18/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 6.9439 - mae: 7.4292 - val_loss: 5.8332 - val_mae: 6.3144\n",
            "\n",
            "Epoch 00018: loss did not improve from 5.85624\n",
            "Epoch 19/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 5.8845 - mae: 6.3621 - val_loss: 5.7483 - val_mae: 6.2294\n",
            "\n",
            "Epoch 00019: loss improved from 5.85624 to 5.70679, saving model to poids.hdf5\n",
            "Epoch 20/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 5.7211 - mae: 6.1998 - val_loss: 5.7695 - val_mae: 6.2458\n",
            "\n",
            "Epoch 00020: loss improved from 5.70679 to 5.68429, saving model to poids.hdf5\n",
            "Epoch 21/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 5.5271 - mae: 6.0091 - val_loss: 5.7097 - val_mae: 6.1942\n",
            "\n",
            "Epoch 00021: loss did not improve from 5.68429\n",
            "Epoch 22/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 5.4419 - mae: 5.9217 - val_loss: 7.3352 - val_mae: 7.8219\n",
            "\n",
            "Epoch 00022: loss did not improve from 5.68429\n",
            "Epoch 23/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 5.7167 - mae: 6.1983 - val_loss: 6.4264 - val_mae: 6.9084\n",
            "\n",
            "Epoch 00023: loss improved from 5.68429 to 5.66745, saving model to poids.hdf5\n",
            "Epoch 24/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 5.5113 - mae: 5.9939 - val_loss: 7.0036 - val_mae: 7.4881\n",
            "\n",
            "Epoch 00024: loss improved from 5.66745 to 5.62688, saving model to poids.hdf5\n",
            "Epoch 25/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 5.4294 - mae: 5.9117 - val_loss: 6.2854 - val_mae: 6.7638\n",
            "\n",
            "Epoch 00025: loss improved from 5.62688 to 5.41357, saving model to poids.hdf5\n",
            "Epoch 26/500\n",
            "45/45 [==============================] - 1s 11ms/step - loss: 5.6250 - mae: 6.1087 - val_loss: 5.2667 - val_mae: 5.7557\n",
            "\n",
            "Epoch 00026: loss did not improve from 5.41357\n",
            "Epoch 27/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 5.6544 - mae: 6.1383 - val_loss: 5.3595 - val_mae: 5.8434\n",
            "\n",
            "Epoch 00027: loss did not improve from 5.41357\n",
            "Epoch 28/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 5.6137 - mae: 6.0943 - val_loss: 5.5642 - val_mae: 6.0389\n",
            "\n",
            "Epoch 00028: loss did not improve from 5.41357\n",
            "Epoch 29/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 5.4414 - mae: 5.9215 - val_loss: 5.3692 - val_mae: 5.8445\n",
            "\n",
            "Epoch 00029: loss improved from 5.41357 to 5.18805, saving model to poids.hdf5\n",
            "Epoch 30/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 5.1774 - mae: 5.6588 - val_loss: 6.2938 - val_mae: 6.7777\n",
            "\n",
            "Epoch 00030: loss did not improve from 5.18805\n",
            "Epoch 31/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 5.6267 - mae: 6.1101 - val_loss: 6.8611 - val_mae: 7.3477\n",
            "\n",
            "Epoch 00031: loss did not improve from 5.18805\n",
            "Epoch 32/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 5.1519 - mae: 5.6280 - val_loss: 5.4314 - val_mae: 5.9091\n",
            "\n",
            "Epoch 00032: loss improved from 5.18805 to 5.12133, saving model to poids.hdf5\n",
            "Epoch 33/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 5.4503 - mae: 5.9311 - val_loss: 5.7957 - val_mae: 6.2755\n",
            "\n",
            "Epoch 00033: loss did not improve from 5.12133\n",
            "Epoch 34/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 5.6355 - mae: 6.1164 - val_loss: 5.4049 - val_mae: 5.8829\n",
            "\n",
            "Epoch 00034: loss did not improve from 5.12133\n",
            "Epoch 35/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 5.6497 - mae: 6.1300 - val_loss: 5.1380 - val_mae: 5.6196\n",
            "\n",
            "Epoch 00035: loss did not improve from 5.12133\n",
            "Epoch 36/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.7673 - mae: 5.2488 - val_loss: 4.7869 - val_mae: 5.2640\n",
            "\n",
            "Epoch 00036: loss improved from 5.12133 to 4.83799, saving model to poids.hdf5\n",
            "Epoch 37/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.6533 - mae: 5.1322 - val_loss: 4.5428 - val_mae: 5.0182\n",
            "\n",
            "Epoch 00037: loss improved from 4.83799 to 4.58954, saving model to poids.hdf5\n",
            "Epoch 38/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 5.0198 - mae: 5.4986 - val_loss: 5.3206 - val_mae: 5.7957\n",
            "\n",
            "Epoch 00038: loss did not improve from 4.58954\n",
            "Epoch 39/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.5480 - mae: 5.0247 - val_loss: 4.4950 - val_mae: 4.9773\n",
            "\n",
            "Epoch 00039: loss did not improve from 4.58954\n",
            "Epoch 40/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 5.1977 - mae: 5.6764 - val_loss: 7.1212 - val_mae: 7.6097\n",
            "\n",
            "Epoch 00040: loss did not improve from 4.58954\n",
            "Epoch 41/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.9480 - mae: 5.4286 - val_loss: 4.4737 - val_mae: 4.9523\n",
            "\n",
            "Epoch 00041: loss did not improve from 4.58954\n",
            "Epoch 42/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.8222 - mae: 5.3010 - val_loss: 4.4964 - val_mae: 4.9686\n",
            "\n",
            "Epoch 00042: loss did not improve from 4.58954\n",
            "Epoch 43/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.4398 - mae: 4.9227 - val_loss: 4.6395 - val_mae: 5.1171\n",
            "\n",
            "Epoch 00043: loss improved from 4.58954 to 4.52527, saving model to poids.hdf5\n",
            "Epoch 44/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.5270 - mae: 5.0038 - val_loss: 4.3619 - val_mae: 4.8389\n",
            "\n",
            "Epoch 00044: loss did not improve from 4.52527\n",
            "Epoch 45/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.4648 - mae: 4.9414 - val_loss: 5.1057 - val_mae: 5.5825\n",
            "\n",
            "Epoch 00045: loss did not improve from 4.52527\n",
            "Epoch 46/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 5.0626 - mae: 5.5384 - val_loss: 5.9984 - val_mae: 6.4860\n",
            "\n",
            "Epoch 00046: loss did not improve from 4.52527\n",
            "Epoch 47/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 5.1506 - mae: 5.6311 - val_loss: 4.2999 - val_mae: 4.7795\n",
            "\n",
            "Epoch 00047: loss did not improve from 4.52527\n",
            "Epoch 48/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.8186 - mae: 5.2988 - val_loss: 4.2159 - val_mae: 4.6933\n",
            "\n",
            "Epoch 00048: loss did not improve from 4.52527\n",
            "Epoch 49/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.3702 - mae: 4.8482 - val_loss: 4.3677 - val_mae: 4.8485\n",
            "\n",
            "Epoch 00049: loss improved from 4.52527 to 4.44399, saving model to poids.hdf5\n",
            "Epoch 50/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.3469 - mae: 4.8214 - val_loss: 4.3254 - val_mae: 4.7968\n",
            "\n",
            "Epoch 00050: loss improved from 4.44399 to 4.39453, saving model to poids.hdf5\n",
            "Epoch 51/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.4947 - mae: 4.9764 - val_loss: 4.3085 - val_mae: 4.7856\n",
            "\n",
            "Epoch 00051: loss did not improve from 4.39453\n",
            "Epoch 52/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.4071 - mae: 4.8875 - val_loss: 4.2304 - val_mae: 4.7048\n",
            "\n",
            "Epoch 00052: loss improved from 4.39453 to 4.34958, saving model to poids.hdf5\n",
            "Epoch 53/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.3280 - mae: 4.8049 - val_loss: 4.7979 - val_mae: 5.2699\n",
            "\n",
            "Epoch 00053: loss did not improve from 4.34958\n",
            "Epoch 54/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.9732 - mae: 5.4509 - val_loss: 4.4540 - val_mae: 4.9266\n",
            "\n",
            "Epoch 00054: loss did not improve from 4.34958\n",
            "Epoch 55/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.6654 - mae: 5.1472 - val_loss: 4.4740 - val_mae: 4.9534\n",
            "\n",
            "Epoch 00055: loss did not improve from 4.34958\n",
            "Epoch 56/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.4102 - mae: 4.8880 - val_loss: 4.1053 - val_mae: 4.5837\n",
            "\n",
            "Epoch 00056: loss improved from 4.34958 to 4.34629, saving model to poids.hdf5\n",
            "Epoch 57/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.5978 - mae: 5.0755 - val_loss: 4.0794 - val_mae: 4.5525\n",
            "\n",
            "Epoch 00057: loss did not improve from 4.34629\n",
            "Epoch 58/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.2845 - mae: 4.7654 - val_loss: 5.7243 - val_mae: 6.2104\n",
            "\n",
            "Epoch 00058: loss did not improve from 4.34629\n",
            "Epoch 59/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.5889 - mae: 5.0666 - val_loss: 4.4207 - val_mae: 4.9024\n",
            "\n",
            "Epoch 00059: loss did not improve from 4.34629\n",
            "Epoch 60/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.6137 - mae: 5.0899 - val_loss: 4.0168 - val_mae: 4.4917\n",
            "\n",
            "Epoch 00060: loss did not improve from 4.34629\n",
            "Epoch 61/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.3285 - mae: 4.8093 - val_loss: 4.0785 - val_mae: 4.5556\n",
            "\n",
            "Epoch 00061: loss improved from 4.34629 to 4.29590, saving model to poids.hdf5\n",
            "Epoch 62/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.3989 - mae: 4.8722 - val_loss: 4.0599 - val_mae: 4.5390\n",
            "\n",
            "Epoch 00062: loss did not improve from 4.29590\n",
            "Epoch 63/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.5980 - mae: 5.0744 - val_loss: 3.9778 - val_mae: 4.4576\n",
            "\n",
            "Epoch 00063: loss did not improve from 4.29590\n",
            "Epoch 64/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.5461 - mae: 5.0260 - val_loss: 4.0719 - val_mae: 4.5497\n",
            "\n",
            "Epoch 00064: loss did not improve from 4.29590\n",
            "Epoch 65/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.3039 - mae: 4.7827 - val_loss: 4.2733 - val_mae: 4.7484\n",
            "\n",
            "Epoch 00065: loss did not improve from 4.29590\n",
            "Epoch 66/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.1234 - mae: 4.5951 - val_loss: 4.1894 - val_mae: 4.6685\n",
            "\n",
            "Epoch 00066: loss improved from 4.29590 to 4.12382, saving model to poids.hdf5\n",
            "Epoch 67/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.6763 - mae: 5.1528 - val_loss: 5.1710 - val_mae: 5.6529\n",
            "\n",
            "Epoch 00067: loss did not improve from 4.12382\n",
            "Epoch 68/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.8778 - mae: 5.3604 - val_loss: 4.1569 - val_mae: 4.6366\n",
            "\n",
            "Epoch 00068: loss did not improve from 4.12382\n",
            "Epoch 69/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.5584 - mae: 5.0381 - val_loss: 4.2178 - val_mae: 4.6968\n",
            "\n",
            "Epoch 00069: loss did not improve from 4.12382\n",
            "Epoch 70/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.6606 - mae: 5.1418 - val_loss: 4.1664 - val_mae: 4.6461\n",
            "\n",
            "Epoch 00070: loss did not improve from 4.12382\n",
            "Epoch 71/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.5820 - mae: 5.0623 - val_loss: 4.1215 - val_mae: 4.5976\n",
            "\n",
            "Epoch 00071: loss did not improve from 4.12382\n",
            "Epoch 72/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.5656 - mae: 5.0433 - val_loss: 4.4371 - val_mae: 4.9173\n",
            "\n",
            "Epoch 00072: loss did not improve from 4.12382\n",
            "Epoch 73/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.4421 - mae: 4.9173 - val_loss: 4.0068 - val_mae: 4.4778\n",
            "\n",
            "Epoch 00073: loss did not improve from 4.12382\n",
            "Epoch 74/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.6889 - mae: 5.1699 - val_loss: 4.3370 - val_mae: 4.8137\n",
            "\n",
            "Epoch 00074: loss did not improve from 4.12382\n",
            "Epoch 75/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.3087 - mae: 4.7884 - val_loss: 5.3201 - val_mae: 5.8040\n",
            "\n",
            "Epoch 00075: loss did not improve from 4.12382\n",
            "Epoch 76/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.6271 - mae: 5.1067 - val_loss: 4.0559 - val_mae: 4.5288\n",
            "\n",
            "Epoch 00076: loss did not improve from 4.12382\n",
            "Epoch 77/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.5398 - mae: 5.0219 - val_loss: 4.5612 - val_mae: 5.0380\n",
            "\n",
            "Epoch 00077: loss did not improve from 4.12382\n",
            "Epoch 78/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.4636 - mae: 4.9430 - val_loss: 4.1880 - val_mae: 4.6685\n",
            "\n",
            "Epoch 00078: loss did not improve from 4.12382\n",
            "Epoch 79/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.4164 - mae: 4.8984 - val_loss: 6.3607 - val_mae: 6.8544\n",
            "\n",
            "Epoch 00079: loss did not improve from 4.12382\n",
            "Epoch 80/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.6795 - mae: 5.1554 - val_loss: 4.1913 - val_mae: 4.6665\n",
            "\n",
            "Epoch 00080: loss did not improve from 4.12382\n",
            "Epoch 81/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.0733 - mae: 4.5565 - val_loss: 3.9814 - val_mae: 4.4615\n",
            "\n",
            "Epoch 00081: loss did not improve from 4.12382\n",
            "Epoch 82/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.2589 - mae: 4.7348 - val_loss: 4.5232 - val_mae: 4.9979\n",
            "\n",
            "Epoch 00082: loss improved from 4.12382 to 4.05429, saving model to poids.hdf5\n",
            "Epoch 83/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.1213 - mae: 4.5941 - val_loss: 5.2200 - val_mae: 5.7018\n",
            "\n",
            "Epoch 00083: loss did not improve from 4.05429\n",
            "Epoch 84/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.1268 - mae: 4.6036 - val_loss: 3.9683 - val_mae: 4.4461\n",
            "\n",
            "Epoch 00084: loss did not improve from 4.05429\n",
            "Epoch 85/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.1601 - mae: 4.6322 - val_loss: 4.6529 - val_mae: 5.1281\n",
            "\n",
            "Epoch 00085: loss did not improve from 4.05429\n",
            "Epoch 86/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 3.9832 - mae: 4.4569 - val_loss: 4.1547 - val_mae: 4.6390\n",
            "\n",
            "Epoch 00086: loss did not improve from 4.05429\n",
            "Epoch 87/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.3823 - mae: 4.8588 - val_loss: 3.9516 - val_mae: 4.4196\n",
            "\n",
            "Epoch 00087: loss did not improve from 4.05429\n",
            "Epoch 88/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.3138 - mae: 4.7868 - val_loss: 4.4445 - val_mae: 4.9206\n",
            "\n",
            "Epoch 00088: loss did not improve from 4.05429\n",
            "Epoch 89/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.0685 - mae: 4.5460 - val_loss: 3.9011 - val_mae: 4.3705\n",
            "\n",
            "Epoch 00089: loss did not improve from 4.05429\n",
            "Epoch 90/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.4398 - mae: 4.9180 - val_loss: 4.2102 - val_mae: 4.6862\n",
            "\n",
            "Epoch 00090: loss did not improve from 4.05429\n",
            "Epoch 91/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.2457 - mae: 4.7227 - val_loss: 3.8524 - val_mae: 4.3250\n",
            "\n",
            "Epoch 00091: loss did not improve from 4.05429\n",
            "Epoch 92/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.1764 - mae: 4.6485 - val_loss: 4.0744 - val_mae: 4.5544\n",
            "\n",
            "Epoch 00092: loss did not improve from 4.05429\n",
            "Epoch 93/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.4526 - mae: 4.9270 - val_loss: 3.9578 - val_mae: 4.4295\n",
            "\n",
            "Epoch 00093: loss did not improve from 4.05429\n",
            "Epoch 94/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.2534 - mae: 4.7280 - val_loss: 6.1300 - val_mae: 6.6149\n",
            "\n",
            "Epoch 00094: loss did not improve from 4.05429\n",
            "Epoch 95/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.4354 - mae: 4.9149 - val_loss: 5.0479 - val_mae: 5.5291\n",
            "\n",
            "Epoch 00095: loss did not improve from 4.05429\n",
            "Epoch 96/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.3865 - mae: 4.8618 - val_loss: 4.0554 - val_mae: 4.5349\n",
            "\n",
            "Epoch 00096: loss did not improve from 4.05429\n",
            "Epoch 97/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2717 - mae: 4.7543 - val_loss: 4.7198 - val_mae: 5.1980\n",
            "\n",
            "Epoch 00097: loss did not improve from 4.05429\n",
            "Epoch 98/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.1540 - mae: 4.6322 - val_loss: 4.3007 - val_mae: 4.7785\n",
            "\n",
            "Epoch 00098: loss did not improve from 4.05429\n",
            "Epoch 99/500\n",
            "45/45 [==============================] - 1s 11ms/step - loss: 4.3651 - mae: 4.8372 - val_loss: 4.2322 - val_mae: 4.7062\n",
            "\n",
            "Epoch 00099: loss did not improve from 4.05429\n",
            "Epoch 100/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.6140 - mae: 5.0952 - val_loss: 4.0389 - val_mae: 4.5150\n",
            "\n",
            "Epoch 00100: loss did not improve from 4.05429\n",
            "Epoch 101/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.3080 - mae: 4.7837 - val_loss: 4.9735 - val_mae: 5.4461\n",
            "\n",
            "Epoch 00101: loss did not improve from 4.05429\n",
            "Epoch 102/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.1518 - mae: 4.6289 - val_loss: 4.2791 - val_mae: 4.7550\n",
            "\n",
            "Epoch 00102: loss did not improve from 4.05429\n",
            "Epoch 103/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.1491 - mae: 4.6286 - val_loss: 4.1108 - val_mae: 4.5912\n",
            "\n",
            "Epoch 00103: loss did not improve from 4.05429\n",
            "Epoch 104/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.0904 - mae: 4.5671 - val_loss: 4.0319 - val_mae: 4.5125\n",
            "\n",
            "Epoch 00104: loss did not improve from 4.05429\n",
            "Epoch 105/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.1804 - mae: 4.6572 - val_loss: 4.5125 - val_mae: 4.9913\n",
            "\n",
            "Epoch 00105: loss did not improve from 4.05429\n",
            "Epoch 106/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.3886 - mae: 4.8665 - val_loss: 4.2325 - val_mae: 4.7147\n",
            "\n",
            "Epoch 00106: loss did not improve from 4.05429\n",
            "Epoch 107/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.5259 - mae: 5.0019 - val_loss: 3.8757 - val_mae: 4.3472\n",
            "\n",
            "Epoch 00107: loss did not improve from 4.05429\n",
            "Epoch 108/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 3.9335 - mae: 4.4118 - val_loss: 4.3362 - val_mae: 4.8122\n",
            "\n",
            "Epoch 00108: loss did not improve from 4.05429\n",
            "Epoch 109/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.3202 - mae: 4.7895 - val_loss: 3.9143 - val_mae: 4.3933\n",
            "\n",
            "Epoch 00109: loss did not improve from 4.05429\n",
            "Epoch 110/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.2663 - mae: 4.7372 - val_loss: 3.9280 - val_mae: 4.3983\n",
            "\n",
            "Epoch 00110: loss did not improve from 4.05429\n",
            "Epoch 111/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.0648 - mae: 4.5392 - val_loss: 6.7002 - val_mae: 7.1921\n",
            "\n",
            "Epoch 00111: loss did not improve from 4.05429\n",
            "Epoch 112/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 5.5435 - mae: 6.0251 - val_loss: 4.2780 - val_mae: 4.7540\n",
            "\n",
            "Epoch 00112: loss did not improve from 4.05429\n",
            "Epoch 113/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2178 - mae: 4.6915 - val_loss: 4.9915 - val_mae: 5.4768\n",
            "\n",
            "Epoch 00113: loss did not improve from 4.05429\n",
            "Epoch 114/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.1576 - mae: 4.6373 - val_loss: 4.8224 - val_mae: 5.3032\n",
            "\n",
            "Epoch 00114: loss did not improve from 4.05429\n",
            "Epoch 115/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.2323 - mae: 4.7047 - val_loss: 4.2527 - val_mae: 4.7303\n",
            "\n",
            "Epoch 00115: loss did not improve from 4.05429\n",
            "Epoch 116/500\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 4.7446 - mae: 5.2208 - val_loss: 5.0123 - val_mae: 5.4858\n",
            "\n",
            "Epoch 00116: loss did not improve from 4.05429\n",
            "Epoch 117/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2224 - mae: 4.6981 - val_loss: 3.8884 - val_mae: 4.3645\n",
            "\n",
            "Epoch 00117: loss did not improve from 4.05429\n",
            "Epoch 118/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.2964 - mae: 4.7749 - val_loss: 4.1176 - val_mae: 4.5959\n",
            "\n",
            "Epoch 00118: loss did not improve from 4.05429\n",
            "Epoch 119/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 3.9581 - mae: 4.4322 - val_loss: 4.1661 - val_mae: 4.6476\n",
            "\n",
            "Epoch 00119: loss did not improve from 4.05429\n",
            "Epoch 120/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.4446 - mae: 4.9130 - val_loss: 4.7767 - val_mae: 5.2498\n",
            "\n",
            "Epoch 00120: loss did not improve from 4.05429\n",
            "Epoch 121/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.4380 - mae: 4.9154 - val_loss: 3.7837 - val_mae: 4.2594\n",
            "\n",
            "Epoch 00121: loss did not improve from 4.05429\n",
            "Epoch 122/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.1177 - mae: 4.5957 - val_loss: 3.8348 - val_mae: 4.3108\n",
            "\n",
            "Epoch 00122: loss did not improve from 4.05429\n",
            "Epoch 123/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.6121 - mae: 5.0859 - val_loss: 3.8875 - val_mae: 4.3632\n",
            "\n",
            "Epoch 00123: loss did not improve from 4.05429\n",
            "Epoch 124/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 3.9436 - mae: 4.4178 - val_loss: 4.1450 - val_mae: 4.6222\n",
            "\n",
            "Epoch 00124: loss did not improve from 4.05429\n",
            "Epoch 125/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.1230 - mae: 4.5980 - val_loss: 4.9620 - val_mae: 5.4356\n",
            "\n",
            "Epoch 00125: loss did not improve from 4.05429\n",
            "Epoch 126/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 3.8846 - mae: 4.3586 - val_loss: 4.0201 - val_mae: 4.4981\n",
            "\n",
            "Epoch 00126: loss improved from 4.05429 to 4.01078, saving model to poids.hdf5\n",
            "Epoch 127/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.4274 - mae: 4.9047 - val_loss: 3.8031 - val_mae: 4.2818\n",
            "\n",
            "Epoch 00127: loss did not improve from 4.01078\n",
            "Epoch 128/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.1904 - mae: 4.6677 - val_loss: 4.0865 - val_mae: 4.5629\n",
            "\n",
            "Epoch 00128: loss did not improve from 4.01078\n",
            "Epoch 129/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.6338 - mae: 5.1176 - val_loss: 3.8816 - val_mae: 4.3583\n",
            "\n",
            "Epoch 00129: loss did not improve from 4.01078\n",
            "Epoch 130/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.0571 - mae: 4.5345 - val_loss: 4.2735 - val_mae: 4.7453\n",
            "\n",
            "Epoch 00130: loss did not improve from 4.01078\n",
            "Epoch 131/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.0797 - mae: 4.5526 - val_loss: 4.9698 - val_mae: 5.4434\n",
            "\n",
            "Epoch 00131: loss did not improve from 4.01078\n",
            "Epoch 132/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.0310 - mae: 4.5029 - val_loss: 3.8375 - val_mae: 4.3073\n",
            "\n",
            "Epoch 00132: loss did not improve from 4.01078\n",
            "Epoch 133/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2129 - mae: 4.6862 - val_loss: 3.9443 - val_mae: 4.4247\n",
            "\n",
            "Epoch 00133: loss did not improve from 4.01078\n",
            "Epoch 134/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2323 - mae: 4.7110 - val_loss: 5.2058 - val_mae: 5.6856\n",
            "\n",
            "Epoch 00134: loss did not improve from 4.01078\n",
            "Epoch 135/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.4754 - mae: 4.9543 - val_loss: 4.8866 - val_mae: 5.3631\n",
            "\n",
            "Epoch 00135: loss did not improve from 4.01078\n",
            "Epoch 136/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2030 - mae: 4.6806 - val_loss: 4.0627 - val_mae: 4.5449\n",
            "\n",
            "Epoch 00136: loss did not improve from 4.01078\n",
            "Epoch 137/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 3.9223 - mae: 4.3938 - val_loss: 3.8858 - val_mae: 4.3576\n",
            "\n",
            "Epoch 00137: loss improved from 4.01078 to 3.98951, saving model to poids.hdf5\n",
            "Epoch 138/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 3.9215 - mae: 4.3914 - val_loss: 4.2955 - val_mae: 4.7724\n",
            "\n",
            "Epoch 00138: loss did not improve from 3.98951\n",
            "Epoch 139/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.3177 - mae: 4.7977 - val_loss: 3.8606 - val_mae: 4.3301\n",
            "\n",
            "Epoch 00139: loss did not improve from 3.98951\n",
            "Epoch 140/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.1550 - mae: 4.6311 - val_loss: 3.8784 - val_mae: 4.3477\n",
            "\n",
            "Epoch 00140: loss did not improve from 3.98951\n",
            "Epoch 141/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.3290 - mae: 4.8055 - val_loss: 4.0230 - val_mae: 4.5042\n",
            "\n",
            "Epoch 00141: loss did not improve from 3.98951\n",
            "Epoch 142/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 3.9357 - mae: 4.4098 - val_loss: 4.5811 - val_mae: 5.0591\n",
            "\n",
            "Epoch 00142: loss did not improve from 3.98951\n",
            "Epoch 143/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2680 - mae: 4.7465 - val_loss: 4.0672 - val_mae: 4.5473\n",
            "\n",
            "Epoch 00143: loss did not improve from 3.98951\n",
            "Epoch 144/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.1374 - mae: 4.6107 - val_loss: 4.5034 - val_mae: 4.9762\n",
            "\n",
            "Epoch 00144: loss did not improve from 3.98951\n",
            "Epoch 145/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.3107 - mae: 4.7844 - val_loss: 5.0890 - val_mae: 5.5668\n",
            "\n",
            "Epoch 00145: loss did not improve from 3.98951\n",
            "Epoch 146/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2963 - mae: 4.7756 - val_loss: 4.2537 - val_mae: 4.7304\n",
            "\n",
            "Epoch 00146: loss did not improve from 3.98951\n",
            "Epoch 147/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.1482 - mae: 4.6215 - val_loss: 4.3698 - val_mae: 4.8459\n",
            "\n",
            "Epoch 00147: loss did not improve from 3.98951\n",
            "Epoch 148/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.2799 - mae: 4.7588 - val_loss: 4.0570 - val_mae: 4.5405\n",
            "\n",
            "Epoch 00148: loss did not improve from 3.98951\n",
            "Epoch 149/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.2004 - mae: 4.6757 - val_loss: 3.9236 - val_mae: 4.3934\n",
            "\n",
            "Epoch 00149: loss did not improve from 3.98951\n",
            "Epoch 150/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.4120 - mae: 4.8894 - val_loss: 4.0365 - val_mae: 4.5174\n",
            "\n",
            "Epoch 00150: loss did not improve from 3.98951\n",
            "Epoch 151/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2244 - mae: 4.7048 - val_loss: 4.5727 - val_mae: 5.0473\n",
            "\n",
            "Epoch 00151: loss did not improve from 3.98951\n",
            "Epoch 152/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2460 - mae: 4.7188 - val_loss: 4.0265 - val_mae: 4.5089\n",
            "\n",
            "Epoch 00152: loss did not improve from 3.98951\n",
            "Epoch 153/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.1753 - mae: 4.6555 - val_loss: 5.5044 - val_mae: 5.9820\n",
            "\n",
            "Epoch 00153: loss did not improve from 3.98951\n",
            "Epoch 154/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.4407 - mae: 4.9170 - val_loss: 4.0787 - val_mae: 4.5628\n",
            "\n",
            "Epoch 00154: loss did not improve from 3.98951\n",
            "Epoch 155/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.3337 - mae: 4.8114 - val_loss: 4.5188 - val_mae: 4.9943\n",
            "\n",
            "Epoch 00155: loss did not improve from 3.98951\n",
            "Epoch 156/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.4548 - mae: 4.9310 - val_loss: 4.0568 - val_mae: 4.5355\n",
            "\n",
            "Epoch 00156: loss did not improve from 3.98951\n",
            "Epoch 157/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.8628 - mae: 5.3440 - val_loss: 4.0180 - val_mae: 4.4953\n",
            "\n",
            "Epoch 00157: loss did not improve from 3.98951\n",
            "Epoch 158/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2160 - mae: 4.6892 - val_loss: 4.9934 - val_mae: 5.4682\n",
            "\n",
            "Epoch 00158: loss did not improve from 3.98951\n",
            "Epoch 159/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.3308 - mae: 4.8074 - val_loss: 5.5619 - val_mae: 6.0469\n",
            "\n",
            "Epoch 00159: loss did not improve from 3.98951\n",
            "Epoch 160/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.3623 - mae: 4.8378 - val_loss: 3.9102 - val_mae: 4.3806\n",
            "\n",
            "Epoch 00160: loss did not improve from 3.98951\n",
            "Epoch 161/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2025 - mae: 4.6753 - val_loss: 4.0180 - val_mae: 4.4934\n",
            "\n",
            "Epoch 00161: loss did not improve from 3.98951\n",
            "Epoch 162/500\n",
            "45/45 [==============================] - 1s 12ms/step - loss: 4.4779 - mae: 4.9563 - val_loss: 4.5369 - val_mae: 5.0106\n",
            "\n",
            "Epoch 00162: loss did not improve from 3.98951\n",
            "Epoch 163/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2177 - mae: 4.6965 - val_loss: 3.9129 - val_mae: 4.3913\n",
            "\n",
            "Epoch 00163: loss did not improve from 3.98951\n",
            "Epoch 164/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.0319 - mae: 4.5089 - val_loss: 3.8996 - val_mae: 4.3761\n",
            "\n",
            "Epoch 00164: loss did not improve from 3.98951\n",
            "Epoch 165/500\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 4.2404 - mae: 4.7157 - val_loss: 4.9670 - val_mae: 5.4418\n",
            "\n",
            "Epoch 00165: loss did not improve from 3.98951\n",
            "Epoch 166/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2542 - mae: 4.7336 - val_loss: 3.8877 - val_mae: 4.3656\n",
            "\n",
            "Epoch 00166: loss did not improve from 3.98951\n",
            "Epoch 167/500\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 4.2114 - mae: 4.6925 - val_loss: 3.9220 - val_mae: 4.4005\n",
            "\n",
            "Epoch 00167: loss did not improve from 3.98951\n",
            "Epoch 168/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.3139 - mae: 4.7913 - val_loss: 3.8834 - val_mae: 4.3523\n",
            "\n",
            "Epoch 00168: loss did not improve from 3.98951\n",
            "Epoch 169/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2365 - mae: 4.7145 - val_loss: 3.8936 - val_mae: 4.3716\n",
            "\n",
            "Epoch 00169: loss did not improve from 3.98951\n",
            "Epoch 170/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2839 - mae: 4.7612 - val_loss: 4.0874 - val_mae: 4.5681\n",
            "\n",
            "Epoch 00170: loss did not improve from 3.98951\n",
            "Epoch 171/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.1127 - mae: 4.5870 - val_loss: 5.2376 - val_mae: 5.7118\n",
            "\n",
            "Epoch 00171: loss did not improve from 3.98951\n",
            "Epoch 172/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.6598 - mae: 5.1440 - val_loss: 4.5179 - val_mae: 4.9939\n",
            "\n",
            "Epoch 00172: loss did not improve from 3.98951\n",
            "Epoch 173/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.3352 - mae: 4.8109 - val_loss: 4.2580 - val_mae: 4.7395\n",
            "\n",
            "Epoch 00173: loss did not improve from 3.98951\n",
            "Epoch 174/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.0640 - mae: 4.5396 - val_loss: 3.8394 - val_mae: 4.3095\n",
            "\n",
            "Epoch 00174: loss did not improve from 3.98951\n",
            "Epoch 175/500\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 3.9118 - mae: 4.3907 - val_loss: 4.0896 - val_mae: 4.5699\n",
            "\n",
            "Epoch 00175: loss did not improve from 3.98951\n",
            "Epoch 176/500\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 4.1334 - mae: 4.6090 - val_loss: 4.2194 - val_mae: 4.6971\n",
            "\n",
            "Epoch 00176: loss did not improve from 3.98951\n",
            "Epoch 177/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 3.9078 - mae: 4.3741 - val_loss: 4.3333 - val_mae: 4.8112\n",
            "\n",
            "Epoch 00177: loss improved from 3.98951 to 3.96949, saving model to poids.hdf5\n",
            "Epoch 178/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2491 - mae: 4.7288 - val_loss: 3.9423 - val_mae: 4.4150\n",
            "\n",
            "Epoch 00178: loss did not improve from 3.96949\n",
            "Epoch 179/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.0725 - mae: 4.5498 - val_loss: 4.5438 - val_mae: 5.0189\n",
            "\n",
            "Epoch 00179: loss did not improve from 3.96949\n",
            "Epoch 180/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.1138 - mae: 4.5894 - val_loss: 4.2614 - val_mae: 4.7420\n",
            "\n",
            "Epoch 00180: loss improved from 3.96949 to 3.96713, saving model to poids.hdf5\n",
            "Epoch 181/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 3.9157 - mae: 4.3953 - val_loss: 3.8857 - val_mae: 4.3625\n",
            "\n",
            "Epoch 00181: loss improved from 3.96713 to 3.96341, saving model to poids.hdf5\n",
            "Epoch 182/500\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 4.1279 - mae: 4.6012 - val_loss: 4.0566 - val_mae: 4.5390\n",
            "\n",
            "Epoch 00182: loss did not improve from 3.96341\n",
            "Epoch 183/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.0946 - mae: 4.5738 - val_loss: 3.9238 - val_mae: 4.4008\n",
            "\n",
            "Epoch 00183: loss did not improve from 3.96341\n",
            "Epoch 184/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.0276 - mae: 4.4995 - val_loss: 3.8979 - val_mae: 4.3655\n",
            "\n",
            "Epoch 00184: loss did not improve from 3.96341\n",
            "Epoch 185/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.0158 - mae: 4.4912 - val_loss: 3.9100 - val_mae: 4.3832\n",
            "\n",
            "Epoch 00185: loss did not improve from 3.96341\n",
            "Epoch 186/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2006 - mae: 4.6761 - val_loss: 4.0366 - val_mae: 4.5132\n",
            "\n",
            "Epoch 00186: loss did not improve from 3.96341\n",
            "Epoch 187/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.0028 - mae: 4.4754 - val_loss: 3.8613 - val_mae: 4.3386\n",
            "\n",
            "Epoch 00187: loss did not improve from 3.96341\n",
            "Epoch 188/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.0802 - mae: 4.5566 - val_loss: 4.0648 - val_mae: 4.5410\n",
            "\n",
            "Epoch 00188: loss did not improve from 3.96341\n",
            "Epoch 189/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.2930 - mae: 4.7703 - val_loss: 3.8317 - val_mae: 4.3054\n",
            "\n",
            "Epoch 00189: loss did not improve from 3.96341\n",
            "Epoch 190/500\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 4.0946 - mae: 4.5683 - val_loss: 4.4806 - val_mae: 4.9569\n",
            "\n",
            "Epoch 00190: loss did not improve from 3.96341\n",
            "Epoch 191/500\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 4.1729 - mae: 4.6465 - val_loss: 4.0789 - val_mae: 4.5585\n",
            "\n",
            "Epoch 00191: loss did not improve from 3.96341\n",
            "Epoch 192/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.5722 - mae: 5.0523 - val_loss: 4.0027 - val_mae: 4.4800\n",
            "\n",
            "Epoch 00192: loss did not improve from 3.96341\n",
            "Epoch 193/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.0600 - mae: 4.5384 - val_loss: 4.1520 - val_mae: 4.6302\n",
            "\n",
            "Epoch 00193: loss did not improve from 3.96341\n",
            "Epoch 194/500\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 4.3039 - mae: 4.7814 - val_loss: 4.5070 - val_mae: 4.9811\n",
            "\n",
            "Epoch 00194: loss did not improve from 3.96341\n",
            "Epoch 195/500\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 4.0757 - mae: 4.5490 - val_loss: 4.0317 - val_mae: 4.5094\n",
            "\n",
            "Epoch 00195: loss did not improve from 3.96341\n",
            "Epoch 196/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 3.7920 - mae: 4.2610 - val_loss: 3.9179 - val_mae: 4.3991\n",
            "\n",
            "Epoch 00196: loss improved from 3.96341 to 3.93000, saving model to poids.hdf5\n",
            "Epoch 197/500\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 3.9928 - mae: 4.4677 - val_loss: 3.9129 - val_mae: 4.3889\n",
            "\n",
            "Epoch 00197: loss did not improve from 3.93000\n",
            "Epoch 198/500\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 4.0098 - mae: 4.4870 - val_loss: 4.6932 - val_mae: 5.1725\n",
            "\n",
            "Epoch 00198: loss did not improve from 3.93000\n",
            "Epoch 199/500\n",
            "45/45 [==============================] - 1s 13ms/step - loss: 4.1811 - mae: 4.6542 - val_loss: 3.9050 - val_mae: 4.3860\n",
            "\n",
            "Epoch 00199: loss did not improve from 3.93000\n",
            "Epoch 200/500\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 4.2992 - mae: 4.7740 - val_loss: 4.1846 - val_mae: 4.6671\n",
            "\n",
            "Epoch 00200: loss did not improve from 3.93000\n",
            "Epoch 201/500\n",
            "45/45 [==============================] - 1s 14ms/step - loss: 4.3178 - mae: 4.7949 - val_loss: 4.1164 - val_mae: 4.5967\n",
            "\n",
            "Epoch 00201: loss did not improve from 3.93000\n",
            "Epoch 202/500\n",
            "44/45 [============================>.] - ETA: 0s - loss: 4.1262 - mae: 4.6009"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COP9u4yitvmw"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\r\n",
        "erreur_validation = historique.history[\"val_loss\"]\r\n",
        "\r\n",
        "# Affiche l'erreur en fonction de la période\r\n",
        "plt.figure(figsize=(10, 6))\r\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\r\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\r\n",
        "plt.legend()\r\n",
        "\r\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o2zh6N9t1b7"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\r\n",
        "erreur_validation = historique.history[\"val_loss\"]\r\n",
        "\r\n",
        "# Affiche l'erreur en fonction de la période\r\n",
        "plt.figure(figsize=(10, 6))\r\n",
        "plt.plot(np.arange(0,len(erreur_entrainement[400:500])),erreur_entrainement[400:500], label=\"Erreurs sur les entrainements\")\r\n",
        "plt.plot(np.arange(0,len(erreur_entrainement[400:500])),erreur_validation[400:500], label =\"Erreurs sur les validations\")\r\n",
        "plt.legend()\r\n",
        "\r\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6Gq2CkeGR_1"
      },
      "source": [
        "**4. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcGnqie_GVNE"
      },
      "source": [
        "# Charge les meilleurs poids\r\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRxXtHRXGXfF"
      },
      "source": [
        "taille_fenetre = 20\r\n",
        "\r\n",
        "# Création d'une liste vide pour recevoir les prédictions\r\n",
        "predictions = []\r\n",
        "\r\n",
        "# Calcul des prédiction pour chaque groupe de 20 valeurs consécutives de la série\r\n",
        "# dans l'intervalle de validation\r\n",
        "for t in temps[temps_separation:-taille_fenetre]:\r\n",
        "    X = np.reshape(serie[t:t+taille_fenetre],(1,taille_fenetre))\r\n",
        "    predictions.append(model.predict(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd2nfDcgG8EO"
      },
      "source": [
        "# Affiche la série et les prédictions\r\n",
        "plt.figure(figsize=(10, 6))\r\n",
        "affiche_serie(temps,serie,label=\"Série temporelle\")\r\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions)[:,0,0],label=\"Prédictions\")\r\n",
        "plt.title('Prédictions avec le modèle de régression linéaire')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# Zoom sur l'intervalle de validation\r\n",
        "plt.figure(figsize=(10, 6))\r\n",
        "affiche_serie(temps[temps_separation:],serie[temps_separation:],label=\"Série temporelle\")\r\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions)[:,0,0],label=\"Prédictions\")\r\n",
        "plt.title(\"Prédictions avec le modèle de régression linéaire (zoom sur l'intervalle de validation)\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDS7BJvZG_e1"
      },
      "source": [
        "# Calcule de l'erreur quadratique moyenne et de l'erreur absolue moyenne \r\n",
        "\r\n",
        "mae = tf.keras.metrics.mean_absolute_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions)[:,0,0]).numpy()\r\n",
        "mse = tf.keras.metrics.mean_squared_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions)[:,0,0]).numpy()\r\n",
        "\r\n",
        "print(mae)\r\n",
        "print(mse)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}