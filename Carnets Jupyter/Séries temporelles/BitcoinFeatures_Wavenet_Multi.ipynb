{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LorenzMap_Wavenet_Multi.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/BitcoinFeatures_Wavenet_Multi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVuCzMYKDp3z"
      },
      "source": [
        "# Chargement des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3nLyKtnliCW"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eo2qVS1lrzuX"
      },
      "source": [
        "On télécharge un script depuis Github permettant de télécharger un fichier stocké sur GoogleDrive, puis on utilise ce script écrit en Python pour télécharger le fichier `bitcoin.zip`. Enfin, on décompresse les données pour obtenir le fichier `bitstampUSD_1-min_data_2012-01-01_to_2021-03-31.csv` :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sOdetggHqKE"
      },
      "source": [
        "# Récupération des données au format .csv\n",
        "!rm *.csv\n",
        "!wget --no-check-certificate --content-disposition \"https://raw.githubusercontent.com/AlexandreBourrieau/ML/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/data/LorenzMap.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPJ6nnrRsUBm"
      },
      "source": [
        "Charge la série sous Pandas et affiche les informations du fichier :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEB_JjihEYTP"
      },
      "source": [
        "# Création de la série sous Pandas\n",
        "serie = pd.read_csv(\"LorenzMap.csv\", names=['X','Y','Z'])\n",
        "serie"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF62FumBty9H"
      },
      "source": [
        "# Pré-traitement des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1QKMBThNQni"
      },
      "source": [
        "# Affiche la série\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(serie.index, serie['X'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU7u1mA1E6jk"
      },
      "source": [
        "# Préparation des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMiDRe1ZcqMR"
      },
      "source": [
        "# Définition des dates de début et de fin\n",
        "\n",
        "temps_debut = 0\n",
        "temps_fin = 1500\n",
        "\n",
        "serie_etude = serie.loc[temps_debut:temps_fin].copy()\n",
        "serie_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG5Fyx5O5oe5"
      },
      "source": [
        "# Prépartion des datasets multivariés"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tShkj2wRIp6a"
      },
      "source": [
        "serie_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7cGUeWb5oe7"
      },
      "source": [
        "**1. Séparation des données en données pour l'entrainement et la validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob-EAw_j5oe8"
      },
      "source": [
        "On réserve 20% des données pour l'entrainement et le reste pour la validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5AWeK_Z5oe8"
      },
      "source": [
        "# Sépare les données en entrainement et tests\n",
        "pourcentage = 0.7\n",
        "temps_separation = int(len(serie_etude) * pourcentage)\n",
        "date_separation = serie_etude.index[temps_separation]\n",
        "\n",
        "serie_entrainement = []\n",
        "serie_test = []\n",
        "\n",
        "serie_entrainement.append(serie_etude['X'].iloc[:temps_separation])\n",
        "serie_test.append(serie_etude['X'].iloc[temps_separation:])\n",
        "\n",
        "serie_entrainement.append(serie_etude['Y'].iloc[:temps_separation])\n",
        "serie_test.append(serie_etude['Y'].iloc[temps_separation:])\n",
        "\n",
        "serie_entrainement.append(serie_etude['Z'].iloc[:temps_separation])\n",
        "serie_test.append(serie_etude['Z'].iloc[temps_separation:])\n",
        "\n",
        "print(\"Taille de l'entrainement : %d\" %len(serie_entrainement[0]))\n",
        "print(\"Taille de la validation : %d\" %len(serie_test[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZUMMMro5oe9"
      },
      "source": [
        "On normalise les données :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu_YxoSI5oe9"
      },
      "source": [
        "# Calcul de la moyenne et de l'écart type de la série\n",
        "mean = tf.math.reduce_mean(np.asarray(serie_entrainement))\n",
        "std = tf.math.reduce_std(np.asarray((serie_entrainement)))\n",
        "\n",
        "# Normalisation des données\n",
        "serie_entrainement = (serie_entrainement-mean)/std\n",
        "serie_test = (serie_test-mean)/std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_4OZJ-p5oe9"
      },
      "source": [
        "# Affiche la série\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "\n",
        "ax.plot(serie_etude.index[:temps_separation].values,serie_entrainement[0], label=\"X_Ent\")\n",
        "ax.plot(serie_etude.index[:temps_separation].values,serie_entrainement[1], label=\"Y_Ent\")\n",
        "ax.plot(serie_etude.index[:temps_separation].values,serie_entrainement[2], label=\"Z_Ent\")\n",
        "\n",
        "ax.plot(serie_etude.index[temps_separation:].values,serie_test[0], label=\"X_Val\")\n",
        "ax.plot(serie_etude.index[temps_separation:].values,serie_test[1], label=\"Y_Val\")\n",
        "ax.plot(serie_etude.index[temps_separation:].values,serie_test[2], label=\"Z_Val\")\n",
        "\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-bANnT35oe-"
      },
      "source": [
        "**2. Création des datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wqy52B5xSXDz"
      },
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "\n",
        "def prepare_dataset_XY(series, taille_fenetre, horizon, batch_size):\n",
        "  serie_concat = tf.expand_dims(series[0],1)\n",
        "\n",
        "  for i in range(1,len(series)):\n",
        "    serie_ = tf.expand_dims(series[i],1)\n",
        "    serie_concat = tf.concat([serie_concat,serie_],1)\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie_concat)\n",
        "  dataset = dataset.window(taille_fenetre+horizon, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre + horizon))\n",
        "  dataset = dataset.map(lambda x: (x[0:taille_fenetre],tf.expand_dims(x[-taille_fenetre:][:,0],1)))\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Jh1RZYo5oe_"
      },
      "source": [
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "taille_fenetre = 16\n",
        "horizon = 1\n",
        "batch_size = 64\n",
        "\n",
        "# Création du dataset\n",
        "dataset = prepare_dataset_XY(serie_entrainement,taille_fenetre,horizon,batch_size)\n",
        "dataset_val = prepare_dataset_XY(serie_test,taille_fenetre,horizon,batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr2pbMox5oe_"
      },
      "source": [
        "print(len(list(dataset.as_numpy_iterator())))\n",
        "for element in dataset.take(1):\n",
        "  print(element[0].shape)\n",
        "  print(element[1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQejjkaeeHxe"
      },
      "source": [
        "for element in dataset.take(1):\n",
        "  print(element)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHCcYn6i5oe_"
      },
      "source": [
        "On extrait maintenant les deux tenseurs (X,Y) pour l'entrainement :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3ZhLIK15ofA"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset))              # #56x((1000,16,5),(1000,16,5)) => x = 56x(1000,16,5) ; y = 56x(1000,16,5)\n",
        "\n",
        "# Recombine les données\n",
        "x = np.asarray(x,dtype=np.float32)      # 56x(1000,16,5) => (56,1000,16,5)\n",
        "y = np.asarray(y,dtype=np.float32)      # 56x(1000,16,5) => (56,1000,16,5)\n",
        "\n",
        "x_train = np.asarray(tf.reshape(x,shape=(x.shape[0]*x.shape[1],taille_fenetre,x.shape[3])))     # (56,1000,16,5) => (56*1000,16,5)\n",
        "y_train = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],taille_fenetre,y.shape[3])))     # (56,1000,16,5) => (56*1000,16,5)\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llnKyLvl5ofA"
      },
      "source": [
        "Puis la même chose pour les données de validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hadrKVrZ5ofB"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset_val))              # #56x((1000,16,5),(1000,16,1)) => x = 56x(1000,16,5) ; y = 56x(1000,16,1)\n",
        "\n",
        "# Recombine les données\n",
        "x = np.asarray(x,dtype=np.float32)      # 56x(1000,16,5) => (56,1000,16,5)\n",
        "y = np.asarray(y,dtype=np.float32)      # 56x(1000,16,1) => (56,1000,16,1)\n",
        "\n",
        "x_val = np.asarray(tf.reshape(x,shape=(x.shape[0]*x.shape[1],taille_fenetre,x.shape[3])))     # (56,1000,16,5) => (56*1000,16,5)\n",
        "y_val = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],taille_fenetre,y.shape[3])))     # (56,1000,16,1) => (56*1000,16,1)\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHfiXLFZhrPs"
      },
      "source": [
        "# Création du modèle type Wavenet Multivarié - Apprentissage univarié"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd9AzWFtjnjs"
      },
      "source": [
        "**1. Création du réseau**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDVH4xsDzM-V"
      },
      "source": [
        "from keras.layers import Conv1D\n",
        "from keras.layers import Conv1D\n",
        "from keras.utils.conv_utils import conv_output_length\n",
        "from keras import layers\n",
        "from keras.regularizers import l2\n",
        "import keras.backend as K\n",
        "from keras.engine import Input\n",
        "from keras.engine import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCtJXABdzKB9"
      },
      "source": [
        "class CausalDilatedConv1D(Conv1D):\n",
        "    def __init__(self, nb_filter, filter_length, init='glorot_uniform', activation=None, weights=None,\n",
        "                 border_mode='valid', subsample_length=1, atrous_rate=1, W_regularizer=None, b_regularizer=None,\n",
        "                 activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True, causal=False, **kwargs):\n",
        "        super(CausalDilatedConv1D, self).__init__(nb_filter, filter_length, weights=weights, activation=activation, \n",
        "                padding=border_mode, strides=subsample_length, dilation_rate=atrous_rate, kernel_regularizer=W_regularizer, \n",
        "                bias_regularizer=b_regularizer, activity_regularizer=activity_regularizer, kernel_constraint=W_constraint, \n",
        "                bias_constraint=b_constraint, use_bias=bias, **kwargs)\n",
        "        self.causal = causal\n",
        "        self.nb_filter = nb_filter\n",
        "        self.atrous_rate = atrous_rate\n",
        "        self.filter_length = filter_length\n",
        "        self.subsample_length = subsample_length\n",
        "        self.border_mode = border_mode\n",
        "        if self.causal and border_mode != 'valid':\n",
        "            raise ValueError(\"Causal mode dictates border_mode=valid.\")\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        input_length = input_shape[1]\n",
        "        if self.causal:\n",
        "            input_length += self.atrous_rate * (self.filter_length - 1)\n",
        "        length = conv_output_length(input_length, self.filter_length, self.border_mode, self.strides[0], dilation=self.atrous_rate)\n",
        "        return (input_shape[0], length, self.nb_filter)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        if self.causal:\n",
        "            x = K.temporal_padding(x, padding=(self.atrous_rate * (self.filter_length - 1), 0))\n",
        "        # return super(CausalAtrousConvolution1D, self).call(x, mask)\n",
        "        return super(CausalDilatedConv1D, self).call(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s19zmYCgzTvV"
      },
      "source": [
        "def _compute_receptive_field(dilation_depth, stacks):\n",
        "  return stacks * (2 ** dilation_depth * 2) - (stacks - 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIX-o-lMzVK0"
      },
      "source": [
        "def build_model_residual_block(x, i, s,nb_filters, dim_filters, use_bias,res_l2):\n",
        "        original_x = x\n",
        "        relu_out = CausalDilatedConv1D(nb_filters, dim_filters, atrous_rate=2 ** i, border_mode='valid', causal=True, bias=True, name='dilated_conv_%d_relu_s%d' % (2 ** i, s), activation='relu', W_regularizer=l2(res_l2))(x)\n",
        "        res_x = layers.Conv1D(1, 1, padding='same', use_bias=use_bias, kernel_regularizer=l2(res_l2))(relu_out)\n",
        "        skip_x = layers.Conv1D(1, 1, padding='same', use_bias=use_bias, kernel_regularizer=l2(res_l2))(relu_out)\n",
        "        res_x = layers.Add()([original_x, res_x])\n",
        "        return res_x, skip_x\n",
        "\n",
        "def build_model_couche_condition(x, nb_input_bins, nb_filters, dim_filters, use_bias,res_l2):\n",
        "        original_x = x\n",
        "        skip_conditions = []\n",
        "\n",
        "        relu_out = CausalDilatedConv1D(nb_filters, dim_filters, atrous_rate=1, border_mode='valid', causal=True, bias=True, name='dilated_conv_%d_condition_%d_s%d' % (1,1, 0), activation='relu',\n",
        "                                       W_regularizer=l2(res_l2))(tf.expand_dims(x[:,:,0],2))\n",
        "\n",
        "        skip_ = layers.Conv1D(1, 1, padding='same', use_bias=use_bias, kernel_regularizer=l2(res_l2))(relu_out)\n",
        "        skip_conditions.append(skip_)\n",
        "\n",
        "        for i in range(1,nb_input_bins):\n",
        "          relu_out = CausalDilatedConv1D(nb_filters, dim_filters, atrous_rate=1, border_mode='valid', causal=True, bias=True, name='dilated_conv_%d_condition_%d_s%d' % (1,i+1,0), activation='relu',\n",
        "                                                    W_regularizer=l2(res_l2))(tf.expand_dims(x[:,:,i],2))\n",
        "\n",
        "          skip_ = layers.Conv1D(1, 1, padding='same', use_bias=use_bias, kernel_regularizer=l2(res_l2))(relu_out)\n",
        "          skip_conditions.append(skip_)\n",
        "\n",
        "        out = layers.Add()(skip_conditions)\n",
        "        return out\n",
        "\n",
        "\n",
        "def build_model(fragment_length, nb_filters, dim_filters, nb_input_bins, dilation_depth, stacks, use_bias, res_l2, final_l2):\n",
        "        input_shape = Input(shape=(fragment_length, nb_input_bins), name='input_part')\n",
        "        out = input_shape\n",
        "\n",
        "        for s in range(stacks):\n",
        "            if nb_input_bins > 1 :\n",
        "                # Couche conditionnée\n",
        "                out = build_model_couche_condition(out, nb_input_bins, nb_filters, dim_filters, use_bias, res_l2)\n",
        "            else:\n",
        "                out, skip_out = build_model_residual_block(out, 0, s , nb_filters, dim_filters, use_bias, res_l2)\n",
        "\n",
        "            # Couches intermédiaires\n",
        "            for i in range(1, dilation_depth + 1):\n",
        "                out, skip_out = build_model_residual_block(out, i, s, nb_filters, dim_filters, use_bias, res_l2)\n",
        "\n",
        "        # Couche de sortie\n",
        "        out = layers.Activation('linear', name=\"output_linear\")(out)\n",
        "        out = layers.Conv1D(1, 1, padding='same', kernel_regularizer=l2(final_l2))(out)\n",
        "        model = Model(input_shape, out)\n",
        "        return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH9rsD5UzZgM"
      },
      "source": [
        "**2. Construction du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln6eVTFezYlq"
      },
      "source": [
        "def compute_receptive_field_(dilation_depth, nb_stacks):\n",
        "    receptive_field = nb_stacks * (2 ** dilation_depth * 2) - (nb_stacks - 1)\n",
        "    return receptive_field\n",
        "\n",
        "nb_filters = 2\n",
        "dim_filters = 2\n",
        "nb_input_bins = 1\n",
        "dilation_depth = 3\n",
        "nb_stacks = 1\n",
        "use_bias = False\n",
        "res_l2 = 0\n",
        "final_l2 = 0\n",
        "\n",
        "fragment_length = compute_receptive_field_(dilation_depth, nb_stacks)\n",
        "fragment_length\n",
        "\n",
        "model = build_model(fragment_length, nb_filters, dim_filters, nb_input_bins, dilation_depth, nb_stacks, use_bias, res_l2, final_l2)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_azfJaeUo2nU"
      },
      "source": [
        "**2. Optimisation de l'apprentissage**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf3lwaQBnjxL"
      },
      "source": [
        "Pour accélérer le traitement des données, nous n'allons pas utiliser l'intégralité des données pendant la mise à jour du gradient, comme cela a été fait jusqu'à présent (en utilisant le dataset).  \n",
        "Cette fois-ci, nous allons forcer les mises à jour du gradient à se produire de manière moins fréquente en attribuant la valeur du batch_size à prendre en compte lors de la regression du modèle.  \n",
        "Pour cela, on utilise l'argument \"batch_size\" dans la méthode fit. En précisant un batch_size=1000, cela signifie que :\n",
        " - Sur notre total de 56000 échantillons, 56 seront utilisés pour les calculs du gradient\n",
        " - Il y aura également 56 itérations à chaque période.\n",
        "  \n",
        "    \n",
        "    \n",
        "Si nous avions pris le dataset comme entrée, nous aurions eu :\n",
        "- Un total de 56000 échantillons également\n",
        "- Chaque période aurait également pris 56 itérations pour se compléter\n",
        "- Mais 1000 échantillons auraient été utilisés pour le calcul du gradient, au lieu de 56 avec la méthode utilisée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6Z35rNWj5SA"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "def  My_MSE(y_true,y_pred):\n",
        "  return(tf.keras.metrics.mse(y_true,y_pred)*std.numpy()+mean.numpy())\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam()\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"mse\", optimizer=optimiseur, metrics=[\"mse\",My_MSE])\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint],batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2aP9J3TkNGG"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 1])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZxCgpuYkQ2Q"
      },
      "source": [
        "# Chargement des poids sauvegardés\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmdbo23qkTKE"
      },
      "source": [
        "max_periodes = 1000\n",
        "\n",
        "# Classe permettant d'arrêter l'entrainement si la variation\n",
        "# devient plus petite qu'une valeur à choisir sur un nombre\n",
        "# de périodes à choisir\n",
        "class StopTrain(keras.callbacks.Callback):\n",
        "    def __init__(self, delta=0.01,periodes=100, term=\"loss\", logs={}):\n",
        "      self.n_periodes = 0\n",
        "      self.periodes = periodes\n",
        "      self.loss_1 = 100\n",
        "      self.delta = delta\n",
        "      self.term = term\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "      diff_loss = abs(self.loss_1 - logs[self.term])\n",
        "      self.loss_1 = logs[self.term]\n",
        "      if (diff_loss < self.delta):\n",
        "        self.n_periodes = self.n_periodes + 1\n",
        "      else:\n",
        "        self.n_periodes = 0\n",
        "      if (self.n_periodes == self.periodes):\n",
        "        print(\"Arrêt de l'entrainement...\")\n",
        "        self.model.stop_training = True\n",
        "\n",
        "def  My_MSE(y_true,y_pred):\n",
        "  return(tf.keras.metrics.mse(y_true,y_pred)*std.numpy()+mean.numpy())\n",
        "  \n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.001,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"mse\", optimizer=optimiseur, metrics=[\"mse\",My_MSE])\n",
        "\n",
        "# Entraine le modèle, avec une réduction des calculs du gradient\n",
        "if nb_input_bins == 1:\n",
        "  historique = model.fit(x=tf.expand_dims(x_train[:,:,0],2),y=y_train,validation_data=(tf.expand_dims(x_val[:,:,0],2),y_val), epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=1e-6,periodes = 50, term=\"loss\")],batch_size=batch_size)\n",
        "else:\n",
        "  historique = model.fit(x=x_train,y=y_train,validation_data=(x_val,y_val), epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=1e-5,periodes = 10, term=\"My_MSE\")],batch_size=batch_size)\n",
        "\n",
        "\n",
        "# Entraine le modèle sans réduction de calculs\n",
        "#historique = model.fit(dataset,validation_data=dataset_val, epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=1e-8,periodes = 10, term=\"val_My_MSE\")])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfbomV0LS9LD"
      },
      "source": [
        "model.load_weights(\"poids_train.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MDY8O1-l6kN"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbzro22hgt4b"
      },
      "source": [
        "**3. Prédictions single step**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMmVn1e5zEAm"
      },
      "source": [
        "# Création des instants d'entrainement et de validation\n",
        "y_train_timing = serie_etude.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_train)]\n",
        "y_val_timing = serie_etude.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_val)]\n",
        "\n",
        "# Calcul des prédictions\n",
        "if nb_input_bins == 1:\n",
        "  pred_ent = model.predict(tf.expand_dims(x_train[:,:,0],2), verbose=1)\n",
        "  pred_val = model.predict(tf.expand_dims(x_val[:,:,0],2), verbose=1)\n",
        "else:\n",
        "  pred_ent = model.predict(x_train, verbose=1)\n",
        "  pred_val = model.predict(x_val, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2xmissP6rXM"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbes originales\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index,y=serie_entrainement[0],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index[temps_separation:],y=serie_test[0],line=dict(color='blue', width=1)))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index[taille_fenetre+horizon-1:],y=pred_ent[:,taille_fenetre-1,0],line=dict(color='green', width=1)))\n",
        "\n",
        "# Courbes des prédictions de validations\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index[temps_separation+taille_fenetre+horizon-1:],y=pred_val[:,taille_fenetre-1,0],line=dict(color='red', width=1)))\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYISrVoIv97W"
      },
      "source": [
        "**Prédictions multi-step ahead**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1tcwq5pv9it"
      },
      "source": [
        "predictions = []\n",
        "\n",
        "data_to_predict = x_val[0,:,0]\n",
        "data_to_predict = tf.expand_dims(data_to_predict,1)\n",
        "data_to_predict = tf.expand_dims(data_to_predict,0)\n",
        "\n",
        "prediction = model.predict(data_to_predict)\n",
        "predictions.append(prediction[0,taille_fenetre-1,0])\n",
        "\n",
        "data_to_predict = x_val[1,0:taille_fenetre-1,0]\n",
        "data_to_predict = np.insert(data_to_predict,taille_fenetre-1,predictions[0])\n",
        "\n",
        "#for t in y_val_timing:\n",
        "for i in range(1,200):\n",
        "  data_to_predict = tf.expand_dims(data_to_predict,1)\n",
        "  data_to_predict = tf.expand_dims(data_to_predict,0)\n",
        "  prediction = model.predict(data_to_predict,1)\n",
        "  predictions.append(prediction[0,taille_fenetre-1,0])\n",
        "\n",
        "  data_to_predict = x_val[i+1,0:taille_fenetre-1,0]\n",
        "  data_to_predict = np.insert(data_to_predict,taille_fenetre-1,predictions[i])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhmBwx6Cxueb"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "n_max = len(predictions)\n",
        "\n",
        "# Courbes originales\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index,y=serie_entrainement[0],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index[temps_separation:],y=serie_test[0],line=dict(color='blue', width=1)))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index[taille_fenetre+horizon-1:],y=pred_ent[:,taille_fenetre-1,0],line=dict(color='green', width=1)))\n",
        "\n",
        "# Courbes des prédictions de validations\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index[temps_separation+taille_fenetre+horizon-1:temps_separation+taille_fenetre+horizon-1+n_max],y=predictions[0:n_max],line=dict(color='red', width=1)))\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1nuQuY-CN1m"
      },
      "source": [
        "# Création du modèle type Wavenet Multivarié - Apprentissage multivarié"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtAbZ43tCN2H"
      },
      "source": [
        "**2. Construction du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7N-oFV6jCN2H"
      },
      "source": [
        "def compute_receptive_field_(dilation_depth, nb_stacks):\n",
        "    receptive_field = nb_stacks * (2 ** dilation_depth * 2) - (nb_stacks - 1)\n",
        "    return receptive_field\n",
        "\n",
        "nb_filters = 2\n",
        "dim_filters = 2\n",
        "nb_input_bins = 3\n",
        "dilation_depth = 3\n",
        "nb_stacks = 1\n",
        "use_bias = False\n",
        "res_l2 = 0\n",
        "final_l2 = 0\n",
        "\n",
        "fragment_length = compute_receptive_field_(dilation_depth, nb_stacks)\n",
        "fragment_length\n",
        "\n",
        "model = build_model(fragment_length, nb_filters, dim_filters, nb_input_bins, dilation_depth, nb_stacks, use_bias, res_l2, final_l2)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-bEVZTKCN2I"
      },
      "source": [
        "**2. Optimisation de l'apprentissage**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsd3q2SxCN2I"
      },
      "source": [
        "Pour accélérer le traitement des données, nous n'allons pas utiliser l'intégralité des données pendant la mise à jour du gradient, comme cela a été fait jusqu'à présent (en utilisant le dataset).  \n",
        "Cette fois-ci, nous allons forcer les mises à jour du gradient à se produire de manière moins fréquente en attribuant la valeur du batch_size à prendre en compte lors de la regression du modèle.  \n",
        "Pour cela, on utilise l'argument \"batch_size\" dans la méthode fit. En précisant un batch_size=1000, cela signifie que :\n",
        " - Sur notre total de 56000 échantillons, 56 seront utilisés pour les calculs du gradient\n",
        " - Il y aura également 56 itérations à chaque période.\n",
        "  \n",
        "    \n",
        "    \n",
        "Si nous avions pris le dataset comme entrée, nous aurions eu :\n",
        "- Un total de 56000 échantillons également\n",
        "- Chaque période aurait également pris 56 itérations pour se compléter\n",
        "- Mais 1000 échantillons auraient été utilisés pour le calcul du gradient, au lieu de 56 avec la méthode utilisée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwjBiyLmCN2J"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "def  My_MSE(y_true,y_pred):\n",
        "  return(tf.keras.metrics.mse(y_true,y_pred)*std.numpy()+mean.numpy())\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam()\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"mse\", optimizer=optimiseur, metrics=[\"mse\",My_MSE])\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint],batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zv8uM_79CN2J"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 1])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7Fe6sb4CN2K"
      },
      "source": [
        "# Chargement des poids sauvegardés\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLVmHMOMCN2K"
      },
      "source": [
        "max_periodes = 1000\n",
        "\n",
        "# Classe permettant d'arrêter l'entrainement si la variation\n",
        "# devient plus petite qu'une valeur à choisir sur un nombre\n",
        "# de périodes à choisir\n",
        "class StopTrain(keras.callbacks.Callback):\n",
        "    def __init__(self, delta=0.01,periodes=100, term=\"loss\", logs={}):\n",
        "      self.n_periodes = 0\n",
        "      self.periodes = periodes\n",
        "      self.loss_1 = 100\n",
        "      self.delta = delta\n",
        "      self.term = term\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "      diff_loss = abs(self.loss_1 - logs[self.term])\n",
        "      self.loss_1 = logs[self.term]\n",
        "      if (diff_loss < self.delta):\n",
        "        self.n_periodes = self.n_periodes + 1\n",
        "      else:\n",
        "        self.n_periodes = 0\n",
        "      if (self.n_periodes == self.periodes):\n",
        "        print(\"Arrêt de l'entrainement...\")\n",
        "        self.model.stop_training = True\n",
        "\n",
        "def  My_MSE(y_true,y_pred):\n",
        "  return(tf.keras.metrics.mse(y_true,y_pred)*std.numpy()+mean.numpy())\n",
        "  \n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.001,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"mse\", optimizer=optimiseur, metrics=[\"mse\",My_MSE])\n",
        "\n",
        "# Entraine le modèle, avec une réduction des calculs du gradient\n",
        "historique = model.fit(x=x_train,y=y_train,validation_data=(x_val,y_val), epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=1e-6,periodes = 50, term=\"loss\")],batch_size=batch_size)\n",
        "\n",
        "\n",
        "# Entraine le modèle sans réduction de calculs\n",
        "#historique = model.fit(dataset,validation_data=dataset_val, epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=1e-8,periodes = 10, term=\"val_My_MSE\")])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS8ZgVeRCN2M"
      },
      "source": [
        "model.load_weights(\"poids_train.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa3Hrx64CN2M"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4TfbnG6CN2N"
      },
      "source": [
        "**3. Prédictions single step**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRggLjz5CN2N"
      },
      "source": [
        "# Création des instants d'entrainement et de validation\n",
        "y_train_timing = serie_etude.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_train)]\n",
        "y_val_timing = serie_etude.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_val)]\n",
        "\n",
        "# Calcul des prédictions\n",
        "pred_ent = model.predict(x_train, verbose=1)\n",
        "pred_val = model.predict(x_val, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pV0yg4QXCN2O"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbes originales\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index,y=serie_entrainement[0],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index[temps_separation:],y=serie_test[0],line=dict(color='blue', width=1)))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index[taille_fenetre+horizon-1:],y=pred_ent[:,taille_fenetre-1,0],line=dict(color='green', width=1)))\n",
        "\n",
        "# Courbes des prédictions de validations\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index[temps_separation+taille_fenetre+horizon-1:],y=pred_val[:,taille_fenetre-1,0],line=dict(color='red', width=1)))\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67HaeJa6CN2O"
      },
      "source": [
        "**Prédictions multi-step ahead (Prédiction de Xt+1 avec Xt, Yt+1, Zt+1)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0q8VJ6oGC_XQ"
      },
      "source": [
        "predictions = []\n",
        "\n",
        "data_to_predict = x_val[0,:,:]                          # [[X0,Y0,Z0],[X1,Y1,Z1], ... , [X15,Y15,Z15]] : (16,3)\n",
        "\n",
        "data_to_predict = tf.expand_dims(data_to_predict,0)     # (1,16,3)\n",
        "prediction = model.predict(data_to_predict)             # [[[X^1][X^2][X^3]...[X1^6]]] : (1,16,1)\n",
        "predictions.append(prediction[0,taille_fenetre-1,0])    # [X^16] : (1,)\n",
        "\n",
        "data_to_predict = x_val[1,0:taille_fenetre-1,:]         # [[X1,Y1,Z1],[X2,Y2,Z2], ... , [X15,Y15,Z15]] : (15,3)\n",
        "\n",
        "data_to_predict = np.insert(data_to_predict,taille_fenetre-1,     # [[X1,Y1,Z1],[X2,Y2,Z2], ... , [X15,Y15,Z15], [X^16,Y16,Z16]] : (16,3)\n",
        "                            [predictions[0],\n",
        "                             x_val[1,taille_fenetre-1,1],\n",
        "                             x_val[1,taille_fenetre-1,2]],axis=0)    \n",
        "\n",
        "\n",
        "for i in range(1,300):\n",
        "  data_to_predict = tf.expand_dims(data_to_predict,0)   # (16,3) => (1,16,3)\n",
        "  prediction = model.predict(data_to_predict)           # [[[X^2][X^3][X^4]...[X^17]]] : (1,16,1)\n",
        "\n",
        "  predictions.append(prediction[0,taille_fenetre-1,0])  # [X^17] : (1,)\n",
        "  data_to_predict = x_val[i+1,0:taille_fenetre-1,:]     # [[X2,Y2,Z2],[X3,Y3,Z3], ... , [X16,Y16,Z16]] : (15,3)\n",
        "\n",
        "  data_to_predict = np.insert(data_to_predict,taille_fenetre-1,       # [[X2,Y2,Z2],[X3,Y3,Z3], ... , [X16,Y16,Z16], [X^17,Y17,Z17]] : (16,3)\n",
        "                            [predictions[i],\n",
        "                             x_val[i+1,taille_fenetre-1,1],\n",
        "                             x_val[i+1,taille_fenetre-1,2]],axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unzkoI-aCN2P"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "n_max = len(predictions)\n",
        "\n",
        "# Courbes originales\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index,y=serie_entrainement[0],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index[temps_separation:],y=serie_test[0],line=dict(color='blue', width=1)))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index[taille_fenetre+horizon-1:],y=pred_ent[:,taille_fenetre-1,0],line=dict(color='green', width=1)))\n",
        "\n",
        "# Courbes des prédictions de validations\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index[temps_separation+taille_fenetre+horizon-1:temps_separation+taille_fenetre+horizon-1+n_max],y=predictions[0:n_max],line=dict(color='red', width=1)))\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzsunU3GQmmt"
      },
      "source": [
        "**Prédictions multi-step ahead (Prédiction de Xt+1 avec Xt, Yt, Zt)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTZ9r4zuQls0"
      },
      "source": [
        "predictions_2 = []\n",
        "\n",
        "data_to_predict = x_val[0,:,:]                          # [[X0,Y0,Z0],[X1,Y1,Z1], ... , [X15,Y15,Z15]] : (16,3)\n",
        "\n",
        "data_to_predict = tf.expand_dims(data_to_predict,0)     # (1,16,3)\n",
        "prediction = model.predict(data_to_predict)             # [[[X^1][X^2][X^3]...[X1^6]]] : (1,16,1)\n",
        "predictions_2.append(prediction[0,taille_fenetre-1,0])  # [X^16] : (1,)\n",
        "\n",
        "data_to_predict = x_val[1,0:taille_fenetre-1,:]         # [[X1,Y1,Z1],[X2,Y2,Z2], ... , [X15,Y15,Z15]] : (15,3)\n",
        "\n",
        "data_to_predict = np.insert(data_to_predict,taille_fenetre-1,     # [[X1,Y1,Z1],[X2,Y2,Z2], ... , [X15,Y15,Z15], [X^16,Y15,Z15]] : (16,3)\n",
        "                            [predictions_2[0],\n",
        "                             x_val[1,taille_fenetre-2,1],\n",
        "                             x_val[1,taille_fenetre-2,2]],axis=0)    \n",
        "\n",
        "\n",
        "for i in range(1,300):\n",
        "  data_to_predict = tf.expand_dims(data_to_predict,0)   # (16,3) => (1,16,3)\n",
        "  prediction = model.predict(data_to_predict)           # [[[X^2][X^3][X^4]...[X^17]]] : (1,16,1)\n",
        "\n",
        "  predictions_2.append(prediction[0,taille_fenetre-1,0])  # [X^17] : (1,)\n",
        "  data_to_predict = x_val[i+1,0:taille_fenetre-1,:]     # [[X2,Y2,Z2],[X3,Y3,Z3], ... , [X16,Y16,Z16]] : (15,3)\n",
        "\n",
        "  data_to_predict = np.insert(data_to_predict,taille_fenetre-1,       # [[X2,Y2,Z2],[X3,Y3,Z3], ... , [X16,Y16,Z16], [X^17,Y16,Z16]] : (16,3)\n",
        "                            [predictions_2[i],\n",
        "                             x_val[i+1,taille_fenetre-2,1],\n",
        "                             x_val[i+1,taille_fenetre-2,2]],axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LjnJlWJV9O_"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "n_max = len(predictions_2)\n",
        "\n",
        "# Courbes originales\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index,y=serie_entrainement[0],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index[temps_separation:],y=serie_test[0],line=dict(color='blue', width=1)))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index[taille_fenetre+horizon-1:],y=pred_ent[:,taille_fenetre-1,0],line=dict(color='green', width=1)))\n",
        "\n",
        "# Courbes des prédictions de validations\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index[temps_separation+taille_fenetre+horizon-1:temps_separation+taille_fenetre+horizon-1+n_max],y=predictions_2[0:n_max],line=dict(color='red', width=1)))\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}