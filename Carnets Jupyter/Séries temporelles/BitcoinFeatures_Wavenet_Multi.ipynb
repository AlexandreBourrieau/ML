{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LorenzMap_Wavenet_Multi.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/BitcoinFeatures_Wavenet_Multi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVuCzMYKDp3z"
      },
      "source": [
        "# Chargement des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3nLyKtnliCW"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eo2qVS1lrzuX"
      },
      "source": [
        "On télécharge un script depuis Github permettant de télécharger un fichier stocké sur GoogleDrive, puis on utilise ce script écrit en Python pour télécharger le fichier `bitcoin.zip`. Enfin, on décompresse les données pour obtenir le fichier `bitstampUSD_1-min_data_2012-01-01_to_2021-03-31.csv` :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sOdetggHqKE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8650947-cb93-401a-c7bc-6cbbcca08c02"
      },
      "source": [
        "# Récupération des données au format .csv\n",
        "!rm *.csv\n",
        "!wget --no-check-certificate --content-disposition \"https://raw.githubusercontent.com/AlexandreBourrieau/ML/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/data/LorenzMap.csv\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '*.csv': No such file or directory\n",
            "--2021-04-27 16:46:48--  https://raw.githubusercontent.com/AlexandreBourrieau/ML/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/data/LorenzMap.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 34567 (34K) [text/plain]\n",
            "Saving to: ‘LorenzMap.csv’\n",
            "\n",
            "LorenzMap.csv       100%[===================>]  33.76K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2021-04-27 16:46:48 (13.7 MB/s) - ‘LorenzMap.csv’ saved [34567/34567]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPJ6nnrRsUBm"
      },
      "source": [
        "Charge la série sous Pandas et affiche les informations du fichier :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEB_JjihEYTP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "30983430-e19c-4dfc-d453-f524623f8aa9"
      },
      "source": [
        "# Création de la série sous Pandas\n",
        "serie = pd.read_csv(\"LorenzMap.csv\", names=['X','Y','Z'])\n",
        "serie"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X</th>\n",
              "      <th>Y</th>\n",
              "      <th>Z</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.0500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.048931</td>\n",
              "      <td>0.99832</td>\n",
              "      <td>1.0362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.095542</td>\n",
              "      <td>1.00310</td>\n",
              "      <td>1.0227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.140250</td>\n",
              "      <td>1.01400</td>\n",
              "      <td>1.0097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.183460</td>\n",
              "      <td>1.03090</td>\n",
              "      <td>0.9971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1552</th>\n",
              "      <td>1.282300</td>\n",
              "      <td>-1.93910</td>\n",
              "      <td>25.2700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1553</th>\n",
              "      <td>0.901650</td>\n",
              "      <td>-1.87140</td>\n",
              "      <td>24.4010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1554</th>\n",
              "      <td>0.574140</td>\n",
              "      <td>-1.81070</td>\n",
              "      <td>23.5710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1555</th>\n",
              "      <td>0.292540</td>\n",
              "      <td>-1.76190</td>\n",
              "      <td>22.7750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1556</th>\n",
              "      <td>0.049573</td>\n",
              "      <td>-1.72800</td>\n",
              "      <td>22.0120</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1557 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             X        Y        Z\n",
              "0     0.000000  1.00000   1.0500\n",
              "1     0.048931  0.99832   1.0362\n",
              "2     0.095542  1.00310   1.0227\n",
              "3     0.140250  1.01400   1.0097\n",
              "4     0.183460  1.03090   0.9971\n",
              "...        ...      ...      ...\n",
              "1552  1.282300 -1.93910  25.2700\n",
              "1553  0.901650 -1.87140  24.4010\n",
              "1554  0.574140 -1.81070  23.5710\n",
              "1555  0.292540 -1.76190  22.7750\n",
              "1556  0.049573 -1.72800  22.0120\n",
              "\n",
              "[1557 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF62FumBty9H"
      },
      "source": [
        "# Pré-traitement des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1QKMBThNQni",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2d4fa744-34ff-4f81-aeb4-a0f7a272f1fd"
      },
      "source": [
        "# Affiche la série\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(serie.index, serie['X'])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f8cca174b50>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU7u1mA1E6jk"
      },
      "source": [
        "# Préparation des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMiDRe1ZcqMR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ea75bf01-fe43-418d-e9a4-ea0c591dff73"
      },
      "source": [
        "# Définition des dates de début et de fin\n",
        "\n",
        "temps_debut = 0\n",
        "temps_fin = 1500\n",
        "\n",
        "serie_etude = serie.loc[temps_debut:temps_fin].copy()\n",
        "serie_etude"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X</th>\n",
              "      <th>Y</th>\n",
              "      <th>Z</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.0500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.048931</td>\n",
              "      <td>0.99832</td>\n",
              "      <td>1.0362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.095542</td>\n",
              "      <td>1.00310</td>\n",
              "      <td>1.0227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.140250</td>\n",
              "      <td>1.01400</td>\n",
              "      <td>1.0097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.183460</td>\n",
              "      <td>1.03090</td>\n",
              "      <td>0.9971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1496</th>\n",
              "      <td>10.584000</td>\n",
              "      <td>3.81220</td>\n",
              "      <td>36.1470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1497</th>\n",
              "      <td>9.564800</td>\n",
              "      <td>2.61540</td>\n",
              "      <td>35.2170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1498</th>\n",
              "      <td>8.540200</td>\n",
              "      <td>1.68650</td>\n",
              "      <td>34.1340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1499</th>\n",
              "      <td>7.547100</td>\n",
              "      <td>1.00680</td>\n",
              "      <td>32.9660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1500</th>\n",
              "      <td>6.612500</td>\n",
              "      <td>0.54020</td>\n",
              "      <td>31.7660</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1501 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              X        Y        Z\n",
              "0      0.000000  1.00000   1.0500\n",
              "1      0.048931  0.99832   1.0362\n",
              "2      0.095542  1.00310   1.0227\n",
              "3      0.140250  1.01400   1.0097\n",
              "4      0.183460  1.03090   0.9971\n",
              "...         ...      ...      ...\n",
              "1496  10.584000  3.81220  36.1470\n",
              "1497   9.564800  2.61540  35.2170\n",
              "1498   8.540200  1.68650  34.1340\n",
              "1499   7.547100  1.00680  32.9660\n",
              "1500   6.612500  0.54020  31.7660\n",
              "\n",
              "[1501 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG5Fyx5O5oe5"
      },
      "source": [
        "# Prépartion des datasets multivariés"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tShkj2wRIp6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a11ba842-d70f-454b-f3cd-d0800acd0060"
      },
      "source": [
        "serie_etude"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X</th>\n",
              "      <th>Y</th>\n",
              "      <th>Z</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.0500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.048931</td>\n",
              "      <td>0.99832</td>\n",
              "      <td>1.0362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.095542</td>\n",
              "      <td>1.00310</td>\n",
              "      <td>1.0227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.140250</td>\n",
              "      <td>1.01400</td>\n",
              "      <td>1.0097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.183460</td>\n",
              "      <td>1.03090</td>\n",
              "      <td>0.9971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1496</th>\n",
              "      <td>10.584000</td>\n",
              "      <td>3.81220</td>\n",
              "      <td>36.1470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1497</th>\n",
              "      <td>9.564800</td>\n",
              "      <td>2.61540</td>\n",
              "      <td>35.2170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1498</th>\n",
              "      <td>8.540200</td>\n",
              "      <td>1.68650</td>\n",
              "      <td>34.1340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1499</th>\n",
              "      <td>7.547100</td>\n",
              "      <td>1.00680</td>\n",
              "      <td>32.9660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1500</th>\n",
              "      <td>6.612500</td>\n",
              "      <td>0.54020</td>\n",
              "      <td>31.7660</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1501 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              X        Y        Z\n",
              "0      0.000000  1.00000   1.0500\n",
              "1      0.048931  0.99832   1.0362\n",
              "2      0.095542  1.00310   1.0227\n",
              "3      0.140250  1.01400   1.0097\n",
              "4      0.183460  1.03090   0.9971\n",
              "...         ...      ...      ...\n",
              "1496  10.584000  3.81220  36.1470\n",
              "1497   9.564800  2.61540  35.2170\n",
              "1498   8.540200  1.68650  34.1340\n",
              "1499   7.547100  1.00680  32.9660\n",
              "1500   6.612500  0.54020  31.7660\n",
              "\n",
              "[1501 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7cGUeWb5oe7"
      },
      "source": [
        "**1. Séparation des données en données pour l'entrainement et la validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob-EAw_j5oe8"
      },
      "source": [
        "On réserve 20% des données pour l'entrainement et le reste pour la validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5AWeK_Z5oe8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc8f2c27-ceb9-4d16-a18e-b395f6205a02"
      },
      "source": [
        "# Sépare les données en entrainement et tests\n",
        "pourcentage = 0.7\n",
        "temps_separation = int(len(serie_etude) * pourcentage)\n",
        "date_separation = serie_etude.index[temps_separation]\n",
        "\n",
        "serie_entrainement = []\n",
        "serie_test = []\n",
        "\n",
        "serie_entrainement.append(serie_etude['X'].iloc[:temps_separation])\n",
        "serie_test.append(serie_etude['X'].iloc[temps_separation:])\n",
        "\n",
        "serie_entrainement.append(serie_etude['Y'].iloc[:temps_separation])\n",
        "serie_test.append(serie_etude['Y'].iloc[temps_separation:])\n",
        "\n",
        "serie_entrainement.append(serie_etude['Z'].iloc[:temps_separation])\n",
        "serie_test.append(serie_etude['Z'].iloc[temps_separation:])\n",
        "\n",
        "print(\"Taille de l'entrainement : %d\" %len(serie_entrainement[0]))\n",
        "print(\"Taille de la validation : %d\" %len(serie_test[0]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Taille de l'entrainement : 1050\n",
            "Taille de la validation : 451\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZUMMMro5oe9"
      },
      "source": [
        "On normalise les données :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu_YxoSI5oe9"
      },
      "source": [
        "# Calcul de la moyenne et de l'écart type de la série\n",
        "mean = tf.math.reduce_mean(np.asarray(serie_entrainement))\n",
        "std = tf.math.reduce_std(np.asarray((serie_entrainement)))\n",
        "\n",
        "# Normalisation des données\n",
        "serie_entrainement = (serie_entrainement-mean)/std\n",
        "serie_test = (serie_test-mean)/std"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_4OZJ-p5oe9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b8e05ce3-2c46-4136-a9d9-ff07f952f4ce"
      },
      "source": [
        "# Affiche la série\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "\n",
        "ax.plot(serie_etude.index[:temps_separation].values,serie_entrainement[0], label=\"X_Ent\")\n",
        "ax.plot(serie_etude.index[:temps_separation].values,serie_entrainement[1], label=\"Y_Ent\")\n",
        "ax.plot(serie_etude.index[:temps_separation].values,serie_entrainement[2], label=\"Z_Ent\")\n",
        "\n",
        "ax.plot(serie_etude.index[temps_separation:].values,serie_test[0], label=\"X_Val\")\n",
        "ax.plot(serie_etude.index[temps_separation:].values,serie_test[1], label=\"Y_Val\")\n",
        "ax.plot(serie_etude.index[temps_separation:].values,serie_test[2], label=\"Z_Val\")\n",
        "\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-bANnT35oe-"
      },
      "source": [
        "**2. Création des datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wqy52B5xSXDz"
      },
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "\n",
        "def prepare_dataset_XY(series, taille_fenetre, horizon, batch_size):\n",
        "  serie_concat = tf.expand_dims(series[0],1)\n",
        "\n",
        "  for i in range(1,len(series)):\n",
        "    serie_ = tf.expand_dims(series[i],1)\n",
        "    serie_concat = tf.concat([serie_concat,serie_],1)\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie_concat)\n",
        "  dataset = dataset.window(taille_fenetre+horizon, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre + horizon))\n",
        "  dataset = dataset.map(lambda x: (x[0:taille_fenetre],tf.expand_dims(x[-taille_fenetre:][:,0],1)))\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "  return dataset"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Jh1RZYo5oe_"
      },
      "source": [
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "taille_fenetre = 16\n",
        "horizon = 1\n",
        "batch_size = 64\n",
        "\n",
        "# Création du dataset\n",
        "dataset = prepare_dataset_XY(serie_entrainement,taille_fenetre,horizon,batch_size)\n",
        "dataset_val = prepare_dataset_XY(serie_test,taille_fenetre,horizon,batch_size)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr2pbMox5oe_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61335896-a6d9-4003-d088-e7297b3bdb61"
      },
      "source": [
        "print(len(list(dataset.as_numpy_iterator())))\n",
        "for element in dataset.take(1):\n",
        "  print(element[0].shape)\n",
        "  print(element[1].shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16\n",
            "(64, 16, 3)\n",
            "(64, 16, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQejjkaeeHxe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "609a8475-74b2-4610-ac57-504a3f57e0ec"
      },
      "source": [
        "for element in dataset.take(1):\n",
        "  print(element)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(<tf.Tensor: shape=(64, 16, 3), dtype=float64, numpy=\n",
            "array([[[-0.39977373, -0.33569354, -0.33248953],\n",
            "        [-0.39663822, -0.33580119, -0.33337384],\n",
            "        [-0.39365138, -0.33549489, -0.33423892],\n",
            "        ...,\n",
            "        [-0.31992341, -0.22381594, -0.34390734],\n",
            "        [-0.30510807, -0.19289085, -0.34235275],\n",
            "        [-0.28778079, -0.15628824, -0.33942813]],\n",
            "\n",
            "       [[-0.39663822, -0.33580119, -0.33337384],\n",
            "        [-0.39365138, -0.33549489, -0.33423892],\n",
            "        [-0.39078648, -0.33479642, -0.33507196],\n",
            "        ...,\n",
            "        [-0.30510807, -0.19289085, -0.34235275],\n",
            "        [-0.28778079, -0.15628824, -0.33942813],\n",
            "        [-0.26746096, -0.11311102, -0.33461699]],\n",
            "\n",
            "       [[-0.39365138, -0.33549489, -0.33423892],\n",
            "        [-0.39078648, -0.33479642, -0.33507196],\n",
            "        [-0.38801758, -0.33371346, -0.33587937],\n",
            "        ...,\n",
            "        [-0.28778079, -0.15628824, -0.33942813],\n",
            "        [-0.26746096, -0.11311102, -0.33461699],\n",
            "        [-0.23571563, -0.04558332, -0.32434494]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[-0.91069786, -1.00825354,  1.2106895 ],\n",
            "        [-0.93062039, -1.02522197,  1.2252357 ],\n",
            "        [-0.9497227 , -1.03993478,  1.24554912],\n",
            "        ...,\n",
            "        [-0.93399742, -0.85028947,  1.42497364],\n",
            "        [-0.90901896, -0.83355173,  1.38620513],\n",
            "        [-0.88769948, -0.82738081,  1.34301508]],\n",
            "\n",
            "       [[-0.93062039, -1.02522197,  1.2252357 ],\n",
            "        [-0.9497227 , -1.03993478,  1.24554912],\n",
            "        [-0.96769078, -1.05121289,  1.27079672],\n",
            "        ...,\n",
            "        [-0.90901896, -0.83355173,  1.38620513],\n",
            "        [-0.88769948, -0.82738081,  1.34301508],\n",
            "        [-0.87390302, -0.82981585,  1.30648938]],\n",
            "\n",
            "       [[-0.9497227 , -1.03993478,  1.24554912],\n",
            "        [-0.96769078, -1.05121289,  1.27079672],\n",
            "        [-0.98698532, -1.05877436,  1.30559226],\n",
            "        ...,\n",
            "        [-0.88769948, -0.82738081,  1.34301508],\n",
            "        [-0.87390302, -0.82981585,  1.30648938],\n",
            "        [-0.86464984, -0.83871659,  1.27156568]]])>, <tf.Tensor: shape=(64, 16, 1), dtype=float64, numpy=\n",
            "array([[[-0.39663822],\n",
            "        [-0.39365138],\n",
            "        [-0.39078648],\n",
            "        ...,\n",
            "        [-0.30510807],\n",
            "        [-0.28778079],\n",
            "        [-0.26746096]],\n",
            "\n",
            "       [[-0.39365138],\n",
            "        [-0.39078648],\n",
            "        [-0.38801758],\n",
            "        ...,\n",
            "        [-0.28778079],\n",
            "        [-0.26746096],\n",
            "        [-0.23571563]],\n",
            "\n",
            "       [[-0.39078648],\n",
            "        [-0.38801758],\n",
            "        [-0.38213245],\n",
            "        ...,\n",
            "        [-0.26746096],\n",
            "        [-0.23571563],\n",
            "        [-0.1966075 ]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[-0.93062039],\n",
            "        [-0.9497227 ],\n",
            "        [-0.96769078],\n",
            "        ...,\n",
            "        [-0.90901896],\n",
            "        [-0.88769948],\n",
            "        [-0.87390302]],\n",
            "\n",
            "       [[-0.9497227 ],\n",
            "        [-0.96769078],\n",
            "        [-0.98698532],\n",
            "        ...,\n",
            "        [-0.88769948],\n",
            "        [-0.87390302],\n",
            "        [-0.86464984]],\n",
            "\n",
            "       [[-0.96769078],\n",
            "        [-0.98698532],\n",
            "        [-1.00282595],\n",
            "        ...,\n",
            "        [-0.87390302],\n",
            "        [-0.86464984],\n",
            "        [-0.86033084]]])>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHCcYn6i5oe_"
      },
      "source": [
        "On extrait maintenant les deux tenseurs (X,Y) pour l'entrainement :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3ZhLIK15ofA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f048348-0444-4ccb-e776-8b35ccec736f"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset))              # #56x((1000,16,5),(1000,16,5)) => x = 56x(1000,16,5) ; y = 56x(1000,16,5)\n",
        "\n",
        "# Recombine les données\n",
        "x = np.asarray(x,dtype=np.float32)      # 56x(1000,16,5) => (56,1000,16,5)\n",
        "y = np.asarray(y,dtype=np.float32)      # 56x(1000,16,5) => (56,1000,16,5)\n",
        "\n",
        "x_train = np.asarray(tf.reshape(x,shape=(x.shape[0]*x.shape[1],taille_fenetre,x.shape[3])))     # (56,1000,16,5) => (56*1000,16,5)\n",
        "y_train = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],taille_fenetre,y.shape[3])))     # (56,1000,16,5) => (56*1000,16,5)\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1024, 16, 3)\n",
            "(1024, 16, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llnKyLvl5ofA"
      },
      "source": [
        "Puis la même chose pour les données de validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hadrKVrZ5ofB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dbc57bc-9f04-4746-fe33-30924c01cb25"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset_val))              # #56x((1000,16,5),(1000,16,1)) => x = 56x(1000,16,5) ; y = 56x(1000,16,1)\n",
        "\n",
        "# Recombine les données\n",
        "x = np.asarray(x,dtype=np.float32)      # 56x(1000,16,5) => (56,1000,16,5)\n",
        "y = np.asarray(y,dtype=np.float32)      # 56x(1000,16,1) => (56,1000,16,1)\n",
        "\n",
        "x_val = np.asarray(tf.reshape(x,shape=(x.shape[0]*x.shape[1],taille_fenetre,x.shape[3])))     # (56,1000,16,5) => (56*1000,16,5)\n",
        "y_val = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],taille_fenetre,y.shape[3])))     # (56,1000,16,1) => (56*1000,16,1)\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(384, 16, 3)\n",
            "(384, 16, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHfiXLFZhrPs"
      },
      "source": [
        "# Création du modèle type Wavenet Multivarié - Apprentissage univarié"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd9AzWFtjnjs"
      },
      "source": [
        "**1. Création du réseau**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDVH4xsDzM-V"
      },
      "source": [
        "from keras.layers import Conv1D\n",
        "from keras.layers import Conv1D\n",
        "from keras.utils.conv_utils import conv_output_length\n",
        "from keras import layers\n",
        "from keras.regularizers import l2\n",
        "import keras.backend as K\n",
        "from keras.engine import Input\n",
        "from keras.engine import Model"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCtJXABdzKB9"
      },
      "source": [
        "class CausalDilatedConv1D(Conv1D):\n",
        "    def __init__(self, nb_filter, filter_length, init='glorot_uniform', activation=None, weights=None,\n",
        "                 border_mode='valid', subsample_length=1, atrous_rate=1, W_regularizer=None, b_regularizer=None,\n",
        "                 activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True, causal=False, **kwargs):\n",
        "        super(CausalDilatedConv1D, self).__init__(nb_filter, filter_length, weights=weights, activation=activation, \n",
        "                padding=border_mode, strides=subsample_length, dilation_rate=atrous_rate, kernel_regularizer=W_regularizer, \n",
        "                bias_regularizer=b_regularizer, activity_regularizer=activity_regularizer, kernel_constraint=W_constraint, \n",
        "                bias_constraint=b_constraint, use_bias=bias, **kwargs)\n",
        "        self.causal = causal\n",
        "        self.nb_filter = nb_filter\n",
        "        self.atrous_rate = atrous_rate\n",
        "        self.filter_length = filter_length\n",
        "        self.subsample_length = subsample_length\n",
        "        self.border_mode = border_mode\n",
        "        if self.causal and border_mode != 'valid':\n",
        "            raise ValueError(\"Causal mode dictates border_mode=valid.\")\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        input_length = input_shape[1]\n",
        "        if self.causal:\n",
        "            input_length += self.atrous_rate * (self.filter_length - 1)\n",
        "        length = conv_output_length(input_length, self.filter_length, self.border_mode, self.strides[0], dilation=self.atrous_rate)\n",
        "        return (input_shape[0], length, self.nb_filter)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        if self.causal:\n",
        "            x = K.temporal_padding(x, padding=(self.atrous_rate * (self.filter_length - 1), 0))\n",
        "        # return super(CausalAtrousConvolution1D, self).call(x, mask)\n",
        "        return super(CausalDilatedConv1D, self).call(x)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s19zmYCgzTvV"
      },
      "source": [
        "def _compute_receptive_field(dilation_depth, stacks):\n",
        "  return stacks * (2 ** dilation_depth * 2) - (stacks - 1)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIX-o-lMzVK0"
      },
      "source": [
        "def build_model_residual_block(x, i, s,nb_filters, dim_filters, use_bias,res_l2):\n",
        "        original_x = x\n",
        "        relu_out = CausalDilatedConv1D(nb_filters, dim_filters, atrous_rate=2 ** i, border_mode='valid', causal=True, bias=True, name='dilated_conv_%d_relu_s%d' % (2 ** i, s), activation='relu', W_regularizer=l2(res_l2))(x)\n",
        "        res_x = layers.Conv1D(1, 1, padding='same', use_bias=use_bias, kernel_regularizer=l2(res_l2))(relu_out)\n",
        "        skip_x = layers.Conv1D(1, 1, padding='same', use_bias=use_bias, kernel_regularizer=l2(res_l2))(relu_out)\n",
        "        res_x = layers.Add()([original_x, res_x])\n",
        "        return res_x, skip_x\n",
        "\n",
        "def build_model_couche_condition(x, nb_input_bins, nb_filters, dim_filters, use_bias,res_l2):\n",
        "        original_x = x\n",
        "        skip_conditions = []\n",
        "\n",
        "        relu_out = CausalDilatedConv1D(nb_filters, dim_filters, atrous_rate=1, border_mode='valid', causal=True, bias=True, name='dilated_conv_%d_condition_%d_s%d' % (1,1, 0), activation='relu',\n",
        "                                       W_regularizer=l2(res_l2))(tf.expand_dims(x[:,:,0],2))\n",
        "\n",
        "        skip_ = layers.Conv1D(1, 1, padding='same', use_bias=use_bias, kernel_regularizer=l2(res_l2))(relu_out)\n",
        "        skip_conditions.append(skip_)\n",
        "\n",
        "        for i in range(1,nb_input_bins):\n",
        "          relu_out = CausalDilatedConv1D(nb_filters, dim_filters, atrous_rate=1, border_mode='valid', causal=True, bias=True, name='dilated_conv_%d_condition_%d_s%d' % (1,i+1,0), activation='relu',\n",
        "                                                    W_regularizer=l2(res_l2))(tf.expand_dims(x[:,:,i],2))\n",
        "\n",
        "          skip_ = layers.Conv1D(1, 1, padding='same', use_bias=use_bias, kernel_regularizer=l2(res_l2))(relu_out)\n",
        "          skip_conditions.append(skip_)\n",
        "\n",
        "        out = layers.Add()(skip_conditions)\n",
        "        return out\n",
        "\n",
        "\n",
        "def build_model(fragment_length, nb_filters, dim_filters, nb_input_bins, dilation_depth, stacks, use_bias, res_l2, final_l2):\n",
        "        input_shape = Input(shape=(fragment_length, nb_input_bins), name='input_part')\n",
        "        out = input_shape\n",
        "\n",
        "        for s in range(stacks):\n",
        "            if nb_input_bins > 1 :\n",
        "                # Couche conditionnée\n",
        "                out = build_model_couche_condition(out, nb_input_bins, nb_filters, dim_filters, use_bias, res_l2)\n",
        "            else:\n",
        "                out, skip_out = build_model_residual_block(out, 0, s , nb_filters, dim_filters, use_bias, res_l2)\n",
        "\n",
        "            # Couches intermédiaires\n",
        "            for i in range(1, dilation_depth + 1):\n",
        "                out, skip_out = build_model_residual_block(out, i, s, nb_filters, dim_filters, use_bias, res_l2)\n",
        "\n",
        "        # Couche de sortie\n",
        "        out = layers.Activation('linear', name=\"output_linear\")(out)\n",
        "        out = layers.Conv1D(1, 1, padding='same', kernel_regularizer=l2(final_l2))(out)\n",
        "        model = Model(input_shape, out)\n",
        "        return model\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH9rsD5UzZgM"
      },
      "source": [
        "**2. Construction du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln6eVTFezYlq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a1b8e40-2e6f-46be-bf96-808cd95c3ed6"
      },
      "source": [
        "def compute_receptive_field_(dilation_depth, nb_stacks):\n",
        "    receptive_field = nb_stacks * (2 ** dilation_depth * 2) - (nb_stacks - 1)\n",
        "    return receptive_field\n",
        "\n",
        "nb_filters = 2\n",
        "dim_filters = 2\n",
        "nb_input_bins = 1\n",
        "dilation_depth = 3\n",
        "nb_stacks = 1\n",
        "use_bias = False\n",
        "res_l2 = 0\n",
        "final_l2 = 0\n",
        "\n",
        "fragment_length = compute_receptive_field_(dilation_depth, nb_stacks)\n",
        "fragment_length\n",
        "\n",
        "model = build_model(fragment_length, nb_filters, dim_filters, nb_input_bins, dilation_depth, nb_stacks, use_bias, res_l2, final_l2)\n",
        "model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_part (InputLayer)         [(None, 16, 1)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_1_relu_s0 (CausalD (None, 16, 2)        6           input_part[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d (Conv1D)                 (None, 16, 1)        2           dilated_conv_1_relu_s0[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 16, 1)        0           input_part[0][0]                 \n",
            "                                                                 conv1d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_2_relu_s0 (CausalD (None, 16, 2)        6           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 16, 1)        2           dilated_conv_2_relu_s0[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 16, 1)        0           add[0][0]                        \n",
            "                                                                 conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_4_relu_s0 (CausalD (None, 16, 2)        6           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, 16, 1)        2           dilated_conv_4_relu_s0[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 16, 1)        0           add_1[0][0]                      \n",
            "                                                                 conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_8_relu_s0 (CausalD (None, 16, 2)        6           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 16, 1)        2           dilated_conv_8_relu_s0[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 16, 1)        0           add_2[0][0]                      \n",
            "                                                                 conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "output_linear (Activation)      (None, 16, 1)        0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_8 (Conv1D)               (None, 16, 1)        2           output_linear[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 34\n",
            "Trainable params: 34\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_azfJaeUo2nU"
      },
      "source": [
        "**2. Optimisation de l'apprentissage**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf3lwaQBnjxL"
      },
      "source": [
        "Pour accélérer le traitement des données, nous n'allons pas utiliser l'intégralité des données pendant la mise à jour du gradient, comme cela a été fait jusqu'à présent (en utilisant le dataset).  \n",
        "Cette fois-ci, nous allons forcer les mises à jour du gradient à se produire de manière moins fréquente en attribuant la valeur du batch_size à prendre en compte lors de la regression du modèle.  \n",
        "Pour cela, on utilise l'argument \"batch_size\" dans la méthode fit. En précisant un batch_size=1000, cela signifie que :\n",
        " - Sur notre total de 56000 échantillons, 56 seront utilisés pour les calculs du gradient\n",
        " - Il y aura également 56 itérations à chaque période.\n",
        "  \n",
        "    \n",
        "    \n",
        "Si nous avions pris le dataset comme entrée, nous aurions eu :\n",
        "- Un total de 56000 échantillons également\n",
        "- Chaque période aurait également pris 56 itérations pour se compléter\n",
        "- Mais 1000 échantillons auraient été utilisés pour le calcul du gradient, au lieu de 56 avec la méthode utilisée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6Z35rNWj5SA"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "def  My_MSE(y_true,y_pred):\n",
        "  return(tf.keras.metrics.mse(y_true,y_pred)*std.numpy()+mean.numpy())\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam()\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"mse\", optimizer=optimiseur, metrics=[\"mse\",My_MSE])\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint],batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2aP9J3TkNGG"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 1])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZxCgpuYkQ2Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "881b6e34-11d1-4a3c-a6c1-630cd7855d83"
      },
      "source": [
        "# Chargement des poids sauvegardés\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-235-a7575e4f6b23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Chargement des poids sauvegardés\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"poids.hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2225\u001b[0m           'first, then load the weights.')\n\u001b[1;32m   2226\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_weights_created\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2227\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2228\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'layer_names'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'model_weights'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2229\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'poids.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmdbo23qkTKE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b835c88-d31b-42ee-e985-f2209e21583c"
      },
      "source": [
        "max_periodes = 1000\n",
        "\n",
        "# Classe permettant d'arrêter l'entrainement si la variation\n",
        "# devient plus petite qu'une valeur à choisir sur un nombre\n",
        "# de périodes à choisir\n",
        "class StopTrain(keras.callbacks.Callback):\n",
        "    def __init__(self, delta=0.01,periodes=100, term=\"loss\", logs={}):\n",
        "      self.n_periodes = 0\n",
        "      self.periodes = periodes\n",
        "      self.loss_1 = 100\n",
        "      self.delta = delta\n",
        "      self.term = term\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "      diff_loss = abs(self.loss_1 - logs[self.term])\n",
        "      self.loss_1 = logs[self.term]\n",
        "      if (diff_loss < self.delta):\n",
        "        self.n_periodes = self.n_periodes + 1\n",
        "      else:\n",
        "        self.n_periodes = 0\n",
        "      if (self.n_periodes == self.periodes):\n",
        "        print(\"Arrêt de l'entrainement...\")\n",
        "        self.model.stop_training = True\n",
        "\n",
        "def  My_MSE(y_true,y_pred):\n",
        "  return(tf.keras.metrics.mse(y_true,y_pred)*std.numpy()+mean.numpy())\n",
        "  \n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.001,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"mse\", optimizer=optimiseur, metrics=[\"mse\",My_MSE])\n",
        "\n",
        "# Entraine le modèle, avec une réduction des calculs du gradient\n",
        "if nb_input_bins == 1:\n",
        "  historique = model.fit(x=tf.expand_dims(x_train[:,:,0],2),y=y_train,validation_data=(tf.expand_dims(x_val[:,:,0],2),y_val), epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=1e-6,periodes = 50, term=\"loss\")],batch_size=batch_size)\n",
        "else:\n",
        "  historique = model.fit(x=x_train,y=y_train,validation_data=(x_val,y_val), epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=1e-5,periodes = 10, term=\"My_MSE\")],batch_size=batch_size)\n",
        "\n",
        "\n",
        "# Entraine le modèle sans réduction de calculs\n",
        "#historique = model.fit(dataset,validation_data=dataset_val, epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=1e-8,periodes = 10, term=\"val_My_MSE\")])\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "16/16 [==============================] - 1s 21ms/step - loss: 0.0014 - mse: 0.0014 - My_MSE: 6.2600 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2635\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.00141, saving model to poids_train.hdf5\n",
            "Epoch 2/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0014 - mse: 0.0014 - My_MSE: 6.2602 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2640\n",
            "\n",
            "Epoch 00002: loss improved from 0.00141 to 0.00138, saving model to poids_train.hdf5\n",
            "Epoch 3/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2590 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2636\n",
            "\n",
            "Epoch 00003: loss improved from 0.00138 to 0.00136, saving model to poids_train.hdf5\n",
            "Epoch 4/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0014 - mse: 0.0014 - My_MSE: 6.2598 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2635\n",
            "\n",
            "Epoch 00004: loss improved from 0.00136 to 0.00135, saving model to poids_train.hdf5\n",
            "Epoch 5/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2594 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2634\n",
            "\n",
            "Epoch 00005: loss improved from 0.00135 to 0.00134, saving model to poids_train.hdf5\n",
            "Epoch 6/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2592 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2643\n",
            "\n",
            "Epoch 00006: loss did not improve from 0.00134\n",
            "Epoch 7/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0014 - mse: 0.0014 - My_MSE: 6.2599 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2634\n",
            "\n",
            "Epoch 00007: loss did not improve from 0.00134\n",
            "Epoch 8/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0014 - mse: 0.0014 - My_MSE: 6.2601 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2633\n",
            "\n",
            "Epoch 00008: loss improved from 0.00134 to 0.00134, saving model to poids_train.hdf5\n",
            "Epoch 9/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2585 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2633\n",
            "\n",
            "Epoch 00009: loss improved from 0.00134 to 0.00133, saving model to poids_train.hdf5\n",
            "Epoch 10/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2591 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2636\n",
            "\n",
            "Epoch 00010: loss improved from 0.00133 to 0.00132, saving model to poids_train.hdf5\n",
            "Epoch 11/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2592 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2634\n",
            "\n",
            "Epoch 00011: loss did not improve from 0.00132\n",
            "Epoch 12/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2590 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2635\n",
            "\n",
            "Epoch 00012: loss improved from 0.00132 to 0.00132, saving model to poids_train.hdf5\n",
            "Epoch 13/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2588 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2632\n",
            "\n",
            "Epoch 00013: loss did not improve from 0.00132\n",
            "Epoch 14/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2588 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2632\n",
            "\n",
            "Epoch 00014: loss improved from 0.00132 to 0.00132, saving model to poids_train.hdf5\n",
            "Epoch 15/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2591 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2631\n",
            "\n",
            "Epoch 00015: loss improved from 0.00132 to 0.00132, saving model to poids_train.hdf5\n",
            "Epoch 16/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2583 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2631\n",
            "\n",
            "Epoch 00016: loss improved from 0.00132 to 0.00131, saving model to poids_train.hdf5\n",
            "Epoch 17/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2594 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2632\n",
            "\n",
            "Epoch 00017: loss did not improve from 0.00131\n",
            "Epoch 18/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2589 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2632\n",
            "\n",
            "Epoch 00018: loss did not improve from 0.00131\n",
            "Epoch 19/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2591 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2630\n",
            "\n",
            "Epoch 00019: loss did not improve from 0.00131\n",
            "Epoch 20/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2593 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2634\n",
            "\n",
            "Epoch 00020: loss improved from 0.00131 to 0.00131, saving model to poids_train.hdf5\n",
            "Epoch 21/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2595 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2631\n",
            "\n",
            "Epoch 00021: loss improved from 0.00131 to 0.00131, saving model to poids_train.hdf5\n",
            "Epoch 22/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2590 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2631\n",
            "\n",
            "Epoch 00022: loss did not improve from 0.00131\n",
            "Epoch 23/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2585 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2632\n",
            "\n",
            "Epoch 00023: loss did not improve from 0.00131\n",
            "Epoch 24/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2595 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2633\n",
            "\n",
            "Epoch 00024: loss improved from 0.00131 to 0.00130, saving model to poids_train.hdf5\n",
            "Epoch 25/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2584 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2632\n",
            "\n",
            "Epoch 00025: loss did not improve from 0.00130\n",
            "Epoch 26/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2590 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2632\n",
            "\n",
            "Epoch 00026: loss did not improve from 0.00130\n",
            "Epoch 27/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2592 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2632\n",
            "\n",
            "Epoch 00027: loss did not improve from 0.00130\n",
            "Epoch 28/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2589 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2630\n",
            "\n",
            "Epoch 00028: loss did not improve from 0.00130\n",
            "Epoch 29/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2588 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2630\n",
            "\n",
            "Epoch 00029: loss improved from 0.00130 to 0.00129, saving model to poids_train.hdf5\n",
            "Epoch 30/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2590 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2629\n",
            "\n",
            "Epoch 00030: loss did not improve from 0.00129\n",
            "Epoch 31/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2594 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2629\n",
            "\n",
            "Epoch 00031: loss did not improve from 0.00129\n",
            "Epoch 32/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2592 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2630\n",
            "\n",
            "Epoch 00032: loss did not improve from 0.00129\n",
            "Epoch 33/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2582 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2635\n",
            "\n",
            "Epoch 00033: loss improved from 0.00129 to 0.00129, saving model to poids_train.hdf5\n",
            "Epoch 34/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2584 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2629\n",
            "\n",
            "Epoch 00034: loss did not improve from 0.00129\n",
            "Epoch 35/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2589 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2629\n",
            "\n",
            "Epoch 00035: loss improved from 0.00129 to 0.00129, saving model to poids_train.hdf5\n",
            "Epoch 36/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2584 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2632\n",
            "\n",
            "Epoch 00036: loss did not improve from 0.00129\n",
            "Epoch 37/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2589 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2630\n",
            "\n",
            "Epoch 00037: loss did not improve from 0.00129\n",
            "Epoch 38/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2587 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2633\n",
            "\n",
            "Epoch 00038: loss did not improve from 0.00129\n",
            "Epoch 39/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2586 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2630\n",
            "\n",
            "Epoch 00039: loss improved from 0.00129 to 0.00129, saving model to poids_train.hdf5\n",
            "Epoch 40/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2586 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2628\n",
            "\n",
            "Epoch 00040: loss did not improve from 0.00129\n",
            "Epoch 41/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2583 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2631\n",
            "\n",
            "Epoch 00041: loss improved from 0.00129 to 0.00128, saving model to poids_train.hdf5\n",
            "Epoch 42/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2586 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2628\n",
            "\n",
            "Epoch 00042: loss did not improve from 0.00128\n",
            "Epoch 43/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2583 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2628\n",
            "\n",
            "Epoch 00043: loss did not improve from 0.00128\n",
            "Epoch 44/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2588 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2629\n",
            "\n",
            "Epoch 00044: loss improved from 0.00128 to 0.00128, saving model to poids_train.hdf5\n",
            "Epoch 45/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2581 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2629\n",
            "\n",
            "Epoch 00045: loss improved from 0.00128 to 0.00128, saving model to poids_train.hdf5\n",
            "Epoch 46/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2585 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2627\n",
            "\n",
            "Epoch 00046: loss did not improve from 0.00128\n",
            "Epoch 47/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2586 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2631\n",
            "\n",
            "Epoch 00047: loss improved from 0.00128 to 0.00127, saving model to poids_train.hdf5\n",
            "Epoch 48/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2585 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2630\n",
            "\n",
            "Epoch 00048: loss did not improve from 0.00127\n",
            "Epoch 49/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2582 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2628\n",
            "\n",
            "Epoch 00049: loss did not improve from 0.00127\n",
            "Epoch 50/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2575 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2627\n",
            "\n",
            "Epoch 00050: loss improved from 0.00127 to 0.00127, saving model to poids_train.hdf5\n",
            "Epoch 51/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2594 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2629\n",
            "\n",
            "Epoch 00051: loss did not improve from 0.00127\n",
            "Epoch 52/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2581 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2627\n",
            "\n",
            "Epoch 00052: loss did not improve from 0.00127\n",
            "Epoch 53/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2587 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2636\n",
            "\n",
            "Epoch 00053: loss did not improve from 0.00127\n",
            "Epoch 54/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2586 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2631\n",
            "\n",
            "Epoch 00054: loss did not improve from 0.00127\n",
            "Epoch 55/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2590 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2626\n",
            "\n",
            "Epoch 00055: loss did not improve from 0.00127\n",
            "Epoch 56/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2590 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2626\n",
            "\n",
            "Epoch 00056: loss improved from 0.00127 to 0.00126, saving model to poids_train.hdf5\n",
            "Epoch 57/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2578 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2625\n",
            "\n",
            "Epoch 00057: loss improved from 0.00126 to 0.00126, saving model to poids_train.hdf5\n",
            "Epoch 58/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2580 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2626\n",
            "\n",
            "Epoch 00058: loss improved from 0.00126 to 0.00126, saving model to poids_train.hdf5\n",
            "Epoch 59/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2583 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2628\n",
            "\n",
            "Epoch 00059: loss did not improve from 0.00126\n",
            "Epoch 60/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2589 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2625\n",
            "\n",
            "Epoch 00060: loss did not improve from 0.00126\n",
            "Epoch 61/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2585 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2625\n",
            "\n",
            "Epoch 00061: loss did not improve from 0.00126\n",
            "Epoch 62/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2579 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2628\n",
            "\n",
            "Epoch 00062: loss did not improve from 0.00126\n",
            "Epoch 63/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2582 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2625\n",
            "\n",
            "Epoch 00063: loss improved from 0.00126 to 0.00126, saving model to poids_train.hdf5\n",
            "Epoch 64/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2579 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2626\n",
            "\n",
            "Epoch 00064: loss improved from 0.00126 to 0.00125, saving model to poids_train.hdf5\n",
            "Epoch 65/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2595 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2629\n",
            "\n",
            "Epoch 00065: loss did not improve from 0.00125\n",
            "Epoch 66/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2585 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2625\n",
            "\n",
            "Epoch 00066: loss did not improve from 0.00125\n",
            "Epoch 67/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2578 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2624\n",
            "\n",
            "Epoch 00067: loss did not improve from 0.00125\n",
            "Epoch 68/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2583 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2625\n",
            "\n",
            "Epoch 00068: loss improved from 0.00125 to 0.00125, saving model to poids_train.hdf5\n",
            "Epoch 69/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2578 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2624\n",
            "\n",
            "Epoch 00069: loss did not improve from 0.00125\n",
            "Epoch 70/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2583 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2626\n",
            "\n",
            "Epoch 00070: loss did not improve from 0.00125\n",
            "Epoch 71/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2584 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2625\n",
            "\n",
            "Epoch 00071: loss did not improve from 0.00125\n",
            "Epoch 72/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2579 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2624\n",
            "\n",
            "Epoch 00072: loss did not improve from 0.00125\n",
            "Epoch 73/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2581 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2623\n",
            "\n",
            "Epoch 00073: loss did not improve from 0.00125\n",
            "Epoch 74/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2586 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2623\n",
            "\n",
            "Epoch 00074: loss did not improve from 0.00125\n",
            "Epoch 75/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2580 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2625\n",
            "\n",
            "Epoch 00075: loss did not improve from 0.00125\n",
            "Epoch 76/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2586 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2624\n",
            "\n",
            "Epoch 00076: loss did not improve from 0.00125\n",
            "Epoch 77/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2577 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2623\n",
            "\n",
            "Epoch 00077: loss improved from 0.00125 to 0.00125, saving model to poids_train.hdf5\n",
            "Epoch 78/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2581 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2624\n",
            "\n",
            "Epoch 00078: loss improved from 0.00125 to 0.00124, saving model to poids_train.hdf5\n",
            "Epoch 79/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2583 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2624\n",
            "\n",
            "Epoch 00079: loss improved from 0.00124 to 0.00124, saving model to poids_train.hdf5\n",
            "Epoch 80/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2580 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2624\n",
            "\n",
            "Epoch 00080: loss did not improve from 0.00124\n",
            "Epoch 81/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2585 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2623\n",
            "\n",
            "Epoch 00081: loss did not improve from 0.00124\n",
            "Epoch 82/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2577 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2623\n",
            "\n",
            "Epoch 00082: loss did not improve from 0.00124\n",
            "Epoch 83/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2575 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2623\n",
            "\n",
            "Epoch 00083: loss did not improve from 0.00124\n",
            "Epoch 84/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2583 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2622\n",
            "\n",
            "Epoch 00084: loss did not improve from 0.00124\n",
            "Epoch 85/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2589 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2626\n",
            "\n",
            "Epoch 00085: loss improved from 0.00124 to 0.00124, saving model to poids_train.hdf5\n",
            "Epoch 86/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2586 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2623\n",
            "\n",
            "Epoch 00086: loss did not improve from 0.00124\n",
            "Epoch 87/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2584 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2624\n",
            "\n",
            "Epoch 00087: loss did not improve from 0.00124\n",
            "Epoch 88/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2580 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2622\n",
            "\n",
            "Epoch 00088: loss did not improve from 0.00124\n",
            "Epoch 89/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2578 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2622\n",
            "\n",
            "Epoch 00089: loss improved from 0.00124 to 0.00124, saving model to poids_train.hdf5\n",
            "Epoch 90/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2587 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2629\n",
            "\n",
            "Epoch 00090: loss did not improve from 0.00124\n",
            "Epoch 91/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2582 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2623\n",
            "\n",
            "Epoch 00091: loss did not improve from 0.00124\n",
            "Epoch 92/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2581 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2623\n",
            "\n",
            "Epoch 00092: loss did not improve from 0.00124\n",
            "Epoch 93/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2594 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2622\n",
            "\n",
            "Epoch 00093: loss did not improve from 0.00124\n",
            "Epoch 94/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2578 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2623\n",
            "\n",
            "Epoch 00094: loss did not improve from 0.00124\n",
            "Epoch 95/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2580 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2624\n",
            "\n",
            "Epoch 00095: loss did not improve from 0.00124\n",
            "Epoch 96/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2584 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2622\n",
            "\n",
            "Epoch 00096: loss did not improve from 0.00124\n",
            "Epoch 97/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2573 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2623\n",
            "\n",
            "Epoch 00097: loss did not improve from 0.00124\n",
            "Epoch 98/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2584 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2624\n",
            "\n",
            "Epoch 00098: loss did not improve from 0.00124\n",
            "Epoch 99/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2576 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2622\n",
            "\n",
            "Epoch 00099: loss did not improve from 0.00124\n",
            "Epoch 100/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2577 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2622\n",
            "\n",
            "Epoch 00100: loss did not improve from 0.00124\n",
            "Epoch 101/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2592 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2627\n",
            "\n",
            "Epoch 00101: loss did not improve from 0.00124\n",
            "Epoch 102/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2580 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2622\n",
            "\n",
            "Epoch 00102: loss did not improve from 0.00124\n",
            "Epoch 103/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2575 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2622\n",
            "\n",
            "Epoch 00103: loss improved from 0.00124 to 0.00123, saving model to poids_train.hdf5\n",
            "Epoch 104/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2577 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2622\n",
            "\n",
            "Epoch 00104: loss improved from 0.00123 to 0.00123, saving model to poids_train.hdf5\n",
            "Epoch 105/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2576 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2622\n",
            "\n",
            "Epoch 00105: loss improved from 0.00123 to 0.00123, saving model to poids_train.hdf5\n",
            "Epoch 106/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2584 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00106: loss improved from 0.00123 to 0.00123, saving model to poids_train.hdf5\n",
            "Epoch 107/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2582 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2622\n",
            "\n",
            "Epoch 00107: loss did not improve from 0.00123\n",
            "Epoch 108/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2577 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2625\n",
            "\n",
            "Epoch 00108: loss did not improve from 0.00123\n",
            "Epoch 109/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2575 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2623\n",
            "\n",
            "Epoch 00109: loss did not improve from 0.00123\n",
            "Epoch 110/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2580 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2623\n",
            "\n",
            "Epoch 00110: loss did not improve from 0.00123\n",
            "Epoch 111/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2581 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2622\n",
            "\n",
            "Epoch 00111: loss did not improve from 0.00123\n",
            "Epoch 112/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2583 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2622\n",
            "\n",
            "Epoch 00112: loss did not improve from 0.00123\n",
            "Epoch 113/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2581 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2622\n",
            "\n",
            "Epoch 00113: loss did not improve from 0.00123\n",
            "Epoch 114/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2584 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2622\n",
            "\n",
            "Epoch 00114: loss improved from 0.00123 to 0.00122, saving model to poids_train.hdf5\n",
            "Epoch 115/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2572 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00115: loss did not improve from 0.00122\n",
            "Epoch 116/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2577 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2622\n",
            "\n",
            "Epoch 00116: loss did not improve from 0.00122\n",
            "Epoch 117/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2583 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00117: loss did not improve from 0.00122\n",
            "Epoch 118/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2579 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00118: loss did not improve from 0.00122\n",
            "Epoch 119/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2577 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2624\n",
            "\n",
            "Epoch 00119: loss did not improve from 0.00122\n",
            "Epoch 120/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2579 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2622\n",
            "\n",
            "Epoch 00120: loss did not improve from 0.00122\n",
            "Epoch 121/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2575 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2622\n",
            "\n",
            "Epoch 00121: loss did not improve from 0.00122\n",
            "Epoch 122/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2589 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00122: loss did not improve from 0.00122\n",
            "Epoch 123/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2577 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2623\n",
            "\n",
            "Epoch 00123: loss improved from 0.00122 to 0.00122, saving model to poids_train.hdf5\n",
            "Epoch 124/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2577 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00124: loss did not improve from 0.00122\n",
            "Epoch 125/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2582 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00125: loss did not improve from 0.00122\n",
            "Epoch 126/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2573 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2624\n",
            "\n",
            "Epoch 00126: loss did not improve from 0.00122\n",
            "Epoch 127/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2582 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2622\n",
            "\n",
            "Epoch 00127: loss did not improve from 0.00122\n",
            "Epoch 128/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2582 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00128: loss did not improve from 0.00122\n",
            "Epoch 129/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2576 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2622\n",
            "\n",
            "Epoch 00129: loss improved from 0.00122 to 0.00122, saving model to poids_train.hdf5\n",
            "Epoch 130/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2578 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00130: loss did not improve from 0.00122\n",
            "Epoch 131/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2581 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00131: loss did not improve from 0.00122\n",
            "Epoch 132/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2578 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2624\n",
            "\n",
            "Epoch 00132: loss did not improve from 0.00122\n",
            "Epoch 133/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2575 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00133: loss did not improve from 0.00122\n",
            "Epoch 134/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2575 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2633\n",
            "\n",
            "Epoch 00134: loss did not improve from 0.00122\n",
            "Epoch 135/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2587 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00135: loss did not improve from 0.00122\n",
            "Epoch 136/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2578 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2623\n",
            "\n",
            "Epoch 00136: loss did not improve from 0.00122\n",
            "Epoch 137/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2578 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00137: loss did not improve from 0.00122\n",
            "Epoch 138/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2572 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2624\n",
            "\n",
            "Epoch 00138: loss did not improve from 0.00122\n",
            "Epoch 139/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2581 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2628\n",
            "\n",
            "Epoch 00139: loss did not improve from 0.00122\n",
            "Epoch 140/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2589 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00140: loss did not improve from 0.00122\n",
            "Epoch 141/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2585 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2625\n",
            "\n",
            "Epoch 00141: loss did not improve from 0.00122\n",
            "Epoch 142/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2573 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00142: loss did not improve from 0.00122\n",
            "Epoch 143/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2578 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2620\n",
            "\n",
            "Epoch 00143: loss did not improve from 0.00122\n",
            "Epoch 144/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2581 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00144: loss did not improve from 0.00122\n",
            "Epoch 145/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2579 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2625\n",
            "\n",
            "Epoch 00145: loss did not improve from 0.00122\n",
            "Epoch 146/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2587 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00146: loss did not improve from 0.00122\n",
            "Epoch 147/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2577 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00147: loss did not improve from 0.00122\n",
            "Epoch 148/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2574 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00148: loss did not improve from 0.00122\n",
            "Epoch 149/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2573 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2622\n",
            "\n",
            "Epoch 00149: loss did not improve from 0.00122\n",
            "Epoch 150/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2580 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2624\n",
            "\n",
            "Epoch 00150: loss did not improve from 0.00122\n",
            "Epoch 151/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2580 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2629\n",
            "\n",
            "Epoch 00151: loss did not improve from 0.00122\n",
            "Epoch 152/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2584 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2623\n",
            "\n",
            "Epoch 00152: loss did not improve from 0.00122\n",
            "Epoch 153/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2572 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2622\n",
            "\n",
            "Epoch 00153: loss did not improve from 0.00122\n",
            "Epoch 154/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2580 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2622\n",
            "\n",
            "Epoch 00154: loss did not improve from 0.00122\n",
            "Epoch 155/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2588 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2622\n",
            "\n",
            "Epoch 00155: loss did not improve from 0.00122\n",
            "Epoch 156/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2582 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2620\n",
            "\n",
            "Epoch 00156: loss did not improve from 0.00122\n",
            "Epoch 157/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2577 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00157: loss did not improve from 0.00122\n",
            "Epoch 158/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2574 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00158: loss did not improve from 0.00122\n",
            "Epoch 159/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2586 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00159: loss did not improve from 0.00122\n",
            "Epoch 160/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2579 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2623\n",
            "\n",
            "Epoch 00160: loss did not improve from 0.00122\n",
            "Epoch 161/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2575 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00161: loss did not improve from 0.00122\n",
            "Epoch 162/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2575 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00162: loss did not improve from 0.00122\n",
            "Epoch 163/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2584 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2620\n",
            "\n",
            "Epoch 00163: loss did not improve from 0.00122\n",
            "Epoch 164/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2578 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00164: loss improved from 0.00122 to 0.00122, saving model to poids_train.hdf5\n",
            "Epoch 165/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2576 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2627\n",
            "\n",
            "Epoch 00165: loss did not improve from 0.00122\n",
            "Epoch 166/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2577 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2627\n",
            "\n",
            "Epoch 00166: loss did not improve from 0.00122\n",
            "Epoch 167/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2579 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2626\n",
            "\n",
            "Epoch 00167: loss did not improve from 0.00122\n",
            "Epoch 168/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2580 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2620\n",
            "\n",
            "Epoch 00168: loss did not improve from 0.00122\n",
            "Epoch 169/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2574 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2620\n",
            "\n",
            "Epoch 00169: loss did not improve from 0.00122\n",
            "Epoch 170/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2578 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2620\n",
            "\n",
            "Epoch 00170: loss did not improve from 0.00122\n",
            "Epoch 171/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2585 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2622\n",
            "\n",
            "Epoch 00171: loss did not improve from 0.00122\n",
            "Epoch 172/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2573 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2625\n",
            "\n",
            "Epoch 00172: loss did not improve from 0.00122\n",
            "Epoch 173/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2572 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2620\n",
            "\n",
            "Epoch 00173: loss did not improve from 0.00122\n",
            "Epoch 174/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2575 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2620\n",
            "\n",
            "Epoch 00174: loss did not improve from 0.00122\n",
            "Epoch 175/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2576 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2622\n",
            "\n",
            "Epoch 00175: loss did not improve from 0.00122\n",
            "Epoch 176/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2583 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00176: loss did not improve from 0.00122\n",
            "Epoch 177/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2620\n",
            "\n",
            "Epoch 00177: loss did not improve from 0.00122\n",
            "Epoch 178/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2573 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2620\n",
            "\n",
            "Epoch 00178: loss did not improve from 0.00122\n",
            "Epoch 179/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2582 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2620\n",
            "\n",
            "Epoch 00179: loss did not improve from 0.00122\n",
            "Epoch 180/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2575 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2620\n",
            "\n",
            "Epoch 00180: loss did not improve from 0.00122\n",
            "Epoch 181/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2576 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2620\n",
            "\n",
            "Epoch 00181: loss did not improve from 0.00122\n",
            "Epoch 182/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2578 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00182: loss did not improve from 0.00122\n",
            "Epoch 183/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2583 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00183: loss did not improve from 0.00122\n",
            "Epoch 184/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2578 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2622\n",
            "\n",
            "Epoch 00184: loss did not improve from 0.00122\n",
            "Epoch 185/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2577 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2620\n",
            "\n",
            "Epoch 00185: loss did not improve from 0.00122\n",
            "Epoch 186/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2581 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00186: loss did not improve from 0.00122\n",
            "Epoch 187/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2584 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00187: loss did not improve from 0.00122\n",
            "Epoch 188/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2580 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2619\n",
            "\n",
            "Epoch 00188: loss did not improve from 0.00122\n",
            "Epoch 189/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2579 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2619\n",
            "\n",
            "Epoch 00189: loss did not improve from 0.00122\n",
            "Epoch 190/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2581 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2619\n",
            "\n",
            "Epoch 00190: loss did not improve from 0.00122\n",
            "Epoch 191/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2582 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2620\n",
            "\n",
            "Epoch 00191: loss did not improve from 0.00122\n",
            "Epoch 192/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2580 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2626\n",
            "\n",
            "Epoch 00192: loss did not improve from 0.00122\n",
            "Epoch 193/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2588 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2619\n",
            "\n",
            "Epoch 00193: loss did not improve from 0.00122\n",
            "Epoch 194/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2576 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2619\n",
            "\n",
            "Epoch 00194: loss did not improve from 0.00122\n",
            "Epoch 195/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2578 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2619\n",
            "\n",
            "Epoch 00195: loss did not improve from 0.00122\n",
            "Epoch 196/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2578 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2620\n",
            "\n",
            "Epoch 00196: loss did not improve from 0.00122\n",
            "Epoch 197/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2575 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2625\n",
            "\n",
            "Epoch 00197: loss did not improve from 0.00122\n",
            "Epoch 198/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2591 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2626\n",
            "\n",
            "Epoch 00198: loss did not improve from 0.00122\n",
            "Epoch 199/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2572 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2619\n",
            "\n",
            "Epoch 00199: loss did not improve from 0.00122\n",
            "Epoch 200/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2577 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2622\n",
            "\n",
            "Epoch 00200: loss did not improve from 0.00122\n",
            "Epoch 201/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2581 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2618\n",
            "\n",
            "Epoch 00201: loss did not improve from 0.00122\n",
            "Epoch 202/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2577 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2620\n",
            "\n",
            "Epoch 00202: loss did not improve from 0.00122\n",
            "Epoch 203/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2579 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2618\n",
            "\n",
            "Epoch 00203: loss did not improve from 0.00122\n",
            "Epoch 204/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2585 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2618\n",
            "\n",
            "Epoch 00204: loss did not improve from 0.00122\n",
            "Epoch 205/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2579 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2619\n",
            "\n",
            "Epoch 00205: loss did not improve from 0.00122\n",
            "Epoch 206/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2575 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00206: loss improved from 0.00122 to 0.00121, saving model to poids_train.hdf5\n",
            "Epoch 207/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2577 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2619\n",
            "\n",
            "Epoch 00207: loss did not improve from 0.00121\n",
            "Epoch 208/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2584 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2618\n",
            "\n",
            "Epoch 00208: loss did not improve from 0.00121\n",
            "Epoch 209/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2576 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2618\n",
            "\n",
            "Epoch 00209: loss did not improve from 0.00121\n",
            "Epoch 210/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2576 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2617\n",
            "\n",
            "Epoch 00210: loss did not improve from 0.00121\n",
            "Epoch 211/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2571 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00211: loss did not improve from 0.00121\n",
            "Epoch 212/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2583 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2618\n",
            "\n",
            "Epoch 00212: loss did not improve from 0.00121\n",
            "Epoch 213/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2583 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00213: loss did not improve from 0.00121\n",
            "Epoch 214/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2581 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2617\n",
            "\n",
            "Epoch 00214: loss did not improve from 0.00121\n",
            "Epoch 215/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2574 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2618\n",
            "\n",
            "Epoch 00215: loss did not improve from 0.00121\n",
            "Epoch 216/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2576 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2618\n",
            "\n",
            "Epoch 00216: loss did not improve from 0.00121\n",
            "Epoch 217/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2570 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2618\n",
            "\n",
            "Epoch 00217: loss did not improve from 0.00121\n",
            "Epoch 218/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2575 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2619\n",
            "\n",
            "Epoch 00218: loss did not improve from 0.00121\n",
            "Epoch 219/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2576 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2618\n",
            "\n",
            "Epoch 00219: loss did not improve from 0.00121\n",
            "Epoch 220/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2582 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2620\n",
            "\n",
            "Epoch 00220: loss did not improve from 0.00121\n",
            "Epoch 221/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2583 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2617\n",
            "\n",
            "Epoch 00221: loss did not improve from 0.00121\n",
            "Epoch 222/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2565 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00222: loss did not improve from 0.00121\n",
            "Epoch 223/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2575 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2617\n",
            "\n",
            "Epoch 00223: loss did not improve from 0.00121\n",
            "Epoch 224/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2581 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2620\n",
            "\n",
            "Epoch 00224: loss did not improve from 0.00121\n",
            "Epoch 225/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2582 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00225: loss improved from 0.00121 to 0.00121, saving model to poids_train.hdf5\n",
            "Epoch 226/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2583 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2618\n",
            "\n",
            "Epoch 00226: loss did not improve from 0.00121\n",
            "Epoch 227/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2584 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2619\n",
            "\n",
            "Epoch 00227: loss improved from 0.00121 to 0.00121, saving model to poids_train.hdf5\n",
            "Epoch 228/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2579 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2617\n",
            "\n",
            "Epoch 00228: loss did not improve from 0.00121\n",
            "Epoch 229/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2580 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2627\n",
            "\n",
            "Epoch 00229: loss did not improve from 0.00121\n",
            "Epoch 230/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2586 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2617\n",
            "\n",
            "Epoch 00230: loss did not improve from 0.00121\n",
            "Epoch 231/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2580 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2619\n",
            "\n",
            "Epoch 00231: loss did not improve from 0.00121\n",
            "Epoch 232/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2578 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2618\n",
            "\n",
            "Epoch 00232: loss improved from 0.00121 to 0.00121, saving model to poids_train.hdf5\n",
            "Epoch 233/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2571 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2617\n",
            "\n",
            "Epoch 00233: loss improved from 0.00121 to 0.00121, saving model to poids_train.hdf5\n",
            "Epoch 234/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2570 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2620\n",
            "\n",
            "Epoch 00234: loss improved from 0.00121 to 0.00120, saving model to poids_train.hdf5\n",
            "Epoch 235/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2581 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2617\n",
            "\n",
            "Epoch 00235: loss did not improve from 0.00120\n",
            "Epoch 236/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2574 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2617\n",
            "\n",
            "Epoch 00236: loss did not improve from 0.00120\n",
            "Epoch 237/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2577 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2617\n",
            "\n",
            "Epoch 00237: loss improved from 0.00120 to 0.00120, saving model to poids_train.hdf5\n",
            "Epoch 238/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2576 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2618\n",
            "\n",
            "Epoch 00238: loss did not improve from 0.00120\n",
            "Epoch 239/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2576 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2617\n",
            "\n",
            "Epoch 00239: loss did not improve from 0.00120\n",
            "Epoch 240/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2576 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2618\n",
            "\n",
            "Epoch 00240: loss improved from 0.00120 to 0.00120, saving model to poids_train.hdf5\n",
            "Epoch 241/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2570 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2620\n",
            "\n",
            "Epoch 00241: loss did not improve from 0.00120\n",
            "Epoch 242/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2576 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2617\n",
            "\n",
            "Epoch 00242: loss did not improve from 0.00120\n",
            "Epoch 243/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2577 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2623\n",
            "\n",
            "Epoch 00243: loss did not improve from 0.00120\n",
            "Epoch 244/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2583 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2617\n",
            "\n",
            "Epoch 00244: loss did not improve from 0.00120\n",
            "Epoch 245/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2574 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2619\n",
            "\n",
            "Epoch 00245: loss did not improve from 0.00120\n",
            "Epoch 246/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2570 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2617\n",
            "\n",
            "Epoch 00246: loss did not improve from 0.00120\n",
            "Epoch 247/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2571 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2617\n",
            "\n",
            "Epoch 00247: loss improved from 0.00120 to 0.00120, saving model to poids_train.hdf5\n",
            "Epoch 248/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2575 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2617\n",
            "\n",
            "Epoch 00248: loss did not improve from 0.00120\n",
            "Epoch 249/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2578 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2616\n",
            "\n",
            "Epoch 00249: loss did not improve from 0.00120\n",
            "Epoch 250/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2571 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2617\n",
            "\n",
            "Epoch 00250: loss did not improve from 0.00120\n",
            "Epoch 251/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2572 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2618\n",
            "\n",
            "Epoch 00251: loss did not improve from 0.00120\n",
            "Epoch 252/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2580 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2616\n",
            "\n",
            "Epoch 00252: loss did not improve from 0.00120\n",
            "Epoch 253/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2579 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2616\n",
            "\n",
            "Epoch 00253: loss did not improve from 0.00120\n",
            "Epoch 254/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2617\n",
            "\n",
            "Epoch 00254: loss did not improve from 0.00120\n",
            "Epoch 255/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2581 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2619\n",
            "\n",
            "Epoch 00255: loss did not improve from 0.00120\n",
            "Epoch 256/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2579 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2618\n",
            "\n",
            "Epoch 00256: loss did not improve from 0.00120\n",
            "Epoch 257/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2580 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2617\n",
            "\n",
            "Epoch 00257: loss did not improve from 0.00120\n",
            "Epoch 258/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2574 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2618\n",
            "\n",
            "Epoch 00258: loss did not improve from 0.00120\n",
            "Epoch 259/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2575 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2616\n",
            "\n",
            "Epoch 00259: loss did not improve from 0.00120\n",
            "Epoch 260/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2575 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2616\n",
            "\n",
            "Epoch 00260: loss did not improve from 0.00120\n",
            "Epoch 261/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2575 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2616\n",
            "\n",
            "Epoch 00261: loss improved from 0.00120 to 0.00120, saving model to poids_train.hdf5\n",
            "Epoch 262/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2616\n",
            "\n",
            "Epoch 00262: loss did not improve from 0.00120\n",
            "Epoch 263/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2566 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2618\n",
            "\n",
            "Epoch 00263: loss did not improve from 0.00120\n",
            "Epoch 264/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2570 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2617\n",
            "\n",
            "Epoch 00264: loss did not improve from 0.00120\n",
            "Epoch 265/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2577 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00265: loss did not improve from 0.00120\n",
            "Epoch 266/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2574 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2616\n",
            "\n",
            "Epoch 00266: loss improved from 0.00120 to 0.00120, saving model to poids_train.hdf5\n",
            "Epoch 267/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2577 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2617\n",
            "\n",
            "Epoch 00267: loss did not improve from 0.00120\n",
            "Epoch 268/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2576 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2616\n",
            "\n",
            "Epoch 00268: loss improved from 0.00120 to 0.00120, saving model to poids_train.hdf5\n",
            "Epoch 269/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2616\n",
            "\n",
            "Epoch 00269: loss improved from 0.00120 to 0.00119, saving model to poids_train.hdf5\n",
            "Epoch 270/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2572 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2616\n",
            "\n",
            "Epoch 00270: loss did not improve from 0.00119\n",
            "Epoch 271/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2573 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2618\n",
            "\n",
            "Epoch 00271: loss did not improve from 0.00119\n",
            "Epoch 272/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2573 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2615\n",
            "\n",
            "Epoch 00272: loss did not improve from 0.00119\n",
            "Epoch 273/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2617\n",
            "\n",
            "Epoch 00273: loss did not improve from 0.00119\n",
            "Epoch 274/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2571 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2615\n",
            "\n",
            "Epoch 00274: loss did not improve from 0.00119\n",
            "Epoch 275/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2581 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2616\n",
            "\n",
            "Epoch 00275: loss did not improve from 0.00119\n",
            "Epoch 276/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2573 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2616\n",
            "\n",
            "Epoch 00276: loss did not improve from 0.00119\n",
            "Epoch 277/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2572 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2615\n",
            "\n",
            "Epoch 00277: loss did not improve from 0.00119\n",
            "Epoch 278/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2573 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2617\n",
            "\n",
            "Epoch 00278: loss did not improve from 0.00119\n",
            "Epoch 279/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2573 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2615\n",
            "\n",
            "Epoch 00279: loss did not improve from 0.00119\n",
            "Epoch 280/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2574 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2617\n",
            "\n",
            "Epoch 00280: loss did not improve from 0.00119\n",
            "Epoch 281/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2578 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2616\n",
            "\n",
            "Epoch 00281: loss did not improve from 0.00119\n",
            "Epoch 282/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2570 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2616\n",
            "\n",
            "Epoch 00282: loss did not improve from 0.00119\n",
            "Epoch 283/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2572 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2615\n",
            "\n",
            "Epoch 00283: loss did not improve from 0.00119\n",
            "Epoch 284/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2572 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2615\n",
            "\n",
            "Epoch 00284: loss did not improve from 0.00119\n",
            "Epoch 285/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2572 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2615\n",
            "\n",
            "Epoch 00285: loss did not improve from 0.00119\n",
            "Epoch 286/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2576 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2615\n",
            "\n",
            "Epoch 00286: loss did not improve from 0.00119\n",
            "Epoch 287/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2566 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2617\n",
            "\n",
            "Epoch 00287: loss did not improve from 0.00119\n",
            "Epoch 288/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2571 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2617\n",
            "\n",
            "Epoch 00288: loss did not improve from 0.00119\n",
            "Epoch 289/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2575 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2617\n",
            "\n",
            "Epoch 00289: loss did not improve from 0.00119\n",
            "Epoch 290/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2577 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2616\n",
            "\n",
            "Epoch 00290: loss did not improve from 0.00119\n",
            "Epoch 291/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2571 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2616\n",
            "\n",
            "Epoch 00291: loss did not improve from 0.00119\n",
            "Epoch 292/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2579 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2618\n",
            "\n",
            "Epoch 00292: loss did not improve from 0.00119\n",
            "Epoch 293/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2574 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2615\n",
            "\n",
            "Epoch 00293: loss did not improve from 0.00119\n",
            "Epoch 294/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2571 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2616\n",
            "\n",
            "Epoch 00294: loss improved from 0.00119 to 0.00119, saving model to poids_train.hdf5\n",
            "Epoch 295/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2575 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2617\n",
            "\n",
            "Epoch 00295: loss improved from 0.00119 to 0.00119, saving model to poids_train.hdf5\n",
            "Epoch 296/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2577 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2615\n",
            "\n",
            "Epoch 00296: loss did not improve from 0.00119\n",
            "Epoch 297/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2575 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2617\n",
            "\n",
            "Epoch 00297: loss did not improve from 0.00119\n",
            "Epoch 298/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2570 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2615\n",
            "\n",
            "Epoch 00298: loss improved from 0.00119 to 0.00119, saving model to poids_train.hdf5\n",
            "Epoch 299/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2615\n",
            "\n",
            "Epoch 00299: loss did not improve from 0.00119\n",
            "Epoch 300/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2574 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2616\n",
            "\n",
            "Epoch 00300: loss did not improve from 0.00119\n",
            "Epoch 301/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2579 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2615\n",
            "\n",
            "Epoch 00301: loss did not improve from 0.00119\n",
            "Epoch 302/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2575 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2615\n",
            "\n",
            "Epoch 00302: loss did not improve from 0.00119\n",
            "Epoch 303/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2573 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2615\n",
            "\n",
            "Epoch 00303: loss did not improve from 0.00119\n",
            "Epoch 304/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2617\n",
            "\n",
            "Epoch 00304: loss did not improve from 0.00119\n",
            "Epoch 305/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2614\n",
            "\n",
            "Epoch 00305: loss did not improve from 0.00119\n",
            "Epoch 306/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2615\n",
            "\n",
            "Epoch 00306: loss did not improve from 0.00119\n",
            "Epoch 307/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2575 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2617\n",
            "\n",
            "Epoch 00307: loss did not improve from 0.00119\n",
            "Epoch 308/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2576 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2615\n",
            "\n",
            "Epoch 00308: loss did not improve from 0.00119\n",
            "Epoch 309/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2580 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2615\n",
            "\n",
            "Epoch 00309: loss did not improve from 0.00119\n",
            "Epoch 310/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2575 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2617\n",
            "\n",
            "Epoch 00310: loss did not improve from 0.00119\n",
            "Epoch 311/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2577 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2616\n",
            "\n",
            "Epoch 00311: loss did not improve from 0.00119\n",
            "Epoch 312/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2574 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2618\n",
            "\n",
            "Epoch 00312: loss did not improve from 0.00119\n",
            "Epoch 313/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2575 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2626\n",
            "\n",
            "Epoch 00313: loss did not improve from 0.00119\n",
            "Epoch 314/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2578 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2618\n",
            "\n",
            "Epoch 00314: loss did not improve from 0.00119\n",
            "Epoch 315/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2618\n",
            "\n",
            "Epoch 00315: loss did not improve from 0.00119\n",
            "Epoch 316/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2573 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2617\n",
            "\n",
            "Epoch 00316: loss did not improve from 0.00119\n",
            "Epoch 317/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2614\n",
            "\n",
            "Epoch 00317: loss did not improve from 0.00119\n",
            "Epoch 318/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2614\n",
            "\n",
            "Epoch 00318: loss did not improve from 0.00119\n",
            "Epoch 319/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2577 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2614\n",
            "\n",
            "Epoch 00319: loss did not improve from 0.00119\n",
            "Epoch 320/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2614\n",
            "\n",
            "Epoch 00320: loss did not improve from 0.00119\n",
            "Epoch 321/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2572 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2615\n",
            "\n",
            "Epoch 00321: loss improved from 0.00119 to 0.00119, saving model to poids_train.hdf5\n",
            "Epoch 322/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2570 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2615\n",
            "\n",
            "Epoch 00322: loss did not improve from 0.00119\n",
            "Epoch 323/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2572 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2614\n",
            "\n",
            "Epoch 00323: loss did not improve from 0.00119\n",
            "Epoch 324/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2579 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2616\n",
            "\n",
            "Epoch 00324: loss improved from 0.00119 to 0.00119, saving model to poids_train.hdf5\n",
            "Epoch 325/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2615\n",
            "\n",
            "Epoch 00325: loss did not improve from 0.00119\n",
            "Epoch 326/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2571 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2618\n",
            "\n",
            "Epoch 00326: loss did not improve from 0.00119\n",
            "Epoch 327/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2574 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2615\n",
            "\n",
            "Epoch 00327: loss did not improve from 0.00119\n",
            "Epoch 328/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2613\n",
            "\n",
            "Epoch 00328: loss improved from 0.00119 to 0.00119, saving model to poids_train.hdf5\n",
            "Epoch 329/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2577 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2613\n",
            "\n",
            "Epoch 00329: loss did not improve from 0.00119\n",
            "Epoch 330/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2573 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2614\n",
            "\n",
            "Epoch 00330: loss did not improve from 0.00119\n",
            "Epoch 331/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2579 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2613\n",
            "\n",
            "Epoch 00331: loss did not improve from 0.00119\n",
            "Epoch 332/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2579 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2613\n",
            "\n",
            "Epoch 00332: loss did not improve from 0.00119\n",
            "Epoch 333/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2579 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2614\n",
            "\n",
            "Epoch 00333: loss did not improve from 0.00119\n",
            "Epoch 334/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2575 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2614\n",
            "\n",
            "Epoch 00334: loss improved from 0.00119 to 0.00119, saving model to poids_train.hdf5\n",
            "Epoch 335/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2574 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00335: loss did not improve from 0.00119\n",
            "Epoch 336/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2570 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2614\n",
            "\n",
            "Epoch 00336: loss did not improve from 0.00119\n",
            "Epoch 337/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2571 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00337: loss did not improve from 0.00119\n",
            "Epoch 338/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2580 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00338: loss did not improve from 0.00119\n",
            "Epoch 339/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2566 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00339: loss improved from 0.00119 to 0.00118, saving model to poids_train.hdf5\n",
            "Epoch 340/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2613\n",
            "\n",
            "Epoch 00340: loss did not improve from 0.00118\n",
            "Epoch 341/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2575 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2620\n",
            "\n",
            "Epoch 00341: loss did not improve from 0.00118\n",
            "Epoch 342/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2580 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2615\n",
            "\n",
            "Epoch 00342: loss did not improve from 0.00118\n",
            "Epoch 343/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2566 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2613\n",
            "\n",
            "Epoch 00343: loss did not improve from 0.00118\n",
            "Epoch 344/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2573 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2616\n",
            "\n",
            "Epoch 00344: loss did not improve from 0.00118\n",
            "Epoch 345/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2573 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00345: loss improved from 0.00118 to 0.00118, saving model to poids_train.hdf5\n",
            "Epoch 346/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2614\n",
            "\n",
            "Epoch 00346: loss did not improve from 0.00118\n",
            "Epoch 347/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2576 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00347: loss did not improve from 0.00118\n",
            "Epoch 348/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2616\n",
            "\n",
            "Epoch 00348: loss did not improve from 0.00118\n",
            "Epoch 349/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2578 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00349: loss did not improve from 0.00118\n",
            "Epoch 350/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00350: loss did not improve from 0.00118\n",
            "Epoch 351/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2570 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2613\n",
            "\n",
            "Epoch 00351: loss did not improve from 0.00118\n",
            "Epoch 352/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2581 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00352: loss did not improve from 0.00118\n",
            "Epoch 353/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2572 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2614\n",
            "\n",
            "Epoch 00353: loss did not improve from 0.00118\n",
            "Epoch 354/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2573 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2613\n",
            "\n",
            "Epoch 00354: loss did not improve from 0.00118\n",
            "Epoch 355/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00355: loss improved from 0.00118 to 0.00118, saving model to poids_train.hdf5\n",
            "Epoch 356/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00356: loss did not improve from 0.00118\n",
            "Epoch 357/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2566 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2619\n",
            "\n",
            "Epoch 00357: loss did not improve from 0.00118\n",
            "Epoch 358/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00358: loss did not improve from 0.00118\n",
            "Epoch 359/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00359: loss did not improve from 0.00118\n",
            "Epoch 360/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2576 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2615\n",
            "\n",
            "Epoch 00360: loss did not improve from 0.00118\n",
            "Epoch 361/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00361: loss did not improve from 0.00118\n",
            "Epoch 362/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2572 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2619\n",
            "\n",
            "Epoch 00362: loss did not improve from 0.00118\n",
            "Epoch 363/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00363: loss did not improve from 0.00118\n",
            "Epoch 364/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2577 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00364: loss improved from 0.00118 to 0.00118, saving model to poids_train.hdf5\n",
            "Epoch 365/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2572 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00365: loss did not improve from 0.00118\n",
            "Epoch 366/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2576 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00366: loss did not improve from 0.00118\n",
            "Epoch 367/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2573 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2623\n",
            "\n",
            "Epoch 00367: loss did not improve from 0.00118\n",
            "Epoch 368/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2573 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00368: loss did not improve from 0.00118\n",
            "Epoch 369/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00369: loss did not improve from 0.00118\n",
            "Epoch 370/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2571 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2614\n",
            "\n",
            "Epoch 00370: loss did not improve from 0.00118\n",
            "Epoch 371/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2572 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00371: loss did not improve from 0.00118\n",
            "Epoch 372/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00372: loss did not improve from 0.00118\n",
            "Epoch 373/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2570 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00373: loss did not improve from 0.00118\n",
            "Epoch 374/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2572 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00374: loss did not improve from 0.00118\n",
            "Epoch 375/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2613\n",
            "\n",
            "Epoch 00375: loss did not improve from 0.00118\n",
            "Epoch 376/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2575 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2615\n",
            "\n",
            "Epoch 00376: loss did not improve from 0.00118\n",
            "Epoch 377/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2615\n",
            "\n",
            "Epoch 00377: loss did not improve from 0.00118\n",
            "Epoch 378/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2576 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2614\n",
            "\n",
            "Epoch 00378: loss did not improve from 0.00118\n",
            "Epoch 379/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2575 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2611\n",
            "\n",
            "Epoch 00379: loss did not improve from 0.00118\n",
            "Epoch 380/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2572 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2613\n",
            "\n",
            "Epoch 00380: loss improved from 0.00118 to 0.00118, saving model to poids_train.hdf5\n",
            "Epoch 381/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00381: loss did not improve from 0.00118\n",
            "Epoch 382/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00382: loss did not improve from 0.00118\n",
            "Epoch 383/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2613\n",
            "\n",
            "Epoch 00383: loss did not improve from 0.00118\n",
            "Epoch 384/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2573 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2613\n",
            "\n",
            "Epoch 00384: loss did not improve from 0.00118\n",
            "Epoch 385/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2611\n",
            "\n",
            "Epoch 00385: loss did not improve from 0.00118\n",
            "Epoch 386/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2575 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2611\n",
            "\n",
            "Epoch 00386: loss did not improve from 0.00118\n",
            "Epoch 387/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2572 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2611\n",
            "\n",
            "Epoch 00387: loss did not improve from 0.00118\n",
            "Epoch 388/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2571 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2611\n",
            "\n",
            "Epoch 00388: loss did not improve from 0.00118\n",
            "Epoch 389/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2566 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00389: loss did not improve from 0.00118\n",
            "Epoch 390/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2571 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2614\n",
            "\n",
            "Epoch 00390: loss did not improve from 0.00118\n",
            "Epoch 391/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2565 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00391: loss did not improve from 0.00118\n",
            "Epoch 392/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2574 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2611\n",
            "\n",
            "Epoch 00392: loss improved from 0.00118 to 0.00118, saving model to poids_train.hdf5\n",
            "Epoch 393/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00393: loss did not improve from 0.00118\n",
            "Epoch 394/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2572 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00394: loss did not improve from 0.00118\n",
            "Epoch 395/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2613\n",
            "\n",
            "Epoch 00395: loss did not improve from 0.00118\n",
            "Epoch 396/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00396: loss did not improve from 0.00118\n",
            "Epoch 397/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2573 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2611\n",
            "\n",
            "Epoch 00397: loss did not improve from 0.00118\n",
            "Epoch 398/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2575 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00398: loss did not improve from 0.00118\n",
            "Epoch 399/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00399: loss did not improve from 0.00118\n",
            "Epoch 400/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2611\n",
            "\n",
            "Epoch 00400: loss did not improve from 0.00118\n",
            "Epoch 401/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2566 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2613\n",
            "\n",
            "Epoch 00401: loss did not improve from 0.00118\n",
            "Epoch 402/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2611\n",
            "\n",
            "Epoch 00402: loss did not improve from 0.00118\n",
            "Epoch 403/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2570 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2611\n",
            "\n",
            "Epoch 00403: loss did not improve from 0.00118\n",
            "Epoch 404/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2571 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2613\n",
            "\n",
            "Epoch 00404: loss did not improve from 0.00118\n",
            "Epoch 405/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2570 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2611\n",
            "\n",
            "Epoch 00405: loss did not improve from 0.00118\n",
            "Epoch 406/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2576 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2623\n",
            "\n",
            "Epoch 00406: loss did not improve from 0.00118\n",
            "Epoch 407/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2571 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2613\n",
            "\n",
            "Epoch 00407: loss did not improve from 0.00118\n",
            "Epoch 408/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2576 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2613\n",
            "\n",
            "Epoch 00408: loss did not improve from 0.00118\n",
            "Epoch 409/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2576 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2610\n",
            "\n",
            "Epoch 00409: loss did not improve from 0.00118\n",
            "Epoch 410/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2565 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2611\n",
            "\n",
            "Epoch 00410: loss improved from 0.00118 to 0.00117, saving model to poids_train.hdf5\n",
            "Epoch 411/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00411: loss did not improve from 0.00117\n",
            "Epoch 412/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2573 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2611\n",
            "\n",
            "Epoch 00412: loss did not improve from 0.00117\n",
            "Epoch 413/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00413: loss did not improve from 0.00117\n",
            "Epoch 414/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2565 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2610\n",
            "\n",
            "Epoch 00414: loss did not improve from 0.00117\n",
            "Epoch 415/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2578 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2611\n",
            "\n",
            "Epoch 00415: loss improved from 0.00117 to 0.00117, saving model to poids_train.hdf5\n",
            "Epoch 416/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2573 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2611\n",
            "\n",
            "Epoch 00416: loss did not improve from 0.00117\n",
            "Epoch 417/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2616\n",
            "\n",
            "Epoch 00417: loss did not improve from 0.00117\n",
            "Epoch 418/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2570 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2610\n",
            "\n",
            "Epoch 00418: loss did not improve from 0.00117\n",
            "Epoch 419/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2574 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2622\n",
            "\n",
            "Epoch 00419: loss did not improve from 0.00117\n",
            "Epoch 420/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2580 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2610\n",
            "\n",
            "Epoch 00420: loss did not improve from 0.00117\n",
            "Epoch 421/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2575 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2611\n",
            "\n",
            "Epoch 00421: loss did not improve from 0.00117\n",
            "Epoch 422/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00422: loss improved from 0.00117 to 0.00117, saving model to poids_train.hdf5\n",
            "Epoch 423/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2576 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2615\n",
            "\n",
            "Epoch 00423: loss did not improve from 0.00117\n",
            "Epoch 424/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2570 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2611\n",
            "\n",
            "Epoch 00424: loss did not improve from 0.00117\n",
            "Epoch 425/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2573 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2610\n",
            "\n",
            "Epoch 00425: loss did not improve from 0.00117\n",
            "Epoch 426/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2565 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2610\n",
            "\n",
            "Epoch 00426: loss improved from 0.00117 to 0.00117, saving model to poids_train.hdf5\n",
            "Epoch 427/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2610\n",
            "\n",
            "Epoch 00427: loss did not improve from 0.00117\n",
            "Epoch 428/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2610\n",
            "\n",
            "Epoch 00428: loss did not improve from 0.00117\n",
            "Epoch 429/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2579 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2610\n",
            "\n",
            "Epoch 00429: loss did not improve from 0.00117\n",
            "Epoch 430/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2610\n",
            "\n",
            "Epoch 00430: loss did not improve from 0.00117\n",
            "Epoch 431/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2574 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2611\n",
            "\n",
            "Epoch 00431: loss did not improve from 0.00117\n",
            "Epoch 432/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2573 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2610\n",
            "\n",
            "Epoch 00432: loss did not improve from 0.00117\n",
            "Epoch 433/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2609\n",
            "\n",
            "Epoch 00433: loss did not improve from 0.00117\n",
            "Epoch 434/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2565 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2610\n",
            "\n",
            "Epoch 00434: loss improved from 0.00117 to 0.00116, saving model to poids_train.hdf5\n",
            "Epoch 435/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2570 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2611\n",
            "\n",
            "Epoch 00435: loss did not improve from 0.00116\n",
            "Epoch 436/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2610\n",
            "\n",
            "Epoch 00436: loss did not improve from 0.00116\n",
            "Epoch 437/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2610\n",
            "\n",
            "Epoch 00437: loss did not improve from 0.00116\n",
            "Epoch 438/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2609\n",
            "\n",
            "Epoch 00438: loss did not improve from 0.00116\n",
            "Epoch 439/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2609\n",
            "\n",
            "Epoch 00439: loss did not improve from 0.00116\n",
            "Epoch 440/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2610\n",
            "\n",
            "Epoch 00440: loss did not improve from 0.00116\n",
            "Epoch 441/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2570 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2609\n",
            "\n",
            "Epoch 00441: loss did not improve from 0.00116\n",
            "Epoch 442/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2609\n",
            "\n",
            "Epoch 00442: loss did not improve from 0.00116\n",
            "Epoch 443/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2573 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2609\n",
            "\n",
            "Epoch 00443: loss did not improve from 0.00116\n",
            "Epoch 444/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2565 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2610\n",
            "\n",
            "Epoch 00444: loss did not improve from 0.00116\n",
            "Epoch 445/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2571 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2609\n",
            "\n",
            "Epoch 00445: loss did not improve from 0.00116\n",
            "Epoch 446/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2572 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2609\n",
            "\n",
            "Epoch 00446: loss did not improve from 0.00116\n",
            "Epoch 447/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2570 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2611\n",
            "\n",
            "Epoch 00447: loss did not improve from 0.00116\n",
            "Epoch 448/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2566 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2610\n",
            "\n",
            "Epoch 00448: loss improved from 0.00116 to 0.00116, saving model to poids_train.hdf5\n",
            "Epoch 449/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2609\n",
            "\n",
            "Epoch 00449: loss improved from 0.00116 to 0.00116, saving model to poids_train.hdf5\n",
            "Epoch 450/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2609\n",
            "\n",
            "Epoch 00450: loss did not improve from 0.00116\n",
            "Epoch 451/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2609\n",
            "\n",
            "Epoch 00451: loss did not improve from 0.00116\n",
            "Epoch 452/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2608\n",
            "\n",
            "Epoch 00452: loss did not improve from 0.00116\n",
            "Epoch 453/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2610\n",
            "\n",
            "Epoch 00453: loss did not improve from 0.00116\n",
            "Epoch 454/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2573 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2608\n",
            "\n",
            "Epoch 00454: loss did not improve from 0.00116\n",
            "Epoch 455/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2609\n",
            "\n",
            "Epoch 00455: loss improved from 0.00116 to 0.00116, saving model to poids_train.hdf5\n",
            "Epoch 456/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2609\n",
            "\n",
            "Epoch 00456: loss improved from 0.00116 to 0.00116, saving model to poids_train.hdf5\n",
            "Epoch 457/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2572 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2609\n",
            "\n",
            "Epoch 00457: loss did not improve from 0.00116\n",
            "Epoch 458/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2608\n",
            "\n",
            "Epoch 00458: loss did not improve from 0.00116\n",
            "Epoch 459/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2610\n",
            "\n",
            "Epoch 00459: loss did not improve from 0.00116\n",
            "Epoch 460/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2608\n",
            "\n",
            "Epoch 00460: loss improved from 0.00116 to 0.00116, saving model to poids_train.hdf5\n",
            "Epoch 461/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2570 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2610\n",
            "\n",
            "Epoch 00461: loss did not improve from 0.00116\n",
            "Epoch 462/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2565 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2608\n",
            "\n",
            "Epoch 00462: loss did not improve from 0.00116\n",
            "Epoch 463/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2565 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2608\n",
            "\n",
            "Epoch 00463: loss did not improve from 0.00116\n",
            "Epoch 464/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2608\n",
            "\n",
            "Epoch 00464: loss improved from 0.00116 to 0.00116, saving model to poids_train.hdf5\n",
            "Epoch 465/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2611\n",
            "\n",
            "Epoch 00465: loss did not improve from 0.00116\n",
            "Epoch 466/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2571 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2608\n",
            "\n",
            "Epoch 00466: loss did not improve from 0.00116\n",
            "Epoch 467/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00467: loss did not improve from 0.00116\n",
            "Epoch 468/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2610\n",
            "\n",
            "Epoch 00468: loss did not improve from 0.00116\n",
            "Epoch 469/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2610\n",
            "\n",
            "Epoch 00469: loss did not improve from 0.00116\n",
            "Epoch 470/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2573 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2608\n",
            "\n",
            "Epoch 00470: loss did not improve from 0.00116\n",
            "Epoch 471/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2610\n",
            "\n",
            "Epoch 00471: loss did not improve from 0.00116\n",
            "Epoch 472/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2565 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2607\n",
            "\n",
            "Epoch 00472: loss did not improve from 0.00116\n",
            "Epoch 473/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2608\n",
            "\n",
            "Epoch 00473: loss did not improve from 0.00116\n",
            "Epoch 474/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2609\n",
            "\n",
            "Epoch 00474: loss improved from 0.00116 to 0.00116, saving model to poids_train.hdf5\n",
            "Epoch 475/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2572 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2608\n",
            "\n",
            "Epoch 00475: loss did not improve from 0.00116\n",
            "Epoch 476/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2571 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2608\n",
            "\n",
            "Epoch 00476: loss did not improve from 0.00116\n",
            "Epoch 477/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2608\n",
            "\n",
            "Epoch 00477: loss did not improve from 0.00116\n",
            "Epoch 478/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2571 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2607\n",
            "\n",
            "Epoch 00478: loss improved from 0.00116 to 0.00115, saving model to poids_train.hdf5\n",
            "Epoch 479/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2571 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2607\n",
            "\n",
            "Epoch 00479: loss did not improve from 0.00115\n",
            "Epoch 480/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2607\n",
            "\n",
            "Epoch 00480: loss did not improve from 0.00115\n",
            "Epoch 481/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2578 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2608\n",
            "\n",
            "Epoch 00481: loss did not improve from 0.00115\n",
            "Epoch 482/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2607\n",
            "\n",
            "Epoch 00482: loss did not improve from 0.00115\n",
            "Epoch 483/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2607\n",
            "\n",
            "Epoch 00483: loss did not improve from 0.00115\n",
            "Epoch 484/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2573 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2607\n",
            "\n",
            "Epoch 00484: loss did not improve from 0.00115\n",
            "Epoch 485/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2571 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2610\n",
            "\n",
            "Epoch 00485: loss did not improve from 0.00115\n",
            "Epoch 486/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2565 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2611\n",
            "\n",
            "Epoch 00486: loss improved from 0.00115 to 0.00115, saving model to poids_train.hdf5\n",
            "Epoch 487/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2610\n",
            "\n",
            "Epoch 00487: loss did not improve from 0.00115\n",
            "Epoch 488/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2571 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2610\n",
            "\n",
            "Epoch 00488: loss did not improve from 0.00115\n",
            "Epoch 489/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2607\n",
            "\n",
            "Epoch 00489: loss improved from 0.00115 to 0.00115, saving model to poids_train.hdf5\n",
            "Epoch 490/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2566 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2611\n",
            "\n",
            "Epoch 00490: loss did not improve from 0.00115\n",
            "Epoch 491/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2572 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2620\n",
            "\n",
            "Epoch 00491: loss did not improve from 0.00115\n",
            "Epoch 492/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2575 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2615\n",
            "\n",
            "Epoch 00492: loss did not improve from 0.00115\n",
            "Epoch 493/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2578 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00493: loss did not improve from 0.00115\n",
            "Epoch 494/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2571 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2606\n",
            "\n",
            "Epoch 00494: loss did not improve from 0.00115\n",
            "Epoch 495/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2575 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2616\n",
            "\n",
            "Epoch 00495: loss did not improve from 0.00115\n",
            "Epoch 496/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2574 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2606\n",
            "\n",
            "Epoch 00496: loss did not improve from 0.00115\n",
            "Epoch 497/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2571 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2610\n",
            "\n",
            "Epoch 00497: loss did not improve from 0.00115\n",
            "Epoch 498/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2571 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2607\n",
            "\n",
            "Epoch 00498: loss did not improve from 0.00115\n",
            "Epoch 499/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2613\n",
            "\n",
            "Epoch 00499: loss did not improve from 0.00115\n",
            "Epoch 500/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2608\n",
            "\n",
            "Epoch 00500: loss did not improve from 0.00115\n",
            "Epoch 501/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2559 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2608\n",
            "\n",
            "Epoch 00501: loss did not improve from 0.00115\n",
            "Epoch 502/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2608\n",
            "\n",
            "Epoch 00502: loss did not improve from 0.00115\n",
            "Epoch 503/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2606\n",
            "\n",
            "Epoch 00503: loss did not improve from 0.00115\n",
            "Epoch 504/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2574 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2609\n",
            "\n",
            "Epoch 00504: loss did not improve from 0.00115\n",
            "Epoch 505/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2606\n",
            "\n",
            "Epoch 00505: loss did not improve from 0.00115\n",
            "Epoch 506/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2611\n",
            "\n",
            "Epoch 00506: loss improved from 0.00115 to 0.00115, saving model to poids_train.hdf5\n",
            "Epoch 507/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2606\n",
            "\n",
            "Epoch 00507: loss did not improve from 0.00115\n",
            "Epoch 508/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2566 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2609\n",
            "\n",
            "Epoch 00508: loss did not improve from 0.00115\n",
            "Epoch 509/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2565 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2606\n",
            "\n",
            "Epoch 00509: loss did not improve from 0.00115\n",
            "Epoch 510/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2606\n",
            "\n",
            "Epoch 00510: loss did not improve from 0.00115\n",
            "Epoch 511/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2609\n",
            "\n",
            "Epoch 00511: loss improved from 0.00115 to 0.00115, saving model to poids_train.hdf5\n",
            "Epoch 512/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2610\n",
            "\n",
            "Epoch 00512: loss did not improve from 0.00115\n",
            "Epoch 513/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2571 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2614\n",
            "\n",
            "Epoch 00513: loss did not improve from 0.00115\n",
            "Epoch 514/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2577 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2606\n",
            "\n",
            "Epoch 00514: loss did not improve from 0.00115\n",
            "Epoch 515/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2608\n",
            "\n",
            "Epoch 00515: loss did not improve from 0.00115\n",
            "Epoch 516/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2606\n",
            "\n",
            "Epoch 00516: loss did not improve from 0.00115\n",
            "Epoch 517/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2606\n",
            "\n",
            "Epoch 00517: loss did not improve from 0.00115\n",
            "Epoch 518/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2609\n",
            "\n",
            "Epoch 00518: loss did not improve from 0.00115\n",
            "Epoch 519/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2605\n",
            "\n",
            "Epoch 00519: loss improved from 0.00115 to 0.00115, saving model to poids_train.hdf5\n",
            "Epoch 520/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2606\n",
            "\n",
            "Epoch 00520: loss did not improve from 0.00115\n",
            "Epoch 521/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2570 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2607\n",
            "\n",
            "Epoch 00521: loss did not improve from 0.00115\n",
            "Epoch 522/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2573 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2605\n",
            "\n",
            "Epoch 00522: loss did not improve from 0.00115\n",
            "Epoch 523/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2559 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2605\n",
            "\n",
            "Epoch 00523: loss did not improve from 0.00115\n",
            "Epoch 524/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2566 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2606\n",
            "\n",
            "Epoch 00524: loss did not improve from 0.00115\n",
            "Epoch 525/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2605\n",
            "\n",
            "Epoch 00525: loss did not improve from 0.00115\n",
            "Epoch 526/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2607\n",
            "\n",
            "Epoch 00526: loss did not improve from 0.00115\n",
            "Epoch 527/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2572 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2605\n",
            "\n",
            "Epoch 00527: loss did not improve from 0.00115\n",
            "Epoch 528/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2607\n",
            "\n",
            "Epoch 00528: loss did not improve from 0.00115\n",
            "Epoch 529/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00529: loss did not improve from 0.00115\n",
            "Epoch 530/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2606\n",
            "\n",
            "Epoch 00530: loss did not improve from 0.00115\n",
            "Epoch 531/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2566 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2607\n",
            "\n",
            "Epoch 00531: loss did not improve from 0.00115\n",
            "Epoch 532/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2558 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2606\n",
            "\n",
            "Epoch 00532: loss did not improve from 0.00115\n",
            "Epoch 533/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2605\n",
            "\n",
            "Epoch 00533: loss did not improve from 0.00115\n",
            "Epoch 534/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2605\n",
            "\n",
            "Epoch 00534: loss did not improve from 0.00115\n",
            "Epoch 535/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2566 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2606\n",
            "\n",
            "Epoch 00535: loss did not improve from 0.00115\n",
            "Epoch 536/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2611\n",
            "\n",
            "Epoch 00536: loss did not improve from 0.00115\n",
            "Epoch 537/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2566 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2605\n",
            "\n",
            "Epoch 00537: loss did not improve from 0.00115\n",
            "Epoch 538/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2605\n",
            "\n",
            "Epoch 00538: loss did not improve from 0.00115\n",
            "Epoch 539/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2572 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2605\n",
            "\n",
            "Epoch 00539: loss did not improve from 0.00115\n",
            "Epoch 540/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2605\n",
            "\n",
            "Epoch 00540: loss did not improve from 0.00115\n",
            "Epoch 541/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2566 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00541: loss improved from 0.00115 to 0.00115, saving model to poids_train.hdf5\n",
            "Epoch 542/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2556 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2605\n",
            "\n",
            "Epoch 00542: loss did not improve from 0.00115\n",
            "Epoch 543/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2559 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00543: loss improved from 0.00115 to 0.00114, saving model to poids_train.hdf5\n",
            "Epoch 544/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2614\n",
            "\n",
            "Epoch 00544: loss did not improve from 0.00114\n",
            "Epoch 545/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2605\n",
            "\n",
            "Epoch 00545: loss did not improve from 0.00114\n",
            "Epoch 546/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2571 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2605\n",
            "\n",
            "Epoch 00546: loss did not improve from 0.00114\n",
            "Epoch 547/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2566 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2606\n",
            "\n",
            "Epoch 00547: loss improved from 0.00114 to 0.00114, saving model to poids_train.hdf5\n",
            "Epoch 548/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2605\n",
            "\n",
            "Epoch 00548: loss did not improve from 0.00114\n",
            "Epoch 549/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2605\n",
            "\n",
            "Epoch 00549: loss did not improve from 0.00114\n",
            "Epoch 550/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00550: loss did not improve from 0.00114\n",
            "Epoch 551/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2571 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00551: loss did not improve from 0.00114\n",
            "Epoch 552/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00552: loss did not improve from 0.00114\n",
            "Epoch 553/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00553: loss improved from 0.00114 to 0.00114, saving model to poids_train.hdf5\n",
            "Epoch 554/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2607\n",
            "\n",
            "Epoch 00554: loss did not improve from 0.00114\n",
            "Epoch 555/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2570 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00555: loss did not improve from 0.00114\n",
            "Epoch 556/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00556: loss improved from 0.00114 to 0.00114, saving model to poids_train.hdf5\n",
            "Epoch 557/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2566 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00557: loss did not improve from 0.00114\n",
            "Epoch 558/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2566 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00558: loss did not improve from 0.00114\n",
            "Epoch 559/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00559: loss did not improve from 0.00114\n",
            "Epoch 560/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00560: loss did not improve from 0.00114\n",
            "Epoch 561/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00561: loss did not improve from 0.00114\n",
            "Epoch 562/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00562: loss did not improve from 0.00114\n",
            "Epoch 563/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00563: loss did not improve from 0.00114\n",
            "Epoch 564/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00564: loss did not improve from 0.00114\n",
            "Epoch 565/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00565: loss did not improve from 0.00114\n",
            "Epoch 566/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00566: loss improved from 0.00114 to 0.00114, saving model to poids_train.hdf5\n",
            "Epoch 567/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00567: loss did not improve from 0.00114\n",
            "Epoch 568/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2613\n",
            "\n",
            "Epoch 00568: loss did not improve from 0.00114\n",
            "Epoch 569/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00569: loss did not improve from 0.00114\n",
            "Epoch 570/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00570: loss improved from 0.00114 to 0.00114, saving model to poids_train.hdf5\n",
            "Epoch 571/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2608\n",
            "\n",
            "Epoch 00571: loss did not improve from 0.00114\n",
            "Epoch 572/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2605\n",
            "\n",
            "Epoch 00572: loss did not improve from 0.00114\n",
            "Epoch 573/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2570 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00573: loss did not improve from 0.00114\n",
            "Epoch 574/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2566 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00574: loss did not improve from 0.00114\n",
            "Epoch 575/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2605\n",
            "\n",
            "Epoch 00575: loss did not improve from 0.00114\n",
            "Epoch 576/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2565 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00576: loss did not improve from 0.00114\n",
            "Epoch 577/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2558 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2608\n",
            "\n",
            "Epoch 00577: loss did not improve from 0.00114\n",
            "Epoch 578/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00578: loss did not improve from 0.00114\n",
            "Epoch 579/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2566 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00579: loss did not improve from 0.00114\n",
            "Epoch 580/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00580: loss did not improve from 0.00114\n",
            "Epoch 581/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00581: loss improved from 0.00114 to 0.00113, saving model to poids_train.hdf5\n",
            "Epoch 582/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00582: loss did not improve from 0.00113\n",
            "Epoch 583/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00583: loss did not improve from 0.00113\n",
            "Epoch 584/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00584: loss did not improve from 0.00113\n",
            "Epoch 585/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2557 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00585: loss did not improve from 0.00113\n",
            "Epoch 586/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00586: loss did not improve from 0.00113\n",
            "Epoch 587/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00587: loss did not improve from 0.00113\n",
            "Epoch 588/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2605\n",
            "\n",
            "Epoch 00588: loss did not improve from 0.00113\n",
            "Epoch 589/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00589: loss did not improve from 0.00113\n",
            "Epoch 590/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00590: loss did not improve from 0.00113\n",
            "Epoch 591/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2557 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00591: loss did not improve from 0.00113\n",
            "Epoch 592/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2566 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00592: loss did not improve from 0.00113\n",
            "Epoch 593/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00593: loss did not improve from 0.00113\n",
            "Epoch 594/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2576 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00594: loss did not improve from 0.00113\n",
            "Epoch 595/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00595: loss did not improve from 0.00113\n",
            "Epoch 596/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00596: loss did not improve from 0.00113\n",
            "Epoch 597/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2605\n",
            "\n",
            "Epoch 00597: loss did not improve from 0.00113\n",
            "Epoch 598/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00598: loss did not improve from 0.00113\n",
            "Epoch 599/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2614\n",
            "\n",
            "Epoch 00599: loss did not improve from 0.00113\n",
            "Epoch 600/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00600: loss did not improve from 0.00113\n",
            "Epoch 601/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00601: loss improved from 0.00113 to 0.00113, saving model to poids_train.hdf5\n",
            "Epoch 602/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00602: loss did not improve from 0.00113\n",
            "Epoch 603/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00603: loss did not improve from 0.00113\n",
            "Epoch 604/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2605\n",
            "\n",
            "Epoch 00604: loss did not improve from 0.00113\n",
            "Epoch 605/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2558 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00605: loss did not improve from 0.00113\n",
            "Epoch 606/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2559 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2606\n",
            "\n",
            "Epoch 00606: loss did not improve from 0.00113\n",
            "Epoch 607/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00607: loss did not improve from 0.00113\n",
            "Epoch 608/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00608: loss did not improve from 0.00113\n",
            "Epoch 609/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00609: loss improved from 0.00113 to 0.00113, saving model to poids_train.hdf5\n",
            "Epoch 610/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2566 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2605\n",
            "\n",
            "Epoch 00610: loss did not improve from 0.00113\n",
            "Epoch 611/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2607\n",
            "\n",
            "Epoch 00611: loss improved from 0.00113 to 0.00113, saving model to poids_train.hdf5\n",
            "Epoch 612/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2607\n",
            "\n",
            "Epoch 00612: loss did not improve from 0.00113\n",
            "Epoch 613/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2605\n",
            "\n",
            "Epoch 00613: loss did not improve from 0.00113\n",
            "Epoch 614/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2605\n",
            "\n",
            "Epoch 00614: loss did not improve from 0.00113\n",
            "Epoch 615/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00615: loss did not improve from 0.00113\n",
            "Epoch 616/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2558 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00616: loss did not improve from 0.00113\n",
            "Epoch 617/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2565 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00617: loss did not improve from 0.00113\n",
            "Epoch 618/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2566 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00618: loss did not improve from 0.00113\n",
            "Epoch 619/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00619: loss did not improve from 0.00113\n",
            "Epoch 620/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00620: loss did not improve from 0.00113\n",
            "Epoch 621/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2572 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2606\n",
            "\n",
            "Epoch 00621: loss did not improve from 0.00113\n",
            "Epoch 622/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00622: loss did not improve from 0.00113\n",
            "Epoch 623/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2566 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00623: loss did not improve from 0.00113\n",
            "Epoch 624/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00624: loss did not improve from 0.00113\n",
            "Epoch 625/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00625: loss did not improve from 0.00113\n",
            "Epoch 626/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00626: loss did not improve from 0.00113\n",
            "Epoch 627/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2566 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00627: loss did not improve from 0.00113\n",
            "Epoch 628/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00628: loss did not improve from 0.00113\n",
            "Epoch 629/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00629: loss did not improve from 0.00113\n",
            "Epoch 630/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2558 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00630: loss did not improve from 0.00113\n",
            "Epoch 631/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00631: loss did not improve from 0.00113\n",
            "Epoch 632/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00632: loss improved from 0.00113 to 0.00113, saving model to poids_train.hdf5\n",
            "Epoch 633/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00633: loss improved from 0.00113 to 0.00113, saving model to poids_train.hdf5\n",
            "Epoch 634/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2570 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00634: loss did not improve from 0.00113\n",
            "Epoch 635/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2565 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2611\n",
            "\n",
            "Epoch 00635: loss did not improve from 0.00113\n",
            "Epoch 636/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2570 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00636: loss did not improve from 0.00113\n",
            "Epoch 637/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2565 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00637: loss did not improve from 0.00113\n",
            "Epoch 638/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00638: loss did not improve from 0.00113\n",
            "Epoch 639/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2554 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00639: loss did not improve from 0.00113\n",
            "Epoch 640/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00640: loss did not improve from 0.00113\n",
            "Epoch 641/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2555 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00641: loss did not improve from 0.00113\n",
            "Epoch 642/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00642: loss did not improve from 0.00113\n",
            "Epoch 643/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2572 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00643: loss did not improve from 0.00113\n",
            "Epoch 644/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2556 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00644: loss did not improve from 0.00113\n",
            "Epoch 645/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2607\n",
            "\n",
            "Epoch 00645: loss did not improve from 0.00113\n",
            "Epoch 646/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2570 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00646: loss did not improve from 0.00113\n",
            "Epoch 647/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2566 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00647: loss did not improve from 0.00113\n",
            "Epoch 648/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2566 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2609\n",
            "\n",
            "Epoch 00648: loss did not improve from 0.00113\n",
            "Epoch 649/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2565 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2605\n",
            "\n",
            "Epoch 00649: loss did not improve from 0.00113\n",
            "Epoch 650/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00650: loss did not improve from 0.00113\n",
            "Epoch 651/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00651: loss did not improve from 0.00113\n",
            "Epoch 652/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2571 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2607\n",
            "\n",
            "Epoch 00652: loss did not improve from 0.00113\n",
            "Epoch 653/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00653: loss did not improve from 0.00113\n",
            "Epoch 654/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00654: loss did not improve from 0.00113\n",
            "Epoch 655/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00655: loss did not improve from 0.00113\n",
            "Epoch 656/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00656: loss did not improve from 0.00113\n",
            "Epoch 657/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2607\n",
            "\n",
            "Epoch 00657: loss did not improve from 0.00113\n",
            "Epoch 658/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2566 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00658: loss did not improve from 0.00113\n",
            "Epoch 659/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00659: loss did not improve from 0.00113\n",
            "Epoch 660/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00660: loss did not improve from 0.00113\n",
            "Epoch 661/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00661: loss did not improve from 0.00113\n",
            "Epoch 662/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00662: loss did not improve from 0.00113\n",
            "Epoch 663/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2565 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00663: loss did not improve from 0.00113\n",
            "Epoch 664/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00664: loss improved from 0.00113 to 0.00112, saving model to poids_train.hdf5\n",
            "Epoch 665/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00665: loss did not improve from 0.00112\n",
            "Epoch 666/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2558 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2605\n",
            "\n",
            "Epoch 00666: loss did not improve from 0.00112\n",
            "Epoch 667/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00667: loss did not improve from 0.00112\n",
            "Epoch 668/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2557 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00668: loss did not improve from 0.00112\n",
            "Epoch 669/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00669: loss did not improve from 0.00112\n",
            "Epoch 670/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00670: loss did not improve from 0.00112\n",
            "Epoch 671/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2566 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00671: loss did not improve from 0.00112\n",
            "Epoch 672/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00672: loss improved from 0.00112 to 0.00112, saving model to poids_train.hdf5\n",
            "Epoch 673/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00673: loss did not improve from 0.00112\n",
            "Epoch 674/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00674: loss did not improve from 0.00112\n",
            "Epoch 675/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00675: loss did not improve from 0.00112\n",
            "Epoch 676/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2559 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00676: loss did not improve from 0.00112\n",
            "Epoch 677/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00677: loss did not improve from 0.00112\n",
            "Epoch 678/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00678: loss did not improve from 0.00112\n",
            "Epoch 679/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00679: loss did not improve from 0.00112\n",
            "Epoch 680/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00680: loss did not improve from 0.00112\n",
            "Epoch 681/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00681: loss did not improve from 0.00112\n",
            "Epoch 682/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00682: loss did not improve from 0.00112\n",
            "Epoch 683/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00683: loss did not improve from 0.00112\n",
            "Epoch 684/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00684: loss did not improve from 0.00112\n",
            "Epoch 685/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2607\n",
            "\n",
            "Epoch 00685: loss did not improve from 0.00112\n",
            "Epoch 686/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2559 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00686: loss did not improve from 0.00112\n",
            "Epoch 687/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00687: loss did not improve from 0.00112\n",
            "Epoch 688/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00688: loss did not improve from 0.00112\n",
            "Epoch 689/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00689: loss did not improve from 0.00112\n",
            "Epoch 690/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2570 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00690: loss did not improve from 0.00112\n",
            "Epoch 691/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2566 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00691: loss did not improve from 0.00112\n",
            "Epoch 692/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00692: loss did not improve from 0.00112\n",
            "Epoch 693/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2606\n",
            "\n",
            "Epoch 00693: loss did not improve from 0.00112\n",
            "Epoch 694/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00694: loss did not improve from 0.00112\n",
            "Epoch 695/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00695: loss did not improve from 0.00112\n",
            "Epoch 696/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00696: loss did not improve from 0.00112\n",
            "Epoch 697/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00697: loss did not improve from 0.00112\n",
            "Epoch 698/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00698: loss improved from 0.00112 to 0.00112, saving model to poids_train.hdf5\n",
            "Epoch 699/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2566 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00699: loss did not improve from 0.00112\n",
            "Epoch 700/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2565 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00700: loss did not improve from 0.00112\n",
            "Epoch 701/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00701: loss did not improve from 0.00112\n",
            "Epoch 702/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2559 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00702: loss did not improve from 0.00112\n",
            "Epoch 703/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00703: loss did not improve from 0.00112\n",
            "Epoch 704/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00704: loss did not improve from 0.00112\n",
            "Epoch 705/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2556 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2606\n",
            "\n",
            "Epoch 00705: loss did not improve from 0.00112\n",
            "Epoch 706/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00706: loss did not improve from 0.00112\n",
            "Epoch 707/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2572 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00707: loss did not improve from 0.00112\n",
            "Epoch 708/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2566 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00708: loss did not improve from 0.00112\n",
            "Epoch 709/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00709: loss did not improve from 0.00112\n",
            "Epoch 710/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00710: loss did not improve from 0.00112\n",
            "Epoch 711/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2606\n",
            "\n",
            "Epoch 00711: loss did not improve from 0.00112\n",
            "Epoch 712/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00712: loss did not improve from 0.00112\n",
            "Epoch 713/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00713: loss did not improve from 0.00112\n",
            "Epoch 714/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2566 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00714: loss did not improve from 0.00112\n",
            "Epoch 715/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00715: loss did not improve from 0.00112\n",
            "Epoch 716/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00716: loss did not improve from 0.00112\n",
            "Epoch 717/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2570 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00717: loss did not improve from 0.00112\n",
            "Epoch 718/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00718: loss did not improve from 0.00112\n",
            "Epoch 719/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00719: loss did not improve from 0.00112\n",
            "Epoch 720/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2559 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00720: loss did not improve from 0.00112\n",
            "Epoch 721/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2556 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00721: loss did not improve from 0.00112\n",
            "Epoch 722/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2570 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00722: loss did not improve from 0.00112\n",
            "Epoch 723/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00723: loss did not improve from 0.00112\n",
            "Epoch 724/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2558 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00724: loss did not improve from 0.00112\n",
            "Epoch 725/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00725: loss did not improve from 0.00112\n",
            "Epoch 726/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2555 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00726: loss did not improve from 0.00112\n",
            "Epoch 727/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2565 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00727: loss did not improve from 0.00112\n",
            "Epoch 728/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00728: loss did not improve from 0.00112\n",
            "Epoch 729/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00729: loss did not improve from 0.00112\n",
            "Epoch 730/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2605\n",
            "\n",
            "Epoch 00730: loss did not improve from 0.00112\n",
            "Epoch 731/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00731: loss did not improve from 0.00112\n",
            "Epoch 732/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00732: loss did not improve from 0.00112\n",
            "Epoch 733/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2556 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00733: loss did not improve from 0.00112\n",
            "Epoch 734/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00734: loss did not improve from 0.00112\n",
            "Epoch 735/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2565 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00735: loss improved from 0.00112 to 0.00112, saving model to poids_train.hdf5\n",
            "Epoch 736/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2570 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00736: loss did not improve from 0.00112\n",
            "Epoch 737/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00737: loss did not improve from 0.00112\n",
            "Epoch 738/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00738: loss improved from 0.00112 to 0.00112, saving model to poids_train.hdf5\n",
            "Epoch 739/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2559 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00739: loss did not improve from 0.00112\n",
            "Epoch 740/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00740: loss did not improve from 0.00112\n",
            "Epoch 741/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00741: loss did not improve from 0.00112\n",
            "Epoch 742/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00742: loss did not improve from 0.00112\n",
            "Epoch 743/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2559 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00743: loss did not improve from 0.00112\n",
            "Epoch 744/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00744: loss did not improve from 0.00112\n",
            "Epoch 745/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00745: loss did not improve from 0.00112\n",
            "Epoch 746/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2558 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00746: loss did not improve from 0.00112\n",
            "Epoch 747/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2557 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00747: loss did not improve from 0.00112\n",
            "Epoch 748/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2553 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00748: loss did not improve from 0.00112\n",
            "Epoch 749/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2565 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00749: loss did not improve from 0.00112\n",
            "Epoch 750/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2557 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00750: loss did not improve from 0.00112\n",
            "Epoch 751/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00751: loss did not improve from 0.00112\n",
            "Epoch 752/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00752: loss did not improve from 0.00112\n",
            "Epoch 753/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2570 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00753: loss did not improve from 0.00112\n",
            "Epoch 754/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00754: loss did not improve from 0.00112\n",
            "Epoch 755/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2559 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00755: loss did not improve from 0.00112\n",
            "Epoch 756/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00756: loss did not improve from 0.00112\n",
            "Epoch 757/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2565 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00757: loss did not improve from 0.00112\n",
            "Epoch 758/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00758: loss did not improve from 0.00112\n",
            "Epoch 759/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2558 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00759: loss did not improve from 0.00112\n",
            "Epoch 760/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2571 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00760: loss did not improve from 0.00112\n",
            "Epoch 761/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00761: loss did not improve from 0.00112\n",
            "Epoch 762/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2605\n",
            "\n",
            "Epoch 00762: loss did not improve from 0.00112\n",
            "Epoch 763/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00763: loss did not improve from 0.00112\n",
            "Epoch 764/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2556 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00764: loss did not improve from 0.00112\n",
            "Epoch 765/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2558 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00765: loss did not improve from 0.00112\n",
            "Epoch 766/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2556 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00766: loss did not improve from 0.00112\n",
            "Epoch 767/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2565 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00767: loss did not improve from 0.00112\n",
            "Epoch 768/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2559 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00768: loss did not improve from 0.00112\n",
            "Epoch 769/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00769: loss did not improve from 0.00112\n",
            "Epoch 770/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00770: loss did not improve from 0.00112\n",
            "Epoch 771/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00771: loss did not improve from 0.00112\n",
            "Epoch 772/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00772: loss did not improve from 0.00112\n",
            "Epoch 773/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00773: loss did not improve from 0.00112\n",
            "Epoch 774/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2558 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00774: loss did not improve from 0.00112\n",
            "Epoch 775/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2558 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00775: loss did not improve from 0.00112\n",
            "Epoch 776/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2553 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00776: loss improved from 0.00112 to 0.00112, saving model to poids_train.hdf5\n",
            "Epoch 777/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2558 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00777: loss did not improve from 0.00112\n",
            "Epoch 778/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00778: loss did not improve from 0.00112\n",
            "Epoch 779/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2557 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00779: loss did not improve from 0.00112\n",
            "Epoch 780/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00780: loss did not improve from 0.00112\n",
            "Epoch 781/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2559 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00781: loss did not improve from 0.00112\n",
            "Epoch 782/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00782: loss did not improve from 0.00112\n",
            "Epoch 783/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2558 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00783: loss did not improve from 0.00112\n",
            "Epoch 784/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00784: loss did not improve from 0.00112\n",
            "Epoch 785/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00785: loss did not improve from 0.00112\n",
            "Epoch 786/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00786: loss did not improve from 0.00112\n",
            "Epoch 787/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00787: loss did not improve from 0.00112\n",
            "Epoch 788/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00788: loss did not improve from 0.00112\n",
            "Epoch 789/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2556 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00789: loss did not improve from 0.00112\n",
            "Epoch 790/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2557 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2608\n",
            "\n",
            "Epoch 00790: loss improved from 0.00112 to 0.00111, saving model to poids_train.hdf5\n",
            "Epoch 791/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2566 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2609\n",
            "\n",
            "Epoch 00791: loss did not improve from 0.00111\n",
            "Epoch 792/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2571 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00792: loss did not improve from 0.00111\n",
            "Epoch 793/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2555 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00793: loss did not improve from 0.00111\n",
            "Epoch 794/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00794: loss did not improve from 0.00111\n",
            "Epoch 795/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00795: loss did not improve from 0.00111\n",
            "Epoch 796/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2557 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00796: loss did not improve from 0.00111\n",
            "Epoch 797/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2557 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00797: loss did not improve from 0.00111\n",
            "Epoch 798/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00798: loss did not improve from 0.00111\n",
            "Epoch 799/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00799: loss did not improve from 0.00111\n",
            "Epoch 800/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00800: loss did not improve from 0.00111\n",
            "Epoch 801/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00801: loss did not improve from 0.00111\n",
            "Epoch 802/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00802: loss did not improve from 0.00111\n",
            "Epoch 803/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00803: loss did not improve from 0.00111\n",
            "Epoch 804/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2558 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00804: loss did not improve from 0.00111\n",
            "Epoch 805/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2559 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00805: loss did not improve from 0.00111\n",
            "Epoch 806/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2558 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00806: loss did not improve from 0.00111\n",
            "Epoch 807/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00807: loss did not improve from 0.00111\n",
            "Epoch 808/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2570 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00808: loss did not improve from 0.00111\n",
            "Epoch 809/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00809: loss did not improve from 0.00111\n",
            "Epoch 810/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2607\n",
            "\n",
            "Epoch 00810: loss did not improve from 0.00111\n",
            "Epoch 811/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00811: loss did not improve from 0.00111\n",
            "Epoch 812/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00812: loss did not improve from 0.00111\n",
            "Epoch 813/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00813: loss did not improve from 0.00111\n",
            "Epoch 814/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2558 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00814: loss did not improve from 0.00111\n",
            "Epoch 815/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2554 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00815: loss did not improve from 0.00111\n",
            "Epoch 816/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2552 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00816: loss did not improve from 0.00111\n",
            "Epoch 817/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2565 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00817: loss did not improve from 0.00111\n",
            "Epoch 818/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2565 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00818: loss did not improve from 0.00111\n",
            "Epoch 819/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00819: loss did not improve from 0.00111\n",
            "Epoch 820/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2558 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00820: loss did not improve from 0.00111\n",
            "Epoch 821/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2565 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2606\n",
            "\n",
            "Epoch 00821: loss did not improve from 0.00111\n",
            "Epoch 822/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2557 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00822: loss did not improve from 0.00111\n",
            "Epoch 823/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00823: loss did not improve from 0.00111\n",
            "Epoch 824/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00824: loss did not improve from 0.00111\n",
            "Epoch 825/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2554 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00825: loss did not improve from 0.00111\n",
            "Epoch 826/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2555 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00826: loss did not improve from 0.00111\n",
            "Epoch 827/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2565 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00827: loss did not improve from 0.00111\n",
            "Epoch 828/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2565 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00828: loss did not improve from 0.00111\n",
            "Epoch 829/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2559 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00829: loss did not improve from 0.00111\n",
            "Epoch 830/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2555 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00830: loss did not improve from 0.00111\n",
            "Epoch 831/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00831: loss did not improve from 0.00111\n",
            "Epoch 832/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2559 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00832: loss did not improve from 0.00111\n",
            "Epoch 833/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00833: loss did not improve from 0.00111\n",
            "Epoch 834/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00834: loss did not improve from 0.00111\n",
            "Epoch 835/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00835: loss did not improve from 0.00111\n",
            "Epoch 836/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2565 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00836: loss did not improve from 0.00111\n",
            "Epoch 837/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00837: loss did not improve from 0.00111\n",
            "Epoch 838/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00838: loss did not improve from 0.00111\n",
            "Epoch 839/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2558 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00839: loss did not improve from 0.00111\n",
            "Epoch 840/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00840: loss improved from 0.00111 to 0.00111, saving model to poids_train.hdf5\n",
            "Epoch 841/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2556 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00841: loss did not improve from 0.00111\n",
            "Epoch 842/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2557 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00842: loss improved from 0.00111 to 0.00111, saving model to poids_train.hdf5\n",
            "Epoch 843/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00843: loss did not improve from 0.00111\n",
            "Epoch 844/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2571 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00844: loss did not improve from 0.00111\n",
            "Epoch 845/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00845: loss did not improve from 0.00111\n",
            "Epoch 846/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00846: loss did not improve from 0.00111\n",
            "Epoch 847/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2565 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00847: loss did not improve from 0.00111\n",
            "Epoch 848/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2557 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00848: loss did not improve from 0.00111\n",
            "Epoch 849/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2558 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00849: loss did not improve from 0.00111\n",
            "Epoch 850/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2558 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00850: loss did not improve from 0.00111\n",
            "Epoch 851/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00851: loss did not improve from 0.00111\n",
            "Epoch 852/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00852: loss did not improve from 0.00111\n",
            "Epoch 853/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00853: loss did not improve from 0.00111\n",
            "Epoch 854/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00854: loss did not improve from 0.00111\n",
            "Epoch 855/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2566 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00855: loss did not improve from 0.00111\n",
            "Epoch 856/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00856: loss did not improve from 0.00111\n",
            "Epoch 857/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00857: loss did not improve from 0.00111\n",
            "Epoch 858/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00858: loss improved from 0.00111 to 0.00111, saving model to poids_train.hdf5\n",
            "Epoch 859/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00859: loss did not improve from 0.00111\n",
            "Epoch 860/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2559 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00860: loss did not improve from 0.00111\n",
            "Epoch 861/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00861: loss did not improve from 0.00111\n",
            "Epoch 862/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2556 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00862: loss did not improve from 0.00111\n",
            "Epoch 863/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00863: loss improved from 0.00111 to 0.00111, saving model to poids_train.hdf5\n",
            "Epoch 864/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00864: loss did not improve from 0.00111\n",
            "Epoch 865/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2556 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00865: loss did not improve from 0.00111\n",
            "Epoch 866/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00866: loss did not improve from 0.00111\n",
            "Epoch 867/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2558 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00867: loss did not improve from 0.00111\n",
            "Epoch 868/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2559 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00868: loss did not improve from 0.00111\n",
            "Epoch 869/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00869: loss did not improve from 0.00111\n",
            "Epoch 870/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00870: loss did not improve from 0.00111\n",
            "Epoch 871/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00871: loss did not improve from 0.00111\n",
            "Epoch 872/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2557 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00872: loss improved from 0.00111 to 0.00111, saving model to poids_train.hdf5\n",
            "Epoch 873/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00873: loss did not improve from 0.00111\n",
            "Epoch 874/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2605\n",
            "\n",
            "Epoch 00874: loss did not improve from 0.00111\n",
            "Epoch 875/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00875: loss did not improve from 0.00111\n",
            "Epoch 876/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00876: loss did not improve from 0.00111\n",
            "Epoch 877/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2558 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00877: loss did not improve from 0.00111\n",
            "Epoch 878/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2566 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00878: loss did not improve from 0.00111\n",
            "Epoch 879/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2569 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00879: loss did not improve from 0.00111\n",
            "Epoch 880/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00880: loss did not improve from 0.00111\n",
            "Epoch 881/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00881: loss did not improve from 0.00111\n",
            "Epoch 882/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2553 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00882: loss did not improve from 0.00111\n",
            "Epoch 883/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2554 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00883: loss did not improve from 0.00111\n",
            "Epoch 884/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00884: loss did not improve from 0.00111\n",
            "Epoch 885/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2571 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00885: loss did not improve from 0.00111\n",
            "Epoch 886/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00886: loss did not improve from 0.00111\n",
            "Epoch 887/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2558 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00887: loss did not improve from 0.00111\n",
            "Epoch 888/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2607\n",
            "\n",
            "Epoch 00888: loss did not improve from 0.00111\n",
            "Epoch 889/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00889: loss did not improve from 0.00111\n",
            "Epoch 890/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00890: loss did not improve from 0.00111\n",
            "Epoch 891/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00891: loss did not improve from 0.00111\n",
            "Epoch 892/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00892: loss did not improve from 0.00111\n",
            "Epoch 893/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2566 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00893: loss did not improve from 0.00111\n",
            "Epoch 894/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00894: loss did not improve from 0.00111\n",
            "Epoch 895/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00895: loss did not improve from 0.00111\n",
            "Epoch 896/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00896: loss did not improve from 0.00111\n",
            "Epoch 897/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00897: loss did not improve from 0.00111\n",
            "Epoch 898/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00898: loss did not improve from 0.00111\n",
            "Epoch 899/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2557 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00899: loss improved from 0.00111 to 0.00111, saving model to poids_train.hdf5\n",
            "Epoch 900/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2556 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00900: loss did not improve from 0.00111\n",
            "Epoch 901/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2558 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00901: loss did not improve from 0.00111\n",
            "Epoch 902/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2555 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00902: loss did not improve from 0.00111\n",
            "Epoch 903/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00903: loss did not improve from 0.00111\n",
            "Epoch 904/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00904: loss did not improve from 0.00111\n",
            "Epoch 905/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2565 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2605\n",
            "\n",
            "Epoch 00905: loss did not improve from 0.00111\n",
            "Epoch 906/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2570 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00906: loss did not improve from 0.00111\n",
            "Epoch 907/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2559 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00907: loss did not improve from 0.00111\n",
            "Epoch 908/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00908: loss did not improve from 0.00111\n",
            "Epoch 909/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00909: loss did not improve from 0.00111\n",
            "Epoch 910/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00910: loss did not improve from 0.00111\n",
            "Epoch 911/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00911: loss did not improve from 0.00111\n",
            "Epoch 912/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00912: loss did not improve from 0.00111\n",
            "Epoch 913/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2556 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00913: loss did not improve from 0.00111\n",
            "Epoch 914/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2558 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00914: loss did not improve from 0.00111\n",
            "Epoch 915/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00915: loss did not improve from 0.00111\n",
            "Epoch 916/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00916: loss did not improve from 0.00111\n",
            "Epoch 917/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2557 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00917: loss did not improve from 0.00111\n",
            "Epoch 918/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2566 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00918: loss did not improve from 0.00111\n",
            "Epoch 919/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2559 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00919: loss did not improve from 0.00111\n",
            "Epoch 920/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2559 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00920: loss did not improve from 0.00111\n",
            "Epoch 921/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00921: loss did not improve from 0.00111\n",
            "Epoch 922/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00922: loss did not improve from 0.00111\n",
            "Epoch 923/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2557 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00923: loss did not improve from 0.00111\n",
            "Epoch 924/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00924: loss did not improve from 0.00111\n",
            "Epoch 925/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2559 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00925: loss did not improve from 0.00111\n",
            "Epoch 926/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00926: loss did not improve from 0.00111\n",
            "Epoch 927/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00927: loss did not improve from 0.00111\n",
            "Epoch 928/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2558 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00928: loss did not improve from 0.00111\n",
            "Epoch 929/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2558 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00929: loss did not improve from 0.00111\n",
            "Epoch 930/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00930: loss did not improve from 0.00111\n",
            "Epoch 931/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2565 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00931: loss did not improve from 0.00111\n",
            "Epoch 932/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00932: loss did not improve from 0.00111\n",
            "Epoch 933/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00933: loss did not improve from 0.00111\n",
            "Epoch 934/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2558 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00934: loss did not improve from 0.00111\n",
            "Epoch 935/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00935: loss did not improve from 0.00111\n",
            "Epoch 936/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00936: loss did not improve from 0.00111\n",
            "Epoch 937/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2552 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00937: loss did not improve from 0.00111\n",
            "Epoch 938/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2557 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00938: loss did not improve from 0.00111\n",
            "Epoch 939/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2559 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00939: loss did not improve from 0.00111\n",
            "Epoch 940/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00940: loss did not improve from 0.00111\n",
            "Epoch 941/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2556 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00941: loss did not improve from 0.00111\n",
            "Epoch 942/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2566 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00942: loss did not improve from 0.00111\n",
            "Epoch 943/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00943: loss did not improve from 0.00111\n",
            "Epoch 944/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2602\n",
            "\n",
            "Epoch 00944: loss did not improve from 0.00111\n",
            "Epoch 945/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00945: loss did not improve from 0.00111\n",
            "Epoch 946/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00946: loss did not improve from 0.00111\n",
            "Epoch 947/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2559 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00947: loss did not improve from 0.00111\n",
            "Epoch 948/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2552 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00948: loss did not improve from 0.00111\n",
            "Epoch 949/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00949: loss did not improve from 0.00111\n",
            "Epoch 950/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2556 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00950: loss did not improve from 0.00111\n",
            "Epoch 951/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2559 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2607\n",
            "\n",
            "Epoch 00951: loss did not improve from 0.00111\n",
            "Epoch 952/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00952: loss did not improve from 0.00111\n",
            "Epoch 953/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00953: loss did not improve from 0.00111\n",
            "Epoch 954/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2556 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2596\n",
            "\n",
            "Epoch 00954: loss did not improve from 0.00111\n",
            "Epoch 955/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2567 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00955: loss did not improve from 0.00111\n",
            "Epoch 956/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00956: loss did not improve from 0.00111\n",
            "Epoch 957/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00957: loss did not improve from 0.00111\n",
            "Epoch 958/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2554 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00958: loss did not improve from 0.00111\n",
            "Epoch 959/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00959: loss did not improve from 0.00111\n",
            "Epoch 960/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2551 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00960: loss did not improve from 0.00111\n",
            "Epoch 961/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00961: loss did not improve from 0.00111\n",
            "Epoch 962/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2556 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2596\n",
            "\n",
            "Epoch 00962: loss did not improve from 0.00111\n",
            "Epoch 963/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00963: loss did not improve from 0.00111\n",
            "Epoch 964/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2565 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00964: loss did not improve from 0.00111\n",
            "Epoch 965/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00965: loss did not improve from 0.00111\n",
            "Epoch 966/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2558 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00966: loss did not improve from 0.00111\n",
            "Epoch 967/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2607\n",
            "\n",
            "Epoch 00967: loss did not improve from 0.00111\n",
            "Epoch 968/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2559 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2606\n",
            "\n",
            "Epoch 00968: loss did not improve from 0.00111\n",
            "Epoch 969/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00969: loss did not improve from 0.00111\n",
            "Epoch 970/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00970: loss did not improve from 0.00111\n",
            "Epoch 971/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2559 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00971: loss did not improve from 0.00111\n",
            "Epoch 972/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00972: loss did not improve from 0.00111\n",
            "Epoch 973/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2566 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2604\n",
            "\n",
            "Epoch 00973: loss did not improve from 0.00111\n",
            "Epoch 974/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2571 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00974: loss did not improve from 0.00111\n",
            "Epoch 975/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2556 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00975: loss did not improve from 0.00111\n",
            "Epoch 976/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2553 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00976: loss did not improve from 0.00111\n",
            "Epoch 977/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00977: loss did not improve from 0.00111\n",
            "Epoch 978/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00978: loss did not improve from 0.00111\n",
            "Epoch 979/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2558 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00979: loss did not improve from 0.00111\n",
            "Epoch 980/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00980: loss did not improve from 0.00111\n",
            "Epoch 981/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2600\n",
            "\n",
            "Epoch 00981: loss did not improve from 0.00111\n",
            "Epoch 982/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00982: loss did not improve from 0.00111\n",
            "Epoch 983/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2557 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00983: loss did not improve from 0.00111\n",
            "Epoch 984/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2596\n",
            "\n",
            "Epoch 00984: loss did not improve from 0.00111\n",
            "Epoch 985/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00985: loss did not improve from 0.00111\n",
            "Epoch 986/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00986: loss did not improve from 0.00111\n",
            "Epoch 987/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00987: loss did not improve from 0.00111\n",
            "Epoch 988/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2554 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00988: loss did not improve from 0.00111\n",
            "Epoch 989/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2555 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00989: loss did not improve from 0.00111\n",
            "Epoch 990/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00990: loss did not improve from 0.00111\n",
            "Epoch 991/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2596\n",
            "\n",
            "Epoch 00991: loss improved from 0.00111 to 0.00110, saving model to poids_train.hdf5\n",
            "Epoch 992/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2555 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2596\n",
            "\n",
            "Epoch 00992: loss did not improve from 0.00110\n",
            "Epoch 993/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2558 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00993: loss did not improve from 0.00110\n",
            "Epoch 994/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2596\n",
            "\n",
            "Epoch 00994: loss did not improve from 0.00110\n",
            "Epoch 995/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2559 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00995: loss did not improve from 0.00110\n",
            "Epoch 996/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2555 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 00996: loss did not improve from 0.00110\n",
            "Epoch 997/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2558 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2601\n",
            "\n",
            "Epoch 00997: loss did not improve from 0.00110\n",
            "Epoch 998/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2565 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00998: loss did not improve from 0.00110\n",
            "Epoch 999/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0010 - mse: 0.0010 - My_MSE: 6.2550 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2596\n",
            "\n",
            "Epoch 00999: loss did not improve from 0.00110\n",
            "Epoch 1000/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2561 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2597\n",
            "\n",
            "Epoch 01000: loss improved from 0.00110 to 0.00110, saving model to poids_train.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfbomV0LS9LD"
      },
      "source": [
        "model.load_weights(\"poids_train.hdf5\")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MDY8O1-l6kN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "af28723e-bc9f-461f-df4d-02f895e92fad"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, \"Evolution de l'erreur en fonction de la période\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbzro22hgt4b"
      },
      "source": [
        "**3. Prédictions single step**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMmVn1e5zEAm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1544ed71-a94b-4285-8d76-5932e3cde267"
      },
      "source": [
        "# Création des instants d'entrainement et de validation\n",
        "y_train_timing = serie_etude.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_train)]\n",
        "y_val_timing = serie_etude.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_val)]\n",
        "\n",
        "# Calcul des prédictions\n",
        "if nb_input_bins == 1:\n",
        "  pred_ent = model.predict(tf.expand_dims(x_train[:,:,0],2), verbose=1)\n",
        "  pred_val = model.predict(tf.expand_dims(x_val[:,:,0],2), verbose=1)\n",
        "else:\n",
        "  pred_ent = model.predict(x_train, verbose=1)\n",
        "  pred_val = model.predict(x_val, verbose=1)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 0s 1ms/step\n",
            "12/12 [==============================] - 0s 2ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2xmissP6rXM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "1ea25bf6-8be5-4831-88f2-5c5cf070f09c"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbes originales\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index,y=serie_entrainement[0],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index[temps_separation:],y=serie_test[0],line=dict(color='blue', width=1)))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index[taille_fenetre+horizon-1:],y=pred_ent[:,taille_fenetre-1,0],line=dict(color='green', width=1)))\n",
        "\n",
        "# Courbes des prédictions de validations\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index[temps_separation+taille_fenetre+horizon-1:],y=pred_val[:,taille_fenetre-1,0],line=dict(color='red', width=1)))\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"a30490cf-ce7a-4d08-8002-52053ba4216f\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"a30490cf-ce7a-4d08-8002-52053ba4216f\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'a30490cf-ce7a-4d08-8002-52053ba4216f',\n",
              "                        [{\"line\": {\"color\": \"blue\", \"width\": 1}, \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500], \"y\": [-0.3997737259124131, -0.39663821834604174, -0.39365137681048884, -0.3907864798683186, -0.38801757504432344, -0.3821324507898519, -0.3763934293587709, -0.3706210862311272, -0.36463022967025455, -0.3577473769244706, -0.35024615038670454, -0.34194712553700274, -0.3326497313941316, -0.3199234065145797, -0.3051080675812845, -0.28778078536864177, -0.2674609584839191, -0.23571563448930655, -0.1966074971580348, -0.14853454184766277, -0.0896512592100983, -0.006225265450336413, 0.09729627454420649, 0.2225345894716806, 0.3675224176311685, 0.45838812095042686, 0.5508558289121261, 0.6407603294459197, 0.722847047324601, 0.7907720441641028, 0.8375505797233825, 0.8571591165468613, 0.8462014047925644, 0.8092912178307217, 0.7480946404894724, 0.6657516018680006, 0.5671962762650251, 0.47056335623297907, 0.36912442227360964, 0.26621164404319425, 0.1646829978238482, 0.06960722630425478, -0.020784283640840975, -0.10554314526311386, -0.18409904490985485, -0.30078906306526343, -0.4001453589493666, -0.4835585687120818, -0.5528548815255133, -0.6036192046351864, -0.6466106012197352, -0.6830658188631247, -0.7141831570379003, -0.7449865023027574, -0.7713362746616284, -0.7942833891599544, -0.8148146806574793, -0.8352498518764578, -0.8543521552329252, -0.8725509279710562, -0.8902306512050359, -0.9106978625168631, -0.9306203922502604, -0.949722695606728, -0.9676907796763472, -0.9869853235899075, -1.0028259454943649, -1.0141168742142899, -1.0201340036512985, -1.0203134281712518, -1.0140335699728829, -1.0017814384674935, -0.9847553331276295, -0.9601933979497229, -0.9339974180365264, -0.9090189616515849, -0.887699483869979, -0.8739030198892763, -0.8646498410745367, -0.8603308365585154, -0.8610421266197593, -0.8688406852191626, -0.8840661373409225, -0.9057765042552841, -0.9318699558713645, -0.9582069121930961, -0.983762090249316, -1.0056582897022008, -1.021018310213926, -1.0272469042637369, -1.0256384916027261, -1.0160841359152073, -0.9996539763023317, -0.9777385527937374, -0.9527985445202147, -0.9272818145754133, -0.9034247614401807, -0.884463434492248, -0.8696288715032435, -0.8596195464972716, -0.8548391646442274, -0.8556786150768666, -0.8629709402092584, -0.8763636990200658, -0.8949661769280917, -0.9198164729416379, -0.9476721296644034, -0.9760916920213083, -1.0017750304489237, -1.020063515447031, -1.0308994748485025, -1.0325399276023621, -1.0247926331515171, -1.010791112576582, -0.9912594719759403, -0.9679663244748469, -0.9429366039413475, -0.9169777207152321, -0.8934282524713483, -0.8739158359264159, -0.8595682823487135, -0.851346794523706, -0.8484119220187538, -0.8508149289824154, -0.858331534764749, -0.8755819207545547, -0.9002720163038566, -0.9307036964916674, -0.9635063435502911, -0.9922655308913934, -1.0165134731593815, -1.0328539205122806, -1.0388838659864288, -1.0348916704174658, -1.022620314856367, -1.0031655704785625, -0.9785523711520977, -0.9484346838742052, -0.9182529164106153, -0.890852229006303, -0.8684690201421161, -0.85476867643996, -0.8459384268508247, -0.8422089600432219, -0.8435930920542909, -0.8539164099701814, -0.8738389397035787, -0.9022264619676347, -0.9360672080345601, -0.9659990627739292, -0.9948159222821595, -1.0194996098128917, -1.0370447646569065, -1.0446126345877982, -1.04320287050245, -1.0326937200480366, -1.014161730344278, -0.9895997951663714, -0.961334025255141, -0.9319853002056202, -0.9039117708514828, -0.8806891115546568, -0.8615483600867706, -0.8472969267876147, -0.8384282290870608, -0.8349999391522369, -0.8381911323999797, -0.8478159762917657, -0.863329789249165, -0.8875328753871651, -0.9173301617365692, -0.9507351625407506, -0.9845438685148273, -1.0143731949570804, -1.0373779816225344, -1.050059450372098, -1.0505080116719814, -1.0413445451172185, -1.0241774633688199, -1.000487018716401, -0.9724839775665309, -0.9395723941922213, -0.9075323013433996, -0.87897176257796, -0.855832407522541, -0.8413502855548735, -0.831956130331599, -0.8278165503355313, -0.8289251375481004, -0.8369672008531547, -0.8520901246777984, -0.8737107793321833, -0.9006372733623331, -0.9313316823115042, -0.9639933529615932, -0.9961167500518218, -1.0245683525015754, -1.0454456770018676, -1.0573005113559315, -1.0580694735843033, -1.0477525636869827, -1.0304701376043282, -1.0068181410633281, -0.9788086918948882, -0.9486397404684378, -0.9174583221079644, -0.8885325262840482, -0.863522029806258, -0.8436251321471399, -0.8299632365564024, -0.8218635010842202, -0.8194092299720005, -0.8225171189783361, -0.8329301491542032, -0.8503407356082527, -0.8741401165763576, -0.9031107685302622, -0.9356955429575138, -0.970004074380032, -1.0034282992399228, -1.032738576178025, -1.0539683416996541, -1.06550277512523, -1.0655668553109277, -1.0539683416996541, -1.0353722718101979, -1.010246430998152, -0.9806990573729688, -0.9489985895083445, -0.9164778952667905, -0.8862448636546425, -0.8599399474257599, -0.83872299794127, -0.8236641543023239, -0.8142379589862007, -0.8105341242528769, -0.8124821618980852, -0.821568732230011, -0.8377297550629565, -0.8604846290041898, -0.8888016630639783, -0.9213543973983811, -0.9565984995320851, -0.9922334907985446, -1.0252219703956915, -1.0524304172429106, -1.0701165484954602, -1.0754992840940623, -1.0679378221817404, -1.0519177757573295, -1.0283106353463176, -0.9989939503896459, -0.9662938316281384, -0.9309600172344581, -0.8971192711675324, -0.8667708952211286, -0.8413887336662922, -0.8229977203710683, -0.8100214827672957, -0.8026202213192178, -0.8007747119711257, -0.8076376998593433, -0.8259902650431483, -0.8552108297212737, -0.893133483617139, -0.928755658846459, -0.966825697169429, -1.0047227189910153, -1.0389992103206847, -1.0654386949395322, -1.0814587413639432, -1.0844705100917325, -1.0740895200087142, -1.0555703463420951, -1.0295089348188635, -0.997987891474193, -0.9633717751603259, -0.9268973334612274, -0.8921402407388256, -0.8609844544526314, -0.8347756585022953, -0.8151478976231071, -0.8010566647881954, -0.7926429364060947, -0.7898810804025262, -0.795571400892477, -0.8130076194208058, -0.8416963185576408, -0.879676644620634, -0.9158370934098141, -0.9552912637438532, -0.9956553727147988, -1.0336100667035129, -1.0657590958680203, -1.0874822788195215, -1.0953000614746342, -1.088123080676498, -1.0714622323951108, -1.0462146392302392, -1.0143475628828011, -0.9783857626692838, -0.9392904413751516, -0.9012652591821702, -0.8664889424040589, -0.8365378636089804, -0.8139047420205727, -0.7966671720679068, -0.7849981702523658, -0.7789169606296595, -0.7793719299481129, -0.7902911935909912, -0.8113351265740973, -0.8413951416848618, -0.8744669255234155, -0.9128509567563039, -0.9549067826296673, -0.9979942994927627, -1.03314228134792, -1.0640930110398819, -1.0880590004908004, -1.1027333630155607, -1.1063218534146289, -1.0979273490882375, -1.0778702509648752, -1.0480729646154712, -1.0085162659843159, -0.9646661949114185, -0.92029707433437, -0.8785103852409369, -0.8467009810606267, -0.8195437983619654, -0.7976219668348016, -0.7812751114633327, -0.7693305648492921, -0.7647360155347711, -0.7672992229626767, -0.7766677461116722, -0.796442891417965, -0.8253046070561836, -0.8626377232436305, -0.9068338273192952, -0.9525037756660056, -0.9997693206365874, -1.0449330355162862, -1.0831889063777795, -1.1075393769428838, -1.118881569811367, -1.115100838855206, -1.096389424631494, -1.0703087890525531, -1.0365897953384533, -0.9978405070470883, -0.9566625797177828, -0.9157537891684072, -0.8771134371927282, -0.8423499364517567, -0.8125590581209223, -0.7883175238715038, -0.7699329185948499, -0.7574565064395188, -0.7507985751455336, -0.7509203274983591, -0.7629033222238184, -0.7864912385791211, -0.8204473289803021, -0.8547622684213902, -0.894767528352429, -0.9392455852451631, -0.9860882009901405, -1.0263433736454002, -1.0638366902970913, -1.09568454258882, -1.119201970739855, -1.1318898475079886, -1.1318898475079886, -1.1184330085114833, -1.0927368540467284, -1.0581335537700012, -1.0168466901250093, -0.9719713360809497, -0.9264487721613438, -0.8854630853891312, -0.8478992805331725, -0.8149108009360259, -0.7872345687332136, -0.7641785179192015, -0.7474471814335469, -0.7369508470162729, -0.7324844580731472, -0.7343043353469603, -0.7432691533260605, -0.7591610393790761, -0.7816339605032396, -0.8103290676586444, -0.8451053844367554, -0.8854246372777126, -0.930197463024656, -0.974598623694553, -1.0195700980171591, -1.0624910063974407, -1.1002342357733526, -1.1294548004514782, -1.1465001298470512, -1.1489351769035616, -1.136119139764033, -1.112922112541486, -1.080048977278595, -1.0398642928276027, -0.9951683633034966, -0.9483129315213797, -0.902380254413309, -0.8594593460330275, -0.8210624987629995, -0.789182606378422, -0.7627879778895628, -0.7420259977235263, -0.7268774418246033, -0.7151635838790742, -0.7121710392069943, -0.7175281427313173, -0.7306838048550434, -0.7507729430712546, -0.7780454701041716, -0.8123924496381085, -0.8533332802803328, -0.9045461646898894, -0.9611994568651758, -1.0203326522269613, -1.0770372085508058, -1.1131784332842767, -1.1421426772196115, -1.1611744923718115, -1.1685437137270405, -1.1629687375713456, -1.1444495639047265, -1.1140755558840436, -1.0740895200087142, -1.0237545341432155, -0.9697477536372415, -0.9158114613355349, -0.8648677137059085, -0.8250867344248116, -0.7899900167182123, -0.7600709780159826, -0.7355538989680642, -0.7137345957380168, -0.6989256648232913, -0.6907938892582605, -0.6889227478358892, -0.6935493372432591, -0.7049940584088581, -0.7231543830355702, -0.747799622454884, -0.7787759842211248, -0.8162244447428275, -0.8599079073329109, -0.9091278979672708, -0.9600716455968974, -1.0131620794473948, -1.0657590958680203, -1.1140755558840436, -1.1541897521307685, -1.181039349938081, -1.1907795381641229, -1.1818723923521501, -1.160021049029254, -1.1263148713522935, -1.0831889063777795, -1.0335844346292338, -0.9805132248344456, -0.9271728782597273, -0.8760497061101473, -0.8289956257523678, -0.7895799035297474, -0.7553674923857756, -0.7265954890075338, -0.7032959334878707, -0.682271224560474, -0.6684042723755038, -0.6612272915773677, -0.6602276406804846, -0.6664177866188769, -0.6805474675652072, -0.7026102755009058, -0.732381929776031, -0.7656908103016661, -0.8055422777870305, -0.8518530279907173, -0.9040976033900058, -0.9586875135858282, -1.0160585038409284, -1.0735127983374353, -1.1272119939520606, -1.1727089257973875, -1.2044926979034185, -1.2178854567142259, -1.2108366362874852, -1.1893056938930768, -1.1546383134306517, -1.1092695419567205, -1.0563393085704669, -0.9995963041352037, -0.941968993137313, -0.8861551513946657, -0.8341476726824584, -0.790541106315212, -0.7519904665995099, -0.718784114370991, -0.6909733137782138, -0.665219487146331, -0.6460659196413053, -0.6330448259075443, -0.6256051163480478, -0.6233174537186419, -0.6287706775215114, -0.6417276910695748, -0.6617463410815186, -0.6835592362929965, -0.7105369944717043, -0.7429295283418631, -0.7809226704419957, -0.8430420024572912, -0.916080598115465, -0.9979302193070649, -1.0829966658206867, -1.136311380321126, -1.1836666375516842, -1.2213457867418984, -1.2456321771213055, -1.2535140399621156, -1.243197130064795, -1.2145532870579485, -1.1700816381837837, -1.1134988342127647, -1.048970087215238, -0.9810835384871547, -0.9135814708732571, -0.8592735134945044, -0.8088744474433078, -0.7632429472080161, -0.7228980622927799, -0.6861352597580417, -0.655293466381766, -0.6300907293468828, -0.6101297515020669, -0.5933920069978426, -0.5820498141293596, -0.5755905314110371, -0.573475885283015, -0.5752573144454094, -0.5807938424896858, -0.5899829411187278, -0.6027477141096984, -0.6206004538450619, -0.6429708466721092, -0.6702177416307471, -0.7027256198351616, -0.7572450418267165, -0.824016595323661, -0.9032389289016575, -0.9931818775468697, -1.0533275398426776, -1.1139473955126484, -1.172068123940411, -1.2241012347268974, -1.2661378365445513, -1.2935641560231428, -1.302855782949301, -1.292410712680585, -1.2658815158017607, -1.2244216356553854, -1.1706583598550626, -1.1079238580570698, -1.0422416677169855, -0.9749895128273086, -0.9088587611873407, -0.8459768749622433, -0.7923609835890252, -0.7434485778460139, -0.6996113228102561, -0.660977378853147, -0.6262395101864544, -0.5966729125055619, -0.5719123287519925, -0.5515156056444325, -0.5343292998403245, -0.5209109089552382, -0.5108375037635685, -0.5036541149468627, -0.4989826694095046, -0.4965732544272732, -0.49620799736879656, -0.49770106569555167, -0.5011934358160732, -0.5066082115075241, -0.5139582088070438, -0.5232626517703416, -0.5368604671753815, -0.5536110277167455, -0.5739052225271892, -0.5982236529994448, -0.6466298252754447, -0.7095886077233793, -0.7897977761611193, -0.8896347054780477, -0.9659734306996502, -1.0494186485151213, -1.1368881019924046, -1.2228196310129444, -1.2819656424118693, -1.3317559466989382, -1.3678330912467114, -1.3866726658418185, -1.385519222499261, -1.3631552376907834, -1.3204137538304555, -1.2601783792746706, -1.1875755288792407, -1.1067063345288146, -1.0221525295007743, -0.9378422291783849, -0.8714743808513357, -0.8086437587747963, -0.7502090374371153, -0.6966828583238738, -0.6409907689340519, -0.5920975872467501, -0.54968291233348, -0.5132533267643697, -0.4740106210431329, -0.44230246355622505, -0.41682225931727107, -0.39617799445236124, -0.38159417722999167, -0.36901844078682916, -0.35795307432056006, -0.34788671794931736, -0.3396184515887504, -0.33156036823727175, -0.32351189691364773, -0.31525836899579124, -0.30513369965556364, -0.29420161997554567, -0.28220580921294686, -0.26883868247641846, -0.2509731267039155, -0.23032008285356498, -0.20635409340264635, -0.17844076451275295, -0.13944797151573693, -0.09287449255068975, -0.03763096446075142, 0.027282263650961296, 0.09890468720521729, 0.1796457211842479, 0.2678136486856354, 0.35932015386187005, 0.42718107051567444, 0.4893388506423885, 0.5413078812431772, 0.5782821483907176, 0.5960964400146623, 0.591482666644432, 0.5634796254945619, 0.5138815617645858, 0.4506984986667096, 0.3757246814004669, 0.29344572296469273, 0.20825111607967595, 0.13704521373245462, 0.06831921457173214, 0.003405986460019419, -0.05680375602148631, -0.11377744912526103, -0.164951885423399, -0.2103911451015979, -0.250332324846939, -0.2968160915520095, -0.3349822501535259, -0.3660880538948759, -0.3914593218181439, -0.40710770316550837, -0.42076895795438896, -0.43281603286554593, -0.4436141849574558, -0.4534972719976033, -0.4627228963324931, -0.4715307178566341, -0.48016872688867646, -0.4901011556718112, -0.5003603934020039, -0.5112091688406148, -0.5229102107490046, -0.5380075024993693, -0.5550207918020936, -0.5743601918456425, -0.5964998960041782, -0.626989248359117, -0.6629190084797855, -0.7051478508545326, -0.7545985301574039, -0.8264003782316133, -0.9112874002252814, -1.0083304334457925, -1.1132425134699744, -1.1796936660384305, -1.242812648950609, -1.298690570878954, -1.3429058990103282, -1.371293421274384, -1.3798801661578681, -1.3666796479041536, -1.331948187256031, -1.282734604640241, -1.2200641830279457, -1.1476535731896087, -1.0694757466384837, -0.9975329221557396, -0.9264615881984835, -0.8581777423190746, -0.7940398844543034, -0.7406931298610153, -0.6917422760065856, -0.6473218912809792, -0.6074191596470566, -0.5563664757017442, -0.5139325767327647, -0.4790793637318165, -0.4506745398176221, -0.43311592813461086, -0.4180391420436694, -0.405061750996554, -0.3938100393502762, -0.3839388712246684, -0.3751810322453715, -0.3672863533674219, -0.3600100482814545, -0.3525075401399744, -0.34526135274128483, -0.33806514788743947, -0.3307081017674931, -0.32230078140396223, -0.3132654752205945, -0.30338431058601795, -0.2924201908131512, -0.27758562782414675, -0.26045699418716667, -0.24057291256518792, -0.21738229336121082, -0.1851115118438776, -0.14641989571964054, -0.10019244975736065, -0.04522446646592216, 0.03494625386039945, 0.1310985724997133, 0.2419893338494852, 0.3613707198041947, 0.43083364110044015, 0.4954905484693622, 0.5507917487264287, 0.5920593883157109, 0.6150641749811647, 0.6160894579523272, 0.5934691524010589, 0.5486130224127086, 0.4892106902709932, 0.41673600024695867, 0.3354182445966491, 0.24955079576180703, 0.17526263648252907, 0.1026597860870992, 0.033369881292237454, -0.03150489870805665, -0.09292575669924787, -0.14860503005193018, -0.19847863858040599, -0.2427003747303497, -0.29475911759111517, -0.3380068349184547, -0.37367642948519086, -0.4031106375024322, -0.4226272833396206, -0.43966364150919607, -0.45470518349800393, -0.46823058829320546, -0.48068777639282734, -0.4924080423569263, -0.5037053790954209, -0.5149065955553689, -0.5280174015491067, -0.5417305612884025, -0.556398515794593, -0.572386522126155, -0.5930844221064939, -0.6165377700718313, -0.6432527994891787, -0.6738190480669546, -0.7162209069430853, -0.765799746617352, -0.8231899609281613, -0.8886991347668621, -0.9618274426850129, -1.0410882243744277, -1.1232390224388067, -1.2023780517753961, -1.2544111625618826, -1.2968963256794201, -1.3260528101718478, -1.338932927497074, -1.33361427208417, -1.309456042076158, -1.2674194402585042, -1.2105803155446946, -1.1425271583337973, -1.0677455816246475, -0.9904584696547198, -0.9141902326373847, -0.8528719029433097, -0.7953983843910934, -0.7425194151533981, -0.6946771485115376, -0.6523201457653953, -0.6149485814665298, -0.5823253589278595, -0.5541428932580359, -0.5272868874321536, -0.5049805747908039, -0.4866600496998477, -0.4717678145437154, -0.4597547021309782, -0.45021700729174097, -0.4427805017415295, -0.4370773652144392, -0.43290318191809474, -0.42987731554945197, -0.4278350800312681, -0.4266169157011559, -0.42608440935800845, -0.42625101784082237, -0.42705778737875566, -0.4284496090121085, -0.43069433791709694, -0.43367150334460947, -0.4374221166134925, -0.44200128668344607, -0.44872393896498586, -0.4570030989571214, -0.46713481711777577, -0.479502292957421, -0.502475039530026, -0.5329451678292555, -0.573283644725922, -0.6269956563776867, -0.685013856508333, -0.7578538035908441, -0.8478736484588936, -0.9563678108635736, -1.0444203940307053, -1.1395153896060082, -1.2377503142804953, -1.3325889891130074, -1.3966691748106508, -1.4490226865256257, -1.485035750887701, -1.5009276369407167, -1.494071057071069, -1.463504808493293, -1.4104464147356441, -1.338420286011493, -1.2737633786425708, -1.203147014003768, -1.128749918408804, -1.0528148983570966, -0.9770721188624822, -0.9031620326788202, -0.8322637152229476, -0.7652358409832127, -0.6817521750563229, -0.6069641903286034, -0.5409103349114726, -0.48324457580216335, -0.4269866583726313, -0.3794346749719811, -0.33930702188625983, -0.30526186002695893, -0.27491989209912476, -0.2485188555916957, -0.22500783545923034, -0.20329746854486877, -0.18354795531285512, -0.16402272273078317, -0.14419631327593233, -0.12354967744415163, -0.09800090740650122, -0.07025418699942167, -0.039892995015878195, -0.006532850341685067, 0.03804132682959564, 0.08728054151966479, 0.14059525602010411, 0.19672949869123965, 0.24807695149076123, 0.29767501522073725, 0.3425311452090877, 0.3793772519852326, 0.4049452460785922, 0.4159670380185869, 0.4105843024199848, 0.3888611194684838, 0.35566758327710457, 0.31202897681700925, 0.26089298863029, 0.20534828366757263, 0.15048923669182018, 0.09691820144859035, 0.0465255434159636, 0.0006313144193114399, -0.03970075445878526, -0.07445784718118706, -0.10356947554362643, -0.1271445758617894, -0.14866270221905806, -0.163157640223865, -0.1711676634360704, -0.17331434965694148, -0.16949517058936192, -0.1593128290820064, -0.14281858928343297, -0.12013420354646723, -0.09316285338632915, -0.06025767803058926, -0.021149540699317543, 0.024276902941741826, 0.09673877692863699, 0.17954960090570143, 0.2681340496141236, 0.3522713334351293, 0.39360305321010924, 0.42564314605893094, 0.4455080036252004, 0.4511470599665932, 0.44127871136915603, 0.4160311182042845, 0.3768781247430244, 0.3265110987846768, 0.2669165260858685, 0.2030670290567366, 0.13864721837489574, 0.07653429437817, 0.025776379287066713, -0.02057281902803873, -0.06194298691443728, -0.09806498759219884, -0.1315853327306361, -0.1591077724877739, -0.18100397194065868, -0.19773530842631334, -0.21015404841451663, -0.21804872729246627, -0.22181664221148772, -0.22184227428576678, -0.21843961642522192, -0.21171119692696935, -0.20170827993956722, -0.1884500895187248, -0.16848270365533916, -0.14353628736324658, -0.1131815033982729, -0.07699542253481374, -0.01661907157049413, 0.05698983774038873, 0.14342760022793993, 0.23930437406875393, 0.29825173689201595, 0.3554753427200115, 0.4078929346206839, 0.4517237816378719, 0.48318715281541486, 0.4986304775685468, 0.49542646828366466, 0.47338288440367526, 0.4371775794845068, 0.3879639968687167, 0.32888206565548944, 0.26332803568680035, 0.19864549624359917, 0.13413597330178156, 0.07199100921220708, 0.013812608617316693, -0.037079874863751666, -0.0829100236747062, -0.12348559725845396, -0.15881300363356476, -0.19114145731802584, -0.21810639945959417, -0.24017561541386254, -0.25786815468498187, -0.27445851476210176, -0.28599294818767756, -0.29313788889296477, -0.29659181090206777, -0.29705959625766054, -0.2954191435038009, -0.29179220499331426, -0.2862749010047472, -0.2780598211983093, -0.2674994065953376, -0.2544462727687277, -0.23868895510567723, -0.21339009779224763, -0.18207411104180932, -0.1437349359389093, -0.09717427301100162, -0.03645829706248455, 0.03672768302279394, 0.12283222854471733, 0.2204455754179375, 0.2887037892230673, 0.3579103897765221, 0.4250023442019545, 0.4856862800576228, 0.5354125041589941, 0.5687342007217686, 0.5813579973042043, 0.5711051675925813, 0.5423331642143396, 0.49632359088343175, 0.43576781539915876, 0.36451064890337925, 0.2910747560938799, 0.21509487991218423, 0.13962123719749991, 0.06690945048638397, 0.006469019336366747, -0.04974852757617577, -0.1013074449884996, -0.14799626828780257, -0.20083038139550952, -0.24609021655375501, -0.2843909435452364, -0.31650152459832553, -0.34036562655398495, -0.3606361116957204, -0.377832029527683, -0.39247691516702243, -0.40506059755321144, -0.4159603808196378, -0.425527552544296, -0.4341085302110674, -0.4419820626277368]}, {\"line\": {\"color\": \"blue\", \"width\": 1}, \"type\": \"scatter\", \"x\": [1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500], \"y\": [-0.4494108785556646, -0.45660964661693787, -0.46379752104664257, -0.471722958413727, -0.48006619859156024, -0.4890117925149512, -0.49879683687098136, -0.5118115225861728, -0.5266524935937469, -0.5436914149707502, -0.5633960720727756, -0.5906814151428321, -0.6231764773101072, -0.6618232373043559, -0.7077046502638684, -0.7730536236383252, -0.8517633157307406, -0.944307919915277, -1.0489060070295404, -1.1197786924111337, -1.191292179649704, -1.2599861387175775, -1.3215671971730127, -1.371293421274384, -1.4037820754230892, -1.4146757069916884, -1.4019237500378576, -1.3702681383032218, -1.3207341547589435, -1.2563976483185095, -1.181039349938081, -1.1045276082150945, -1.0255680033984587, -0.9470056957331479, -0.8711411638857078, -0.8089961997961334, -0.7508498392940918, -0.6971442356608969, -0.6481100775650601, -0.5848052621143583, -0.5309714981097682, -0.4857308870072319, -0.44805045621330364, -0.423295639676447, -0.4018375564531771, -0.3831820842315793, -0.36683779206753836, -0.35234221326087445, -0.3393224011308273, -0.3274271962597737, -0.3163156920598024, -0.30460824213284293, -0.29313788889296477, -0.28160345546738896, -0.26969094894619705, -0.25512552273712275, -0.2392977168698048, -0.22182945824862724, -0.20230422566655532, -0.1761979580133354, -0.14606104667973374, -0.11127191386448314, -0.07111286148777007, -0.014119944328286059, 0.0526003450201002, 0.1290480065573887, 0.2138389082725104, 0.27928400192551367, 0.34509435263699334, 0.4081492553634743, 0.4643475782203076, 0.5091396280229604, 0.5374630701013187, 0.5454090131278264, 0.5314395326457402, 0.500745123696569, 0.4542869890657776, 0.39507689748115526, 0.32663925915607195, 0.25672777655994306, 0.18527196148850097, 0.1149439576853375, 0.047813555148486245, -0.009128097862439665, -0.06154568976311191, -0.10905473943934467, -0.15152708651974273, -0.19250636527338563, -0.22783377164849644, -0.2579450509078191, -0.2833464365183649, -0.3039161761273084, -0.32107684985713725, -0.3352834270263048, -0.3469601184641294, -0.3565176781609329, -0.3642611278006361, -0.37046985699288076, -0.3754149249231679, -0.3794269853496974, -0.3825226991207505, -0.38485457707828774, -0.38656808124384273, -0.38786570500422, -0.38866798892915455, -0.38904734362848453, -0.3890761797120485, -0.38875577878356027, -0.38807652881516524, -0.3870480418347181, -0.38568121147378737, -0.38363833515374646, -0.38105205885898963, -0.37783651514068184, -0.37388469008870817, -0.36780988848457163, -0.3600600308262986, -0.3501743805787232, -0.3375172622997247, -0.32127549843279996, -0.3004814781739147, -0.2738497529979741, -0.2396245258168628, -0.19713295468075548, -0.1431582142676305, -0.07506020092674487, 0.00996138945688831, 0.08719082925968812, 0.17611490295230778, 0.27601591245493373, 0.3839269451697652, 0.46582142249135333, 0.5457934942420123, 0.619357547422907, 0.6811949266211327, 0.72585881605239, 0.7477742395609842, 0.7431604661907537, 0.7112485337133275, 0.6601766257123057, 0.5906496242303626, 0.5065764205950546, 0.41250670799091405, 0.3253576554421191, 0.23688214304938296, 0.1495792980549137, 0.06529462980680337, -0.018099323860109713, -0.09571965279566509, -0.16699604334715382, -0.2317042148646341, -0.312464472899374, -0.38118790885266857, -0.4391817585127498, -0.48794806143237035, -0.532060861266628, -0.5691632887855635, -0.6008317165573389, -0.6286873732801044, -0.6518908085212212, -0.6738190480669546, -0.6951385258485606, -0.7164900437230154, -0.741487724163666, -0.7677029281325719, -0.7955073207067794, -0.8252405268704859, -0.8634835816948395, -0.9044628604484823, -0.9475375612744382, -0.991624729034417, -1.0358144250915118, -1.0769090481794106, -1.1119609097560215, -1.1381697057063576, -1.1528440682311178, -1.15386935120228, -1.140412512205775, -1.1136269945841601, -1.0776139302220846, -1.0340906680962452, -0.9863573377700707, -0.9374000758970712, -0.8926785142986858, -0.8511032898180547, -0.8139175580577124, -0.7819415453945883, -0.7544575537488691, -0.733170116060112, -0.717970296012631, -0.7086530370121936, -0.7047569617217769, -0.7079225228952404, -0.7179062158269333, -0.7343555994955184, -0.7570079451396353, -0.785946557000691, -0.8210496827258601, -0.8619392492195262, -0.9178299871850107, -0.9791034607490973, -1.0419853469741949, -1.1002983159590503, -1.1336840927075225, -1.1586112849439056, -1.1728370861687827, -1.174759491739712, -1.1637376997997173, -1.1401561914629845, -1.105552891186257, -1.062426926211743, -1.0112717139693144, -0.9576814546703752, -0.9049050137297961, -0.8554479264083551, -0.815666947127258, -0.7807047978106239, -0.7509715916469173, -0.7266339371189524, -0.7047633697403467, -0.6899480308070515, -0.6818162552420206, -0.6799130737268005, -0.6861801158880302, -0.7024885231480804, -0.7288511115440908, -0.7647488315719105, -0.8008772402682419, -0.8433880354600585, -0.8918454718846164, -0.945205042515044, -0.9954631321577058, -1.0462787194159369, -1.094979660546146, -1.1382978660777527, -1.1725166852402944, -1.1937272267062145, -1.1989818019334213, -1.1872551279507524, -1.1629687375713456, -1.1272119939520606, -1.0823558639637103, -1.0313928922783744, -0.9776232084594818, -0.9238150765291706, -0.8723586874139632, -0.8249842061276954, -0.7846905853610172, -0.7496515398215459, -0.720091350159223, -0.6960100163740485, -0.6741458570140126, -0.6593177020435781, -0.6510449500700122, -0.6487829195148854, -0.6532749405322903, -0.6654950319448308, -0.6853983376225189, -0.7127669849339824, -0.7433076014374792, -0.7801344841579148, -0.8233181212995567, -0.8726534562681724, -0.9305947601759814, -0.9931946935840092, -1.0576209122844198, -1.1194582914826456, -1.1617512140430901, -1.1959059530199343, -1.218910739685388, -1.228458687354337, -1.223075951755735, -1.2025062121467915, -1.1677747514986687, -1.12150885742497, -1.0643493317826724, -1.002281263915935, -0.9392968493937214, -0.8785103852409369, -0.8298991563707045, -0.7856902362579005, -0.746492386666652, -0.7125875604140289, -0.6811049651807768, -0.6559086361644634, -0.6366140922509029, -0.6227663641216422, -0.6135836735111699, -0.6093351571994161, -0.6096747821836137, -0.6142565154609951, -0.6228368523259097, -0.635415792778357, -0.6520510089854653, -0.6728193971700714, -0.7043019924033236, -0.7428141840076072, -0.7887853092270966, -0.8423819765446056, -0.9060584570723538, -0.9767389018968543, -1.0517255352002366, -1.1259303902381077, -1.1754002935966883, -1.217308735042947, -1.2481313043635136, -1.2647921526449009, -1.2651766337590866, -1.248195384549211, -1.2144892068722508, -1.1666213081561112, -1.105552891186257, -1.0375894462353366, -0.9671845462093357, -0.8981061060272761, -0.8437853326113839, -0.7935977311729897, -0.7482609997919069, -0.7081788436380311, -0.6715698335489674, -0.6407280401726917, -0.6153330625807156, -0.5949747875845742, -0.577634689334792, -0.5653441097179841, -0.5575904072485692, -0.5538353083666873, -0.5536302517724548, -0.5567701808716393, -0.563114119255706, -0.5725531306089688, -0.5859715214940554, -0.6030617070196169, -0.6240864159470136, -0.649366049204734, -0.6907041769982837, -0.7416543326464801, -0.8032033510090665, -0.8758959136644731, -0.9470697759188456, -1.0247605930586683, -1.1058092119290475, -1.184435599780056, -1.2361483096380543, -1.278761633126987, -1.3086229996620888, -1.3227206405155703, -1.3190039897451071, -1.2966400049366296, -1.2567180492469976, -1.201865410289815, -1.1353501775356611, -1.0620424450975574, -0.9861394651386985, -0.9112745841881419, -0.8513083464122873, -0.7952381839268493, -0.7438202429230604, -0.6974902686636641, -0.6572927681755325, -0.6219397297261426, -0.5912196887026924, -0.5648250602138332, -0.5400132123117056, -0.5195972651484364, -0.5030645772384444, -0.4898896910590089, -0.4795791891802581, -0.47177422256228513, -0.4661351662208925, -0.462343541633163, -0.4601295712173094, -0.45933177290537375, -0.45981814151481887, -0.46146884709839014, -0.46434092102135854, -0.46837797272031007, -0.4736069158732377, -0.48004697453585093, -0.4897423066319044, -0.5016291810788173, -0.5160087747493683, -0.5332719767763135, -0.5664270648562743, -0.609610701997916, -0.6653925036477146, -0.7371430875733659, -0.8201077039961048, -0.9214761497512068, -1.040402566387463, -1.1700816381837837, -1.241402884865261, -1.308110358176508, -1.3657825253043867, -1.4096774525072724, -1.4353736069720273, -1.4388980171853978, -1.4183923577621518, -1.3749459918591498, -1.316312621945806, -1.243709771550376, -1.161430813114602, -1.0738331992659236, -0.9971484410415538, -0.921770918605416, -0.8494115729156371, -0.7812815194819025, -0.7152340720833416, -0.6551524899732312, -0.6011585255043969, -0.5531496503797225, -0.4972460963770984, -0.4504816584586722, -0.41160164658848414, -0.3792520464427428, -0.3563094175574156, -0.33629717556404154, -0.318609762707778, -0.3026345724133555, -0.28781923348006033, -0.27375363271942765, -0.26003406496156223, -0.24626323305513867, -0.22997404985079772, -0.21271725584242238, -0.1940763298229779, -0.17361552652972037, -0.14693253720522167, -0.11664824144451541, -0.08222436568774143, -0.04310982033789991, 0.011928651157805985, 0.07511812227425207, 0.146010031711555, 0.22286139841873862, 0.28325697343876755, 0.3427233857661805, 0.3980886662089443, 0.44557208381089813, 0.4812006670587878, 0.5008092038822667, 0.5013859255535454, 0.4823541104013453, 0.4487120129100826, 0.4014208358652218, 0.34355642818024984, 0.27845095951144433, 0.21314043424840612, 0.14748387598260077, 0.08384584356627113, 0.023956502013253615]}, {\"line\": {\"color\": \"green\", \"width\": 1}, \"type\": \"scatter\", \"x\": [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500], \"y\": [-0.27307790517807007, -0.24625147879123688, -0.20242346823215485, -0.14809216558933258, -0.09159816801548004, -0.028796792030334473, 0.06199105083942413, 0.17487899959087372, 0.2969359755516052, 0.43389302492141724, 0.5211577415466309, 0.6075464487075806, 0.690931499004364, 0.7660244107246399, 0.816062867641449, 0.8372583389282227, 0.8236343860626221, 0.781032383441925, 0.7236737608909607, 0.6452173590660095, 0.5486011505126953, 0.4397316575050354, 0.34074264764785767, 0.24236616492271423, 0.14437399804592133, 0.051443442702293396, -0.03227095305919647, -0.10922494530677795, -0.179734006524086, -0.24313460290431976, -0.33459120988845825, -0.41463083028793335, -0.4992128610610962, -0.5678592324256897, -0.6163685917854309, -0.6574708819389343, -0.6928642392158508, -0.7231007218360901, -0.7534038424491882, -0.779367983341217, -0.8017251491546631, -0.8218030333518982, -0.8419901728630066, -0.860934853553772, -0.8789216876029968, -0.8964428305625916, -0.9169630408287048, -0.9369686245918274, -0.9559161067008972, -0.9736941456794739, -0.9929178357124329, -1.0085850954055786, -1.019330382347107, -1.024701476097107, -1.0241732597351074, -1.017134428024292, -1.0041314363479614, -0.9864758849143982, -0.9612309336662292, -0.9345199465751648, -0.9095907807350159, -0.8886488080024719, -0.8755984902381897, -0.8671626448631287, -0.8634651303291321, -0.8648099303245544, -0.8733438849449158, -0.8894374966621399, -0.9119603037834167, -0.9386618733406067, -0.9652174115180969, -0.9906505942344666, -1.0121960639953613, -1.026862621307373, -1.032081127166748, -1.0293896198272705, -1.018862009048462, -1.0015407800674438, -0.9789096713066101, -0.9535059332847595, -0.9278408885002136, -0.9041295647621155, -0.8856410384178162, -0.8714398741722107, -0.8620370030403137, -0.857914388179779, -0.8594419360160828, -0.8674744963645935, -0.8816208243370056, -0.9008813500404358, -0.9263669848442078, -0.9547253251075745, -0.9832807183265686, -1.0087471008300781, -1.0263441801071167, -1.036194086074829, -1.0367729663848877, -1.0278688669204712, -1.0129050016403198, -0.9926876425743103, -0.9688785672187805, -0.9435798525810242, -0.917542040348053, -0.8941583037376404, -0.8751206994056702, -0.8614082932472229, -0.8539335131645203, -0.8517423272132874, -0.8548155426979065, -0.862973153591156, -0.8810864090919495, -0.9067903161048889, -0.937960684299469, -0.9711766839027405, -0.9997546076774597, -1.0233718156814575, -1.0388743877410889, -1.0437310934066772, -1.0384544134140015, -1.0250507593154907, -1.0046740770339966, -0.9793532490730286, -0.9486666321754456, -0.9182156920433044, -0.8910736441612244, -0.8692596554756165, -0.8564671277999878, -0.8485516309738159, -0.8454743027687073, -0.8475058078765869, -0.8586705923080444, -0.8796983361244202, -0.9091398119926453, -0.9437591433525085, -0.9737246632575989, -1.002105474472046, -1.0263749361038208, -1.043150544166565, -1.0496118068695068, -1.0469928979873657, -1.035367727279663, -1.0158052444458008, -0.9904265999794006, -0.961631715297699, -0.9320723414421082, -0.904102623462677, -0.8813394904136658, -0.8628422617912292, -0.8492135405540466, -0.8410314321517944, -0.8383069038391113, -0.8422512412071228, -0.8526747226715088, -0.8689208626747131, -0.8939527869224548, -0.9245650172233582, -0.958445131778717, -0.9924009442329407, -1.0219136476516724, -1.0441585779190063, -1.0557154417037964, -1.054732084274292, -1.0441954135894775, -1.0259572267532349, -1.0014184713363647, -0.9728158116340637, -0.9394388794898987, -0.9072327017784119, -0.8790406584739685, -0.8565452694892883, -0.8430001139640808, -0.8345342874526978, -0.8310722708702087, -0.8328477144241333, -0.8416303992271423, -0.8575909733772278, -0.8800017237663269, -0.9075884222984314, -0.93875652551651, -0.9716728329658508, -1.0037811994552612, -1.0318742990493774, -1.0519757270812988, -1.0627351999282837, -1.0622268915176392, -1.0505326986312866, -1.0321414470672607, -1.0077201128005981, -0.9791192412376404, -0.9486343264579773, -0.9173592925071716, -0.8886043429374695, -0.8640713095664978, -0.8448187112808228, -0.8319265842437744, -0.8246135115623474, -0.8228845000267029, -0.8266971707344055, -0.8378857374191284, -0.8561459183692932, -0.8807154297828674, -0.9103232026100159, -0.943356454372406, -0.9778884053230286, -1.0112556219100952, -1.0401544570922852, -1.0605467557907104, -1.070912480354309, -1.06963312625885, -1.0566014051437378, -1.0368599891662598, -1.010958194732666, -0.9808104634284973, -0.9487956166267395, -0.916198194026947, -0.8861557841300964, -0.8603362441062927, -0.8397677540779114, -0.8254777789115906, -0.8168418407440186, -0.813877284526825, -0.816545307636261, -0.826408326625824, -0.8434168696403503, -0.8669630885124207, -0.895956814289093, -0.9290216565132141, -0.9645854830741882, -1.000296950340271, -1.033034324645996, -1.0596197843551636, -1.0762821435928345, -1.0802654027938843, -1.071123480796814, -1.0537742376327515, -1.0292227268218994, -0.9991559386253357, -0.965984046459198, -0.9303922057151794, -0.8965844511985779, -0.8666554093360901, -0.8418999314308167, -0.8243308663368225, -0.8122017979621887, -0.8055306077003479, -0.8044030666351318, -0.8121371269226074, -0.8316971659660339, -0.8622391819953918, -0.9012822508811951, -0.9372344613075256, -0.9751562476158142, -1.0130677223205566, -1.0469921827316284, -1.0726234912872314, -1.087431788444519, -1.0889493227005005, -1.0769230127334595, -1.0570721626281738, -1.0300997495651245, -0.9978546500205994, -0.9628080725669861, -0.9261453747749329, -0.8914907574653625, -0.8607796430587769, -0.8352070450782776, -0.8163802027702332, -0.8031197190284729, -0.7954537868499756, -0.7934262752532959, -0.7999832630157471, -0.8186282515525818, -0.8486803770065308, -0.8878419995307922, -0.9243994355201721, -0.963783323764801, -1.0042744874954224, -1.0420094728469849, -1.0735321044921875, -1.0941612720489502, -1.1004143953323364, -1.0914244651794434, -1.0732358694076538, -1.046919822692871, -1.0141897201538086, -0.9776635766029358, -0.9382457137107849, -0.900209367275238, -0.8658254742622375, -0.8364928960800171, -0.8147072196006775, -0.7983537316322327, -0.7874289155006409, -0.7820829749107361, -0.7833064794540405, -0.7952523827552795, -0.8175331950187683, -0.8487100005149841, -0.8824566006660461, -0.9212223887443542, -0.9637027382850647, -1.006944179534912, -1.0415643453598022, -1.0716116428375244, -1.094785213470459, -1.1083775758743286, -1.110660433769226, -1.1008284091949463, -1.079333782196045, -1.0482702255249023, -1.0076135396957397, -0.9630023837089539, -0.9184694886207581, -0.8769529461860657, -0.8460274338722229, -0.8199270963668823, -0.7987188696861267, -0.7831215262413025, -0.7718643546104431, -0.7680245637893677, -0.7715044021606445, -0.7817322611808777, -0.8025168180465698, -0.8325211405754089, -0.8708412051200867, -0.9158609509468079, -0.9619205594062805, -1.0092090368270874, -1.0541901588439941, -1.0917530059814453, -1.114719033241272, -1.1242990493774414, -1.1187442541122437, -1.0981823205947876, -1.0707374811172485, -1.0361818075180054, -0.9967545866966248, -0.9552392363548279, -0.9143301844596863, -0.8759844899177551, -0.8417340517044067, -0.8126132488250732, -0.7891307473182678, -0.7715415358543396, -0.7598627805709839, -0.7539794445037842, -0.7549013495445251, -0.7680061459541321, -0.7930042743682861, -0.8282442688941956, -0.8631654381752014, -0.903389036655426, -0.9483539462089539, -0.9954701066017151, -1.035349726676941, -1.07209312915802, -1.1032969951629639, -1.125848412513733, -1.1372781991958618, -1.135785698890686, -1.1207131147384644, -1.093464970588684, -1.0576224327087402, -1.0154801607131958, -0.9700990319252014, -0.9244592785835266, -0.8838481307029724, -0.8469348549842834, -0.8145750761032104, -0.7876293063163757, -0.7652965188026428, -0.7493190169334412, -0.73966383934021, -0.735998809337616, -0.7385977506637573, -0.7483833432197571, -0.7651158571243286, -0.7883702516555786, -0.8177788257598877, -0.8532129526138306, -0.8941277861595154, -0.939378559589386, -0.9838991761207581, -1.0287145376205444, -1.0713934898376465, -1.108550786972046, -1.1368050575256348, -1.1524837017059326, -1.1532117128372192, -1.138539433479309, -1.1137681007385254, -1.0797291994094849, -1.0385966300964355, -0.9932966828346252, -0.9461763501167297, -0.9003191590309143, -0.8577974438667297, -0.8200165033340454, -0.7889502048492432, -0.7634243965148926, -0.7434499263763428, -0.7290763258934021, -0.7179944515228271, -0.7157658338546753, -0.7221928238868713, -0.7363342642784119, -0.7572945952415466, -0.7853670716285706, -0.8204980492591858, -0.8621624112129211, -0.9142455458641052, -0.9717132449150085, -1.0311497449874878, -1.0876719951629639, -1.1222985982894897, -1.1493293046951294, -1.1672025918960571, -1.1731743812561035, -1.1660547256469727, -1.1459507942199707, -1.1140998601913452, -1.0728793144226074, -1.0214580297470093, -0.966724693775177, -0.9127370715141296, -0.8621622920036316, -0.8233932256698608, -0.789465606212616, -0.760289192199707, -0.7365379929542542, -0.7153299450874329, -0.7011774778366089, -0.6939496994018555, -0.6929144263267517, -0.6983382105827332, -0.7105896472930908, -0.7295476198196411, -0.7549459338188171, -0.7866270542144775, -0.8247538805007935, -0.8690881133079529, -0.9188809990882874, -0.9701178669929504, -1.0232586860656738, -1.0757780075073242, -1.1236295700073242, -1.1628177165985107, -1.188222050666809, -1.196006417274475, -1.1848570108413696, -1.1610417366027832, -1.1258676052093506, -1.0815303325653076, -1.0310789346694946, -0.9775534272193909, -0.9241556525230408, -0.8733331561088562, -0.8268455266952515, -0.7883062362670898, -0.7550511956214905, -0.7270664572715759, -0.7045505046844482, -0.6841140389442444, -0.670880138874054, -0.6646140217781067, -0.6644430756568909, -0.6714598536491394, -0.6864997148513794, -0.7094943523406982, -0.7401559948921204, -0.7740797996520996, -0.8144152760505676, -0.8613834977149963, -0.9142283797264099, -0.9691618084907532, -1.0266473293304443, -1.0840836763381958, -1.1373685598373413, -1.1819556951522827, -1.2122498750686646, -1.2235665321350098, -1.214074969291687, -1.1903650760650635, -1.1540520191192627, -1.1073161363601685, -1.0533993244171143, -0.9961206316947937, -0.9383912682533264, -0.8828297257423401, -0.8313636183738708, -0.7886761426925659, -0.7511418461799622, -0.7187240123748779, -0.6917024254798889, -0.666536271572113, -0.647987425327301, -0.6358279585838318, -0.6291673183441162, -0.6275763511657715, -0.6338211894035339, -0.6477038264274597, -0.6685811281204224, -0.6909041404724121, -0.7182134389877319, -0.7511584162712097, -0.7897132039070129, -0.8534132242202759, -0.928472101688385, -1.011296033859253, -1.096842646598816, -1.1483052968978882, -1.1929723024368286, -1.2295151948928833, -1.252256989479065, -1.258221983909607, -1.2457424402236938, -1.2148725986480713, -1.1683719158172607, -1.1101950407028198, -1.044602394104004, -0.97622150182724, -0.9087598919868469, -0.8554583787918091, -0.8063762784004211, -0.7614699602127075, -0.721914529800415, -0.6858627796173096, -0.6557327508926392, -0.6313623189926147, -0.6121729016304016, -0.5960479974746704, -0.5852895379066467, -0.5795122981071472, -0.5779979825019836, -0.5803017616271973, -0.5863076448440552, -0.5959364771842957, -0.6091161966323853, -0.6274490356445312, -0.6503519415855408, -0.6780965328216553, -0.711129367351532, -0.767109215259552, -0.8358588814735413, -0.9163700938224792, -1.0074676275253296, -1.0663644075393677, -1.1247645616531372, -1.1825367212295532, -1.2338464260101318, -1.2747198343276978, -1.300495982170105, -1.3076800107955933, -1.2948635816574097, -1.266127347946167, -1.2228320837020874, -1.1675151586532593, -1.1036368608474731, -1.0374066829681396, -0.9701055884361267, -0.9041784405708313, -0.8417977690696716, -0.7891509532928467, -0.741331160068512, -0.6982825398445129, -0.6604481339454651, -0.6264268159866333, -0.5975441336631775, -0.5735142230987549, -0.5537815093994141, -0.5371487736701965, -0.5242241621017456, -0.5146443843841553, -0.5078886151313782, -0.503581702709198, -0.501487672328949, -0.5014001727104187, -0.5031380653381348, -0.5068634152412415, -0.5125089287757874, -0.5200781226158142, -0.5295965075492859, -0.5435318350791931, -0.5607000589370728, -0.5813454985618591, -0.6060547232627869, -0.6559818387031555, -0.721209704875946, -0.8031456470489502, -0.9049555659294128, -0.9809665083885193, -1.0629336833953857, -1.1507865190505981, -1.2365858554840088, -1.2938252687454224, -1.3410528898239136, -1.375532627105713, -1.392343521118164, -1.388830542564392, -1.3639240264892578, -1.3186805248260498, -1.2562347650527954, -1.1819676160812378, -1.1000579595565796, -1.015085220336914, -0.9307922720909119, -0.8657432198524475, -0.8045805096626282, -0.7468738555908203, -0.69413822889328, -0.6388616561889648, -0.59040766954422, -0.5489954352378845, -0.5135000944137573, -0.4746479392051697, -0.44331520795822144, -0.4188094139099121, -0.39895617961883545, -0.3851178288459778, -0.3731229305267334, -0.3623281717300415, -0.3450085520744324, -0.33878493309020996, -0.3321801424026489, -0.3223453760147095, -0.31212133169174194, -0.29897427558898926, -0.2846348285675049, -0.26925212144851685, -0.2519955039024353, -0.2280697375535965, -0.20024608075618744, -0.16873501241207123, -0.1320098489522934, -0.0902513712644577, -0.04011571407318115, 0.017883576452732086, 0.08587788045406342, 0.15962757170200348, 0.24202612042427063, 0.3317505121231079, 0.42192375659942627, 0.4818114638328552, 0.5320911407470703, 0.5753598213195801, 0.5936079621315002, 0.5886709094047546, 0.5591496825218201, 0.5093900561332703, 0.4434749484062195, 0.3694295287132263, 0.28772443532943726, 0.20160073041915894, 0.1170303076505661, 0.05311568081378937, -0.005259677767753601, -0.06117945909500122, -0.11094993352890015, -0.1575629562139511, -0.20096509158611298, -0.2373698502779007, -0.26842790842056274, -0.32327282428741455, -0.3501971960067749, -0.3747962713241577, -0.4004315137863159, -0.4154897928237915, -0.4283176064491272, -0.4400835633277893, -0.4506567120552063, -0.4603630304336548, -0.46945250034332275, -0.47815752029418945, -0.4867245554924011, -0.4966849088668823, -0.5070133209228516, -0.5178797245025635, -0.5296279788017273, -0.5449284315109253, -0.5622236132621765, -0.5817610621452332, -0.6041436195373535, -0.6352055668830872, -0.6718935966491699, -0.7147133946418762, -0.7648394703865051, -0.8381919264793396, -0.9250633120536804, -1.02336847782135, -1.1291910409927368, -1.1936631202697754, -1.2537364959716797, -1.3087297677993774, -1.3515961170196533, -1.3781437873840332, -1.3844431638717651, -1.3686460256576538, -1.331275224685669, -1.2799097299575806, -1.2156779766082764, -1.142035961151123, -1.0631366968154907, -0.9914315342903137, -0.9210969805717468, -0.8532946705818176, -0.78983074426651, -0.7376042604446411, -0.6898323893547058, -0.6461504101753235, -0.6069740056991577, -0.5557616949081421, -0.5132387280464172, -0.4795438051223755, -0.4521469473838806, -0.4357384443283081, -0.4216066002845764, -0.40897679328918457, -0.39801329374313354, -0.38837915658950806, -0.37981462478637695, -0.37207692861557007, -0.3649236559867859, -0.3574782609939575, -0.3463696241378784, -0.3396620750427246, -0.33171534538269043, -0.32096731662750244, -0.3092923164367676, -0.2966434955596924, -0.2825116515159607, -0.26261889934539795, -0.23950310051441193, -0.2133629471063614, -0.1828189641237259, -0.1390136331319809, -0.09552307426929474, -0.046883419156074524, 0.01091572642326355, 0.09794831275939941, 0.202494278550148, 0.31295228004455566, 0.42654120922088623, 0.49128222465515137, 0.5429155230522156, 0.5905442833900452, 0.6146843433380127, 0.6141652464866638, 0.5893318057060242, 0.5421663522720337, 0.4789724349975586, 0.4077340364456177, 0.3276638984680176, 0.24137046933174133, 0.15489479899406433, 0.08655837178230286, 0.023192480206489563, -0.03752608597278595, -0.09222166240215302, -0.1432316154241562, -0.1898181289434433, -0.2305992990732193, -0.2657199501991272, -0.3239886164665222, -0.3522408604621887, -0.3829089403152466, -0.41268390417099, -0.4314597249031067, -0.4476073384284973, -0.46230047941207886, -0.47554874420166016, -0.48779213428497314, -0.4993516206741333, -0.5105313062667847, -0.5216559767723083, -0.5348241925239563, -0.5486519932746887, -0.5633640885353088, -0.5794360637664795, -0.6004260778427124, -0.6242766976356506, -0.651275634765625, -0.6821798086166382, -0.72538822889328, -0.7760153412818909, -0.8341497778892517, -0.9004316926002502, -0.9742961525917053, -1.054148554801941, -1.1365869045257568, -1.215453863143921, -1.2654870748519897, -1.3054026365280151, -1.3329969644546509, -1.3439533710479736, -1.336462378501892, -1.3100311756134033, -1.2658003568649292, -1.207068920135498, -1.137587308883667, -1.0619137287139893, -0.9843011498451233, -0.9082021117210388, -0.8480636477470398, -0.7920495867729187, -0.7399092316627502, -0.6928575038909912, -0.6513277888298035, -0.6147662997245789, -0.5828877687454224, -0.5553987622261047, -0.5290259718894958, -0.5071645379066467, -0.48945844173431396, -0.475098192691803, -0.4635356664657593, -0.45437783002853394, -0.4472634196281433, -0.4418284296989441, -0.43787938356399536, -0.43503785133361816, -0.4331403374671936, -0.43204307556152344, -0.43160974979400635, -0.43186426162719727, -0.43275392055511475, -0.4342191219329834, -0.4365462064743042, -0.43961405754089355, -0.4434468150138855, -0.44811105728149414, -0.454992413520813, -0.463479220867157, -0.4737900495529175, -0.48637086153030396, -0.5100387930870056, -0.5415624976158142, -0.5828513503074646, -0.6378309726715088, -0.6968043446540833, -0.7705512046813965, -0.8622643351554871, -0.9726261496543884, -1.0604100227355957, -1.1541873216629028, -1.2527105808258057, -1.347213864326477, -1.4090070724487305, -1.458376407623291, -1.4924733638763428, -1.5059969425201416, -1.4964408874511719, -1.463025450706482, -1.4072068929672241, -1.3327815532684326, -1.2674968242645264, -1.197281002998352, -1.1225353479385376, -1.0465304851531982, -0.970973551273346, -0.8974587321281433, -0.8271268606185913, -0.7607830166816711, -0.6769013404846191, -0.6070058941841125, -0.5370784401893616, -0.4807225465774536, -0.42533302307128906, -0.3663838505744934, -0.29645079374313354, -0.2720365524291992, -0.2466210275888443, -0.21664194762706757, -0.18995238840579987, -0.16477398574352264, -0.14170308411121368, -0.11838878691196442, -0.10234549641609192, -0.08294598758220673, -0.05754649639129639, -0.029851451516151428, -1.0371208190917969e-05, 0.033079639077186584, 0.07961475849151611, 0.1311800181865692, 0.18482279777526855, 0.2406897097826004, 0.28910091519355774, 0.3344152569770813, 0.3749349117279053, 0.40569251775741577, 0.42273253202438354, 0.41902709007263184, 0.3951336145401001, 0.35503143072128296, 0.3096744418144226, 0.25893434882164, 0.20272283256053925, 0.1448344588279724, 0.09077804535627365, 0.0409531369805336, -0.0037278085947036743, -0.0421537309885025, -0.07199510931968689, -0.10025769472122192, -0.12255428731441498, -0.1392497569322586, -0.1550578624010086, -0.16373233497142792, -0.16508804261684418, -0.161302849650383, -0.1523006409406662, -0.13696716725826263, -0.11567692458629608, -0.08867810666561127, -0.05805449187755585, -0.022118285298347473, 0.019953913986682892, 0.06800670176744461, 0.14786401391029358, 0.23858147859573364, 0.3289701044559479, 0.41067010164260864, 0.4370654821395874, 0.4526389241218567, 0.4627434015274048, 0.4460705518722534, 0.4182816743850708, 0.37581008672714233, 0.32437509298324585, 0.26412898302078247, 0.20016223192214966, 0.1349354237318039, 0.07202599942684174, 0.01496383547782898, -0.02734515070915222, -0.06391642987728119, -0.0954405665397644, -0.12482611835002899, -0.15180079638957977, -0.17253415286540985, -0.18704654276371002, -0.19695119559764862, -0.20386551320552826, -0.20679862797260284, -0.20631565153598785, -0.2037539929151535, -0.19720254838466644, -0.18669448792934418, -0.17220644652843475, -0.15601511299610138, -0.1330510526895523, -0.10499727725982666, -0.07221156358718872, -0.033673107624053955, 0.03348632901906967, 0.11545106768608093, 0.20686563849449158, 0.30638426542282104, 0.35583633184432983, 0.40029144287109375, 0.4481664299964905, 0.48536932468414307, 0.5039336085319519, 0.5000252723693848, 0.47386735677719116, 0.42845380306243896, 0.38126471638679504, 0.32378941774368286, 0.2581601142883301, 0.18911048769950867, 0.12504808604717255, 0.06451912224292755, 0.008249357342720032, -0.04190985858440399, -0.08356258273124695, -0.11876796185970306, -0.15203525125980377, -0.17981068789958954, -0.20547576248645782, -0.22595717012882233, -0.24129237234592438, -0.25995272397994995, -0.27856898307800293, -0.2907378077507019, -0.296420156955719, -0.29800885915756226, -0.2963842749595642, -0.2926861643791199, -0.2868722677230835, -0.2788032293319702, -0.2671789526939392, -0.25251781940460205, -0.2348557859659195, -0.21374689042568207, -0.17857004702091217, -0.13535277545452118, -0.09438320994377136, -0.04483683407306671, 0.020375043153762817, 0.09885485470294952, 0.1895981729030609, 0.2912636697292328, 0.35206979513168335, 0.4105810523033142, 0.4737953543663025, 0.5281811356544495, 0.5683931708335876, 0.5828953385353088, 0.5702064633369446, 0.5324742794036865, 0.4862901568412781, 0.4283244013786316, 0.3576061427593231, 0.27938705682754517, 0.20352229475975037, 0.12928365170955658, 0.058243148028850555, -0.006994783878326416, -0.05671960115432739, -0.10122999548912048, -0.14244507253170013, -0.18106330931186676, -0.2288012057542801, -0.26835280656814575]}, {\"line\": {\"color\": \"red\", \"width\": 1}, \"type\": \"scatter\", \"x\": [1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500], \"y\": [-0.7844164967536926, -0.8649535775184631, -0.9588934779167175, -1.0647653341293335, -1.1342082023620605, -1.2032047510147095, -1.2715011835098267, -1.332241177558899, -1.3805955648422241, -1.4111328125, -1.4195282459259033, -1.403944969177246, -1.3696666955947876, -1.3179588317871094, -1.2517576217651367, -1.1750094890594482, -1.0979551076889038, -1.0190593004226685, -0.9406450390815735, -0.8652475476264954, -0.804328978061676, -0.7475706338882446, -0.6946073770523071, -0.6463363766670227, -0.5827430486679077, -0.5287007689476013, -0.4847731590270996, -0.448270320892334, -0.4248916506767273, -0.4045925736427307, -0.38640397787094116, -0.37044984102249146, -0.3487709164619446, -0.33293968439102173, -0.32312607765197754, -0.31121331453323364, -0.2968640923500061, -0.2826264500617981, -0.26848530769348145, -0.25367599725723267, -0.2347479611635208, -0.21397463977336884, -0.19151563942432404, -0.16626279056072235, -0.13223935663700104, -0.10044130682945251, -0.06455762684345245, -0.023056715726852417, 0.03805696219205856, 0.10982322692871094, 0.1892222911119461, 0.2763424515724182, 0.3361908793449402, 0.3940010666847229, 0.45321041345596313, 0.5033811330795288, 0.5387289524078369, 0.5489363670349121, 0.5328097939491272, 0.49318981170654297, 0.4463971257209778, 0.38901573419570923, 0.32081738114356995, 0.2465328425168991, 0.17497465014457703, 0.10581854730844498, 0.04035940766334534, -0.019144296646118164, -0.06598114967346191, -0.10744655132293701, -0.14544926583766937, -0.18013779819011688, -0.2143750935792923, -0.2429266721010208, -0.26850903034210205, -0.2965056300163269, -0.31868135929107666, -0.337089478969574, -0.3483784794807434, -0.35453325510025024, -0.3633708357810974, -0.37119442224502563, -0.3772173523902893, -0.3819637894630432, -0.38581883907318115, -0.38878464698791504, -0.3909997344017029, -0.39261776208877563, -0.39384347200393677, -0.3945847749710083, -0.3949037790298462, -0.3948826193809509, -0.3945186734199524, -0.39379674196243286, -0.3927267789840698, -0.391321063041687, -0.38922256231307983, -0.38656753301620483, -0.3832893371582031, -0.3792654275894165, -0.3730372190475464, -0.3650720715522766, -0.3518029451370239, -0.3342525362968445, -0.31485670804977417, -0.2870991826057434, -0.250893235206604, -0.20433788001537323, -0.1468067616224289, -0.08267255127429962, -0.009409606456756592, 0.08176559209823608, 0.15936823189258575, 0.24669396877288818, 0.3477282226085663, 0.4494197964668274, 0.5254417061805725, 0.5988086462020874, 0.6648216247558594, 0.7103384733200073, 0.7320994734764099, 0.7268195152282715, 0.6921490430831909, 0.6395618915557861, 0.5725427269935608, 0.49077320098876953, 0.3972906470298767, 0.29828596115112305, 0.21345040202140808, 0.13165150582790375, 0.05165240913629532, -0.02299332618713379, -0.09610800445079803, -0.16202352941036224, -0.21964381635189056, -0.2747069001197815, -0.3401978015899658, -0.3929551839828491, -0.45177215337753296, -0.5002312064170837, -0.543270468711853, -0.579498291015625, -0.6102477312088013, -0.6374003887176514, -0.6599798202514648, -0.6814322471618652, -0.7025470733642578, -0.7237789630889893, -0.7489013075828552, -0.7753393054008484, -0.8032193183898926, -0.8330650925636292, -0.8718017935752869, -0.9133709073066711, -0.9566006064414978, -1.0007282495498657, -1.0448299646377563, -1.0855963230133057, -1.1199500560760498, -1.1451250314712524, -1.1584572792053223, -1.1578810214996338, -1.1426817178726196, -1.114213466644287, -1.0768728256225586, -1.0324311256408691, -0.9841096997261047, -0.934971034526825, -0.8905803561210632, -0.8496333956718445, -0.8130620121955872, -0.7818136811256409, -0.7550546526908875, -0.7345216274261475, -0.7201627492904663, -0.7116460800170898, -0.708489179611206, -0.7124330997467041, -0.7232668399810791, -0.7405056953430176, -0.7638848423957825, -0.7935137748718262, -0.8292826414108276, -0.8707857728004456, -0.927753746509552, -0.9901425242424011, -1.0532609224319458, -1.1111993789672852, -1.1426767110824585, -1.165311336517334, -1.178261160850525, -1.1787152290344238, -1.166124939918518, -1.1409999132156372, -1.105010986328125, -1.0607835054397583, -1.0087568759918213, -0.954670250415802, -0.9019675850868225, -0.8529446125030518, -0.8141103386878967, -0.7802237272262573, -0.7512438893318176, -0.7276746034622192, -0.7064014077186584, -0.6922294497489929, -0.6850050687789917, -0.6839362978935242, -0.6910912990570068, -0.7084928154945374, -0.7360404133796692, -0.7730590105056763, -0.8097216486930847, -0.8524731993675232, -0.9015370011329651, -0.9553967118263245, -1.0056219100952148, -1.0560734272003174, -1.1045043468475342, -1.1471997499465942, -1.1803853511810303, -1.200124979019165, -1.2035253047943115, -1.1897441148757935, -1.163640022277832, -1.1264728307724, -1.0804572105407715, -1.0286962985992432, -0.9745393395423889, -0.9207343459129333, -0.8696039319038391, -0.8228070139884949, -0.7833612561225891, -0.7492475509643555, -0.7204830050468445, -0.6971912384033203, -0.675915539264679, -0.6617127060890198, -0.6543388366699219, -0.6528909802436829, -0.6581788063049316, -0.6712738871574402, -0.6920876502990723, -0.7203237414360046, -0.7514510750770569, -0.788733184337616, -0.8325621485710144, -0.8825144171714783, -0.9411718249320984, -1.0044043064117432, -1.0690211057662964, -1.1306027173995972, -1.1714072227478027, -1.2036216259002686, -1.2253162860870361, -1.2332688570022583, -1.2261006832122803, -1.2036848068237305, -1.1672022342681885, -1.1194530725479126, -1.0610682964324951, -0.9981923699378967, -0.9350473284721375, -0.8745514750480652, -0.8269900679588318, -0.7840360999107361, -0.7455869317054749, -0.7124648094177246, -0.6816055774688721, -0.6570426821708679, -0.6385982632637024, -0.6255295276641846, -0.6170310974121094, -0.613411009311676, -0.6143593192100525, -0.6194900274276733, -0.6285665035247803, -0.6416099071502686, -0.658697783946991, -0.6799118518829346, -0.7121949195861816, -0.7517116069793701, -0.7984379529953003, -0.8527981042861938, -0.9173577427864075, -0.988880455493927, -1.0643256902694702, -1.1385059356689453, -1.1862781047821045, -1.225899338722229, -1.2554227113723755, -1.2704250812530518, -1.2688747644424438, -1.249813199043274, -1.214051365852356, -1.1643750667572021, -1.1017792224884033, -1.0327534675598145, -0.9619887471199036, -0.8930752873420715, -0.839918315410614, -0.7911670804023743, -0.7465754151344299, -0.7072857022285461, -0.6713786721229553, -0.6412320137023926, -0.616648256778717, -0.5970376133918762, -0.5802863836288452, -0.5685485005378723, -0.5614374876022339, -0.5582446455955505, -0.5585252046585083, -0.5620952248573303, -0.5688349008560181, -0.578641951084137, -0.5924584269523621, -0.6099750399589539, -0.6314066052436829, -0.6571138501167297, -0.6995401978492737, -0.751965343952179, -0.8145760893821716, -0.8883847594261169, -0.9599691033363342, -1.037659764289856, -1.1190485954284668, -1.1974718570709229, -1.2472376823425293, -1.2873207330703735, -1.3156819343566895, -1.3279215097427368, -1.3220844268798828, -1.2974789142608643, -1.2553868293762207, -1.1986488103866577, -1.130664348602295, -1.0564247369766235, -0.9801811575889587, -0.9054753184318542, -0.8466823101043701, -0.7920658588409424, -0.7413878440856934, -0.6958498954772949, -0.656513512134552, -0.6219987869262695, -0.5920003056526184, -0.5662777423858643, -0.5419487357139587, -0.5219789743423462, -0.5060375332832336, -0.493377149105072, -0.48350512981414795, -0.47607433795928955, -0.47075653076171875, -0.4672371745109558, -0.4652518033981323, -0.4646492004394531, -0.46530699729919434, -0.46710872650146484, -0.4701223373413086, -0.47429704666137695, -0.4796571135520935, -0.4862266778945923, -0.49616163969039917, -0.5083563327789307, -0.5229811072349548, -0.5405215620994568, -0.5746912956237793, -0.619391143321991, -0.6764028072357178, -0.7496946454048157, -0.834108829498291, -0.9369617104530334, -1.0577441453933716, -1.1887832880020142, -1.2571296691894531, -1.3193496465682983, -1.3759255409240723, -1.4182249307632446, -1.4418059587478638, -1.4427587985992432, -1.4193880558013916, -1.3730977773666382]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"rangeslider\": {\"visible\": true}}, \"yaxis\": {\"autorange\": true, \"fixedrange\": false}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('a30490cf-ce7a-4d08-8002-52053ba4216f');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYISrVoIv97W"
      },
      "source": [
        "**Prédictions multi-step ahead**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1tcwq5pv9it"
      },
      "source": [
        "predictions = []\n",
        "\n",
        "data_to_predict = x_val[0,:,0]\n",
        "data_to_predict = tf.expand_dims(data_to_predict,1)\n",
        "data_to_predict = tf.expand_dims(data_to_predict,0)\n",
        "\n",
        "prediction = model.predict(data_to_predict)\n",
        "predictions.append(prediction[0,taille_fenetre-1,0])\n",
        "\n",
        "data_to_predict = x_val[1,0:taille_fenetre-1,0]\n",
        "data_to_predict = np.insert(data_to_predict,taille_fenetre-1,predictions[0])\n",
        "\n",
        "#for t in y_val_timing:\n",
        "for i in range(1,200):\n",
        "  data_to_predict = tf.expand_dims(data_to_predict,1)\n",
        "  data_to_predict = tf.expand_dims(data_to_predict,0)\n",
        "  prediction = model.predict(data_to_predict,1)\n",
        "  predictions.append(prediction[0,taille_fenetre-1,0])\n",
        "\n",
        "  data_to_predict = x_val[i+1,0:taille_fenetre-1,0]\n",
        "  data_to_predict = np.insert(data_to_predict,taille_fenetre-1,predictions[i])\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "xhmBwx6Cxueb",
        "outputId": "54420da3-fd6b-4ac6-9499-74387ba31dcf"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "n_max = len(predictions)\n",
        "\n",
        "# Courbes originales\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index,y=serie_entrainement[0],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index[temps_separation:],y=serie_test[0],line=dict(color='blue', width=1)))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index[taille_fenetre+horizon-1:],y=pred_ent[:,taille_fenetre-1,0],line=dict(color='green', width=1)))\n",
        "\n",
        "# Courbes des prédictions de validations\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index[temps_separation+taille_fenetre+horizon-1:temps_separation+taille_fenetre+horizon-1+n_max],y=predictions[0:n_max],line=dict(color='red', width=1)))\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"b8aafb79-a5de-42d2-b205-d4244cc190d4\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"b8aafb79-a5de-42d2-b205-d4244cc190d4\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'b8aafb79-a5de-42d2-b205-d4244cc190d4',\n",
              "                        [{\"line\": {\"color\": \"blue\", \"width\": 1}, \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500], \"y\": [-0.3997737259124131, -0.39663821834604174, -0.39365137681048884, -0.3907864798683186, -0.38801757504432344, -0.3821324507898519, -0.3763934293587709, -0.3706210862311272, -0.36463022967025455, -0.3577473769244706, -0.35024615038670454, -0.34194712553700274, -0.3326497313941316, -0.3199234065145797, -0.3051080675812845, -0.28778078536864177, -0.2674609584839191, -0.23571563448930655, -0.1966074971580348, -0.14853454184766277, -0.0896512592100983, -0.006225265450336413, 0.09729627454420649, 0.2225345894716806, 0.3675224176311685, 0.45838812095042686, 0.5508558289121261, 0.6407603294459197, 0.722847047324601, 0.7907720441641028, 0.8375505797233825, 0.8571591165468613, 0.8462014047925644, 0.8092912178307217, 0.7480946404894724, 0.6657516018680006, 0.5671962762650251, 0.47056335623297907, 0.36912442227360964, 0.26621164404319425, 0.1646829978238482, 0.06960722630425478, -0.020784283640840975, -0.10554314526311386, -0.18409904490985485, -0.30078906306526343, -0.4001453589493666, -0.4835585687120818, -0.5528548815255133, -0.6036192046351864, -0.6466106012197352, -0.6830658188631247, -0.7141831570379003, -0.7449865023027574, -0.7713362746616284, -0.7942833891599544, -0.8148146806574793, -0.8352498518764578, -0.8543521552329252, -0.8725509279710562, -0.8902306512050359, -0.9106978625168631, -0.9306203922502604, -0.949722695606728, -0.9676907796763472, -0.9869853235899075, -1.0028259454943649, -1.0141168742142899, -1.0201340036512985, -1.0203134281712518, -1.0140335699728829, -1.0017814384674935, -0.9847553331276295, -0.9601933979497229, -0.9339974180365264, -0.9090189616515849, -0.887699483869979, -0.8739030198892763, -0.8646498410745367, -0.8603308365585154, -0.8610421266197593, -0.8688406852191626, -0.8840661373409225, -0.9057765042552841, -0.9318699558713645, -0.9582069121930961, -0.983762090249316, -1.0056582897022008, -1.021018310213926, -1.0272469042637369, -1.0256384916027261, -1.0160841359152073, -0.9996539763023317, -0.9777385527937374, -0.9527985445202147, -0.9272818145754133, -0.9034247614401807, -0.884463434492248, -0.8696288715032435, -0.8596195464972716, -0.8548391646442274, -0.8556786150768666, -0.8629709402092584, -0.8763636990200658, -0.8949661769280917, -0.9198164729416379, -0.9476721296644034, -0.9760916920213083, -1.0017750304489237, -1.020063515447031, -1.0308994748485025, -1.0325399276023621, -1.0247926331515171, -1.010791112576582, -0.9912594719759403, -0.9679663244748469, -0.9429366039413475, -0.9169777207152321, -0.8934282524713483, -0.8739158359264159, -0.8595682823487135, -0.851346794523706, -0.8484119220187538, -0.8508149289824154, -0.858331534764749, -0.8755819207545547, -0.9002720163038566, -0.9307036964916674, -0.9635063435502911, -0.9922655308913934, -1.0165134731593815, -1.0328539205122806, -1.0388838659864288, -1.0348916704174658, -1.022620314856367, -1.0031655704785625, -0.9785523711520977, -0.9484346838742052, -0.9182529164106153, -0.890852229006303, -0.8684690201421161, -0.85476867643996, -0.8459384268508247, -0.8422089600432219, -0.8435930920542909, -0.8539164099701814, -0.8738389397035787, -0.9022264619676347, -0.9360672080345601, -0.9659990627739292, -0.9948159222821595, -1.0194996098128917, -1.0370447646569065, -1.0446126345877982, -1.04320287050245, -1.0326937200480366, -1.014161730344278, -0.9895997951663714, -0.961334025255141, -0.9319853002056202, -0.9039117708514828, -0.8806891115546568, -0.8615483600867706, -0.8472969267876147, -0.8384282290870608, -0.8349999391522369, -0.8381911323999797, -0.8478159762917657, -0.863329789249165, -0.8875328753871651, -0.9173301617365692, -0.9507351625407506, -0.9845438685148273, -1.0143731949570804, -1.0373779816225344, -1.050059450372098, -1.0505080116719814, -1.0413445451172185, -1.0241774633688199, -1.000487018716401, -0.9724839775665309, -0.9395723941922213, -0.9075323013433996, -0.87897176257796, -0.855832407522541, -0.8413502855548735, -0.831956130331599, -0.8278165503355313, -0.8289251375481004, -0.8369672008531547, -0.8520901246777984, -0.8737107793321833, -0.9006372733623331, -0.9313316823115042, -0.9639933529615932, -0.9961167500518218, -1.0245683525015754, -1.0454456770018676, -1.0573005113559315, -1.0580694735843033, -1.0477525636869827, -1.0304701376043282, -1.0068181410633281, -0.9788086918948882, -0.9486397404684378, -0.9174583221079644, -0.8885325262840482, -0.863522029806258, -0.8436251321471399, -0.8299632365564024, -0.8218635010842202, -0.8194092299720005, -0.8225171189783361, -0.8329301491542032, -0.8503407356082527, -0.8741401165763576, -0.9031107685302622, -0.9356955429575138, -0.970004074380032, -1.0034282992399228, -1.032738576178025, -1.0539683416996541, -1.06550277512523, -1.0655668553109277, -1.0539683416996541, -1.0353722718101979, -1.010246430998152, -0.9806990573729688, -0.9489985895083445, -0.9164778952667905, -0.8862448636546425, -0.8599399474257599, -0.83872299794127, -0.8236641543023239, -0.8142379589862007, -0.8105341242528769, -0.8124821618980852, -0.821568732230011, -0.8377297550629565, -0.8604846290041898, -0.8888016630639783, -0.9213543973983811, -0.9565984995320851, -0.9922334907985446, -1.0252219703956915, -1.0524304172429106, -1.0701165484954602, -1.0754992840940623, -1.0679378221817404, -1.0519177757573295, -1.0283106353463176, -0.9989939503896459, -0.9662938316281384, -0.9309600172344581, -0.8971192711675324, -0.8667708952211286, -0.8413887336662922, -0.8229977203710683, -0.8100214827672957, -0.8026202213192178, -0.8007747119711257, -0.8076376998593433, -0.8259902650431483, -0.8552108297212737, -0.893133483617139, -0.928755658846459, -0.966825697169429, -1.0047227189910153, -1.0389992103206847, -1.0654386949395322, -1.0814587413639432, -1.0844705100917325, -1.0740895200087142, -1.0555703463420951, -1.0295089348188635, -0.997987891474193, -0.9633717751603259, -0.9268973334612274, -0.8921402407388256, -0.8609844544526314, -0.8347756585022953, -0.8151478976231071, -0.8010566647881954, -0.7926429364060947, -0.7898810804025262, -0.795571400892477, -0.8130076194208058, -0.8416963185576408, -0.879676644620634, -0.9158370934098141, -0.9552912637438532, -0.9956553727147988, -1.0336100667035129, -1.0657590958680203, -1.0874822788195215, -1.0953000614746342, -1.088123080676498, -1.0714622323951108, -1.0462146392302392, -1.0143475628828011, -0.9783857626692838, -0.9392904413751516, -0.9012652591821702, -0.8664889424040589, -0.8365378636089804, -0.8139047420205727, -0.7966671720679068, -0.7849981702523658, -0.7789169606296595, -0.7793719299481129, -0.7902911935909912, -0.8113351265740973, -0.8413951416848618, -0.8744669255234155, -0.9128509567563039, -0.9549067826296673, -0.9979942994927627, -1.03314228134792, -1.0640930110398819, -1.0880590004908004, -1.1027333630155607, -1.1063218534146289, -1.0979273490882375, -1.0778702509648752, -1.0480729646154712, -1.0085162659843159, -0.9646661949114185, -0.92029707433437, -0.8785103852409369, -0.8467009810606267, -0.8195437983619654, -0.7976219668348016, -0.7812751114633327, -0.7693305648492921, -0.7647360155347711, -0.7672992229626767, -0.7766677461116722, -0.796442891417965, -0.8253046070561836, -0.8626377232436305, -0.9068338273192952, -0.9525037756660056, -0.9997693206365874, -1.0449330355162862, -1.0831889063777795, -1.1075393769428838, -1.118881569811367, -1.115100838855206, -1.096389424631494, -1.0703087890525531, -1.0365897953384533, -0.9978405070470883, -0.9566625797177828, -0.9157537891684072, -0.8771134371927282, -0.8423499364517567, -0.8125590581209223, -0.7883175238715038, -0.7699329185948499, -0.7574565064395188, -0.7507985751455336, -0.7509203274983591, -0.7629033222238184, -0.7864912385791211, -0.8204473289803021, -0.8547622684213902, -0.894767528352429, -0.9392455852451631, -0.9860882009901405, -1.0263433736454002, -1.0638366902970913, -1.09568454258882, -1.119201970739855, -1.1318898475079886, -1.1318898475079886, -1.1184330085114833, -1.0927368540467284, -1.0581335537700012, -1.0168466901250093, -0.9719713360809497, -0.9264487721613438, -0.8854630853891312, -0.8478992805331725, -0.8149108009360259, -0.7872345687332136, -0.7641785179192015, -0.7474471814335469, -0.7369508470162729, -0.7324844580731472, -0.7343043353469603, -0.7432691533260605, -0.7591610393790761, -0.7816339605032396, -0.8103290676586444, -0.8451053844367554, -0.8854246372777126, -0.930197463024656, -0.974598623694553, -1.0195700980171591, -1.0624910063974407, -1.1002342357733526, -1.1294548004514782, -1.1465001298470512, -1.1489351769035616, -1.136119139764033, -1.112922112541486, -1.080048977278595, -1.0398642928276027, -0.9951683633034966, -0.9483129315213797, -0.902380254413309, -0.8594593460330275, -0.8210624987629995, -0.789182606378422, -0.7627879778895628, -0.7420259977235263, -0.7268774418246033, -0.7151635838790742, -0.7121710392069943, -0.7175281427313173, -0.7306838048550434, -0.7507729430712546, -0.7780454701041716, -0.8123924496381085, -0.8533332802803328, -0.9045461646898894, -0.9611994568651758, -1.0203326522269613, -1.0770372085508058, -1.1131784332842767, -1.1421426772196115, -1.1611744923718115, -1.1685437137270405, -1.1629687375713456, -1.1444495639047265, -1.1140755558840436, -1.0740895200087142, -1.0237545341432155, -0.9697477536372415, -0.9158114613355349, -0.8648677137059085, -0.8250867344248116, -0.7899900167182123, -0.7600709780159826, -0.7355538989680642, -0.7137345957380168, -0.6989256648232913, -0.6907938892582605, -0.6889227478358892, -0.6935493372432591, -0.7049940584088581, -0.7231543830355702, -0.747799622454884, -0.7787759842211248, -0.8162244447428275, -0.8599079073329109, -0.9091278979672708, -0.9600716455968974, -1.0131620794473948, -1.0657590958680203, -1.1140755558840436, -1.1541897521307685, -1.181039349938081, -1.1907795381641229, -1.1818723923521501, -1.160021049029254, -1.1263148713522935, -1.0831889063777795, -1.0335844346292338, -0.9805132248344456, -0.9271728782597273, -0.8760497061101473, -0.8289956257523678, -0.7895799035297474, -0.7553674923857756, -0.7265954890075338, -0.7032959334878707, -0.682271224560474, -0.6684042723755038, -0.6612272915773677, -0.6602276406804846, -0.6664177866188769, -0.6805474675652072, -0.7026102755009058, -0.732381929776031, -0.7656908103016661, -0.8055422777870305, -0.8518530279907173, -0.9040976033900058, -0.9586875135858282, -1.0160585038409284, -1.0735127983374353, -1.1272119939520606, -1.1727089257973875, -1.2044926979034185, -1.2178854567142259, -1.2108366362874852, -1.1893056938930768, -1.1546383134306517, -1.1092695419567205, -1.0563393085704669, -0.9995963041352037, -0.941968993137313, -0.8861551513946657, -0.8341476726824584, -0.790541106315212, -0.7519904665995099, -0.718784114370991, -0.6909733137782138, -0.665219487146331, -0.6460659196413053, -0.6330448259075443, -0.6256051163480478, -0.6233174537186419, -0.6287706775215114, -0.6417276910695748, -0.6617463410815186, -0.6835592362929965, -0.7105369944717043, -0.7429295283418631, -0.7809226704419957, -0.8430420024572912, -0.916080598115465, -0.9979302193070649, -1.0829966658206867, -1.136311380321126, -1.1836666375516842, -1.2213457867418984, -1.2456321771213055, -1.2535140399621156, -1.243197130064795, -1.2145532870579485, -1.1700816381837837, -1.1134988342127647, -1.048970087215238, -0.9810835384871547, -0.9135814708732571, -0.8592735134945044, -0.8088744474433078, -0.7632429472080161, -0.7228980622927799, -0.6861352597580417, -0.655293466381766, -0.6300907293468828, -0.6101297515020669, -0.5933920069978426, -0.5820498141293596, -0.5755905314110371, -0.573475885283015, -0.5752573144454094, -0.5807938424896858, -0.5899829411187278, -0.6027477141096984, -0.6206004538450619, -0.6429708466721092, -0.6702177416307471, -0.7027256198351616, -0.7572450418267165, -0.824016595323661, -0.9032389289016575, -0.9931818775468697, -1.0533275398426776, -1.1139473955126484, -1.172068123940411, -1.2241012347268974, -1.2661378365445513, -1.2935641560231428, -1.302855782949301, -1.292410712680585, -1.2658815158017607, -1.2244216356553854, -1.1706583598550626, -1.1079238580570698, -1.0422416677169855, -0.9749895128273086, -0.9088587611873407, -0.8459768749622433, -0.7923609835890252, -0.7434485778460139, -0.6996113228102561, -0.660977378853147, -0.6262395101864544, -0.5966729125055619, -0.5719123287519925, -0.5515156056444325, -0.5343292998403245, -0.5209109089552382, -0.5108375037635685, -0.5036541149468627, -0.4989826694095046, -0.4965732544272732, -0.49620799736879656, -0.49770106569555167, -0.5011934358160732, -0.5066082115075241, -0.5139582088070438, -0.5232626517703416, -0.5368604671753815, -0.5536110277167455, -0.5739052225271892, -0.5982236529994448, -0.6466298252754447, -0.7095886077233793, -0.7897977761611193, -0.8896347054780477, -0.9659734306996502, -1.0494186485151213, -1.1368881019924046, -1.2228196310129444, -1.2819656424118693, -1.3317559466989382, -1.3678330912467114, -1.3866726658418185, -1.385519222499261, -1.3631552376907834, -1.3204137538304555, -1.2601783792746706, -1.1875755288792407, -1.1067063345288146, -1.0221525295007743, -0.9378422291783849, -0.8714743808513357, -0.8086437587747963, -0.7502090374371153, -0.6966828583238738, -0.6409907689340519, -0.5920975872467501, -0.54968291233348, -0.5132533267643697, -0.4740106210431329, -0.44230246355622505, -0.41682225931727107, -0.39617799445236124, -0.38159417722999167, -0.36901844078682916, -0.35795307432056006, -0.34788671794931736, -0.3396184515887504, -0.33156036823727175, -0.32351189691364773, -0.31525836899579124, -0.30513369965556364, -0.29420161997554567, -0.28220580921294686, -0.26883868247641846, -0.2509731267039155, -0.23032008285356498, -0.20635409340264635, -0.17844076451275295, -0.13944797151573693, -0.09287449255068975, -0.03763096446075142, 0.027282263650961296, 0.09890468720521729, 0.1796457211842479, 0.2678136486856354, 0.35932015386187005, 0.42718107051567444, 0.4893388506423885, 0.5413078812431772, 0.5782821483907176, 0.5960964400146623, 0.591482666644432, 0.5634796254945619, 0.5138815617645858, 0.4506984986667096, 0.3757246814004669, 0.29344572296469273, 0.20825111607967595, 0.13704521373245462, 0.06831921457173214, 0.003405986460019419, -0.05680375602148631, -0.11377744912526103, -0.164951885423399, -0.2103911451015979, -0.250332324846939, -0.2968160915520095, -0.3349822501535259, -0.3660880538948759, -0.3914593218181439, -0.40710770316550837, -0.42076895795438896, -0.43281603286554593, -0.4436141849574558, -0.4534972719976033, -0.4627228963324931, -0.4715307178566341, -0.48016872688867646, -0.4901011556718112, -0.5003603934020039, -0.5112091688406148, -0.5229102107490046, -0.5380075024993693, -0.5550207918020936, -0.5743601918456425, -0.5964998960041782, -0.626989248359117, -0.6629190084797855, -0.7051478508545326, -0.7545985301574039, -0.8264003782316133, -0.9112874002252814, -1.0083304334457925, -1.1132425134699744, -1.1796936660384305, -1.242812648950609, -1.298690570878954, -1.3429058990103282, -1.371293421274384, -1.3798801661578681, -1.3666796479041536, -1.331948187256031, -1.282734604640241, -1.2200641830279457, -1.1476535731896087, -1.0694757466384837, -0.9975329221557396, -0.9264615881984835, -0.8581777423190746, -0.7940398844543034, -0.7406931298610153, -0.6917422760065856, -0.6473218912809792, -0.6074191596470566, -0.5563664757017442, -0.5139325767327647, -0.4790793637318165, -0.4506745398176221, -0.43311592813461086, -0.4180391420436694, -0.405061750996554, -0.3938100393502762, -0.3839388712246684, -0.3751810322453715, -0.3672863533674219, -0.3600100482814545, -0.3525075401399744, -0.34526135274128483, -0.33806514788743947, -0.3307081017674931, -0.32230078140396223, -0.3132654752205945, -0.30338431058601795, -0.2924201908131512, -0.27758562782414675, -0.26045699418716667, -0.24057291256518792, -0.21738229336121082, -0.1851115118438776, -0.14641989571964054, -0.10019244975736065, -0.04522446646592216, 0.03494625386039945, 0.1310985724997133, 0.2419893338494852, 0.3613707198041947, 0.43083364110044015, 0.4954905484693622, 0.5507917487264287, 0.5920593883157109, 0.6150641749811647, 0.6160894579523272, 0.5934691524010589, 0.5486130224127086, 0.4892106902709932, 0.41673600024695867, 0.3354182445966491, 0.24955079576180703, 0.17526263648252907, 0.1026597860870992, 0.033369881292237454, -0.03150489870805665, -0.09292575669924787, -0.14860503005193018, -0.19847863858040599, -0.2427003747303497, -0.29475911759111517, -0.3380068349184547, -0.37367642948519086, -0.4031106375024322, -0.4226272833396206, -0.43966364150919607, -0.45470518349800393, -0.46823058829320546, -0.48068777639282734, -0.4924080423569263, -0.5037053790954209, -0.5149065955553689, -0.5280174015491067, -0.5417305612884025, -0.556398515794593, -0.572386522126155, -0.5930844221064939, -0.6165377700718313, -0.6432527994891787, -0.6738190480669546, -0.7162209069430853, -0.765799746617352, -0.8231899609281613, -0.8886991347668621, -0.9618274426850129, -1.0410882243744277, -1.1232390224388067, -1.2023780517753961, -1.2544111625618826, -1.2968963256794201, -1.3260528101718478, -1.338932927497074, -1.33361427208417, -1.309456042076158, -1.2674194402585042, -1.2105803155446946, -1.1425271583337973, -1.0677455816246475, -0.9904584696547198, -0.9141902326373847, -0.8528719029433097, -0.7953983843910934, -0.7425194151533981, -0.6946771485115376, -0.6523201457653953, -0.6149485814665298, -0.5823253589278595, -0.5541428932580359, -0.5272868874321536, -0.5049805747908039, -0.4866600496998477, -0.4717678145437154, -0.4597547021309782, -0.45021700729174097, -0.4427805017415295, -0.4370773652144392, -0.43290318191809474, -0.42987731554945197, -0.4278350800312681, -0.4266169157011559, -0.42608440935800845, -0.42625101784082237, -0.42705778737875566, -0.4284496090121085, -0.43069433791709694, -0.43367150334460947, -0.4374221166134925, -0.44200128668344607, -0.44872393896498586, -0.4570030989571214, -0.46713481711777577, -0.479502292957421, -0.502475039530026, -0.5329451678292555, -0.573283644725922, -0.6269956563776867, -0.685013856508333, -0.7578538035908441, -0.8478736484588936, -0.9563678108635736, -1.0444203940307053, -1.1395153896060082, -1.2377503142804953, -1.3325889891130074, -1.3966691748106508, -1.4490226865256257, -1.485035750887701, -1.5009276369407167, -1.494071057071069, -1.463504808493293, -1.4104464147356441, -1.338420286011493, -1.2737633786425708, -1.203147014003768, -1.128749918408804, -1.0528148983570966, -0.9770721188624822, -0.9031620326788202, -0.8322637152229476, -0.7652358409832127, -0.6817521750563229, -0.6069641903286034, -0.5409103349114726, -0.48324457580216335, -0.4269866583726313, -0.3794346749719811, -0.33930702188625983, -0.30526186002695893, -0.27491989209912476, -0.2485188555916957, -0.22500783545923034, -0.20329746854486877, -0.18354795531285512, -0.16402272273078317, -0.14419631327593233, -0.12354967744415163, -0.09800090740650122, -0.07025418699942167, -0.039892995015878195, -0.006532850341685067, 0.03804132682959564, 0.08728054151966479, 0.14059525602010411, 0.19672949869123965, 0.24807695149076123, 0.29767501522073725, 0.3425311452090877, 0.3793772519852326, 0.4049452460785922, 0.4159670380185869, 0.4105843024199848, 0.3888611194684838, 0.35566758327710457, 0.31202897681700925, 0.26089298863029, 0.20534828366757263, 0.15048923669182018, 0.09691820144859035, 0.0465255434159636, 0.0006313144193114399, -0.03970075445878526, -0.07445784718118706, -0.10356947554362643, -0.1271445758617894, -0.14866270221905806, -0.163157640223865, -0.1711676634360704, -0.17331434965694148, -0.16949517058936192, -0.1593128290820064, -0.14281858928343297, -0.12013420354646723, -0.09316285338632915, -0.06025767803058926, -0.021149540699317543, 0.024276902941741826, 0.09673877692863699, 0.17954960090570143, 0.2681340496141236, 0.3522713334351293, 0.39360305321010924, 0.42564314605893094, 0.4455080036252004, 0.4511470599665932, 0.44127871136915603, 0.4160311182042845, 0.3768781247430244, 0.3265110987846768, 0.2669165260858685, 0.2030670290567366, 0.13864721837489574, 0.07653429437817, 0.025776379287066713, -0.02057281902803873, -0.06194298691443728, -0.09806498759219884, -0.1315853327306361, -0.1591077724877739, -0.18100397194065868, -0.19773530842631334, -0.21015404841451663, -0.21804872729246627, -0.22181664221148772, -0.22184227428576678, -0.21843961642522192, -0.21171119692696935, -0.20170827993956722, -0.1884500895187248, -0.16848270365533916, -0.14353628736324658, -0.1131815033982729, -0.07699542253481374, -0.01661907157049413, 0.05698983774038873, 0.14342760022793993, 0.23930437406875393, 0.29825173689201595, 0.3554753427200115, 0.4078929346206839, 0.4517237816378719, 0.48318715281541486, 0.4986304775685468, 0.49542646828366466, 0.47338288440367526, 0.4371775794845068, 0.3879639968687167, 0.32888206565548944, 0.26332803568680035, 0.19864549624359917, 0.13413597330178156, 0.07199100921220708, 0.013812608617316693, -0.037079874863751666, -0.0829100236747062, -0.12348559725845396, -0.15881300363356476, -0.19114145731802584, -0.21810639945959417, -0.24017561541386254, -0.25786815468498187, -0.27445851476210176, -0.28599294818767756, -0.29313788889296477, -0.29659181090206777, -0.29705959625766054, -0.2954191435038009, -0.29179220499331426, -0.2862749010047472, -0.2780598211983093, -0.2674994065953376, -0.2544462727687277, -0.23868895510567723, -0.21339009779224763, -0.18207411104180932, -0.1437349359389093, -0.09717427301100162, -0.03645829706248455, 0.03672768302279394, 0.12283222854471733, 0.2204455754179375, 0.2887037892230673, 0.3579103897765221, 0.4250023442019545, 0.4856862800576228, 0.5354125041589941, 0.5687342007217686, 0.5813579973042043, 0.5711051675925813, 0.5423331642143396, 0.49632359088343175, 0.43576781539915876, 0.36451064890337925, 0.2910747560938799, 0.21509487991218423, 0.13962123719749991, 0.06690945048638397, 0.006469019336366747, -0.04974852757617577, -0.1013074449884996, -0.14799626828780257, -0.20083038139550952, -0.24609021655375501, -0.2843909435452364, -0.31650152459832553, -0.34036562655398495, -0.3606361116957204, -0.377832029527683, -0.39247691516702243, -0.40506059755321144, -0.4159603808196378, -0.425527552544296, -0.4341085302110674, -0.4419820626277368]}, {\"line\": {\"color\": \"blue\", \"width\": 1}, \"type\": \"scatter\", \"x\": [1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500], \"y\": [-0.4494108785556646, -0.45660964661693787, -0.46379752104664257, -0.471722958413727, -0.48006619859156024, -0.4890117925149512, -0.49879683687098136, -0.5118115225861728, -0.5266524935937469, -0.5436914149707502, -0.5633960720727756, -0.5906814151428321, -0.6231764773101072, -0.6618232373043559, -0.7077046502638684, -0.7730536236383252, -0.8517633157307406, -0.944307919915277, -1.0489060070295404, -1.1197786924111337, -1.191292179649704, -1.2599861387175775, -1.3215671971730127, -1.371293421274384, -1.4037820754230892, -1.4146757069916884, -1.4019237500378576, -1.3702681383032218, -1.3207341547589435, -1.2563976483185095, -1.181039349938081, -1.1045276082150945, -1.0255680033984587, -0.9470056957331479, -0.8711411638857078, -0.8089961997961334, -0.7508498392940918, -0.6971442356608969, -0.6481100775650601, -0.5848052621143583, -0.5309714981097682, -0.4857308870072319, -0.44805045621330364, -0.423295639676447, -0.4018375564531771, -0.3831820842315793, -0.36683779206753836, -0.35234221326087445, -0.3393224011308273, -0.3274271962597737, -0.3163156920598024, -0.30460824213284293, -0.29313788889296477, -0.28160345546738896, -0.26969094894619705, -0.25512552273712275, -0.2392977168698048, -0.22182945824862724, -0.20230422566655532, -0.1761979580133354, -0.14606104667973374, -0.11127191386448314, -0.07111286148777007, -0.014119944328286059, 0.0526003450201002, 0.1290480065573887, 0.2138389082725104, 0.27928400192551367, 0.34509435263699334, 0.4081492553634743, 0.4643475782203076, 0.5091396280229604, 0.5374630701013187, 0.5454090131278264, 0.5314395326457402, 0.500745123696569, 0.4542869890657776, 0.39507689748115526, 0.32663925915607195, 0.25672777655994306, 0.18527196148850097, 0.1149439576853375, 0.047813555148486245, -0.009128097862439665, -0.06154568976311191, -0.10905473943934467, -0.15152708651974273, -0.19250636527338563, -0.22783377164849644, -0.2579450509078191, -0.2833464365183649, -0.3039161761273084, -0.32107684985713725, -0.3352834270263048, -0.3469601184641294, -0.3565176781609329, -0.3642611278006361, -0.37046985699288076, -0.3754149249231679, -0.3794269853496974, -0.3825226991207505, -0.38485457707828774, -0.38656808124384273, -0.38786570500422, -0.38866798892915455, -0.38904734362848453, -0.3890761797120485, -0.38875577878356027, -0.38807652881516524, -0.3870480418347181, -0.38568121147378737, -0.38363833515374646, -0.38105205885898963, -0.37783651514068184, -0.37388469008870817, -0.36780988848457163, -0.3600600308262986, -0.3501743805787232, -0.3375172622997247, -0.32127549843279996, -0.3004814781739147, -0.2738497529979741, -0.2396245258168628, -0.19713295468075548, -0.1431582142676305, -0.07506020092674487, 0.00996138945688831, 0.08719082925968812, 0.17611490295230778, 0.27601591245493373, 0.3839269451697652, 0.46582142249135333, 0.5457934942420123, 0.619357547422907, 0.6811949266211327, 0.72585881605239, 0.7477742395609842, 0.7431604661907537, 0.7112485337133275, 0.6601766257123057, 0.5906496242303626, 0.5065764205950546, 0.41250670799091405, 0.3253576554421191, 0.23688214304938296, 0.1495792980549137, 0.06529462980680337, -0.018099323860109713, -0.09571965279566509, -0.16699604334715382, -0.2317042148646341, -0.312464472899374, -0.38118790885266857, -0.4391817585127498, -0.48794806143237035, -0.532060861266628, -0.5691632887855635, -0.6008317165573389, -0.6286873732801044, -0.6518908085212212, -0.6738190480669546, -0.6951385258485606, -0.7164900437230154, -0.741487724163666, -0.7677029281325719, -0.7955073207067794, -0.8252405268704859, -0.8634835816948395, -0.9044628604484823, -0.9475375612744382, -0.991624729034417, -1.0358144250915118, -1.0769090481794106, -1.1119609097560215, -1.1381697057063576, -1.1528440682311178, -1.15386935120228, -1.140412512205775, -1.1136269945841601, -1.0776139302220846, -1.0340906680962452, -0.9863573377700707, -0.9374000758970712, -0.8926785142986858, -0.8511032898180547, -0.8139175580577124, -0.7819415453945883, -0.7544575537488691, -0.733170116060112, -0.717970296012631, -0.7086530370121936, -0.7047569617217769, -0.7079225228952404, -0.7179062158269333, -0.7343555994955184, -0.7570079451396353, -0.785946557000691, -0.8210496827258601, -0.8619392492195262, -0.9178299871850107, -0.9791034607490973, -1.0419853469741949, -1.1002983159590503, -1.1336840927075225, -1.1586112849439056, -1.1728370861687827, -1.174759491739712, -1.1637376997997173, -1.1401561914629845, -1.105552891186257, -1.062426926211743, -1.0112717139693144, -0.9576814546703752, -0.9049050137297961, -0.8554479264083551, -0.815666947127258, -0.7807047978106239, -0.7509715916469173, -0.7266339371189524, -0.7047633697403467, -0.6899480308070515, -0.6818162552420206, -0.6799130737268005, -0.6861801158880302, -0.7024885231480804, -0.7288511115440908, -0.7647488315719105, -0.8008772402682419, -0.8433880354600585, -0.8918454718846164, -0.945205042515044, -0.9954631321577058, -1.0462787194159369, -1.094979660546146, -1.1382978660777527, -1.1725166852402944, -1.1937272267062145, -1.1989818019334213, -1.1872551279507524, -1.1629687375713456, -1.1272119939520606, -1.0823558639637103, -1.0313928922783744, -0.9776232084594818, -0.9238150765291706, -0.8723586874139632, -0.8249842061276954, -0.7846905853610172, -0.7496515398215459, -0.720091350159223, -0.6960100163740485, -0.6741458570140126, -0.6593177020435781, -0.6510449500700122, -0.6487829195148854, -0.6532749405322903, -0.6654950319448308, -0.6853983376225189, -0.7127669849339824, -0.7433076014374792, -0.7801344841579148, -0.8233181212995567, -0.8726534562681724, -0.9305947601759814, -0.9931946935840092, -1.0576209122844198, -1.1194582914826456, -1.1617512140430901, -1.1959059530199343, -1.218910739685388, -1.228458687354337, -1.223075951755735, -1.2025062121467915, -1.1677747514986687, -1.12150885742497, -1.0643493317826724, -1.002281263915935, -0.9392968493937214, -0.8785103852409369, -0.8298991563707045, -0.7856902362579005, -0.746492386666652, -0.7125875604140289, -0.6811049651807768, -0.6559086361644634, -0.6366140922509029, -0.6227663641216422, -0.6135836735111699, -0.6093351571994161, -0.6096747821836137, -0.6142565154609951, -0.6228368523259097, -0.635415792778357, -0.6520510089854653, -0.6728193971700714, -0.7043019924033236, -0.7428141840076072, -0.7887853092270966, -0.8423819765446056, -0.9060584570723538, -0.9767389018968543, -1.0517255352002366, -1.1259303902381077, -1.1754002935966883, -1.217308735042947, -1.2481313043635136, -1.2647921526449009, -1.2651766337590866, -1.248195384549211, -1.2144892068722508, -1.1666213081561112, -1.105552891186257, -1.0375894462353366, -0.9671845462093357, -0.8981061060272761, -0.8437853326113839, -0.7935977311729897, -0.7482609997919069, -0.7081788436380311, -0.6715698335489674, -0.6407280401726917, -0.6153330625807156, -0.5949747875845742, -0.577634689334792, -0.5653441097179841, -0.5575904072485692, -0.5538353083666873, -0.5536302517724548, -0.5567701808716393, -0.563114119255706, -0.5725531306089688, -0.5859715214940554, -0.6030617070196169, -0.6240864159470136, -0.649366049204734, -0.6907041769982837, -0.7416543326464801, -0.8032033510090665, -0.8758959136644731, -0.9470697759188456, -1.0247605930586683, -1.1058092119290475, -1.184435599780056, -1.2361483096380543, -1.278761633126987, -1.3086229996620888, -1.3227206405155703, -1.3190039897451071, -1.2966400049366296, -1.2567180492469976, -1.201865410289815, -1.1353501775356611, -1.0620424450975574, -0.9861394651386985, -0.9112745841881419, -0.8513083464122873, -0.7952381839268493, -0.7438202429230604, -0.6974902686636641, -0.6572927681755325, -0.6219397297261426, -0.5912196887026924, -0.5648250602138332, -0.5400132123117056, -0.5195972651484364, -0.5030645772384444, -0.4898896910590089, -0.4795791891802581, -0.47177422256228513, -0.4661351662208925, -0.462343541633163, -0.4601295712173094, -0.45933177290537375, -0.45981814151481887, -0.46146884709839014, -0.46434092102135854, -0.46837797272031007, -0.4736069158732377, -0.48004697453585093, -0.4897423066319044, -0.5016291810788173, -0.5160087747493683, -0.5332719767763135, -0.5664270648562743, -0.609610701997916, -0.6653925036477146, -0.7371430875733659, -0.8201077039961048, -0.9214761497512068, -1.040402566387463, -1.1700816381837837, -1.241402884865261, -1.308110358176508, -1.3657825253043867, -1.4096774525072724, -1.4353736069720273, -1.4388980171853978, -1.4183923577621518, -1.3749459918591498, -1.316312621945806, -1.243709771550376, -1.161430813114602, -1.0738331992659236, -0.9971484410415538, -0.921770918605416, -0.8494115729156371, -0.7812815194819025, -0.7152340720833416, -0.6551524899732312, -0.6011585255043969, -0.5531496503797225, -0.4972460963770984, -0.4504816584586722, -0.41160164658848414, -0.3792520464427428, -0.3563094175574156, -0.33629717556404154, -0.318609762707778, -0.3026345724133555, -0.28781923348006033, -0.27375363271942765, -0.26003406496156223, -0.24626323305513867, -0.22997404985079772, -0.21271725584242238, -0.1940763298229779, -0.17361552652972037, -0.14693253720522167, -0.11664824144451541, -0.08222436568774143, -0.04310982033789991, 0.011928651157805985, 0.07511812227425207, 0.146010031711555, 0.22286139841873862, 0.28325697343876755, 0.3427233857661805, 0.3980886662089443, 0.44557208381089813, 0.4812006670587878, 0.5008092038822667, 0.5013859255535454, 0.4823541104013453, 0.4487120129100826, 0.4014208358652218, 0.34355642818024984, 0.27845095951144433, 0.21314043424840612, 0.14748387598260077, 0.08384584356627113, 0.023956502013253615]}, {\"line\": {\"color\": \"green\", \"width\": 1}, \"type\": \"scatter\", \"x\": [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500], \"y\": [-0.27307790517807007, -0.24625147879123688, -0.20242346823215485, -0.14809216558933258, -0.09159816801548004, -0.028796792030334473, 0.06199105083942413, 0.17487899959087372, 0.2969359755516052, 0.43389302492141724, 0.5211577415466309, 0.6075464487075806, 0.690931499004364, 0.7660244107246399, 0.816062867641449, 0.8372583389282227, 0.8236343860626221, 0.781032383441925, 0.7236737608909607, 0.6452173590660095, 0.5486011505126953, 0.4397316575050354, 0.34074264764785767, 0.24236616492271423, 0.14437399804592133, 0.051443442702293396, -0.03227095305919647, -0.10922494530677795, -0.179734006524086, -0.24313460290431976, -0.33459120988845825, -0.41463083028793335, -0.4992128610610962, -0.5678592324256897, -0.6163685917854309, -0.6574708819389343, -0.6928642392158508, -0.7231007218360901, -0.7534038424491882, -0.779367983341217, -0.8017251491546631, -0.8218030333518982, -0.8419901728630066, -0.860934853553772, -0.8789216876029968, -0.8964428305625916, -0.9169630408287048, -0.9369686245918274, -0.9559161067008972, -0.9736941456794739, -0.9929178357124329, -1.0085850954055786, -1.019330382347107, -1.024701476097107, -1.0241732597351074, -1.017134428024292, -1.0041314363479614, -0.9864758849143982, -0.9612309336662292, -0.9345199465751648, -0.9095907807350159, -0.8886488080024719, -0.8755984902381897, -0.8671626448631287, -0.8634651303291321, -0.8648099303245544, -0.8733438849449158, -0.8894374966621399, -0.9119603037834167, -0.9386618733406067, -0.9652174115180969, -0.9906505942344666, -1.0121960639953613, -1.026862621307373, -1.032081127166748, -1.0293896198272705, -1.018862009048462, -1.0015407800674438, -0.9789096713066101, -0.9535059332847595, -0.9278408885002136, -0.9041295647621155, -0.8856410384178162, -0.8714398741722107, -0.8620370030403137, -0.857914388179779, -0.8594419360160828, -0.8674744963645935, -0.8816208243370056, -0.9008813500404358, -0.9263669848442078, -0.9547253251075745, -0.9832807183265686, -1.0087471008300781, -1.0263441801071167, -1.036194086074829, -1.0367729663848877, -1.0278688669204712, -1.0129050016403198, -0.9926876425743103, -0.9688785672187805, -0.9435798525810242, -0.917542040348053, -0.8941583037376404, -0.8751206994056702, -0.8614082932472229, -0.8539335131645203, -0.8517423272132874, -0.8548155426979065, -0.862973153591156, -0.8810864090919495, -0.9067903161048889, -0.937960684299469, -0.9711766839027405, -0.9997546076774597, -1.0233718156814575, -1.0388743877410889, -1.0437310934066772, -1.0384544134140015, -1.0250507593154907, -1.0046740770339966, -0.9793532490730286, -0.9486666321754456, -0.9182156920433044, -0.8910736441612244, -0.8692596554756165, -0.8564671277999878, -0.8485516309738159, -0.8454743027687073, -0.8475058078765869, -0.8586705923080444, -0.8796983361244202, -0.9091398119926453, -0.9437591433525085, -0.9737246632575989, -1.002105474472046, -1.0263749361038208, -1.043150544166565, -1.0496118068695068, -1.0469928979873657, -1.035367727279663, -1.0158052444458008, -0.9904265999794006, -0.961631715297699, -0.9320723414421082, -0.904102623462677, -0.8813394904136658, -0.8628422617912292, -0.8492135405540466, -0.8410314321517944, -0.8383069038391113, -0.8422512412071228, -0.8526747226715088, -0.8689208626747131, -0.8939527869224548, -0.9245650172233582, -0.958445131778717, -0.9924009442329407, -1.0219136476516724, -1.0441585779190063, -1.0557154417037964, -1.054732084274292, -1.0441954135894775, -1.0259572267532349, -1.0014184713363647, -0.9728158116340637, -0.9394388794898987, -0.9072327017784119, -0.8790406584739685, -0.8565452694892883, -0.8430001139640808, -0.8345342874526978, -0.8310722708702087, -0.8328477144241333, -0.8416303992271423, -0.8575909733772278, -0.8800017237663269, -0.9075884222984314, -0.93875652551651, -0.9716728329658508, -1.0037811994552612, -1.0318742990493774, -1.0519757270812988, -1.0627351999282837, -1.0622268915176392, -1.0505326986312866, -1.0321414470672607, -1.0077201128005981, -0.9791192412376404, -0.9486343264579773, -0.9173592925071716, -0.8886043429374695, -0.8640713095664978, -0.8448187112808228, -0.8319265842437744, -0.8246135115623474, -0.8228845000267029, -0.8266971707344055, -0.8378857374191284, -0.8561459183692932, -0.8807154297828674, -0.9103232026100159, -0.943356454372406, -0.9778884053230286, -1.0112556219100952, -1.0401544570922852, -1.0605467557907104, -1.070912480354309, -1.06963312625885, -1.0566014051437378, -1.0368599891662598, -1.010958194732666, -0.9808104634284973, -0.9487956166267395, -0.916198194026947, -0.8861557841300964, -0.8603362441062927, -0.8397677540779114, -0.8254777789115906, -0.8168418407440186, -0.813877284526825, -0.816545307636261, -0.826408326625824, -0.8434168696403503, -0.8669630885124207, -0.895956814289093, -0.9290216565132141, -0.9645854830741882, -1.000296950340271, -1.033034324645996, -1.0596197843551636, -1.0762821435928345, -1.0802654027938843, -1.071123480796814, -1.0537742376327515, -1.0292227268218994, -0.9991559386253357, -0.965984046459198, -0.9303922057151794, -0.8965844511985779, -0.8666554093360901, -0.8418999314308167, -0.8243308663368225, -0.8122017979621887, -0.8055306077003479, -0.8044030666351318, -0.8121371269226074, -0.8316971659660339, -0.8622391819953918, -0.9012822508811951, -0.9372344613075256, -0.9751562476158142, -1.0130677223205566, -1.0469921827316284, -1.0726234912872314, -1.087431788444519, -1.0889493227005005, -1.0769230127334595, -1.0570721626281738, -1.0300997495651245, -0.9978546500205994, -0.9628080725669861, -0.9261453747749329, -0.8914907574653625, -0.8607796430587769, -0.8352070450782776, -0.8163802027702332, -0.8031197190284729, -0.7954537868499756, -0.7934262752532959, -0.7999832630157471, -0.8186282515525818, -0.8486803770065308, -0.8878419995307922, -0.9243994355201721, -0.963783323764801, -1.0042744874954224, -1.0420094728469849, -1.0735321044921875, -1.0941612720489502, -1.1004143953323364, -1.0914244651794434, -1.0732358694076538, -1.046919822692871, -1.0141897201538086, -0.9776635766029358, -0.9382457137107849, -0.900209367275238, -0.8658254742622375, -0.8364928960800171, -0.8147072196006775, -0.7983537316322327, -0.7874289155006409, -0.7820829749107361, -0.7833064794540405, -0.7952523827552795, -0.8175331950187683, -0.8487100005149841, -0.8824566006660461, -0.9212223887443542, -0.9637027382850647, -1.006944179534912, -1.0415643453598022, -1.0716116428375244, -1.094785213470459, -1.1083775758743286, -1.110660433769226, -1.1008284091949463, -1.079333782196045, -1.0482702255249023, -1.0076135396957397, -0.9630023837089539, -0.9184694886207581, -0.8769529461860657, -0.8460274338722229, -0.8199270963668823, -0.7987188696861267, -0.7831215262413025, -0.7718643546104431, -0.7680245637893677, -0.7715044021606445, -0.7817322611808777, -0.8025168180465698, -0.8325211405754089, -0.8708412051200867, -0.9158609509468079, -0.9619205594062805, -1.0092090368270874, -1.0541901588439941, -1.0917530059814453, -1.114719033241272, -1.1242990493774414, -1.1187442541122437, -1.0981823205947876, -1.0707374811172485, -1.0361818075180054, -0.9967545866966248, -0.9552392363548279, -0.9143301844596863, -0.8759844899177551, -0.8417340517044067, -0.8126132488250732, -0.7891307473182678, -0.7715415358543396, -0.7598627805709839, -0.7539794445037842, -0.7549013495445251, -0.7680061459541321, -0.7930042743682861, -0.8282442688941956, -0.8631654381752014, -0.903389036655426, -0.9483539462089539, -0.9954701066017151, -1.035349726676941, -1.07209312915802, -1.1032969951629639, -1.125848412513733, -1.1372781991958618, -1.135785698890686, -1.1207131147384644, -1.093464970588684, -1.0576224327087402, -1.0154801607131958, -0.9700990319252014, -0.9244592785835266, -0.8838481307029724, -0.8469348549842834, -0.8145750761032104, -0.7876293063163757, -0.7652965188026428, -0.7493190169334412, -0.73966383934021, -0.735998809337616, -0.7385977506637573, -0.7483833432197571, -0.7651158571243286, -0.7883702516555786, -0.8177788257598877, -0.8532129526138306, -0.8941277861595154, -0.939378559589386, -0.9838991761207581, -1.0287145376205444, -1.0713934898376465, -1.108550786972046, -1.1368050575256348, -1.1524837017059326, -1.1532117128372192, -1.138539433479309, -1.1137681007385254, -1.0797291994094849, -1.0385966300964355, -0.9932966828346252, -0.9461763501167297, -0.9003191590309143, -0.8577974438667297, -0.8200165033340454, -0.7889502048492432, -0.7634243965148926, -0.7434499263763428, -0.7290763258934021, -0.7179944515228271, -0.7157658338546753, -0.7221928238868713, -0.7363342642784119, -0.7572945952415466, -0.7853670716285706, -0.8204980492591858, -0.8621624112129211, -0.9142455458641052, -0.9717132449150085, -1.0311497449874878, -1.0876719951629639, -1.1222985982894897, -1.1493293046951294, -1.1672025918960571, -1.1731743812561035, -1.1660547256469727, -1.1459507942199707, -1.1140998601913452, -1.0728793144226074, -1.0214580297470093, -0.966724693775177, -0.9127370715141296, -0.8621622920036316, -0.8233932256698608, -0.789465606212616, -0.760289192199707, -0.7365379929542542, -0.7153299450874329, -0.7011774778366089, -0.6939496994018555, -0.6929144263267517, -0.6983382105827332, -0.7105896472930908, -0.7295476198196411, -0.7549459338188171, -0.7866270542144775, -0.8247538805007935, -0.8690881133079529, -0.9188809990882874, -0.9701178669929504, -1.0232586860656738, -1.0757780075073242, -1.1236295700073242, -1.1628177165985107, -1.188222050666809, -1.196006417274475, -1.1848570108413696, -1.1610417366027832, -1.1258676052093506, -1.0815303325653076, -1.0310789346694946, -0.9775534272193909, -0.9241556525230408, -0.8733331561088562, -0.8268455266952515, -0.7883062362670898, -0.7550511956214905, -0.7270664572715759, -0.7045505046844482, -0.6841140389442444, -0.670880138874054, -0.6646140217781067, -0.6644430756568909, -0.6714598536491394, -0.6864997148513794, -0.7094943523406982, -0.7401559948921204, -0.7740797996520996, -0.8144152760505676, -0.8613834977149963, -0.9142283797264099, -0.9691618084907532, -1.0266473293304443, -1.0840836763381958, -1.1373685598373413, -1.1819556951522827, -1.2122498750686646, -1.2235665321350098, -1.214074969291687, -1.1903650760650635, -1.1540520191192627, -1.1073161363601685, -1.0533993244171143, -0.9961206316947937, -0.9383912682533264, -0.8828297257423401, -0.8313636183738708, -0.7886761426925659, -0.7511418461799622, -0.7187240123748779, -0.6917024254798889, -0.666536271572113, -0.647987425327301, -0.6358279585838318, -0.6291673183441162, -0.6275763511657715, -0.6338211894035339, -0.6477038264274597, -0.6685811281204224, -0.6909041404724121, -0.7182134389877319, -0.7511584162712097, -0.7897132039070129, -0.8534132242202759, -0.928472101688385, -1.011296033859253, -1.096842646598816, -1.1483052968978882, -1.1929723024368286, -1.2295151948928833, -1.252256989479065, -1.258221983909607, -1.2457424402236938, -1.2148725986480713, -1.1683719158172607, -1.1101950407028198, -1.044602394104004, -0.97622150182724, -0.9087598919868469, -0.8554583787918091, -0.8063762784004211, -0.7614699602127075, -0.721914529800415, -0.6858627796173096, -0.6557327508926392, -0.6313623189926147, -0.6121729016304016, -0.5960479974746704, -0.5852895379066467, -0.5795122981071472, -0.5779979825019836, -0.5803017616271973, -0.5863076448440552, -0.5959364771842957, -0.6091161966323853, -0.6274490356445312, -0.6503519415855408, -0.6780965328216553, -0.711129367351532, -0.767109215259552, -0.8358588814735413, -0.9163700938224792, -1.0074676275253296, -1.0663644075393677, -1.1247645616531372, -1.1825367212295532, -1.2338464260101318, -1.2747198343276978, -1.300495982170105, -1.3076800107955933, -1.2948635816574097, -1.266127347946167, -1.2228320837020874, -1.1675151586532593, -1.1036368608474731, -1.0374066829681396, -0.9701055884361267, -0.9041784405708313, -0.8417977690696716, -0.7891509532928467, -0.741331160068512, -0.6982825398445129, -0.6604481339454651, -0.6264268159866333, -0.5975441336631775, -0.5735142230987549, -0.5537815093994141, -0.5371487736701965, -0.5242241621017456, -0.5146443843841553, -0.5078886151313782, -0.503581702709198, -0.501487672328949, -0.5014001727104187, -0.5031380653381348, -0.5068634152412415, -0.5125089287757874, -0.5200781226158142, -0.5295965075492859, -0.5435318350791931, -0.5607000589370728, -0.5813454985618591, -0.6060547232627869, -0.6559818387031555, -0.721209704875946, -0.8031456470489502, -0.9049555659294128, -0.9809665083885193, -1.0629336833953857, -1.1507865190505981, -1.2365858554840088, -1.2938252687454224, -1.3410528898239136, -1.375532627105713, -1.392343521118164, -1.388830542564392, -1.3639240264892578, -1.3186805248260498, -1.2562347650527954, -1.1819676160812378, -1.1000579595565796, -1.015085220336914, -0.9307922720909119, -0.8657432198524475, -0.8045805096626282, -0.7468738555908203, -0.69413822889328, -0.6388616561889648, -0.59040766954422, -0.5489954352378845, -0.5135000944137573, -0.4746479392051697, -0.44331520795822144, -0.4188094139099121, -0.39895617961883545, -0.3851178288459778, -0.3731229305267334, -0.3623281717300415, -0.3450085520744324, -0.33878493309020996, -0.3321801424026489, -0.3223453760147095, -0.31212133169174194, -0.29897427558898926, -0.2846348285675049, -0.26925212144851685, -0.2519955039024353, -0.2280697375535965, -0.20024608075618744, -0.16873501241207123, -0.1320098489522934, -0.0902513712644577, -0.04011571407318115, 0.017883576452732086, 0.08587788045406342, 0.15962757170200348, 0.24202612042427063, 0.3317505121231079, 0.42192375659942627, 0.4818114638328552, 0.5320911407470703, 0.5753598213195801, 0.5936079621315002, 0.5886709094047546, 0.5591496825218201, 0.5093900561332703, 0.4434749484062195, 0.3694295287132263, 0.28772443532943726, 0.20160073041915894, 0.1170303076505661, 0.05311568081378937, -0.005259677767753601, -0.06117945909500122, -0.11094993352890015, -0.1575629562139511, -0.20096509158611298, -0.2373698502779007, -0.26842790842056274, -0.32327282428741455, -0.3501971960067749, -0.3747962713241577, -0.4004315137863159, -0.4154897928237915, -0.4283176064491272, -0.4400835633277893, -0.4506567120552063, -0.4603630304336548, -0.46945250034332275, -0.47815752029418945, -0.4867245554924011, -0.4966849088668823, -0.5070133209228516, -0.5178797245025635, -0.5296279788017273, -0.5449284315109253, -0.5622236132621765, -0.5817610621452332, -0.6041436195373535, -0.6352055668830872, -0.6718935966491699, -0.7147133946418762, -0.7648394703865051, -0.8381919264793396, -0.9250633120536804, -1.02336847782135, -1.1291910409927368, -1.1936631202697754, -1.2537364959716797, -1.3087297677993774, -1.3515961170196533, -1.3781437873840332, -1.3844431638717651, -1.3686460256576538, -1.331275224685669, -1.2799097299575806, -1.2156779766082764, -1.142035961151123, -1.0631366968154907, -0.9914315342903137, -0.9210969805717468, -0.8532946705818176, -0.78983074426651, -0.7376042604446411, -0.6898323893547058, -0.6461504101753235, -0.6069740056991577, -0.5557616949081421, -0.5132387280464172, -0.4795438051223755, -0.4521469473838806, -0.4357384443283081, -0.4216066002845764, -0.40897679328918457, -0.39801329374313354, -0.38837915658950806, -0.37981462478637695, -0.37207692861557007, -0.3649236559867859, -0.3574782609939575, -0.3463696241378784, -0.3396620750427246, -0.33171534538269043, -0.32096731662750244, -0.3092923164367676, -0.2966434955596924, -0.2825116515159607, -0.26261889934539795, -0.23950310051441193, -0.2133629471063614, -0.1828189641237259, -0.1390136331319809, -0.09552307426929474, -0.046883419156074524, 0.01091572642326355, 0.09794831275939941, 0.202494278550148, 0.31295228004455566, 0.42654120922088623, 0.49128222465515137, 0.5429155230522156, 0.5905442833900452, 0.6146843433380127, 0.6141652464866638, 0.5893318057060242, 0.5421663522720337, 0.4789724349975586, 0.4077340364456177, 0.3276638984680176, 0.24137046933174133, 0.15489479899406433, 0.08655837178230286, 0.023192480206489563, -0.03752608597278595, -0.09222166240215302, -0.1432316154241562, -0.1898181289434433, -0.2305992990732193, -0.2657199501991272, -0.3239886164665222, -0.3522408604621887, -0.3829089403152466, -0.41268390417099, -0.4314597249031067, -0.4476073384284973, -0.46230047941207886, -0.47554874420166016, -0.48779213428497314, -0.4993516206741333, -0.5105313062667847, -0.5216559767723083, -0.5348241925239563, -0.5486519932746887, -0.5633640885353088, -0.5794360637664795, -0.6004260778427124, -0.6242766976356506, -0.651275634765625, -0.6821798086166382, -0.72538822889328, -0.7760153412818909, -0.8341497778892517, -0.9004316926002502, -0.9742961525917053, -1.054148554801941, -1.1365869045257568, -1.215453863143921, -1.2654870748519897, -1.3054026365280151, -1.3329969644546509, -1.3439533710479736, -1.336462378501892, -1.3100311756134033, -1.2658003568649292, -1.207068920135498, -1.137587308883667, -1.0619137287139893, -0.9843011498451233, -0.9082021117210388, -0.8480636477470398, -0.7920495867729187, -0.7399092316627502, -0.6928575038909912, -0.6513277888298035, -0.6147662997245789, -0.5828877687454224, -0.5553987622261047, -0.5290259718894958, -0.5071645379066467, -0.48945844173431396, -0.475098192691803, -0.4635356664657593, -0.45437783002853394, -0.4472634196281433, -0.4418284296989441, -0.43787938356399536, -0.43503785133361816, -0.4331403374671936, -0.43204307556152344, -0.43160974979400635, -0.43186426162719727, -0.43275392055511475, -0.4342191219329834, -0.4365462064743042, -0.43961405754089355, -0.4434468150138855, -0.44811105728149414, -0.454992413520813, -0.463479220867157, -0.4737900495529175, -0.48637086153030396, -0.5100387930870056, -0.5415624976158142, -0.5828513503074646, -0.6378309726715088, -0.6968043446540833, -0.7705512046813965, -0.8622643351554871, -0.9726261496543884, -1.0604100227355957, -1.1541873216629028, -1.2527105808258057, -1.347213864326477, -1.4090070724487305, -1.458376407623291, -1.4924733638763428, -1.5059969425201416, -1.4964408874511719, -1.463025450706482, -1.4072068929672241, -1.3327815532684326, -1.2674968242645264, -1.197281002998352, -1.1225353479385376, -1.0465304851531982, -0.970973551273346, -0.8974587321281433, -0.8271268606185913, -0.7607830166816711, -0.6769013404846191, -0.6070058941841125, -0.5370784401893616, -0.4807225465774536, -0.42533302307128906, -0.3663838505744934, -0.29645079374313354, -0.2720365524291992, -0.2466210275888443, -0.21664194762706757, -0.18995238840579987, -0.16477398574352264, -0.14170308411121368, -0.11838878691196442, -0.10234549641609192, -0.08294598758220673, -0.05754649639129639, -0.029851451516151428, -1.0371208190917969e-05, 0.033079639077186584, 0.07961475849151611, 0.1311800181865692, 0.18482279777526855, 0.2406897097826004, 0.28910091519355774, 0.3344152569770813, 0.3749349117279053, 0.40569251775741577, 0.42273253202438354, 0.41902709007263184, 0.3951336145401001, 0.35503143072128296, 0.3096744418144226, 0.25893434882164, 0.20272283256053925, 0.1448344588279724, 0.09077804535627365, 0.0409531369805336, -0.0037278085947036743, -0.0421537309885025, -0.07199510931968689, -0.10025769472122192, -0.12255428731441498, -0.1392497569322586, -0.1550578624010086, -0.16373233497142792, -0.16508804261684418, -0.161302849650383, -0.1523006409406662, -0.13696716725826263, -0.11567692458629608, -0.08867810666561127, -0.05805449187755585, -0.022118285298347473, 0.019953913986682892, 0.06800670176744461, 0.14786401391029358, 0.23858147859573364, 0.3289701044559479, 0.41067010164260864, 0.4370654821395874, 0.4526389241218567, 0.4627434015274048, 0.4460705518722534, 0.4182816743850708, 0.37581008672714233, 0.32437509298324585, 0.26412898302078247, 0.20016223192214966, 0.1349354237318039, 0.07202599942684174, 0.01496383547782898, -0.02734515070915222, -0.06391642987728119, -0.0954405665397644, -0.12482611835002899, -0.15180079638957977, -0.17253415286540985, -0.18704654276371002, -0.19695119559764862, -0.20386551320552826, -0.20679862797260284, -0.20631565153598785, -0.2037539929151535, -0.19720254838466644, -0.18669448792934418, -0.17220644652843475, -0.15601511299610138, -0.1330510526895523, -0.10499727725982666, -0.07221156358718872, -0.033673107624053955, 0.03348632901906967, 0.11545106768608093, 0.20686563849449158, 0.30638426542282104, 0.35583633184432983, 0.40029144287109375, 0.4481664299964905, 0.48536932468414307, 0.5039336085319519, 0.5000252723693848, 0.47386735677719116, 0.42845380306243896, 0.38126471638679504, 0.32378941774368286, 0.2581601142883301, 0.18911048769950867, 0.12504808604717255, 0.06451912224292755, 0.008249357342720032, -0.04190985858440399, -0.08356258273124695, -0.11876796185970306, -0.15203525125980377, -0.17981068789958954, -0.20547576248645782, -0.22595717012882233, -0.24129237234592438, -0.25995272397994995, -0.27856898307800293, -0.2907378077507019, -0.296420156955719, -0.29800885915756226, -0.2963842749595642, -0.2926861643791199, -0.2868722677230835, -0.2788032293319702, -0.2671789526939392, -0.25251781940460205, -0.2348557859659195, -0.21374689042568207, -0.17857004702091217, -0.13535277545452118, -0.09438320994377136, -0.04483683407306671, 0.020375043153762817, 0.09885485470294952, 0.1895981729030609, 0.2912636697292328, 0.35206979513168335, 0.4105810523033142, 0.4737953543663025, 0.5281811356544495, 0.5683931708335876, 0.5828953385353088, 0.5702064633369446, 0.5324742794036865, 0.4862901568412781, 0.4283244013786316, 0.3576061427593231, 0.27938705682754517, 0.20352229475975037, 0.12928365170955658, 0.058243148028850555, -0.006994783878326416, -0.05671960115432739, -0.10122999548912048, -0.14244507253170013, -0.18106330931186676, -0.2288012057542801, -0.26835280656814575]}, {\"line\": {\"color\": \"red\", \"width\": 1}, \"type\": \"scatter\", \"x\": [1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265], \"y\": [-0.7844164967536926, -0.7938733100891113, -0.8001192212104797, -0.8054235577583313, -0.8116531372070312, -0.8480663299560547, -0.9020710587501526, -0.9629707932472229, -1.024846076965332, -1.0829955339431763, -1.1329289674758911, -1.169922947883606, -1.189766526222229, -1.1898562908172607, -1.1846816539764404, -1.1788538694381714, -1.1764017343521118, -1.1782547235488892, -1.1847137212753296, -1.196203589439392, -1.2130018472671509, -1.2353428602218628, -1.2626417875289917, -1.2949355840682983, -1.3322405815124512, -1.374389410018921, -1.4226957559585571, -1.4721630811691284, -1.5151770114898682, -1.552693486213684, -1.5852993726730347, -1.613832950592041, -1.6398351192474365, -1.6636143922805786, -1.6849859952926636, -1.7056645154953003, -1.7261343002319336, -1.7463500499725342, -1.7662813663482666, -1.786054015159607, -1.805503487586975, -1.8248448371887207, -1.8443901538848877, -1.8641033172607422, -1.8840227127075195, -1.9042103290557861, -1.9249026775360107, -1.9462368488311768, -1.968388557434082, -1.9914557933807373, -2.015991449356079, -2.0422985553741455, -2.0708229541778564, -2.1020939350128174, -2.135436534881592, -2.170825242996216, -2.2065021991729736, -2.2417562007904053, -2.276005983352661, -2.3082292079925537, -2.338809013366699, -2.3666810989379883, -2.3906943798065186, -2.4101908206939697, -2.4250388145446777, -2.4357340335845947, -2.44317626953125, -2.4482152462005615, -2.4513485431671143, -2.4531409740448, -2.454483985900879, -2.4559409618377686, -2.457904100418091, -2.4606306552886963, -2.4641213417053223, -2.4684557914733887, -2.47320818901062, -2.478438377380371, -2.4843244552612305, -2.490943193435669, -2.498476505279541, -2.5067996978759766, -2.5156052112579346, -2.5248494148254395, -2.5343332290649414, -2.54400634765625, -2.553687334060669, -2.563260078430176, -2.5728914737701416, -2.582599639892578, -2.5923542976379395, -2.602147102355957, -2.6119778156280518, -2.6218414306640625, -2.6317379474639893, -2.6416726112365723, -2.6516573429107666, -2.6616973876953125, -2.6718225479125977, -2.682056188583374, -2.692434549331665, -2.702984571456909, -2.71382737159729, -2.72505521774292, -2.736801862716675, -2.7489185333251953, -2.761530876159668, -2.774815082550049, -2.788864850997925, -2.8039944171905518, -2.820378065109253, -2.8383612632751465, -2.858464479446411, -2.881361961364746, -2.906777858734131, -2.9351274967193604, -2.965909957885742, -2.9993040561676025, -3.0340545177459717, -3.0693044662475586, -3.1048624515533447, -3.139920234680176, -3.1732125282287598, -3.203706979751587, -3.2313642501831055, -3.254779815673828, -3.2729575634002686, -3.285536289215088, -3.2928266525268555, -3.295269012451172, -3.294140338897705, -3.290046215057373, -3.2841427326202393, -3.27728009223938, -3.2700178623199463, -3.262949228286743, -3.2556591033935547, -3.2486324310302734, -3.2414228916168213, -3.2334370613098145, -3.224055528640747, -3.214175224304199, -3.2035434246063232, -3.192699670791626, -3.1822962760925293, -3.17156982421875, -3.1611673831939697, -3.1508476734161377, -3.140545606613159, -3.130053758621216, -3.118948459625244, -3.1070268154144287, -3.0939443111419678, -3.079582452774048, -3.063384532928467, -3.0451607704162598, -3.024745464324951, -3.0059728622436523, -2.992079973220825, -2.983102560043335, -2.97938871383667, -2.9807932376861572, -2.9869208335876465, -2.997166395187378, -3.0106725692749023, -3.026226758956909, -3.0422492027282715, -3.0572266578674316, -3.0699706077575684, -3.0793521404266357, -3.0843207836151123, -3.0843701362609863, -3.082662343978882, -3.082005262374878, -3.0819075107574463, -3.08186936378479, -3.0816307067871094, -3.0808515548706055, -3.0792758464813232, -3.0765864849090576, -3.072457790374756, -3.066614866256714, -3.0588042736053467, -3.0487661361694336, -3.036234140396118, -3.0211098194122314, -3.0026228427886963, -2.9805729389190674, -2.963080406188965, -2.9517877101898193, -2.9480926990509033, -2.951601982116699, -2.95993709564209, -2.9726309776306152, -2.9890079498291016, -3.007863759994507, -3.0271077156066895, -3.0453622341156006, -3.0615057945251465, -3.074371099472046]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"rangeslider\": {\"visible\": true}}, \"yaxis\": {\"autorange\": true, \"fixedrange\": false}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('b8aafb79-a5de-42d2-b205-d4244cc190d4');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1nuQuY-CN1m"
      },
      "source": [
        "# Création du modèle type Wavenet Multivarié - Apprentissage multivarié"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtAbZ43tCN2H"
      },
      "source": [
        "**2. Construction du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7N-oFV6jCN2H",
        "outputId": "10129a37-2607-482c-9a45-0418265680a2"
      },
      "source": [
        "def compute_receptive_field_(dilation_depth, nb_stacks):\n",
        "    receptive_field = nb_stacks * (2 ** dilation_depth * 2) - (nb_stacks - 1)\n",
        "    return receptive_field\n",
        "\n",
        "nb_filters = 2\n",
        "dim_filters = 2\n",
        "nb_input_bins = 3\n",
        "dilation_depth = 3\n",
        "nb_stacks = 1\n",
        "use_bias = False\n",
        "res_l2 = 0\n",
        "final_l2 = 0\n",
        "\n",
        "fragment_length = compute_receptive_field_(dilation_depth, nb_stacks)\n",
        "fragment_length\n",
        "\n",
        "model = build_model(fragment_length, nb_filters, dim_filters, nb_input_bins, dilation_depth, nb_stacks, use_bias, res_l2, final_l2)\n",
        "model.summary()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_part (InputLayer)         [(None, 16, 3)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_3 (Sli (None, 16)           0           input_part[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_4 (Sli (None, 16)           0           input_part[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_5 (Sli (None, 16)           0           input_part[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims_3 (TFOpLambda)   (None, 16, 1)        0           tf.__operators__.getitem_3[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims_4 (TFOpLambda)   (None, 16, 1)        0           tf.__operators__.getitem_4[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims_5 (TFOpLambda)   (None, 16, 1)        0           tf.__operators__.getitem_5[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_1_condition_1_s0 ( (None, 16, 2)        6           tf.expand_dims_3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_1_condition_2_s0 ( (None, 16, 2)        6           tf.expand_dims_4[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_1_condition_3_s0 ( (None, 16, 2)        6           tf.expand_dims_5[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_19 (Conv1D)              (None, 16, 1)        2           dilated_conv_1_condition_1_s0[0][\n",
            "__________________________________________________________________________________________________\n",
            "conv1d_20 (Conv1D)              (None, 16, 1)        2           dilated_conv_1_condition_2_s0[0][\n",
            "__________________________________________________________________________________________________\n",
            "conv1d_21 (Conv1D)              (None, 16, 1)        2           dilated_conv_1_condition_3_s0[0][\n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 16, 1)        0           conv1d_19[0][0]                  \n",
            "                                                                 conv1d_20[0][0]                  \n",
            "                                                                 conv1d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_2_relu_s0 (CausalD (None, 16, 2)        6           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_22 (Conv1D)              (None, 16, 1)        2           dilated_conv_2_relu_s0[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 16, 1)        0           add_8[0][0]                      \n",
            "                                                                 conv1d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_4_relu_s0 (CausalD (None, 16, 2)        6           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_24 (Conv1D)              (None, 16, 1)        2           dilated_conv_4_relu_s0[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 16, 1)        0           add_9[0][0]                      \n",
            "                                                                 conv1d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_8_relu_s0 (CausalD (None, 16, 2)        6           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_26 (Conv1D)              (None, 16, 1)        2           dilated_conv_8_relu_s0[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 16, 1)        0           add_10[0][0]                     \n",
            "                                                                 conv1d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "output_linear (Activation)      (None, 16, 1)        0           add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_28 (Conv1D)              (None, 16, 1)        2           output_linear[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 50\n",
            "Trainable params: 50\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-bEVZTKCN2I"
      },
      "source": [
        "**2. Optimisation de l'apprentissage**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsd3q2SxCN2I"
      },
      "source": [
        "Pour accélérer le traitement des données, nous n'allons pas utiliser l'intégralité des données pendant la mise à jour du gradient, comme cela a été fait jusqu'à présent (en utilisant le dataset).  \n",
        "Cette fois-ci, nous allons forcer les mises à jour du gradient à se produire de manière moins fréquente en attribuant la valeur du batch_size à prendre en compte lors de la regression du modèle.  \n",
        "Pour cela, on utilise l'argument \"batch_size\" dans la méthode fit. En précisant un batch_size=1000, cela signifie que :\n",
        " - Sur notre total de 56000 échantillons, 56 seront utilisés pour les calculs du gradient\n",
        " - Il y aura également 56 itérations à chaque période.\n",
        "  \n",
        "    \n",
        "    \n",
        "Si nous avions pris le dataset comme entrée, nous aurions eu :\n",
        "- Un total de 56000 échantillons également\n",
        "- Chaque période aurait également pris 56 itérations pour se compléter\n",
        "- Mais 1000 échantillons auraient été utilisés pour le calcul du gradient, au lieu de 56 avec la méthode utilisée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwjBiyLmCN2J"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "def  My_MSE(y_true,y_pred):\n",
        "  return(tf.keras.metrics.mse(y_true,y_pred)*std.numpy()+mean.numpy())\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam()\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"mse\", optimizer=optimiseur, metrics=[\"mse\",My_MSE])\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint],batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zv8uM_79CN2J"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 1])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "v7Fe6sb4CN2K",
        "outputId": "881b6e34-11d1-4a3c-a6c1-630cd7855d83"
      },
      "source": [
        "# Chargement des poids sauvegardés\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-235-a7575e4f6b23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Chargement des poids sauvegardés\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"poids.hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2225\u001b[0m           'first, then load the weights.')\n\u001b[1;32m   2226\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_weights_created\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2227\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2228\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'layer_names'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'model_weights'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2229\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'poids.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLVmHMOMCN2K",
        "outputId": "00afecb8-c2ca-43cf-b550-9926f9e5751e"
      },
      "source": [
        "max_periodes = 1000\n",
        "\n",
        "# Classe permettant d'arrêter l'entrainement si la variation\n",
        "# devient plus petite qu'une valeur à choisir sur un nombre\n",
        "# de périodes à choisir\n",
        "class StopTrain(keras.callbacks.Callback):\n",
        "    def __init__(self, delta=0.01,periodes=100, term=\"loss\", logs={}):\n",
        "      self.n_periodes = 0\n",
        "      self.periodes = periodes\n",
        "      self.loss_1 = 100\n",
        "      self.delta = delta\n",
        "      self.term = term\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "      diff_loss = abs(self.loss_1 - logs[self.term])\n",
        "      self.loss_1 = logs[self.term]\n",
        "      if (diff_loss < self.delta):\n",
        "        self.n_periodes = self.n_periodes + 1\n",
        "      else:\n",
        "        self.n_periodes = 0\n",
        "      if (self.n_periodes == self.periodes):\n",
        "        print(\"Arrêt de l'entrainement...\")\n",
        "        self.model.stop_training = True\n",
        "\n",
        "def  My_MSE(y_true,y_pred):\n",
        "  return(tf.keras.metrics.mse(y_true,y_pred)*std.numpy()+mean.numpy())\n",
        "  \n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.001,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"mse\", optimizer=optimiseur, metrics=[\"mse\",My_MSE])\n",
        "\n",
        "# Entraine le modèle, avec une réduction des calculs du gradient\n",
        "historique = model.fit(x=x_train,y=y_train,validation_data=(x_val,y_val), epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=1e-6,periodes = 50, term=\"loss\")],batch_size=batch_size)\n",
        "\n",
        "\n",
        "# Entraine le modèle sans réduction de calculs\n",
        "#historique = model.fit(dataset,validation_data=dataset_val, epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=1e-8,periodes = 10, term=\"val_My_MSE\")])\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "16/16 [==============================] - 2s 28ms/step - loss: 1.0671 - mse: 1.0671 - My_MSE: 22.8910 - val_loss: 0.7252 - val_mse: 0.7252 - val_My_MSE: 17.5560\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.99664, saving model to poids_train.hdf5\n",
            "Epoch 2/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6753 - mse: 0.6753 - My_MSE: 16.7770 - val_loss: 0.4872 - val_mse: 0.4872 - val_My_MSE: 13.8409\n",
            "\n",
            "Epoch 00002: loss improved from 0.99664 to 0.62800, saving model to poids_train.hdf5\n",
            "Epoch 3/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.4715 - mse: 0.4715 - My_MSE: 13.5961 - val_loss: 0.3452 - val_mse: 0.3452 - val_My_MSE: 11.6256\n",
            "\n",
            "Epoch 00003: loss improved from 0.62800 to 0.43973, saving model to poids_train.hdf5\n",
            "Epoch 4/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.3390 - mse: 0.3390 - My_MSE: 11.5286 - val_loss: 0.2357 - val_mse: 0.2357 - val_My_MSE: 9.9163\n",
            "\n",
            "Epoch 00004: loss improved from 0.43973 to 0.31278, saving model to poids_train.hdf5\n",
            "Epoch 5/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.2373 - mse: 0.2373 - My_MSE: 9.9424 - val_loss: 0.1536 - val_mse: 0.1536 - val_My_MSE: 8.6359\n",
            "\n",
            "Epoch 00005: loss improved from 0.31278 to 0.21037, saving model to poids_train.hdf5\n",
            "Epoch 6/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1452 - mse: 0.1452 - My_MSE: 8.5048 - val_loss: 0.1060 - val_mse: 0.1060 - val_My_MSE: 7.8932\n",
            "\n",
            "Epoch 00006: loss improved from 0.21037 to 0.13636, saving model to poids_train.hdf5\n",
            "Epoch 7/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1050 - mse: 0.1050 - My_MSE: 7.8769 - val_loss: 0.0855 - val_mse: 0.0855 - val_My_MSE: 7.5723\n",
            "\n",
            "Epoch 00007: loss improved from 0.13636 to 0.09828, saving model to poids_train.hdf5\n",
            "Epoch 8/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0857 - mse: 0.0857 - My_MSE: 7.5760 - val_loss: 0.0780 - val_mse: 0.0780 - val_My_MSE: 7.4564\n",
            "\n",
            "Epoch 00008: loss improved from 0.09828 to 0.08213, saving model to poids_train.hdf5\n",
            "Epoch 9/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0766 - mse: 0.0766 - My_MSE: 7.4348 - val_loss: 0.0713 - val_mse: 0.0713 - val_My_MSE: 7.3511\n",
            "\n",
            "Epoch 00009: loss improved from 0.08213 to 0.07413, saving model to poids_train.hdf5\n",
            "Epoch 10/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0693 - mse: 0.0693 - My_MSE: 7.3199 - val_loss: 0.0671 - val_mse: 0.0671 - val_My_MSE: 7.2863\n",
            "\n",
            "Epoch 00010: loss improved from 0.07413 to 0.06865, saving model to poids_train.hdf5\n",
            "Epoch 11/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0682 - mse: 0.0682 - My_MSE: 7.3027 - val_loss: 0.0639 - val_mse: 0.0639 - val_My_MSE: 7.2359\n",
            "\n",
            "Epoch 00011: loss improved from 0.06865 to 0.06517, saving model to poids_train.hdf5\n",
            "Epoch 12/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0634 - mse: 0.0634 - My_MSE: 7.2282 - val_loss: 0.0614 - val_mse: 0.0614 - val_My_MSE: 7.1966\n",
            "\n",
            "Epoch 00012: loss improved from 0.06517 to 0.06262, saving model to poids_train.hdf5\n",
            "Epoch 13/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0630 - mse: 0.0630 - My_MSE: 7.2213 - val_loss: 0.0593 - val_mse: 0.0593 - val_My_MSE: 7.1644\n",
            "\n",
            "Epoch 00013: loss improved from 0.06262 to 0.06064, saving model to poids_train.hdf5\n",
            "Epoch 14/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0603 - mse: 0.0603 - My_MSE: 7.1799 - val_loss: 0.0575 - val_mse: 0.0575 - val_My_MSE: 7.1366\n",
            "\n",
            "Epoch 00014: loss improved from 0.06064 to 0.05896, saving model to poids_train.hdf5\n",
            "Epoch 15/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0569 - mse: 0.0569 - My_MSE: 7.1267 - val_loss: 0.0559 - val_mse: 0.0559 - val_My_MSE: 7.1117\n",
            "\n",
            "Epoch 00015: loss improved from 0.05896 to 0.05746, saving model to poids_train.hdf5\n",
            "Epoch 16/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0582 - mse: 0.0582 - My_MSE: 7.1467 - val_loss: 0.0545 - val_mse: 0.0545 - val_My_MSE: 7.0888\n",
            "\n",
            "Epoch 00016: loss improved from 0.05746 to 0.05608, saving model to poids_train.hdf5\n",
            "Epoch 17/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0528 - mse: 0.0528 - My_MSE: 7.0622 - val_loss: 0.0531 - val_mse: 0.0531 - val_My_MSE: 7.0680\n",
            "\n",
            "Epoch 00017: loss improved from 0.05608 to 0.05476, saving model to poids_train.hdf5\n",
            "Epoch 18/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0550 - mse: 0.0550 - My_MSE: 7.0975 - val_loss: 0.0519 - val_mse: 0.0519 - val_My_MSE: 7.0486\n",
            "\n",
            "Epoch 00018: loss improved from 0.05476 to 0.05359, saving model to poids_train.hdf5\n",
            "Epoch 19/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0539 - mse: 0.0539 - My_MSE: 7.0799 - val_loss: 0.0508 - val_mse: 0.0508 - val_My_MSE: 7.0313\n",
            "\n",
            "Epoch 00019: loss improved from 0.05359 to 0.05248, saving model to poids_train.hdf5\n",
            "Epoch 20/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0542 - mse: 0.0542 - My_MSE: 7.0849 - val_loss: 0.0497 - val_mse: 0.0497 - val_My_MSE: 7.0146\n",
            "\n",
            "Epoch 00020: loss improved from 0.05248 to 0.05146, saving model to poids_train.hdf5\n",
            "Epoch 21/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0494 - mse: 0.0494 - My_MSE: 7.0103 - val_loss: 0.0488 - val_mse: 0.0488 - val_My_MSE: 6.9999\n",
            "\n",
            "Epoch 00021: loss improved from 0.05146 to 0.05049, saving model to poids_train.hdf5\n",
            "Epoch 22/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0500 - mse: 0.0500 - My_MSE: 7.0193 - val_loss: 0.0479 - val_mse: 0.0479 - val_My_MSE: 6.9859\n",
            "\n",
            "Epoch 00022: loss improved from 0.05049 to 0.04964, saving model to poids_train.hdf5\n",
            "Epoch 23/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0481 - mse: 0.0481 - My_MSE: 6.9889 - val_loss: 0.0471 - val_mse: 0.0471 - val_My_MSE: 6.9730\n",
            "\n",
            "Epoch 00023: loss improved from 0.04964 to 0.04884, saving model to poids_train.hdf5\n",
            "Epoch 24/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0491 - mse: 0.0491 - My_MSE: 7.0045 - val_loss: 0.0463 - val_mse: 0.0463 - val_My_MSE: 6.9615\n",
            "\n",
            "Epoch 00024: loss improved from 0.04884 to 0.04810, saving model to poids_train.hdf5\n",
            "Epoch 25/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0488 - mse: 0.0488 - My_MSE: 6.9994 - val_loss: 0.0456 - val_mse: 0.0456 - val_My_MSE: 6.9501\n",
            "\n",
            "Epoch 00025: loss improved from 0.04810 to 0.04743, saving model to poids_train.hdf5\n",
            "Epoch 26/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0472 - mse: 0.0472 - My_MSE: 6.9751 - val_loss: 0.0450 - val_mse: 0.0450 - val_My_MSE: 6.9405\n",
            "\n",
            "Epoch 00026: loss improved from 0.04743 to 0.04677, saving model to poids_train.hdf5\n",
            "Epoch 27/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0480 - mse: 0.0480 - My_MSE: 6.9876 - val_loss: 0.0444 - val_mse: 0.0444 - val_My_MSE: 6.9309\n",
            "\n",
            "Epoch 00027: loss improved from 0.04677 to 0.04619, saving model to poids_train.hdf5\n",
            "Epoch 28/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0473 - mse: 0.0473 - My_MSE: 6.9769 - val_loss: 0.0438 - val_mse: 0.0438 - val_My_MSE: 6.9217\n",
            "\n",
            "Epoch 00028: loss improved from 0.04619 to 0.04564, saving model to poids_train.hdf5\n",
            "Epoch 29/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0462 - mse: 0.0462 - My_MSE: 6.9594 - val_loss: 0.0432 - val_mse: 0.0432 - val_My_MSE: 6.9132\n",
            "\n",
            "Epoch 00029: loss improved from 0.04564 to 0.04511, saving model to poids_train.hdf5\n",
            "Epoch 30/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0437 - mse: 0.0437 - My_MSE: 6.9209 - val_loss: 0.0427 - val_mse: 0.0427 - val_My_MSE: 6.9057\n",
            "\n",
            "Epoch 00030: loss improved from 0.04511 to 0.04462, saving model to poids_train.hdf5\n",
            "Epoch 31/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0439 - mse: 0.0439 - My_MSE: 6.9238 - val_loss: 0.0423 - val_mse: 0.0423 - val_My_MSE: 6.8985\n",
            "\n",
            "Epoch 00031: loss improved from 0.04462 to 0.04415, saving model to poids_train.hdf5\n",
            "Epoch 32/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0439 - mse: 0.0439 - My_MSE: 6.9238 - val_loss: 0.0418 - val_mse: 0.0418 - val_My_MSE: 6.8908\n",
            "\n",
            "Epoch 00032: loss improved from 0.04415 to 0.04371, saving model to poids_train.hdf5\n",
            "Epoch 33/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0445 - mse: 0.0445 - My_MSE: 6.9334 - val_loss: 0.0413 - val_mse: 0.0413 - val_My_MSE: 6.8839\n",
            "\n",
            "Epoch 00033: loss improved from 0.04371 to 0.04327, saving model to poids_train.hdf5\n",
            "Epoch 34/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0417 - mse: 0.0417 - My_MSE: 6.8888 - val_loss: 0.0410 - val_mse: 0.0410 - val_My_MSE: 6.8778\n",
            "\n",
            "Epoch 00034: loss improved from 0.04327 to 0.04285, saving model to poids_train.hdf5\n",
            "Epoch 35/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0419 - mse: 0.0419 - My_MSE: 6.8930 - val_loss: 0.0405 - val_mse: 0.0405 - val_My_MSE: 6.8707\n",
            "\n",
            "Epoch 00035: loss improved from 0.04285 to 0.04245, saving model to poids_train.hdf5\n",
            "Epoch 36/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0418 - mse: 0.0418 - My_MSE: 6.8913 - val_loss: 0.0401 - val_mse: 0.0401 - val_My_MSE: 6.8649\n",
            "\n",
            "Epoch 00036: loss improved from 0.04245 to 0.04207, saving model to poids_train.hdf5\n",
            "Epoch 37/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0401 - mse: 0.0401 - My_MSE: 6.8642 - val_loss: 0.0397 - val_mse: 0.0397 - val_My_MSE: 6.8589\n",
            "\n",
            "Epoch 00037: loss improved from 0.04207 to 0.04169, saving model to poids_train.hdf5\n",
            "Epoch 38/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0419 - mse: 0.0419 - My_MSE: 6.8929 - val_loss: 0.0394 - val_mse: 0.0394 - val_My_MSE: 6.8529\n",
            "\n",
            "Epoch 00038: loss improved from 0.04169 to 0.04132, saving model to poids_train.hdf5\n",
            "Epoch 39/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0414 - mse: 0.0414 - My_MSE: 6.8849 - val_loss: 0.0390 - val_mse: 0.0390 - val_My_MSE: 6.8468\n",
            "\n",
            "Epoch 00039: loss improved from 0.04132 to 0.04096, saving model to poids_train.hdf5\n",
            "Epoch 40/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0392 - mse: 0.0392 - My_MSE: 6.8500 - val_loss: 0.0386 - val_mse: 0.0386 - val_My_MSE: 6.8417\n",
            "\n",
            "Epoch 00040: loss improved from 0.04096 to 0.04060, saving model to poids_train.hdf5\n",
            "Epoch 41/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0409 - mse: 0.0409 - My_MSE: 6.8775 - val_loss: 0.0383 - val_mse: 0.0383 - val_My_MSE: 6.8365\n",
            "\n",
            "Epoch 00041: loss improved from 0.04060 to 0.04025, saving model to poids_train.hdf5\n",
            "Epoch 42/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0398 - mse: 0.0398 - My_MSE: 6.8603 - val_loss: 0.0379 - val_mse: 0.0379 - val_My_MSE: 6.8299\n",
            "\n",
            "Epoch 00042: loss improved from 0.04025 to 0.03991, saving model to poids_train.hdf5\n",
            "Epoch 43/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0395 - mse: 0.0395 - My_MSE: 6.8551 - val_loss: 0.0376 - val_mse: 0.0376 - val_My_MSE: 6.8253\n",
            "\n",
            "Epoch 00043: loss improved from 0.03991 to 0.03956, saving model to poids_train.hdf5\n",
            "Epoch 44/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0382 - mse: 0.0382 - My_MSE: 6.8355 - val_loss: 0.0373 - val_mse: 0.0373 - val_My_MSE: 6.8202\n",
            "\n",
            "Epoch 00044: loss improved from 0.03956 to 0.03922, saving model to poids_train.hdf5\n",
            "Epoch 45/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0376 - mse: 0.0376 - My_MSE: 6.8255 - val_loss: 0.0369 - val_mse: 0.0369 - val_My_MSE: 6.8139\n",
            "\n",
            "Epoch 00045: loss improved from 0.03922 to 0.03888, saving model to poids_train.hdf5\n",
            "Epoch 46/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0410 - mse: 0.0410 - My_MSE: 6.8778 - val_loss: 0.0365 - val_mse: 0.0365 - val_My_MSE: 6.8090\n",
            "\n",
            "Epoch 00046: loss improved from 0.03888 to 0.03855, saving model to poids_train.hdf5\n",
            "Epoch 47/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0389 - mse: 0.0389 - My_MSE: 6.8457 - val_loss: 0.0362 - val_mse: 0.0362 - val_My_MSE: 6.8033\n",
            "\n",
            "Epoch 00047: loss improved from 0.03855 to 0.03822, saving model to poids_train.hdf5\n",
            "Epoch 48/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0376 - mse: 0.0376 - My_MSE: 6.8247 - val_loss: 0.0359 - val_mse: 0.0359 - val_My_MSE: 6.7994\n",
            "\n",
            "Epoch 00048: loss improved from 0.03822 to 0.03788, saving model to poids_train.hdf5\n",
            "Epoch 49/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0384 - mse: 0.0384 - My_MSE: 6.8382 - val_loss: 0.0355 - val_mse: 0.0355 - val_My_MSE: 6.7919\n",
            "\n",
            "Epoch 00049: loss improved from 0.03788 to 0.03757, saving model to poids_train.hdf5\n",
            "Epoch 50/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0371 - mse: 0.0371 - My_MSE: 6.8171 - val_loss: 0.0352 - val_mse: 0.0352 - val_My_MSE: 6.7884\n",
            "\n",
            "Epoch 00050: loss improved from 0.03757 to 0.03723, saving model to poids_train.hdf5\n",
            "Epoch 51/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0359 - mse: 0.0359 - My_MSE: 6.7983 - val_loss: 0.0348 - val_mse: 0.0348 - val_My_MSE: 6.7809\n",
            "\n",
            "Epoch 00051: loss improved from 0.03723 to 0.03690, saving model to poids_train.hdf5\n",
            "Epoch 52/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0372 - mse: 0.0372 - My_MSE: 6.8187 - val_loss: 0.0345 - val_mse: 0.0345 - val_My_MSE: 6.7767\n",
            "\n",
            "Epoch 00052: loss improved from 0.03690 to 0.03658, saving model to poids_train.hdf5\n",
            "Epoch 53/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0359 - mse: 0.0359 - My_MSE: 6.7985 - val_loss: 0.0341 - val_mse: 0.0341 - val_My_MSE: 6.7715\n",
            "\n",
            "Epoch 00053: loss improved from 0.03658 to 0.03623, saving model to poids_train.hdf5\n",
            "Epoch 54/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0360 - mse: 0.0360 - My_MSE: 6.8012 - val_loss: 0.0337 - val_mse: 0.0337 - val_My_MSE: 6.7653\n",
            "\n",
            "Epoch 00054: loss improved from 0.03623 to 0.03590, saving model to poids_train.hdf5\n",
            "Epoch 55/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0350 - mse: 0.0350 - My_MSE: 6.7843 - val_loss: 0.0335 - val_mse: 0.0335 - val_My_MSE: 6.7618\n",
            "\n",
            "Epoch 00055: loss improved from 0.03590 to 0.03558, saving model to poids_train.hdf5\n",
            "Epoch 56/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0349 - mse: 0.0349 - My_MSE: 6.7837 - val_loss: 0.0331 - val_mse: 0.0331 - val_My_MSE: 6.7549\n",
            "\n",
            "Epoch 00056: loss improved from 0.03558 to 0.03524, saving model to poids_train.hdf5\n",
            "Epoch 57/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0341 - mse: 0.0341 - My_MSE: 6.7700 - val_loss: 0.0327 - val_mse: 0.0327 - val_My_MSE: 6.7497\n",
            "\n",
            "Epoch 00057: loss improved from 0.03524 to 0.03490, saving model to poids_train.hdf5\n",
            "Epoch 58/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0351 - mse: 0.0351 - My_MSE: 6.7866 - val_loss: 0.0324 - val_mse: 0.0324 - val_My_MSE: 6.7447\n",
            "\n",
            "Epoch 00058: loss improved from 0.03490 to 0.03457, saving model to poids_train.hdf5\n",
            "Epoch 59/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0338 - mse: 0.0338 - My_MSE: 6.7654 - val_loss: 0.0321 - val_mse: 0.0321 - val_My_MSE: 6.7391\n",
            "\n",
            "Epoch 00059: loss improved from 0.03457 to 0.03422, saving model to poids_train.hdf5\n",
            "Epoch 60/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0337 - mse: 0.0337 - My_MSE: 6.7640 - val_loss: 0.0317 - val_mse: 0.0317 - val_My_MSE: 6.7326\n",
            "\n",
            "Epoch 00060: loss improved from 0.03422 to 0.03389, saving model to poids_train.hdf5\n",
            "Epoch 61/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0333 - mse: 0.0333 - My_MSE: 6.7589 - val_loss: 0.0314 - val_mse: 0.0314 - val_My_MSE: 6.7285\n",
            "\n",
            "Epoch 00061: loss improved from 0.03389 to 0.03355, saving model to poids_train.hdf5\n",
            "Epoch 62/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0336 - mse: 0.0336 - My_MSE: 6.7629 - val_loss: 0.0310 - val_mse: 0.0310 - val_My_MSE: 6.7232\n",
            "\n",
            "Epoch 00062: loss improved from 0.03355 to 0.03320, saving model to poids_train.hdf5\n",
            "Epoch 63/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0333 - mse: 0.0333 - My_MSE: 6.7580 - val_loss: 0.0306 - val_mse: 0.0306 - val_My_MSE: 6.7169\n",
            "\n",
            "Epoch 00063: loss improved from 0.03320 to 0.03285, saving model to poids_train.hdf5\n",
            "Epoch 64/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0323 - mse: 0.0323 - My_MSE: 6.7428 - val_loss: 0.0304 - val_mse: 0.0304 - val_My_MSE: 6.7127\n",
            "\n",
            "Epoch 00064: loss improved from 0.03285 to 0.03249, saving model to poids_train.hdf5\n",
            "Epoch 65/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0312 - mse: 0.0312 - My_MSE: 6.7259 - val_loss: 0.0299 - val_mse: 0.0299 - val_My_MSE: 6.7056\n",
            "\n",
            "Epoch 00065: loss improved from 0.03249 to 0.03213, saving model to poids_train.hdf5\n",
            "Epoch 66/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0312 - mse: 0.0312 - My_MSE: 6.7255 - val_loss: 0.0296 - val_mse: 0.0296 - val_My_MSE: 6.7005\n",
            "\n",
            "Epoch 00066: loss improved from 0.03213 to 0.03179, saving model to poids_train.hdf5\n",
            "Epoch 67/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0317 - mse: 0.0317 - My_MSE: 6.7337 - val_loss: 0.0293 - val_mse: 0.0293 - val_My_MSE: 6.6953\n",
            "\n",
            "Epoch 00067: loss improved from 0.03179 to 0.03143, saving model to poids_train.hdf5\n",
            "Epoch 68/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0313 - mse: 0.0313 - My_MSE: 6.7264 - val_loss: 0.0289 - val_mse: 0.0289 - val_My_MSE: 6.6899\n",
            "\n",
            "Epoch 00068: loss improved from 0.03143 to 0.03109, saving model to poids_train.hdf5\n",
            "Epoch 69/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0309 - mse: 0.0309 - My_MSE: 6.7208 - val_loss: 0.0286 - val_mse: 0.0286 - val_My_MSE: 6.6844\n",
            "\n",
            "Epoch 00069: loss improved from 0.03109 to 0.03074, saving model to poids_train.hdf5\n",
            "Epoch 70/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0304 - mse: 0.0304 - My_MSE: 6.7128 - val_loss: 0.0282 - val_mse: 0.0282 - val_My_MSE: 6.6792\n",
            "\n",
            "Epoch 00070: loss improved from 0.03074 to 0.03041, saving model to poids_train.hdf5\n",
            "Epoch 71/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0303 - mse: 0.0303 - My_MSE: 6.7108 - val_loss: 0.0278 - val_mse: 0.0278 - val_My_MSE: 6.6723\n",
            "\n",
            "Epoch 00071: loss improved from 0.03041 to 0.03004, saving model to poids_train.hdf5\n",
            "Epoch 72/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0296 - mse: 0.0296 - My_MSE: 6.7006 - val_loss: 0.0276 - val_mse: 0.0276 - val_My_MSE: 6.6692\n",
            "\n",
            "Epoch 00072: loss improved from 0.03004 to 0.02969, saving model to poids_train.hdf5\n",
            "Epoch 73/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0301 - mse: 0.0301 - My_MSE: 6.7086 - val_loss: 0.0271 - val_mse: 0.0271 - val_My_MSE: 6.6619\n",
            "\n",
            "Epoch 00073: loss improved from 0.02969 to 0.02933, saving model to poids_train.hdf5\n",
            "Epoch 74/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0283 - mse: 0.0283 - My_MSE: 6.6804 - val_loss: 0.0268 - val_mse: 0.0268 - val_My_MSE: 6.6572\n",
            "\n",
            "Epoch 00074: loss improved from 0.02933 to 0.02898, saving model to poids_train.hdf5\n",
            "Epoch 75/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0289 - mse: 0.0289 - My_MSE: 6.6897 - val_loss: 0.0265 - val_mse: 0.0265 - val_My_MSE: 6.6517\n",
            "\n",
            "Epoch 00075: loss improved from 0.02898 to 0.02862, saving model to poids_train.hdf5\n",
            "Epoch 76/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0290 - mse: 0.0290 - My_MSE: 6.6910 - val_loss: 0.0262 - val_mse: 0.0262 - val_My_MSE: 6.6471\n",
            "\n",
            "Epoch 00076: loss improved from 0.02862 to 0.02827, saving model to poids_train.hdf5\n",
            "Epoch 77/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0291 - mse: 0.0291 - My_MSE: 6.6920 - val_loss: 0.0258 - val_mse: 0.0258 - val_My_MSE: 6.6420\n",
            "\n",
            "Epoch 00077: loss improved from 0.02827 to 0.02792, saving model to poids_train.hdf5\n",
            "Epoch 78/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0271 - mse: 0.0271 - My_MSE: 6.6616 - val_loss: 0.0255 - val_mse: 0.0255 - val_My_MSE: 6.6365\n",
            "\n",
            "Epoch 00078: loss improved from 0.02792 to 0.02758, saving model to poids_train.hdf5\n",
            "Epoch 79/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0269 - mse: 0.0269 - My_MSE: 6.6581 - val_loss: 0.0251 - val_mse: 0.0251 - val_My_MSE: 6.6310\n",
            "\n",
            "Epoch 00079: loss improved from 0.02758 to 0.02723, saving model to poids_train.hdf5\n",
            "Epoch 80/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0258 - mse: 0.0258 - My_MSE: 6.6417 - val_loss: 0.0248 - val_mse: 0.0248 - val_My_MSE: 6.6261\n",
            "\n",
            "Epoch 00080: loss improved from 0.02723 to 0.02689, saving model to poids_train.hdf5\n",
            "Epoch 81/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0266 - mse: 0.0266 - My_MSE: 6.6534 - val_loss: 0.0245 - val_mse: 0.0245 - val_My_MSE: 6.6210\n",
            "\n",
            "Epoch 00081: loss improved from 0.02689 to 0.02656, saving model to poids_train.hdf5\n",
            "Epoch 82/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0261 - mse: 0.0261 - My_MSE: 6.6453 - val_loss: 0.0242 - val_mse: 0.0242 - val_My_MSE: 6.6158\n",
            "\n",
            "Epoch 00082: loss improved from 0.02656 to 0.02623, saving model to poids_train.hdf5\n",
            "Epoch 83/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0257 - mse: 0.0257 - My_MSE: 6.6395 - val_loss: 0.0239 - val_mse: 0.0239 - val_My_MSE: 6.6120\n",
            "\n",
            "Epoch 00083: loss improved from 0.02623 to 0.02590, saving model to poids_train.hdf5\n",
            "Epoch 84/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0251 - mse: 0.0251 - My_MSE: 6.6308 - val_loss: 0.0235 - val_mse: 0.0235 - val_My_MSE: 6.6058\n",
            "\n",
            "Epoch 00084: loss improved from 0.02590 to 0.02559, saving model to poids_train.hdf5\n",
            "Epoch 85/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0254 - mse: 0.0254 - My_MSE: 6.6357 - val_loss: 0.0232 - val_mse: 0.0232 - val_My_MSE: 6.6013\n",
            "\n",
            "Epoch 00085: loss improved from 0.02559 to 0.02527, saving model to poids_train.hdf5\n",
            "Epoch 86/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0252 - mse: 0.0252 - My_MSE: 6.6325 - val_loss: 0.0230 - val_mse: 0.0230 - val_My_MSE: 6.5969\n",
            "\n",
            "Epoch 00086: loss improved from 0.02527 to 0.02495, saving model to poids_train.hdf5\n",
            "Epoch 87/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0242 - mse: 0.0242 - My_MSE: 6.6166 - val_loss: 0.0226 - val_mse: 0.0226 - val_My_MSE: 6.5919\n",
            "\n",
            "Epoch 00087: loss improved from 0.02495 to 0.02463, saving model to poids_train.hdf5\n",
            "Epoch 88/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0243 - mse: 0.0243 - My_MSE: 6.6183 - val_loss: 0.0223 - val_mse: 0.0223 - val_My_MSE: 6.5872\n",
            "\n",
            "Epoch 00088: loss improved from 0.02463 to 0.02433, saving model to poids_train.hdf5\n",
            "Epoch 89/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0239 - mse: 0.0239 - My_MSE: 6.6114 - val_loss: 0.0220 - val_mse: 0.0220 - val_My_MSE: 6.5823\n",
            "\n",
            "Epoch 00089: loss improved from 0.02433 to 0.02401, saving model to poids_train.hdf5\n",
            "Epoch 90/1000\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0241 - mse: 0.0241 - My_MSE: 6.6155 - val_loss: 0.0217 - val_mse: 0.0217 - val_My_MSE: 6.5777\n",
            "\n",
            "Epoch 00090: loss improved from 0.02401 to 0.02371, saving model to poids_train.hdf5\n",
            "Epoch 91/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0231 - mse: 0.0231 - My_MSE: 6.5998 - val_loss: 0.0215 - val_mse: 0.0215 - val_My_MSE: 6.5734\n",
            "\n",
            "Epoch 00091: loss improved from 0.02371 to 0.02340, saving model to poids_train.hdf5\n",
            "Epoch 92/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0228 - mse: 0.0228 - My_MSE: 6.5950 - val_loss: 0.0211 - val_mse: 0.0211 - val_My_MSE: 6.5685\n",
            "\n",
            "Epoch 00092: loss improved from 0.02340 to 0.02309, saving model to poids_train.hdf5\n",
            "Epoch 93/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0232 - mse: 0.0232 - My_MSE: 6.6005 - val_loss: 0.0208 - val_mse: 0.0208 - val_My_MSE: 6.5640\n",
            "\n",
            "Epoch 00093: loss improved from 0.02309 to 0.02279, saving model to poids_train.hdf5\n",
            "Epoch 94/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0236 - mse: 0.0236 - My_MSE: 6.6072 - val_loss: 0.0206 - val_mse: 0.0206 - val_My_MSE: 6.5597\n",
            "\n",
            "Epoch 00094: loss improved from 0.02279 to 0.02249, saving model to poids_train.hdf5\n",
            "Epoch 95/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0233 - mse: 0.0233 - My_MSE: 6.6018 - val_loss: 0.0202 - val_mse: 0.0202 - val_My_MSE: 6.5544\n",
            "\n",
            "Epoch 00095: loss improved from 0.02249 to 0.02218, saving model to poids_train.hdf5\n",
            "Epoch 96/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0226 - mse: 0.0226 - My_MSE: 6.5918 - val_loss: 0.0200 - val_mse: 0.0200 - val_My_MSE: 6.5502\n",
            "\n",
            "Epoch 00096: loss improved from 0.02218 to 0.02187, saving model to poids_train.hdf5\n",
            "Epoch 97/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0210 - mse: 0.0210 - My_MSE: 6.5661 - val_loss: 0.0197 - val_mse: 0.0197 - val_My_MSE: 6.5456\n",
            "\n",
            "Epoch 00097: loss improved from 0.02187 to 0.02156, saving model to poids_train.hdf5\n",
            "Epoch 98/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0217 - mse: 0.0217 - My_MSE: 6.5771 - val_loss: 0.0194 - val_mse: 0.0194 - val_My_MSE: 6.5411\n",
            "\n",
            "Epoch 00098: loss improved from 0.02156 to 0.02126, saving model to poids_train.hdf5\n",
            "Epoch 99/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0214 - mse: 0.0214 - My_MSE: 6.5719 - val_loss: 0.0191 - val_mse: 0.0191 - val_My_MSE: 6.5363\n",
            "\n",
            "Epoch 00099: loss improved from 0.02126 to 0.02094, saving model to poids_train.hdf5\n",
            "Epoch 100/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0204 - mse: 0.0204 - My_MSE: 6.5568 - val_loss: 0.0188 - val_mse: 0.0188 - val_My_MSE: 6.5319\n",
            "\n",
            "Epoch 00100: loss improved from 0.02094 to 0.02062, saving model to poids_train.hdf5\n",
            "Epoch 101/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0200 - mse: 0.0200 - My_MSE: 6.5508 - val_loss: 0.0185 - val_mse: 0.0185 - val_My_MSE: 6.5268\n",
            "\n",
            "Epoch 00101: loss improved from 0.02062 to 0.02031, saving model to poids_train.hdf5\n",
            "Epoch 102/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0206 - mse: 0.0206 - My_MSE: 6.5599 - val_loss: 0.0182 - val_mse: 0.0182 - val_My_MSE: 6.5226\n",
            "\n",
            "Epoch 00102: loss improved from 0.02031 to 0.01999, saving model to poids_train.hdf5\n",
            "Epoch 103/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0192 - mse: 0.0192 - My_MSE: 6.5376 - val_loss: 0.0179 - val_mse: 0.0179 - val_My_MSE: 6.5174\n",
            "\n",
            "Epoch 00103: loss improved from 0.01999 to 0.01966, saving model to poids_train.hdf5\n",
            "Epoch 104/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0197 - mse: 0.0197 - My_MSE: 6.5468 - val_loss: 0.0176 - val_mse: 0.0176 - val_My_MSE: 6.5131\n",
            "\n",
            "Epoch 00104: loss improved from 0.01966 to 0.01935, saving model to poids_train.hdf5\n",
            "Epoch 105/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0194 - mse: 0.0194 - My_MSE: 6.5419 - val_loss: 0.0173 - val_mse: 0.0173 - val_My_MSE: 6.5088\n",
            "\n",
            "Epoch 00105: loss improved from 0.01935 to 0.01902, saving model to poids_train.hdf5\n",
            "Epoch 106/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0185 - mse: 0.0185 - My_MSE: 6.5275 - val_loss: 0.0170 - val_mse: 0.0170 - val_My_MSE: 6.5037\n",
            "\n",
            "Epoch 00106: loss improved from 0.01902 to 0.01870, saving model to poids_train.hdf5\n",
            "Epoch 107/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0181 - mse: 0.0181 - My_MSE: 6.5215 - val_loss: 0.0167 - val_mse: 0.0167 - val_My_MSE: 6.4996\n",
            "\n",
            "Epoch 00107: loss improved from 0.01870 to 0.01838, saving model to poids_train.hdf5\n",
            "Epoch 108/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0180 - mse: 0.0180 - My_MSE: 6.5203 - val_loss: 0.0164 - val_mse: 0.0164 - val_My_MSE: 6.4952\n",
            "\n",
            "Epoch 00108: loss improved from 0.01838 to 0.01807, saving model to poids_train.hdf5\n",
            "Epoch 109/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0171 - mse: 0.0171 - My_MSE: 6.5054 - val_loss: 0.0161 - val_mse: 0.0161 - val_My_MSE: 6.4901\n",
            "\n",
            "Epoch 00109: loss improved from 0.01807 to 0.01774, saving model to poids_train.hdf5\n",
            "Epoch 110/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0173 - mse: 0.0173 - My_MSE: 6.5085 - val_loss: 0.0159 - val_mse: 0.0159 - val_My_MSE: 6.4863\n",
            "\n",
            "Epoch 00110: loss improved from 0.01774 to 0.01743, saving model to poids_train.hdf5\n",
            "Epoch 111/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0170 - mse: 0.0170 - My_MSE: 6.5032 - val_loss: 0.0156 - val_mse: 0.0156 - val_My_MSE: 6.4818\n",
            "\n",
            "Epoch 00111: loss improved from 0.01743 to 0.01711, saving model to poids_train.hdf5\n",
            "Epoch 112/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0160 - mse: 0.0160 - My_MSE: 6.4891 - val_loss: 0.0153 - val_mse: 0.0153 - val_My_MSE: 6.4774\n",
            "\n",
            "Epoch 00112: loss improved from 0.01711 to 0.01680, saving model to poids_train.hdf5\n",
            "Epoch 113/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0159 - mse: 0.0159 - My_MSE: 6.4861 - val_loss: 0.0150 - val_mse: 0.0150 - val_My_MSE: 6.4728\n",
            "\n",
            "Epoch 00113: loss improved from 0.01680 to 0.01649, saving model to poids_train.hdf5\n",
            "Epoch 114/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0162 - mse: 0.0162 - My_MSE: 6.4921 - val_loss: 0.0148 - val_mse: 0.0148 - val_My_MSE: 6.4692\n",
            "\n",
            "Epoch 00114: loss improved from 0.01649 to 0.01618, saving model to poids_train.hdf5\n",
            "Epoch 115/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0156 - mse: 0.0156 - My_MSE: 6.4818 - val_loss: 0.0145 - val_mse: 0.0145 - val_My_MSE: 6.4645\n",
            "\n",
            "Epoch 00115: loss improved from 0.01618 to 0.01587, saving model to poids_train.hdf5\n",
            "Epoch 116/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0152 - mse: 0.0152 - My_MSE: 6.4754 - val_loss: 0.0142 - val_mse: 0.0142 - val_My_MSE: 6.4605\n",
            "\n",
            "Epoch 00116: loss improved from 0.01587 to 0.01557, saving model to poids_train.hdf5\n",
            "Epoch 117/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0152 - mse: 0.0152 - My_MSE: 6.4757 - val_loss: 0.0139 - val_mse: 0.0139 - val_My_MSE: 6.4560\n",
            "\n",
            "Epoch 00117: loss improved from 0.01557 to 0.01527, saving model to poids_train.hdf5\n",
            "Epoch 118/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0157 - mse: 0.0157 - My_MSE: 6.4841 - val_loss: 0.0137 - val_mse: 0.0137 - val_My_MSE: 6.4525\n",
            "\n",
            "Epoch 00118: loss improved from 0.01527 to 0.01497, saving model to poids_train.hdf5\n",
            "Epoch 119/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0151 - mse: 0.0151 - My_MSE: 6.4738 - val_loss: 0.0134 - val_mse: 0.0134 - val_My_MSE: 6.4479\n",
            "\n",
            "Epoch 00119: loss improved from 0.01497 to 0.01469, saving model to poids_train.hdf5\n",
            "Epoch 120/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0139 - mse: 0.0139 - My_MSE: 6.4549 - val_loss: 0.0132 - val_mse: 0.0132 - val_My_MSE: 6.4444\n",
            "\n",
            "Epoch 00120: loss improved from 0.01469 to 0.01440, saving model to poids_train.hdf5\n",
            "Epoch 121/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0144 - mse: 0.0144 - My_MSE: 6.4636 - val_loss: 0.0129 - val_mse: 0.0129 - val_My_MSE: 6.4398\n",
            "\n",
            "Epoch 00121: loss improved from 0.01440 to 0.01413, saving model to poids_train.hdf5\n",
            "Epoch 122/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0140 - mse: 0.0140 - My_MSE: 6.4573 - val_loss: 0.0127 - val_mse: 0.0127 - val_My_MSE: 6.4363\n",
            "\n",
            "Epoch 00122: loss improved from 0.01413 to 0.01386, saving model to poids_train.hdf5\n",
            "Epoch 123/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0134 - mse: 0.0134 - My_MSE: 6.4484 - val_loss: 0.0124 - val_mse: 0.0124 - val_My_MSE: 6.4326\n",
            "\n",
            "Epoch 00123: loss improved from 0.01386 to 0.01358, saving model to poids_train.hdf5\n",
            "Epoch 124/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0132 - mse: 0.0132 - My_MSE: 6.4445 - val_loss: 0.0122 - val_mse: 0.0122 - val_My_MSE: 6.4290\n",
            "\n",
            "Epoch 00124: loss improved from 0.01358 to 0.01331, saving model to poids_train.hdf5\n",
            "Epoch 125/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0126 - mse: 0.0126 - My_MSE: 6.4360 - val_loss: 0.0120 - val_mse: 0.0120 - val_My_MSE: 6.4254\n",
            "\n",
            "Epoch 00125: loss improved from 0.01331 to 0.01305, saving model to poids_train.hdf5\n",
            "Epoch 126/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0127 - mse: 0.0127 - My_MSE: 6.4368 - val_loss: 0.0117 - val_mse: 0.0117 - val_My_MSE: 6.4216\n",
            "\n",
            "Epoch 00126: loss improved from 0.01305 to 0.01279, saving model to poids_train.hdf5\n",
            "Epoch 127/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0126 - mse: 0.0126 - My_MSE: 6.4348 - val_loss: 0.0115 - val_mse: 0.0115 - val_My_MSE: 6.4179\n",
            "\n",
            "Epoch 00127: loss improved from 0.01279 to 0.01254, saving model to poids_train.hdf5\n",
            "Epoch 128/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0121 - mse: 0.0121 - My_MSE: 6.4278 - val_loss: 0.0113 - val_mse: 0.0113 - val_My_MSE: 6.4147\n",
            "\n",
            "Epoch 00128: loss improved from 0.01254 to 0.01228, saving model to poids_train.hdf5\n",
            "Epoch 129/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0120 - mse: 0.0120 - My_MSE: 6.4253 - val_loss: 0.0111 - val_mse: 0.0111 - val_My_MSE: 6.4118\n",
            "\n",
            "Epoch 00129: loss improved from 0.01228 to 0.01204, saving model to poids_train.hdf5\n",
            "Epoch 130/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0119 - mse: 0.0119 - My_MSE: 6.4243 - val_loss: 0.0109 - val_mse: 0.0109 - val_My_MSE: 6.4083\n",
            "\n",
            "Epoch 00130: loss improved from 0.01204 to 0.01180, saving model to poids_train.hdf5\n",
            "Epoch 131/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0119 - mse: 0.0119 - My_MSE: 6.4236 - val_loss: 0.0106 - val_mse: 0.0106 - val_My_MSE: 6.4046\n",
            "\n",
            "Epoch 00131: loss improved from 0.01180 to 0.01155, saving model to poids_train.hdf5\n",
            "Epoch 132/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0116 - mse: 0.0116 - My_MSE: 6.4190 - val_loss: 0.0105 - val_mse: 0.0105 - val_My_MSE: 6.4018\n",
            "\n",
            "Epoch 00132: loss improved from 0.01155 to 0.01131, saving model to poids_train.hdf5\n",
            "Epoch 133/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0114 - mse: 0.0114 - My_MSE: 6.4162 - val_loss: 0.0102 - val_mse: 0.0102 - val_My_MSE: 6.3984\n",
            "\n",
            "Epoch 00133: loss improved from 0.01131 to 0.01107, saving model to poids_train.hdf5\n",
            "Epoch 134/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0110 - mse: 0.0110 - My_MSE: 6.4109 - val_loss: 0.0101 - val_mse: 0.0101 - val_My_MSE: 6.3955\n",
            "\n",
            "Epoch 00134: loss improved from 0.01107 to 0.01085, saving model to poids_train.hdf5\n",
            "Epoch 135/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0105 - mse: 0.0105 - My_MSE: 6.4026 - val_loss: 0.0099 - val_mse: 0.0099 - val_My_MSE: 6.3932\n",
            "\n",
            "Epoch 00135: loss improved from 0.01085 to 0.01062, saving model to poids_train.hdf5\n",
            "Epoch 136/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0100 - mse: 0.0100 - My_MSE: 6.3949 - val_loss: 0.0097 - val_mse: 0.0097 - val_My_MSE: 6.3896\n",
            "\n",
            "Epoch 00136: loss improved from 0.01062 to 0.01040, saving model to poids_train.hdf5\n",
            "Epoch 137/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0106 - mse: 0.0106 - My_MSE: 6.4037 - val_loss: 0.0095 - val_mse: 0.0095 - val_My_MSE: 6.3866\n",
            "\n",
            "Epoch 00137: loss improved from 0.01040 to 0.01017, saving model to poids_train.hdf5\n",
            "Epoch 138/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0101 - mse: 0.0101 - My_MSE: 6.3968 - val_loss: 0.0093 - val_mse: 0.0093 - val_My_MSE: 6.3837\n",
            "\n",
            "Epoch 00138: loss improved from 0.01017 to 0.00996, saving model to poids_train.hdf5\n",
            "Epoch 139/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0099 - mse: 0.0099 - My_MSE: 6.3929 - val_loss: 0.0091 - val_mse: 0.0091 - val_My_MSE: 6.3811\n",
            "\n",
            "Epoch 00139: loss improved from 0.00996 to 0.00973, saving model to poids_train.hdf5\n",
            "Epoch 140/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0094 - mse: 0.0094 - My_MSE: 6.3850 - val_loss: 0.0090 - val_mse: 0.0090 - val_My_MSE: 6.3786\n",
            "\n",
            "Epoch 00140: loss improved from 0.00973 to 0.00951, saving model to poids_train.hdf5\n",
            "Epoch 141/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0090 - mse: 0.0090 - My_MSE: 6.3795 - val_loss: 0.0088 - val_mse: 0.0088 - val_My_MSE: 6.3755\n",
            "\n",
            "Epoch 00141: loss improved from 0.00951 to 0.00930, saving model to poids_train.hdf5\n",
            "Epoch 142/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0091 - mse: 0.0091 - My_MSE: 6.3806 - val_loss: 0.0086 - val_mse: 0.0086 - val_My_MSE: 6.3733\n",
            "\n",
            "Epoch 00142: loss improved from 0.00930 to 0.00908, saving model to poids_train.hdf5\n",
            "Epoch 143/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0094 - mse: 0.0094 - My_MSE: 6.3849 - val_loss: 0.0085 - val_mse: 0.0085 - val_My_MSE: 6.3705\n",
            "\n",
            "Epoch 00143: loss improved from 0.00908 to 0.00886, saving model to poids_train.hdf5\n",
            "Epoch 144/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0088 - mse: 0.0088 - My_MSE: 6.3766 - val_loss: 0.0083 - val_mse: 0.0083 - val_My_MSE: 6.3674\n",
            "\n",
            "Epoch 00144: loss improved from 0.00886 to 0.00865, saving model to poids_train.hdf5\n",
            "Epoch 145/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0085 - mse: 0.0085 - My_MSE: 6.3708 - val_loss: 0.0081 - val_mse: 0.0081 - val_My_MSE: 6.3655\n",
            "\n",
            "Epoch 00145: loss improved from 0.00865 to 0.00845, saving model to poids_train.hdf5\n",
            "Epoch 146/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0080 - mse: 0.0080 - My_MSE: 6.3632 - val_loss: 0.0081 - val_mse: 0.0081 - val_My_MSE: 6.3647\n",
            "\n",
            "Epoch 00146: loss improved from 0.00845 to 0.00824, saving model to poids_train.hdf5\n",
            "Epoch 147/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0082 - mse: 0.0082 - My_MSE: 6.3666 - val_loss: 0.0079 - val_mse: 0.0079 - val_My_MSE: 6.3625\n",
            "\n",
            "Epoch 00147: loss improved from 0.00824 to 0.00805, saving model to poids_train.hdf5\n",
            "Epoch 148/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0078 - mse: 0.0078 - My_MSE: 6.3602 - val_loss: 0.0078 - val_mse: 0.0078 - val_My_MSE: 6.3600\n",
            "\n",
            "Epoch 00148: loss improved from 0.00805 to 0.00787, saving model to poids_train.hdf5\n",
            "Epoch 149/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0077 - mse: 0.0077 - My_MSE: 6.3584 - val_loss: 0.0077 - val_mse: 0.0077 - val_My_MSE: 6.3586\n",
            "\n",
            "Epoch 00149: loss improved from 0.00787 to 0.00769, saving model to poids_train.hdf5\n",
            "Epoch 150/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0074 - mse: 0.0074 - My_MSE: 6.3536 - val_loss: 0.0076 - val_mse: 0.0076 - val_My_MSE: 6.3566\n",
            "\n",
            "Epoch 00150: loss improved from 0.00769 to 0.00753, saving model to poids_train.hdf5\n",
            "Epoch 151/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0071 - mse: 0.0071 - My_MSE: 6.3491 - val_loss: 0.0073 - val_mse: 0.0073 - val_My_MSE: 6.3530\n",
            "\n",
            "Epoch 00151: loss improved from 0.00753 to 0.00739, saving model to poids_train.hdf5\n",
            "Epoch 152/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0070 - mse: 0.0070 - My_MSE: 6.3486 - val_loss: 0.0073 - val_mse: 0.0073 - val_My_MSE: 6.3531\n",
            "\n",
            "Epoch 00152: loss improved from 0.00739 to 0.00720, saving model to poids_train.hdf5\n",
            "Epoch 153/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0066 - mse: 0.0066 - My_MSE: 6.3424 - val_loss: 0.0072 - val_mse: 0.0072 - val_My_MSE: 6.3510\n",
            "\n",
            "Epoch 00153: loss improved from 0.00720 to 0.00705, saving model to poids_train.hdf5\n",
            "Epoch 154/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0071 - mse: 0.0071 - My_MSE: 6.3491 - val_loss: 0.0071 - val_mse: 0.0071 - val_My_MSE: 6.3493\n",
            "\n",
            "Epoch 00154: loss improved from 0.00705 to 0.00692, saving model to poids_train.hdf5\n",
            "Epoch 155/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0063 - mse: 0.0063 - My_MSE: 6.3377 - val_loss: 0.0070 - val_mse: 0.0070 - val_My_MSE: 6.3484\n",
            "\n",
            "Epoch 00155: loss improved from 0.00692 to 0.00675, saving model to poids_train.hdf5\n",
            "Epoch 156/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0069 - mse: 0.0069 - My_MSE: 6.3459 - val_loss: 0.0069 - val_mse: 0.0069 - val_My_MSE: 6.3459\n",
            "\n",
            "Epoch 00156: loss improved from 0.00675 to 0.00661, saving model to poids_train.hdf5\n",
            "Epoch 157/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0066 - mse: 0.0066 - My_MSE: 6.3421 - val_loss: 0.0067 - val_mse: 0.0067 - val_My_MSE: 6.3437\n",
            "\n",
            "Epoch 00157: loss improved from 0.00661 to 0.00646, saving model to poids_train.hdf5\n",
            "Epoch 158/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0063 - mse: 0.0063 - My_MSE: 6.3374 - val_loss: 0.0067 - val_mse: 0.0067 - val_My_MSE: 6.3431\n",
            "\n",
            "Epoch 00158: loss improved from 0.00646 to 0.00633, saving model to poids_train.hdf5\n",
            "Epoch 159/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0061 - mse: 0.0061 - My_MSE: 6.3333 - val_loss: 0.0067 - val_mse: 0.0067 - val_My_MSE: 6.3429\n",
            "\n",
            "Epoch 00159: loss improved from 0.00633 to 0.00620, saving model to poids_train.hdf5\n",
            "Epoch 160/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0060 - mse: 0.0060 - My_MSE: 6.3321 - val_loss: 0.0064 - val_mse: 0.0064 - val_My_MSE: 6.3389\n",
            "\n",
            "Epoch 00160: loss improved from 0.00620 to 0.00608, saving model to poids_train.hdf5\n",
            "Epoch 161/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0063 - mse: 0.0063 - My_MSE: 6.3368 - val_loss: 0.0064 - val_mse: 0.0064 - val_My_MSE: 6.3386\n",
            "\n",
            "Epoch 00161: loss improved from 0.00608 to 0.00595, saving model to poids_train.hdf5\n",
            "Epoch 162/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0059 - mse: 0.0059 - My_MSE: 6.3312 - val_loss: 0.0063 - val_mse: 0.0063 - val_My_MSE: 6.3365\n",
            "\n",
            "Epoch 00162: loss improved from 0.00595 to 0.00585, saving model to poids_train.hdf5\n",
            "Epoch 163/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0060 - mse: 0.0060 - My_MSE: 6.3326 - val_loss: 0.0061 - val_mse: 0.0061 - val_My_MSE: 6.3342\n",
            "\n",
            "Epoch 00163: loss improved from 0.00585 to 0.00574, saving model to poids_train.hdf5\n",
            "Epoch 164/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0056 - mse: 0.0056 - My_MSE: 6.3253 - val_loss: 0.0060 - val_mse: 0.0060 - val_My_MSE: 6.3330\n",
            "\n",
            "Epoch 00164: loss improved from 0.00574 to 0.00561, saving model to poids_train.hdf5\n",
            "Epoch 165/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0054 - mse: 0.0054 - My_MSE: 6.3228 - val_loss: 0.0060 - val_mse: 0.0060 - val_My_MSE: 6.3323\n",
            "\n",
            "Epoch 00165: loss improved from 0.00561 to 0.00550, saving model to poids_train.hdf5\n",
            "Epoch 166/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0056 - mse: 0.0056 - My_MSE: 6.3259 - val_loss: 0.0059 - val_mse: 0.0059 - val_My_MSE: 6.3315\n",
            "\n",
            "Epoch 00166: loss improved from 0.00550 to 0.00540, saving model to poids_train.hdf5\n",
            "Epoch 167/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0053 - mse: 0.0053 - My_MSE: 6.3219 - val_loss: 0.0059 - val_mse: 0.0059 - val_My_MSE: 6.3303\n",
            "\n",
            "Epoch 00167: loss improved from 0.00540 to 0.00531, saving model to poids_train.hdf5\n",
            "Epoch 168/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0056 - mse: 0.0056 - My_MSE: 6.3254 - val_loss: 0.0057 - val_mse: 0.0057 - val_My_MSE: 6.3277\n",
            "\n",
            "Epoch 00168: loss improved from 0.00531 to 0.00521, saving model to poids_train.hdf5\n",
            "Epoch 169/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0051 - mse: 0.0051 - My_MSE: 6.3188 - val_loss: 0.0056 - val_mse: 0.0056 - val_My_MSE: 6.3263\n",
            "\n",
            "Epoch 00169: loss improved from 0.00521 to 0.00511, saving model to poids_train.hdf5\n",
            "Epoch 170/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0050 - mse: 0.0050 - My_MSE: 6.3167 - val_loss: 0.0056 - val_mse: 0.0056 - val_My_MSE: 6.3255\n",
            "\n",
            "Epoch 00170: loss improved from 0.00511 to 0.00504, saving model to poids_train.hdf5\n",
            "Epoch 171/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0051 - mse: 0.0051 - My_MSE: 6.3180 - val_loss: 0.0056 - val_mse: 0.0056 - val_My_MSE: 6.3258\n",
            "\n",
            "Epoch 00171: loss improved from 0.00504 to 0.00495, saving model to poids_train.hdf5\n",
            "Epoch 172/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0050 - mse: 0.0050 - My_MSE: 6.3170 - val_loss: 0.0056 - val_mse: 0.0056 - val_My_MSE: 6.3254\n",
            "\n",
            "Epoch 00172: loss improved from 0.00495 to 0.00486, saving model to poids_train.hdf5\n",
            "Epoch 173/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0047 - mse: 0.0047 - My_MSE: 6.3119 - val_loss: 0.0054 - val_mse: 0.0054 - val_My_MSE: 6.3235\n",
            "\n",
            "Epoch 00173: loss improved from 0.00486 to 0.00475, saving model to poids_train.hdf5\n",
            "Epoch 174/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0047 - mse: 0.0047 - My_MSE: 6.3119 - val_loss: 0.0053 - val_mse: 0.0053 - val_My_MSE: 6.3206\n",
            "\n",
            "Epoch 00174: loss improved from 0.00475 to 0.00467, saving model to poids_train.hdf5\n",
            "Epoch 175/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0047 - mse: 0.0047 - My_MSE: 6.3123 - val_loss: 0.0051 - val_mse: 0.0051 - val_My_MSE: 6.3184\n",
            "\n",
            "Epoch 00175: loss improved from 0.00467 to 0.00458, saving model to poids_train.hdf5\n",
            "Epoch 176/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0044 - mse: 0.0044 - My_MSE: 6.3077 - val_loss: 0.0050 - val_mse: 0.0050 - val_My_MSE: 6.3168\n",
            "\n",
            "Epoch 00176: loss improved from 0.00458 to 0.00450, saving model to poids_train.hdf5\n",
            "Epoch 177/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0042 - mse: 0.0042 - My_MSE: 6.3036 - val_loss: 0.0051 - val_mse: 0.0051 - val_My_MSE: 6.3177\n",
            "\n",
            "Epoch 00177: loss improved from 0.00450 to 0.00444, saving model to poids_train.hdf5\n",
            "Epoch 178/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0044 - mse: 0.0044 - My_MSE: 6.3070 - val_loss: 0.0049 - val_mse: 0.0049 - val_My_MSE: 6.3145\n",
            "\n",
            "Epoch 00178: loss improved from 0.00444 to 0.00435, saving model to poids_train.hdf5\n",
            "Epoch 179/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0043 - mse: 0.0043 - My_MSE: 6.3056 - val_loss: 0.0048 - val_mse: 0.0048 - val_My_MSE: 6.3142\n",
            "\n",
            "Epoch 00179: loss improved from 0.00435 to 0.00428, saving model to poids_train.hdf5\n",
            "Epoch 180/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0044 - mse: 0.0044 - My_MSE: 6.3066 - val_loss: 0.0047 - val_mse: 0.0047 - val_My_MSE: 6.3126\n",
            "\n",
            "Epoch 00180: loss improved from 0.00428 to 0.00420, saving model to poids_train.hdf5\n",
            "Epoch 181/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0040 - mse: 0.0040 - My_MSE: 6.3015 - val_loss: 0.0046 - val_mse: 0.0046 - val_My_MSE: 6.3106\n",
            "\n",
            "Epoch 00181: loss improved from 0.00420 to 0.00413, saving model to poids_train.hdf5\n",
            "Epoch 182/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0041 - mse: 0.0041 - My_MSE: 6.3024 - val_loss: 0.0045 - val_mse: 0.0045 - val_My_MSE: 6.3096\n",
            "\n",
            "Epoch 00182: loss improved from 0.00413 to 0.00407, saving model to poids_train.hdf5\n",
            "Epoch 183/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0043 - mse: 0.0043 - My_MSE: 6.3056 - val_loss: 0.0045 - val_mse: 0.0045 - val_My_MSE: 6.3095\n",
            "\n",
            "Epoch 00183: loss improved from 0.00407 to 0.00399, saving model to poids_train.hdf5\n",
            "Epoch 184/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0042 - mse: 0.0042 - My_MSE: 6.3038 - val_loss: 0.0044 - val_mse: 0.0044 - val_My_MSE: 6.3076\n",
            "\n",
            "Epoch 00184: loss improved from 0.00399 to 0.00393, saving model to poids_train.hdf5\n",
            "Epoch 185/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0040 - mse: 0.0040 - My_MSE: 6.3004 - val_loss: 0.0044 - val_mse: 0.0044 - val_My_MSE: 6.3067\n",
            "\n",
            "Epoch 00185: loss improved from 0.00393 to 0.00387, saving model to poids_train.hdf5\n",
            "Epoch 186/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0036 - mse: 0.0036 - My_MSE: 6.2949 - val_loss: 0.0043 - val_mse: 0.0043 - val_My_MSE: 6.3061\n",
            "\n",
            "Epoch 00186: loss improved from 0.00387 to 0.00380, saving model to poids_train.hdf5\n",
            "Epoch 187/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0039 - mse: 0.0039 - My_MSE: 6.2999 - val_loss: 0.0042 - val_mse: 0.0042 - val_My_MSE: 6.3044\n",
            "\n",
            "Epoch 00187: loss improved from 0.00380 to 0.00375, saving model to poids_train.hdf5\n",
            "Epoch 188/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0037 - mse: 0.0037 - My_MSE: 6.2957 - val_loss: 0.0042 - val_mse: 0.0042 - val_My_MSE: 6.3043\n",
            "\n",
            "Epoch 00188: loss improved from 0.00375 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 189/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0035 - mse: 0.0035 - My_MSE: 6.2939 - val_loss: 0.0041 - val_mse: 0.0041 - val_My_MSE: 6.3027\n",
            "\n",
            "Epoch 00189: loss improved from 0.00369 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 190/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0035 - mse: 0.0035 - My_MSE: 6.2931 - val_loss: 0.0041 - val_mse: 0.0041 - val_My_MSE: 6.3019\n",
            "\n",
            "Epoch 00190: loss improved from 0.00363 to 0.00357, saving model to poids_train.hdf5\n",
            "Epoch 191/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0035 - mse: 0.0035 - My_MSE: 6.2939 - val_loss: 0.0040 - val_mse: 0.0040 - val_My_MSE: 6.3014\n",
            "\n",
            "Epoch 00191: loss improved from 0.00357 to 0.00353, saving model to poids_train.hdf5\n",
            "Epoch 192/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0035 - mse: 0.0035 - My_MSE: 6.2935 - val_loss: 0.0041 - val_mse: 0.0041 - val_My_MSE: 6.3023\n",
            "\n",
            "Epoch 00192: loss improved from 0.00353 to 0.00348, saving model to poids_train.hdf5\n",
            "Epoch 193/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0033 - mse: 0.0033 - My_MSE: 6.2898 - val_loss: 0.0038 - val_mse: 0.0038 - val_My_MSE: 6.2986\n",
            "\n",
            "Epoch 00193: loss improved from 0.00348 to 0.00340, saving model to poids_train.hdf5\n",
            "Epoch 194/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0033 - mse: 0.0033 - My_MSE: 6.2905 - val_loss: 0.0038 - val_mse: 0.0038 - val_My_MSE: 6.2987\n",
            "\n",
            "Epoch 00194: loss improved from 0.00340 to 0.00335, saving model to poids_train.hdf5\n",
            "Epoch 195/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0032 - mse: 0.0032 - My_MSE: 6.2890 - val_loss: 0.0038 - val_mse: 0.0038 - val_My_MSE: 6.2978\n",
            "\n",
            "Epoch 00195: loss improved from 0.00335 to 0.00329, saving model to poids_train.hdf5\n",
            "Epoch 196/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0033 - mse: 0.0033 - My_MSE: 6.2894 - val_loss: 0.0037 - val_mse: 0.0037 - val_My_MSE: 6.2969\n",
            "\n",
            "Epoch 00196: loss improved from 0.00329 to 0.00325, saving model to poids_train.hdf5\n",
            "Epoch 197/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0033 - mse: 0.0033 - My_MSE: 6.2898 - val_loss: 0.0036 - val_mse: 0.0036 - val_My_MSE: 6.2947\n",
            "\n",
            "Epoch 00197: loss improved from 0.00325 to 0.00320, saving model to poids_train.hdf5\n",
            "Epoch 198/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0032 - mse: 0.0032 - My_MSE: 6.2882 - val_loss: 0.0036 - val_mse: 0.0036 - val_My_MSE: 6.2944\n",
            "\n",
            "Epoch 00198: loss improved from 0.00320 to 0.00315, saving model to poids_train.hdf5\n",
            "Epoch 199/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0032 - mse: 0.0032 - My_MSE: 6.2890 - val_loss: 0.0035 - val_mse: 0.0035 - val_My_MSE: 6.2937\n",
            "\n",
            "Epoch 00199: loss improved from 0.00315 to 0.00310, saving model to poids_train.hdf5\n",
            "Epoch 200/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0033 - mse: 0.0033 - My_MSE: 6.2909 - val_loss: 0.0034 - val_mse: 0.0034 - val_My_MSE: 6.2922\n",
            "\n",
            "Epoch 00200: loss improved from 0.00310 to 0.00306, saving model to poids_train.hdf5\n",
            "Epoch 201/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0031 - mse: 0.0031 - My_MSE: 6.2877 - val_loss: 0.0034 - val_mse: 0.0034 - val_My_MSE: 6.2921\n",
            "\n",
            "Epoch 00201: loss improved from 0.00306 to 0.00302, saving model to poids_train.hdf5\n",
            "Epoch 202/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0030 - mse: 0.0030 - My_MSE: 6.2859 - val_loss: 0.0034 - val_mse: 0.0034 - val_My_MSE: 6.2914\n",
            "\n",
            "Epoch 00202: loss improved from 0.00302 to 0.00298, saving model to poids_train.hdf5\n",
            "Epoch 203/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0030 - mse: 0.0030 - My_MSE: 6.2853 - val_loss: 0.0033 - val_mse: 0.0033 - val_My_MSE: 6.2902\n",
            "\n",
            "Epoch 00203: loss improved from 0.00298 to 0.00294, saving model to poids_train.hdf5\n",
            "Epoch 204/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0028 - mse: 0.0028 - My_MSE: 6.2821 - val_loss: 0.0033 - val_mse: 0.0033 - val_My_MSE: 6.2896\n",
            "\n",
            "Epoch 00204: loss improved from 0.00294 to 0.00289, saving model to poids_train.hdf5\n",
            "Epoch 205/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0029 - mse: 0.0029 - My_MSE: 6.2838 - val_loss: 0.0032 - val_mse: 0.0032 - val_My_MSE: 6.2887\n",
            "\n",
            "Epoch 00205: loss improved from 0.00289 to 0.00286, saving model to poids_train.hdf5\n",
            "Epoch 206/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0027 - mse: 0.0027 - My_MSE: 6.2808 - val_loss: 0.0032 - val_mse: 0.0032 - val_My_MSE: 6.2886\n",
            "\n",
            "Epoch 00206: loss improved from 0.00286 to 0.00282, saving model to poids_train.hdf5\n",
            "Epoch 207/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0027 - mse: 0.0027 - My_MSE: 6.2807 - val_loss: 0.0031 - val_mse: 0.0031 - val_My_MSE: 6.2872\n",
            "\n",
            "Epoch 00207: loss improved from 0.00282 to 0.00276, saving model to poids_train.hdf5\n",
            "Epoch 208/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0027 - mse: 0.0027 - My_MSE: 6.2804 - val_loss: 0.0031 - val_mse: 0.0031 - val_My_MSE: 6.2864\n",
            "\n",
            "Epoch 00208: loss improved from 0.00276 to 0.00273, saving model to poids_train.hdf5\n",
            "Epoch 209/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0029 - mse: 0.0029 - My_MSE: 6.2840 - val_loss: 0.0031 - val_mse: 0.0031 - val_My_MSE: 6.2870\n",
            "\n",
            "Epoch 00209: loss improved from 0.00273 to 0.00269, saving model to poids_train.hdf5\n",
            "Epoch 210/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0028 - mse: 0.0028 - My_MSE: 6.2825 - val_loss: 0.0031 - val_mse: 0.0031 - val_My_MSE: 6.2878\n",
            "\n",
            "Epoch 00210: loss improved from 0.00269 to 0.00267, saving model to poids_train.hdf5\n",
            "Epoch 211/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0028 - mse: 0.0028 - My_MSE: 6.2819 - val_loss: 0.0030 - val_mse: 0.0030 - val_My_MSE: 6.2851\n",
            "\n",
            "Epoch 00211: loss improved from 0.00267 to 0.00265, saving model to poids_train.hdf5\n",
            "Epoch 212/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0026 - mse: 0.0026 - My_MSE: 6.2792 - val_loss: 0.0029 - val_mse: 0.0029 - val_My_MSE: 6.2839\n",
            "\n",
            "Epoch 00212: loss improved from 0.00265 to 0.00262, saving model to poids_train.hdf5\n",
            "Epoch 213/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0026 - mse: 0.0026 - My_MSE: 6.2796 - val_loss: 0.0029 - val_mse: 0.0029 - val_My_MSE: 6.2835\n",
            "\n",
            "Epoch 00213: loss improved from 0.00262 to 0.00255, saving model to poids_train.hdf5\n",
            "Epoch 214/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0027 - mse: 0.0027 - My_MSE: 6.2801 - val_loss: 0.0028 - val_mse: 0.0028 - val_My_MSE: 6.2827\n",
            "\n",
            "Epoch 00214: loss improved from 0.00255 to 0.00253, saving model to poids_train.hdf5\n",
            "Epoch 215/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0026 - mse: 0.0026 - My_MSE: 6.2793 - val_loss: 0.0028 - val_mse: 0.0028 - val_My_MSE: 6.2827\n",
            "\n",
            "Epoch 00215: loss improved from 0.00253 to 0.00249, saving model to poids_train.hdf5\n",
            "Epoch 216/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0025 - mse: 0.0025 - My_MSE: 6.2774 - val_loss: 0.0028 - val_mse: 0.0028 - val_My_MSE: 6.2826\n",
            "\n",
            "Epoch 00216: loss improved from 0.00249 to 0.00245, saving model to poids_train.hdf5\n",
            "Epoch 217/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0024 - mse: 0.0024 - My_MSE: 6.2755 - val_loss: 0.0028 - val_mse: 0.0028 - val_My_MSE: 6.2824\n",
            "\n",
            "Epoch 00217: loss improved from 0.00245 to 0.00242, saving model to poids_train.hdf5\n",
            "Epoch 218/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0024 - mse: 0.0024 - My_MSE: 6.2762 - val_loss: 0.0027 - val_mse: 0.0027 - val_My_MSE: 6.2807\n",
            "\n",
            "Epoch 00218: loss improved from 0.00242 to 0.00241, saving model to poids_train.hdf5\n",
            "Epoch 219/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0024 - mse: 0.0024 - My_MSE: 6.2768 - val_loss: 0.0026 - val_mse: 0.0026 - val_My_MSE: 6.2799\n",
            "\n",
            "Epoch 00219: loss improved from 0.00241 to 0.00239, saving model to poids_train.hdf5\n",
            "Epoch 220/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0024 - mse: 0.0024 - My_MSE: 6.2762 - val_loss: 0.0026 - val_mse: 0.0026 - val_My_MSE: 6.2793\n",
            "\n",
            "Epoch 00220: loss improved from 0.00239 to 0.00234, saving model to poids_train.hdf5\n",
            "Epoch 221/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0023 - mse: 0.0023 - My_MSE: 6.2740 - val_loss: 0.0026 - val_mse: 0.0026 - val_My_MSE: 6.2789\n",
            "\n",
            "Epoch 00221: loss improved from 0.00234 to 0.00230, saving model to poids_train.hdf5\n",
            "Epoch 222/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0021 - mse: 0.0021 - My_MSE: 6.2718 - val_loss: 0.0026 - val_mse: 0.0026 - val_My_MSE: 6.2785\n",
            "\n",
            "Epoch 00222: loss improved from 0.00230 to 0.00227, saving model to poids_train.hdf5\n",
            "Epoch 223/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0023 - mse: 0.0023 - My_MSE: 6.2740 - val_loss: 0.0026 - val_mse: 0.0026 - val_My_MSE: 6.2784\n",
            "\n",
            "Epoch 00223: loss improved from 0.00227 to 0.00224, saving model to poids_train.hdf5\n",
            "Epoch 224/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0022 - mse: 0.0022 - My_MSE: 6.2732 - val_loss: 0.0025 - val_mse: 0.0025 - val_My_MSE: 6.2781\n",
            "\n",
            "Epoch 00224: loss improved from 0.00224 to 0.00222, saving model to poids_train.hdf5\n",
            "Epoch 225/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0022 - mse: 0.0022 - My_MSE: 6.2727 - val_loss: 0.0025 - val_mse: 0.0025 - val_My_MSE: 6.2770\n",
            "\n",
            "Epoch 00225: loss improved from 0.00222 to 0.00219, saving model to poids_train.hdf5\n",
            "Epoch 226/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0022 - mse: 0.0022 - My_MSE: 6.2725 - val_loss: 0.0024 - val_mse: 0.0024 - val_My_MSE: 6.2768\n",
            "\n",
            "Epoch 00226: loss improved from 0.00219 to 0.00216, saving model to poids_train.hdf5\n",
            "Epoch 227/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0021 - mse: 0.0021 - My_MSE: 6.2719 - val_loss: 0.0024 - val_mse: 0.0024 - val_My_MSE: 6.2765\n",
            "\n",
            "Epoch 00227: loss improved from 0.00216 to 0.00214, saving model to poids_train.hdf5\n",
            "Epoch 228/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0021 - mse: 0.0021 - My_MSE: 6.2712 - val_loss: 0.0024 - val_mse: 0.0024 - val_My_MSE: 6.2767\n",
            "\n",
            "Epoch 00228: loss improved from 0.00214 to 0.00212, saving model to poids_train.hdf5\n",
            "Epoch 229/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0021 - mse: 0.0021 - My_MSE: 6.2718 - val_loss: 0.0023 - val_mse: 0.0023 - val_My_MSE: 6.2752\n",
            "\n",
            "Epoch 00229: loss improved from 0.00212 to 0.00211, saving model to poids_train.hdf5\n",
            "Epoch 230/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0021 - mse: 0.0021 - My_MSE: 6.2709 - val_loss: 0.0023 - val_mse: 0.0023 - val_My_MSE: 6.2752\n",
            "\n",
            "Epoch 00230: loss improved from 0.00211 to 0.00208, saving model to poids_train.hdf5\n",
            "Epoch 231/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0020 - mse: 0.0020 - My_MSE: 6.2696 - val_loss: 0.0023 - val_mse: 0.0023 - val_My_MSE: 6.2743\n",
            "\n",
            "Epoch 00231: loss improved from 0.00208 to 0.00206, saving model to poids_train.hdf5\n",
            "Epoch 232/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0022 - mse: 0.0022 - My_MSE: 6.2730 - val_loss: 0.0023 - val_mse: 0.0023 - val_My_MSE: 6.2746\n",
            "\n",
            "Epoch 00232: loss improved from 0.00206 to 0.00204, saving model to poids_train.hdf5\n",
            "Epoch 233/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0020 - mse: 0.0020 - My_MSE: 6.2704 - val_loss: 0.0023 - val_mse: 0.0023 - val_My_MSE: 6.2740\n",
            "\n",
            "Epoch 00233: loss improved from 0.00204 to 0.00202, saving model to poids_train.hdf5\n",
            "Epoch 234/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0019 - mse: 0.0019 - My_MSE: 6.2682 - val_loss: 0.0022 - val_mse: 0.0022 - val_My_MSE: 6.2730\n",
            "\n",
            "Epoch 00234: loss improved from 0.00202 to 0.00200, saving model to poids_train.hdf5\n",
            "Epoch 235/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0019 - mse: 0.0019 - My_MSE: 6.2678 - val_loss: 0.0022 - val_mse: 0.0022 - val_My_MSE: 6.2728\n",
            "\n",
            "Epoch 00235: loss improved from 0.00200 to 0.00196, saving model to poids_train.hdf5\n",
            "Epoch 236/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0019 - mse: 0.0019 - My_MSE: 6.2679 - val_loss: 0.0022 - val_mse: 0.0022 - val_My_MSE: 6.2725\n",
            "\n",
            "Epoch 00236: loss improved from 0.00196 to 0.00194, saving model to poids_train.hdf5\n",
            "Epoch 237/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0019 - mse: 0.0019 - My_MSE: 6.2675 - val_loss: 0.0022 - val_mse: 0.0022 - val_My_MSE: 6.2729\n",
            "\n",
            "Epoch 00237: loss improved from 0.00194 to 0.00194, saving model to poids_train.hdf5\n",
            "Epoch 238/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0019 - mse: 0.0019 - My_MSE: 6.2687 - val_loss: 0.0021 - val_mse: 0.0021 - val_My_MSE: 6.2714\n",
            "\n",
            "Epoch 00238: loss improved from 0.00194 to 0.00191, saving model to poids_train.hdf5\n",
            "Epoch 239/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0019 - mse: 0.0019 - My_MSE: 6.2682 - val_loss: 0.0022 - val_mse: 0.0022 - val_My_MSE: 6.2723\n",
            "\n",
            "Epoch 00239: loss improved from 0.00191 to 0.00190, saving model to poids_train.hdf5\n",
            "Epoch 240/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0020 - mse: 0.0020 - My_MSE: 6.2693 - val_loss: 0.0021 - val_mse: 0.0021 - val_My_MSE: 6.2718\n",
            "\n",
            "Epoch 00240: loss improved from 0.00190 to 0.00187, saving model to poids_train.hdf5\n",
            "Epoch 241/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0018 - mse: 0.0018 - My_MSE: 6.2661 - val_loss: 0.0022 - val_mse: 0.0022 - val_My_MSE: 6.2727\n",
            "\n",
            "Epoch 00241: loss improved from 0.00187 to 0.00185, saving model to poids_train.hdf5\n",
            "Epoch 242/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0020 - mse: 0.0020 - My_MSE: 6.2696 - val_loss: 0.0020 - val_mse: 0.0020 - val_My_MSE: 6.2701\n",
            "\n",
            "Epoch 00242: loss improved from 0.00185 to 0.00185, saving model to poids_train.hdf5\n",
            "Epoch 243/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0018 - mse: 0.0018 - My_MSE: 6.2664 - val_loss: 0.0021 - val_mse: 0.0021 - val_My_MSE: 6.2721\n",
            "\n",
            "Epoch 00243: loss improved from 0.00185 to 0.00181, saving model to poids_train.hdf5\n",
            "Epoch 244/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0019 - mse: 0.0019 - My_MSE: 6.2682 - val_loss: 0.0020 - val_mse: 0.0020 - val_My_MSE: 6.2693\n",
            "\n",
            "Epoch 00244: loss did not improve from 0.00181\n",
            "Epoch 245/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0018 - mse: 0.0018 - My_MSE: 6.2670 - val_loss: 0.0020 - val_mse: 0.0020 - val_My_MSE: 6.2691\n",
            "\n",
            "Epoch 00245: loss did not improve from 0.00181\n",
            "Epoch 246/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0017 - mse: 0.0017 - My_MSE: 6.2659 - val_loss: 0.0020 - val_mse: 0.0020 - val_My_MSE: 6.2705\n",
            "\n",
            "Epoch 00246: loss improved from 0.00181 to 0.00180, saving model to poids_train.hdf5\n",
            "Epoch 247/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0018 - mse: 0.0018 - My_MSE: 6.2665 - val_loss: 0.0020 - val_mse: 0.0020 - val_My_MSE: 6.2699\n",
            "\n",
            "Epoch 00247: loss improved from 0.00180 to 0.00176, saving model to poids_train.hdf5\n",
            "Epoch 248/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0017 - mse: 0.0017 - My_MSE: 6.2653 - val_loss: 0.0019 - val_mse: 0.0019 - val_My_MSE: 6.2686\n",
            "\n",
            "Epoch 00248: loss improved from 0.00176 to 0.00172, saving model to poids_train.hdf5\n",
            "Epoch 249/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0017 - mse: 0.0017 - My_MSE: 6.2646 - val_loss: 0.0019 - val_mse: 0.0019 - val_My_MSE: 6.2686\n",
            "\n",
            "Epoch 00249: loss improved from 0.00172 to 0.00171, saving model to poids_train.hdf5\n",
            "Epoch 250/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0018 - mse: 0.0018 - My_MSE: 6.2667 - val_loss: 0.0018 - val_mse: 0.0018 - val_My_MSE: 6.2672\n",
            "\n",
            "Epoch 00250: loss did not improve from 0.00171\n",
            "Epoch 251/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0016 - mse: 0.0016 - My_MSE: 6.2636 - val_loss: 0.0018 - val_mse: 0.0018 - val_My_MSE: 6.2671\n",
            "\n",
            "Epoch 00251: loss improved from 0.00171 to 0.00168, saving model to poids_train.hdf5\n",
            "Epoch 252/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0016 - mse: 0.0016 - My_MSE: 6.2637 - val_loss: 0.0018 - val_mse: 0.0018 - val_My_MSE: 6.2673\n",
            "\n",
            "Epoch 00252: loss improved from 0.00168 to 0.00166, saving model to poids_train.hdf5\n",
            "Epoch 253/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0017 - mse: 0.0017 - My_MSE: 6.2656 - val_loss: 0.0018 - val_mse: 0.0018 - val_My_MSE: 6.2663\n",
            "\n",
            "Epoch 00253: loss improved from 0.00166 to 0.00165, saving model to poids_train.hdf5\n",
            "Epoch 254/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0016 - mse: 0.0016 - My_MSE: 6.2642 - val_loss: 0.0018 - val_mse: 0.0018 - val_My_MSE: 6.2667\n",
            "\n",
            "Epoch 00254: loss improved from 0.00165 to 0.00164, saving model to poids_train.hdf5\n",
            "Epoch 255/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0016 - mse: 0.0016 - My_MSE: 6.2639 - val_loss: 0.0017 - val_mse: 0.0017 - val_My_MSE: 6.2657\n",
            "\n",
            "Epoch 00255: loss improved from 0.00164 to 0.00162, saving model to poids_train.hdf5\n",
            "Epoch 256/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0016 - mse: 0.0016 - My_MSE: 6.2629 - val_loss: 0.0018 - val_mse: 0.0018 - val_My_MSE: 6.2661\n",
            "\n",
            "Epoch 00256: loss improved from 0.00162 to 0.00161, saving model to poids_train.hdf5\n",
            "Epoch 257/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0015 - mse: 0.0015 - My_MSE: 6.2617 - val_loss: 0.0017 - val_mse: 0.0017 - val_My_MSE: 6.2657\n",
            "\n",
            "Epoch 00257: loss improved from 0.00161 to 0.00158, saving model to poids_train.hdf5\n",
            "Epoch 258/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0016 - mse: 0.0016 - My_MSE: 6.2631 - val_loss: 0.0017 - val_mse: 0.0017 - val_My_MSE: 6.2650\n",
            "\n",
            "Epoch 00258: loss improved from 0.00158 to 0.00157, saving model to poids_train.hdf5\n",
            "Epoch 259/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0015 - mse: 0.0015 - My_MSE: 6.2626 - val_loss: 0.0017 - val_mse: 0.0017 - val_My_MSE: 6.2658\n",
            "\n",
            "Epoch 00259: loss improved from 0.00157 to 0.00157, saving model to poids_train.hdf5\n",
            "Epoch 260/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0015 - mse: 0.0015 - My_MSE: 6.2624 - val_loss: 0.0017 - val_mse: 0.0017 - val_My_MSE: 6.2645\n",
            "\n",
            "Epoch 00260: loss improved from 0.00157 to 0.00154, saving model to poids_train.hdf5\n",
            "Epoch 261/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0015 - mse: 0.0015 - My_MSE: 6.2628 - val_loss: 0.0017 - val_mse: 0.0017 - val_My_MSE: 6.2656\n",
            "\n",
            "Epoch 00261: loss improved from 0.00154 to 0.00153, saving model to poids_train.hdf5\n",
            "Epoch 262/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0016 - mse: 0.0016 - My_MSE: 6.2629 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2643\n",
            "\n",
            "Epoch 00262: loss did not improve from 0.00153\n",
            "Epoch 263/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0015 - mse: 0.0015 - My_MSE: 6.2624 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2636\n",
            "\n",
            "Epoch 00263: loss improved from 0.00153 to 0.00151, saving model to poids_train.hdf5\n",
            "Epoch 264/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0016 - mse: 0.0016 - My_MSE: 6.2631 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2639\n",
            "\n",
            "Epoch 00264: loss improved from 0.00151 to 0.00148, saving model to poids_train.hdf5\n",
            "Epoch 265/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0015 - mse: 0.0015 - My_MSE: 6.2618 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2640\n",
            "\n",
            "Epoch 00265: loss improved from 0.00148 to 0.00147, saving model to poids_train.hdf5\n",
            "Epoch 266/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0014 - mse: 0.0014 - My_MSE: 6.2612 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2630\n",
            "\n",
            "Epoch 00266: loss improved from 0.00147 to 0.00146, saving model to poids_train.hdf5\n",
            "Epoch 267/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0014 - mse: 0.0014 - My_MSE: 6.2612 - val_loss: 0.0016 - val_mse: 0.0016 - val_My_MSE: 6.2629\n",
            "\n",
            "Epoch 00267: loss improved from 0.00146 to 0.00144, saving model to poids_train.hdf5\n",
            "Epoch 268/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0014 - mse: 0.0014 - My_MSE: 6.2604 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2626\n",
            "\n",
            "Epoch 00268: loss improved from 0.00144 to 0.00142, saving model to poids_train.hdf5\n",
            "Epoch 269/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0015 - mse: 0.0015 - My_MSE: 6.2620 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2621\n",
            "\n",
            "Epoch 00269: loss improved from 0.00142 to 0.00141, saving model to poids_train.hdf5\n",
            "Epoch 270/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2595 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2622\n",
            "\n",
            "Epoch 00270: loss improved from 0.00141 to 0.00140, saving model to poids_train.hdf5\n",
            "Epoch 271/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0014 - mse: 0.0014 - My_MSE: 6.2601 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2615\n",
            "\n",
            "Epoch 00271: loss improved from 0.00140 to 0.00138, saving model to poids_train.hdf5\n",
            "Epoch 272/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0014 - mse: 0.0014 - My_MSE: 6.2604 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2612\n",
            "\n",
            "Epoch 00272: loss improved from 0.00138 to 0.00137, saving model to poids_train.hdf5\n",
            "Epoch 273/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2592 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2610\n",
            "\n",
            "Epoch 00273: loss improved from 0.00137 to 0.00136, saving model to poids_train.hdf5\n",
            "Epoch 274/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2594 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2611\n",
            "\n",
            "Epoch 00274: loss improved from 0.00136 to 0.00134, saving model to poids_train.hdf5\n",
            "Epoch 275/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0014 - mse: 0.0014 - My_MSE: 6.2597 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2605\n",
            "\n",
            "Epoch 00275: loss improved from 0.00134 to 0.00133, saving model to poids_train.hdf5\n",
            "Epoch 276/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2596 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2615\n",
            "\n",
            "Epoch 00276: loss did not improve from 0.00133\n",
            "Epoch 277/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2583 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2603\n",
            "\n",
            "Epoch 00277: loss improved from 0.00133 to 0.00132, saving model to poids_train.hdf5\n",
            "Epoch 278/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2595 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2599\n",
            "\n",
            "Epoch 00278: loss improved from 0.00132 to 0.00129, saving model to poids_train.hdf5\n",
            "Epoch 279/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0014 - mse: 0.0014 - My_MSE: 6.2599 - val_loss: 0.0015 - val_mse: 0.0015 - val_My_MSE: 6.2614\n",
            "\n",
            "Epoch 00279: loss did not improve from 0.00129\n",
            "Epoch 280/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2588 - val_loss: 0.0014 - val_mse: 0.0014 - val_My_MSE: 6.2598\n",
            "\n",
            "Epoch 00280: loss did not improve from 0.00129\n",
            "Epoch 281/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2581 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2592\n",
            "\n",
            "Epoch 00281: loss improved from 0.00129 to 0.00126, saving model to poids_train.hdf5\n",
            "Epoch 282/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2578 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2594\n",
            "\n",
            "Epoch 00282: loss improved from 0.00126 to 0.00125, saving model to poids_train.hdf5\n",
            "Epoch 283/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2573 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2588\n",
            "\n",
            "Epoch 00283: loss improved from 0.00125 to 0.00124, saving model to poids_train.hdf5\n",
            "Epoch 284/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2574 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2592\n",
            "\n",
            "Epoch 00284: loss improved from 0.00124 to 0.00123, saving model to poids_train.hdf5\n",
            "Epoch 285/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0013 - mse: 0.0013 - My_MSE: 6.2592 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2586\n",
            "\n",
            "Epoch 00285: loss improved from 0.00123 to 0.00123, saving model to poids_train.hdf5\n",
            "Epoch 286/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2587\n",
            "\n",
            "Epoch 00286: loss improved from 0.00123 to 0.00121, saving model to poids_train.hdf5\n",
            "Epoch 287/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2576 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2582\n",
            "\n",
            "Epoch 00287: loss improved from 0.00121 to 0.00120, saving model to poids_train.hdf5\n",
            "Epoch 288/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2580 - val_loss: 0.0013 - val_mse: 0.0013 - val_My_MSE: 6.2586\n",
            "\n",
            "Epoch 00288: loss improved from 0.00120 to 0.00120, saving model to poids_train.hdf5\n",
            "Epoch 289/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2571 - val_loss: 0.0012 - val_mse: 0.0012 - val_My_MSE: 6.2579\n",
            "\n",
            "Epoch 00289: loss improved from 0.00120 to 0.00118, saving model to poids_train.hdf5\n",
            "Epoch 290/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2568 - val_loss: 0.0012 - val_mse: 0.0012 - val_My_MSE: 6.2579\n",
            "\n",
            "Epoch 00290: loss improved from 0.00118 to 0.00118, saving model to poids_train.hdf5\n",
            "Epoch 291/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2560 - val_loss: 0.0012 - val_mse: 0.0012 - val_My_MSE: 6.2578\n",
            "\n",
            "Epoch 00291: loss improved from 0.00118 to 0.00116, saving model to poids_train.hdf5\n",
            "Epoch 292/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2557 - val_loss: 0.0012 - val_mse: 0.0012 - val_My_MSE: 6.2574\n",
            "\n",
            "Epoch 00292: loss did not improve from 0.00116\n",
            "Epoch 293/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2574 - val_loss: 0.0012 - val_mse: 0.0012 - val_My_MSE: 6.2576\n",
            "\n",
            "Epoch 00293: loss improved from 0.00116 to 0.00115, saving model to poids_train.hdf5\n",
            "Epoch 294/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2566 - val_loss: 0.0012 - val_mse: 0.0012 - val_My_MSE: 6.2578\n",
            "\n",
            "Epoch 00294: loss did not improve from 0.00115\n",
            "Epoch 295/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2573 - val_loss: 0.0012 - val_mse: 0.0012 - val_My_MSE: 6.2568\n",
            "\n",
            "Epoch 00295: loss did not improve from 0.00115\n",
            "Epoch 296/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2556 - val_loss: 0.0012 - val_mse: 0.0012 - val_My_MSE: 6.2567\n",
            "\n",
            "Epoch 00296: loss improved from 0.00115 to 0.00113, saving model to poids_train.hdf5\n",
            "Epoch 297/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2559 - val_loss: 0.0012 - val_mse: 0.0012 - val_My_MSE: 6.2573\n",
            "\n",
            "Epoch 00297: loss improved from 0.00113 to 0.00112, saving model to poids_train.hdf5\n",
            "Epoch 298/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0011 - val_mse: 0.0011 - val_My_MSE: 6.2565\n",
            "\n",
            "Epoch 00298: loss improved from 0.00112 to 0.00112, saving model to poids_train.hdf5\n",
            "Epoch 299/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0012 - mse: 0.0012 - My_MSE: 6.2566 - val_loss: 0.0011 - val_mse: 0.0011 - val_My_MSE: 6.2564\n",
            "\n",
            "Epoch 00299: loss improved from 0.00112 to 0.00110, saving model to poids_train.hdf5\n",
            "Epoch 300/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2564 - val_loss: 0.0011 - val_mse: 0.0011 - val_My_MSE: 6.2563\n",
            "\n",
            "Epoch 00300: loss improved from 0.00110 to 0.00110, saving model to poids_train.hdf5\n",
            "Epoch 301/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0011 - val_mse: 0.0011 - val_My_MSE: 6.2564\n",
            "\n",
            "Epoch 00301: loss did not improve from 0.00110\n",
            "Epoch 302/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2562 - val_loss: 0.0012 - val_mse: 0.0012 - val_My_MSE: 6.2568\n",
            "\n",
            "Epoch 00302: loss improved from 0.00110 to 0.00107, saving model to poids_train.hdf5\n",
            "Epoch 303/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0010 - mse: 0.0010 - My_MSE: 6.2546 - val_loss: 0.0011 - val_mse: 0.0011 - val_My_MSE: 6.2558\n",
            "\n",
            "Epoch 00303: loss improved from 0.00107 to 0.00107, saving model to poids_train.hdf5\n",
            "Epoch 304/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2556 - val_loss: 0.0011 - val_mse: 0.0011 - val_My_MSE: 6.2558\n",
            "\n",
            "Epoch 00304: loss improved from 0.00107 to 0.00107, saving model to poids_train.hdf5\n",
            "Epoch 305/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2556 - val_loss: 0.0011 - val_mse: 0.0011 - val_My_MSE: 6.2561\n",
            "\n",
            "Epoch 00305: loss improved from 0.00107 to 0.00106, saving model to poids_train.hdf5\n",
            "Epoch 306/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0010 - mse: 0.0010 - My_MSE: 6.2549 - val_loss: 0.0011 - val_mse: 0.0011 - val_My_MSE: 6.2557\n",
            "\n",
            "Epoch 00306: loss improved from 0.00106 to 0.00106, saving model to poids_train.hdf5\n",
            "Epoch 307/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0010 - mse: 0.0010 - My_MSE: 6.2547 - val_loss: 0.0011 - val_mse: 0.0011 - val_My_MSE: 6.2553\n",
            "\n",
            "Epoch 00307: loss improved from 0.00106 to 0.00105, saving model to poids_train.hdf5\n",
            "Epoch 308/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2551 - val_loss: 0.0011 - val_mse: 0.0011 - val_My_MSE: 6.2551\n",
            "\n",
            "Epoch 00308: loss improved from 0.00105 to 0.00104, saving model to poids_train.hdf5\n",
            "Epoch 309/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 9.8701e-04 - mse: 9.8701e-04 - My_MSE: 6.2541 - val_loss: 0.0011 - val_mse: 0.0011 - val_My_MSE: 6.2554\n",
            "\n",
            "Epoch 00309: loss improved from 0.00104 to 0.00103, saving model to poids_train.hdf5\n",
            "Epoch 310/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0010 - mse: 0.0010 - My_MSE: 6.2550 - val_loss: 0.0010 - val_mse: 0.0010 - val_My_MSE: 6.2548\n",
            "\n",
            "Epoch 00310: loss did not improve from 0.00103\n",
            "Epoch 311/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0010 - mse: 0.0010 - My_MSE: 6.2544 - val_loss: 0.0011 - val_mse: 0.0011 - val_My_MSE: 6.2558\n",
            "\n",
            "Epoch 00311: loss improved from 0.00103 to 0.00101, saving model to poids_train.hdf5\n",
            "Epoch 312/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2563 - val_loss: 0.0010 - val_mse: 0.0010 - val_My_MSE: 6.2546\n",
            "\n",
            "Epoch 00312: loss did not improve from 0.00101\n",
            "Epoch 313/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - My_MSE: 6.2555 - val_loss: 0.0010 - val_mse: 0.0010 - val_My_MSE: 6.2544\n",
            "\n",
            "Epoch 00313: loss improved from 0.00101 to 0.00100, saving model to poids_train.hdf5\n",
            "Epoch 314/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0010 - mse: 0.0010 - My_MSE: 6.2546 - val_loss: 0.0010 - val_mse: 0.0010 - val_My_MSE: 6.2548\n",
            "\n",
            "Epoch 00314: loss improved from 0.00100 to 0.00099, saving model to poids_train.hdf5\n",
            "Epoch 315/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 9.7723e-04 - mse: 9.7723e-04 - My_MSE: 6.2539 - val_loss: 0.0010 - val_mse: 0.0010 - val_My_MSE: 6.2543\n",
            "\n",
            "Epoch 00315: loss improved from 0.00099 to 0.00099, saving model to poids_train.hdf5\n",
            "Epoch 316/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 9.0287e-04 - mse: 9.0287e-04 - My_MSE: 6.2527 - val_loss: 9.8899e-04 - val_mse: 9.8899e-04 - val_My_MSE: 6.2541\n",
            "\n",
            "Epoch 00316: loss did not improve from 0.00099\n",
            "Epoch 317/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 9.1710e-04 - mse: 9.1710e-04 - My_MSE: 6.2530 - val_loss: 0.0010 - val_mse: 0.0010 - val_My_MSE: 6.2546\n",
            "\n",
            "Epoch 00317: loss did not improve from 0.00099\n",
            "Epoch 318/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 9.4414e-04 - mse: 9.4414e-04 - My_MSE: 6.2534 - val_loss: 9.8568e-04 - val_mse: 9.8568e-04 - val_My_MSE: 6.2540\n",
            "\n",
            "Epoch 00318: loss improved from 0.00099 to 0.00097, saving model to poids_train.hdf5\n",
            "Epoch 319/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0010 - mse: 0.0010 - My_MSE: 6.2548 - val_loss: 9.7984e-04 - val_mse: 9.7984e-04 - val_My_MSE: 6.2539\n",
            "\n",
            "Epoch 00319: loss did not improve from 0.00097\n",
            "Epoch 320/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 9.2492e-04 - mse: 9.2492e-04 - My_MSE: 6.2531 - val_loss: 9.7616e-04 - val_mse: 9.7616e-04 - val_My_MSE: 6.2539\n",
            "\n",
            "Epoch 00320: loss improved from 0.00097 to 0.00096, saving model to poids_train.hdf5\n",
            "Epoch 321/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0010 - mse: 0.0010 - My_MSE: 6.2544 - val_loss: 9.4600e-04 - val_mse: 9.4600e-04 - val_My_MSE: 6.2534\n",
            "\n",
            "Epoch 00321: loss improved from 0.00096 to 0.00096, saving model to poids_train.hdf5\n",
            "Epoch 322/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 9.5466e-04 - mse: 9.5466e-04 - My_MSE: 6.2535 - val_loss: 0.0010 - val_mse: 0.0010 - val_My_MSE: 6.2543\n",
            "\n",
            "Epoch 00322: loss improved from 0.00096 to 0.00094, saving model to poids_train.hdf5\n",
            "Epoch 323/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 9.6156e-04 - mse: 9.6156e-04 - My_MSE: 6.2537 - val_loss: 9.4089e-04 - val_mse: 9.4089e-04 - val_My_MSE: 6.2533\n",
            "\n",
            "Epoch 00323: loss did not improve from 0.00094\n",
            "Epoch 324/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 9.5598e-04 - mse: 9.5598e-04 - My_MSE: 6.2536 - val_loss: 9.5491e-04 - val_mse: 9.5491e-04 - val_My_MSE: 6.2535\n",
            "\n",
            "Epoch 00324: loss did not improve from 0.00094\n",
            "Epoch 325/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 9.0956e-04 - mse: 9.0956e-04 - My_MSE: 6.2528 - val_loss: 9.2955e-04 - val_mse: 9.2955e-04 - val_My_MSE: 6.2532\n",
            "\n",
            "Epoch 00325: loss improved from 0.00094 to 0.00092, saving model to poids_train.hdf5\n",
            "Epoch 326/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 9.1394e-04 - mse: 9.1394e-04 - My_MSE: 6.2529 - val_loss: 9.1151e-04 - val_mse: 9.1151e-04 - val_My_MSE: 6.2529\n",
            "\n",
            "Epoch 00326: loss improved from 0.00092 to 0.00091, saving model to poids_train.hdf5\n",
            "Epoch 327/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 8.5927e-04 - mse: 8.5927e-04 - My_MSE: 6.2521 - val_loss: 9.1802e-04 - val_mse: 9.1802e-04 - val_My_MSE: 6.2530\n",
            "\n",
            "Epoch 00327: loss did not improve from 0.00091\n",
            "Epoch 328/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 8.6226e-04 - mse: 8.6226e-04 - My_MSE: 6.2521 - val_loss: 9.3864e-04 - val_mse: 9.3864e-04 - val_My_MSE: 6.2533\n",
            "\n",
            "Epoch 00328: loss improved from 0.00091 to 0.00091, saving model to poids_train.hdf5\n",
            "Epoch 329/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 9.0329e-04 - mse: 9.0329e-04 - My_MSE: 6.2527 - val_loss: 8.8044e-04 - val_mse: 8.8044e-04 - val_My_MSE: 6.2524\n",
            "\n",
            "Epoch 00329: loss improved from 0.00091 to 0.00089, saving model to poids_train.hdf5\n",
            "Epoch 330/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 9.9232e-04 - mse: 9.9232e-04 - My_MSE: 6.2541 - val_loss: 9.1556e-04 - val_mse: 9.1556e-04 - val_My_MSE: 6.2529\n",
            "\n",
            "Epoch 00330: loss did not improve from 0.00089\n",
            "Epoch 331/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 9.4155e-04 - mse: 9.4155e-04 - My_MSE: 6.2533 - val_loss: 8.7987e-04 - val_mse: 8.7987e-04 - val_My_MSE: 6.2524\n",
            "\n",
            "Epoch 00331: loss improved from 0.00089 to 0.00089, saving model to poids_train.hdf5\n",
            "Epoch 332/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 8.5083e-04 - mse: 8.5083e-04 - My_MSE: 6.2519 - val_loss: 8.9263e-04 - val_mse: 8.9263e-04 - val_My_MSE: 6.2526\n",
            "\n",
            "Epoch 00332: loss improved from 0.00089 to 0.00087, saving model to poids_train.hdf5\n",
            "Epoch 333/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 8.7971e-04 - mse: 8.7971e-04 - My_MSE: 6.2524 - val_loss: 8.5834e-04 - val_mse: 8.5834e-04 - val_My_MSE: 6.2520\n",
            "\n",
            "Epoch 00333: loss improved from 0.00087 to 0.00087, saving model to poids_train.hdf5\n",
            "Epoch 334/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 8.6346e-04 - mse: 8.6346e-04 - My_MSE: 6.2521 - val_loss: 8.5580e-04 - val_mse: 8.5580e-04 - val_My_MSE: 6.2520\n",
            "\n",
            "Epoch 00334: loss improved from 0.00087 to 0.00086, saving model to poids_train.hdf5\n",
            "Epoch 335/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 8.7482e-04 - mse: 8.7482e-04 - My_MSE: 6.2523 - val_loss: 8.6135e-04 - val_mse: 8.6135e-04 - val_My_MSE: 6.2521\n",
            "\n",
            "Epoch 00335: loss improved from 0.00086 to 0.00086, saving model to poids_train.hdf5\n",
            "Epoch 336/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 8.4947e-04 - mse: 8.4947e-04 - My_MSE: 6.2519 - val_loss: 8.9600e-04 - val_mse: 8.9600e-04 - val_My_MSE: 6.2526\n",
            "\n",
            "Epoch 00336: loss did not improve from 0.00086\n",
            "Epoch 337/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 9.1785e-04 - mse: 9.1785e-04 - My_MSE: 6.2530 - val_loss: 8.3515e-04 - val_mse: 8.3515e-04 - val_My_MSE: 6.2517\n",
            "\n",
            "Epoch 00337: loss improved from 0.00086 to 0.00085, saving model to poids_train.hdf5\n",
            "Epoch 338/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 8.3055e-04 - mse: 8.3055e-04 - My_MSE: 6.2516 - val_loss: 8.2846e-04 - val_mse: 8.2846e-04 - val_My_MSE: 6.2516\n",
            "\n",
            "Epoch 00338: loss improved from 0.00085 to 0.00085, saving model to poids_train.hdf5\n",
            "Epoch 339/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 8.3107e-04 - mse: 8.3107e-04 - My_MSE: 6.2516 - val_loss: 8.6450e-04 - val_mse: 8.6450e-04 - val_My_MSE: 6.2521\n",
            "\n",
            "Epoch 00339: loss improved from 0.00085 to 0.00084, saving model to poids_train.hdf5\n",
            "Epoch 340/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 7.9082e-04 - mse: 7.9082e-04 - My_MSE: 6.2510 - val_loss: 8.3090e-04 - val_mse: 8.3090e-04 - val_My_MSE: 6.2516\n",
            "\n",
            "Epoch 00340: loss improved from 0.00084 to 0.00084, saving model to poids_train.hdf5\n",
            "Epoch 341/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 8.9694e-04 - mse: 8.9694e-04 - My_MSE: 6.2526 - val_loss: 8.7514e-04 - val_mse: 8.7514e-04 - val_My_MSE: 6.2523\n",
            "\n",
            "Epoch 00341: loss did not improve from 0.00084\n",
            "Epoch 342/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 8.6058e-04 - mse: 8.6058e-04 - My_MSE: 6.2521 - val_loss: 8.1999e-04 - val_mse: 8.1999e-04 - val_My_MSE: 6.2514\n",
            "\n",
            "Epoch 00342: loss did not improve from 0.00084\n",
            "Epoch 343/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 8.9334e-04 - mse: 8.9334e-04 - My_MSE: 6.2526 - val_loss: 8.2013e-04 - val_mse: 8.2013e-04 - val_My_MSE: 6.2514\n",
            "\n",
            "Epoch 00343: loss improved from 0.00084 to 0.00084, saving model to poids_train.hdf5\n",
            "Epoch 344/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 8.5439e-04 - mse: 8.5439e-04 - My_MSE: 6.2520 - val_loss: 8.2385e-04 - val_mse: 8.2385e-04 - val_My_MSE: 6.2515\n",
            "\n",
            "Epoch 00344: loss improved from 0.00084 to 0.00083, saving model to poids_train.hdf5\n",
            "Epoch 345/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 8.3874e-04 - mse: 8.3874e-04 - My_MSE: 6.2517 - val_loss: 8.2113e-04 - val_mse: 8.2113e-04 - val_My_MSE: 6.2515\n",
            "\n",
            "Epoch 00345: loss improved from 0.00083 to 0.00082, saving model to poids_train.hdf5\n",
            "Epoch 346/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 7.7080e-04 - mse: 7.7080e-04 - My_MSE: 6.2507 - val_loss: 7.9166e-04 - val_mse: 7.9166e-04 - val_My_MSE: 6.2510\n",
            "\n",
            "Epoch 00346: loss improved from 0.00082 to 0.00081, saving model to poids_train.hdf5\n",
            "Epoch 347/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 7.9843e-04 - mse: 7.9843e-04 - My_MSE: 6.2511 - val_loss: 7.9255e-04 - val_mse: 7.9255e-04 - val_My_MSE: 6.2510\n",
            "\n",
            "Epoch 00347: loss improved from 0.00081 to 0.00080, saving model to poids_train.hdf5\n",
            "Epoch 348/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 7.8462e-04 - mse: 7.8462e-04 - My_MSE: 6.2509 - val_loss: 8.4556e-04 - val_mse: 8.4556e-04 - val_My_MSE: 6.2518\n",
            "\n",
            "Epoch 00348: loss did not improve from 0.00080\n",
            "Epoch 349/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 7.4125e-04 - mse: 7.4125e-04 - My_MSE: 6.2502 - val_loss: 7.9317e-04 - val_mse: 7.9317e-04 - val_My_MSE: 6.2510\n",
            "\n",
            "Epoch 00349: loss did not improve from 0.00080\n",
            "Epoch 350/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 8.2117e-04 - mse: 8.2117e-04 - My_MSE: 6.2515 - val_loss: 7.8076e-04 - val_mse: 7.8076e-04 - val_My_MSE: 6.2508\n",
            "\n",
            "Epoch 00350: loss improved from 0.00080 to 0.00080, saving model to poids_train.hdf5\n",
            "Epoch 351/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 8.0809e-04 - mse: 8.0809e-04 - My_MSE: 6.2513 - val_loss: 7.7784e-04 - val_mse: 7.7784e-04 - val_My_MSE: 6.2508\n",
            "\n",
            "Epoch 00351: loss improved from 0.00080 to 0.00079, saving model to poids_train.hdf5\n",
            "Epoch 352/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 7.5713e-04 - mse: 7.5713e-04 - My_MSE: 6.2505 - val_loss: 7.7549e-04 - val_mse: 7.7549e-04 - val_My_MSE: 6.2508\n",
            "\n",
            "Epoch 00352: loss did not improve from 0.00079\n",
            "Epoch 353/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 8.2317e-04 - mse: 8.2317e-04 - My_MSE: 6.2515 - val_loss: 7.8677e-04 - val_mse: 7.8677e-04 - val_My_MSE: 6.2509\n",
            "\n",
            "Epoch 00353: loss improved from 0.00079 to 0.00078, saving model to poids_train.hdf5\n",
            "Epoch 354/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 7.2203e-04 - mse: 7.2203e-04 - My_MSE: 6.2499 - val_loss: 7.5924e-04 - val_mse: 7.5924e-04 - val_My_MSE: 6.2505\n",
            "\n",
            "Epoch 00354: loss improved from 0.00078 to 0.00078, saving model to poids_train.hdf5\n",
            "Epoch 355/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 7.7017e-04 - mse: 7.7017e-04 - My_MSE: 6.2507 - val_loss: 7.8332e-04 - val_mse: 7.8332e-04 - val_My_MSE: 6.2509\n",
            "\n",
            "Epoch 00355: loss did not improve from 0.00078\n",
            "Epoch 356/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 7.4921e-04 - mse: 7.4921e-04 - My_MSE: 6.2503 - val_loss: 7.6801e-04 - val_mse: 7.6801e-04 - val_My_MSE: 6.2506\n",
            "\n",
            "Epoch 00356: loss improved from 0.00078 to 0.00077, saving model to poids_train.hdf5\n",
            "Epoch 357/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 7.7233e-04 - mse: 7.7233e-04 - My_MSE: 6.2507 - val_loss: 7.5687e-04 - val_mse: 7.5687e-04 - val_My_MSE: 6.2505\n",
            "\n",
            "Epoch 00357: loss did not improve from 0.00077\n",
            "Epoch 358/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 8.3321e-04 - mse: 8.3321e-04 - My_MSE: 6.2517 - val_loss: 7.4938e-04 - val_mse: 7.4938e-04 - val_My_MSE: 6.2503\n",
            "\n",
            "Epoch 00358: loss did not improve from 0.00077\n",
            "Epoch 359/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 7.7478e-04 - mse: 7.7478e-04 - My_MSE: 6.2507 - val_loss: 7.5647e-04 - val_mse: 7.5647e-04 - val_My_MSE: 6.2505\n",
            "\n",
            "Epoch 00359: loss improved from 0.00077 to 0.00076, saving model to poids_train.hdf5\n",
            "Epoch 360/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 7.4887e-04 - mse: 7.4887e-04 - My_MSE: 6.2503 - val_loss: 7.5507e-04 - val_mse: 7.5507e-04 - val_My_MSE: 6.2504\n",
            "\n",
            "Epoch 00360: loss improved from 0.00076 to 0.00076, saving model to poids_train.hdf5\n",
            "Epoch 361/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 7.7131e-04 - mse: 7.7131e-04 - My_MSE: 6.2507 - val_loss: 7.3404e-04 - val_mse: 7.3404e-04 - val_My_MSE: 6.2501\n",
            "\n",
            "Epoch 00361: loss improved from 0.00076 to 0.00075, saving model to poids_train.hdf5\n",
            "Epoch 362/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 7.2532e-04 - mse: 7.2532e-04 - My_MSE: 6.2500 - val_loss: 7.5030e-04 - val_mse: 7.5030e-04 - val_My_MSE: 6.2504\n",
            "\n",
            "Epoch 00362: loss did not improve from 0.00075\n",
            "Epoch 363/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 7.8607e-04 - mse: 7.8607e-04 - My_MSE: 6.2509 - val_loss: 7.3901e-04 - val_mse: 7.3901e-04 - val_My_MSE: 6.2502\n",
            "\n",
            "Epoch 00363: loss improved from 0.00075 to 0.00075, saving model to poids_train.hdf5\n",
            "Epoch 364/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 7.7367e-04 - mse: 7.7367e-04 - My_MSE: 6.2507 - val_loss: 7.3854e-04 - val_mse: 7.3854e-04 - val_My_MSE: 6.2502\n",
            "\n",
            "Epoch 00364: loss improved from 0.00075 to 0.00075, saving model to poids_train.hdf5\n",
            "Epoch 365/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 7.0935e-04 - mse: 7.0935e-04 - My_MSE: 6.2497 - val_loss: 7.3028e-04 - val_mse: 7.3028e-04 - val_My_MSE: 6.2500\n",
            "\n",
            "Epoch 00365: loss improved from 0.00075 to 0.00075, saving model to poids_train.hdf5\n",
            "Epoch 366/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 7.7517e-04 - mse: 7.7517e-04 - My_MSE: 6.2507 - val_loss: 7.2715e-04 - val_mse: 7.2715e-04 - val_My_MSE: 6.2500\n",
            "\n",
            "Epoch 00366: loss improved from 0.00075 to 0.00075, saving model to poids_train.hdf5\n",
            "Epoch 367/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 7.0855e-04 - mse: 7.0855e-04 - My_MSE: 6.2497 - val_loss: 7.2206e-04 - val_mse: 7.2206e-04 - val_My_MSE: 6.2499\n",
            "\n",
            "Epoch 00367: loss improved from 0.00075 to 0.00074, saving model to poids_train.hdf5\n",
            "Epoch 368/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 7.7818e-04 - mse: 7.7818e-04 - My_MSE: 6.2508 - val_loss: 7.2531e-04 - val_mse: 7.2531e-04 - val_My_MSE: 6.2500\n",
            "\n",
            "Epoch 00368: loss improved from 0.00074 to 0.00074, saving model to poids_train.hdf5\n",
            "Epoch 369/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 7.3402e-04 - mse: 7.3402e-04 - My_MSE: 6.2501 - val_loss: 7.2638e-04 - val_mse: 7.2638e-04 - val_My_MSE: 6.2500\n",
            "\n",
            "Epoch 00369: loss improved from 0.00074 to 0.00073, saving model to poids_train.hdf5\n",
            "Epoch 370/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 7.0272e-04 - mse: 7.0272e-04 - My_MSE: 6.2496 - val_loss: 7.2683e-04 - val_mse: 7.2683e-04 - val_My_MSE: 6.2500\n",
            "\n",
            "Epoch 00370: loss improved from 0.00073 to 0.00072, saving model to poids_train.hdf5\n",
            "Epoch 371/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 7.6130e-04 - mse: 7.6130e-04 - My_MSE: 6.2505 - val_loss: 7.1065e-04 - val_mse: 7.1065e-04 - val_My_MSE: 6.2497\n",
            "\n",
            "Epoch 00371: loss did not improve from 0.00072\n",
            "Epoch 372/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.7958e-04 - mse: 6.7958e-04 - My_MSE: 6.2493 - val_loss: 7.0891e-04 - val_mse: 7.0891e-04 - val_My_MSE: 6.2497\n",
            "\n",
            "Epoch 00372: loss improved from 0.00072 to 0.00072, saving model to poids_train.hdf5\n",
            "Epoch 373/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.8164e-04 - mse: 6.8164e-04 - My_MSE: 6.2493 - val_loss: 7.0637e-04 - val_mse: 7.0637e-04 - val_My_MSE: 6.2497\n",
            "\n",
            "Epoch 00373: loss improved from 0.00072 to 0.00072, saving model to poids_train.hdf5\n",
            "Epoch 374/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 7.5122e-04 - mse: 7.5122e-04 - My_MSE: 6.2504 - val_loss: 7.1263e-04 - val_mse: 7.1263e-04 - val_My_MSE: 6.2498\n",
            "\n",
            "Epoch 00374: loss did not improve from 0.00072\n",
            "Epoch 375/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 7.4059e-04 - mse: 7.4059e-04 - My_MSE: 6.2502 - val_loss: 7.0661e-04 - val_mse: 7.0661e-04 - val_My_MSE: 6.2497\n",
            "\n",
            "Epoch 00375: loss did not improve from 0.00072\n",
            "Epoch 376/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 7.2017e-04 - mse: 7.2017e-04 - My_MSE: 6.2499 - val_loss: 7.1445e-04 - val_mse: 7.1445e-04 - val_My_MSE: 6.2498\n",
            "\n",
            "Epoch 00376: loss improved from 0.00072 to 0.00071, saving model to poids_train.hdf5\n",
            "Epoch 377/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.8375e-04 - mse: 6.8375e-04 - My_MSE: 6.2493 - val_loss: 6.9062e-04 - val_mse: 6.9062e-04 - val_My_MSE: 6.2494\n",
            "\n",
            "Epoch 00377: loss did not improve from 0.00071\n",
            "Epoch 378/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 6.8056e-04 - mse: 6.8056e-04 - My_MSE: 6.2493 - val_loss: 6.8750e-04 - val_mse: 6.8750e-04 - val_My_MSE: 6.2494\n",
            "\n",
            "Epoch 00378: loss improved from 0.00071 to 0.00070, saving model to poids_train.hdf5\n",
            "Epoch 379/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.8344e-04 - mse: 6.8344e-04 - My_MSE: 6.2493 - val_loss: 7.0091e-04 - val_mse: 7.0091e-04 - val_My_MSE: 6.2496\n",
            "\n",
            "Epoch 00379: loss improved from 0.00070 to 0.00070, saving model to poids_train.hdf5\n",
            "Epoch 380/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.9567e-04 - mse: 6.9567e-04 - My_MSE: 6.2495 - val_loss: 7.0101e-04 - val_mse: 7.0101e-04 - val_My_MSE: 6.2496\n",
            "\n",
            "Epoch 00380: loss did not improve from 0.00070\n",
            "Epoch 381/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 7.7222e-04 - mse: 7.7222e-04 - My_MSE: 6.2507 - val_loss: 6.9368e-04 - val_mse: 6.9368e-04 - val_My_MSE: 6.2495\n",
            "\n",
            "Epoch 00381: loss did not improve from 0.00070\n",
            "Epoch 382/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 7.0898e-04 - mse: 7.0898e-04 - My_MSE: 6.2497 - val_loss: 7.1182e-04 - val_mse: 7.1182e-04 - val_My_MSE: 6.2498\n",
            "\n",
            "Epoch 00382: loss improved from 0.00070 to 0.00070, saving model to poids_train.hdf5\n",
            "Epoch 383/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.8113e-04 - mse: 6.8113e-04 - My_MSE: 6.2493 - val_loss: 6.7687e-04 - val_mse: 6.7687e-04 - val_My_MSE: 6.2492\n",
            "\n",
            "Epoch 00383: loss did not improve from 0.00070\n",
            "Epoch 384/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 7.1778e-04 - mse: 7.1778e-04 - My_MSE: 6.2498 - val_loss: 6.8146e-04 - val_mse: 6.8146e-04 - val_My_MSE: 6.2493\n",
            "\n",
            "Epoch 00384: loss did not improve from 0.00070\n",
            "Epoch 385/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 7.6305e-04 - mse: 7.6305e-04 - My_MSE: 6.2506 - val_loss: 6.9817e-04 - val_mse: 6.9817e-04 - val_My_MSE: 6.2495\n",
            "\n",
            "Epoch 00385: loss did not improve from 0.00070\n",
            "Epoch 386/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.5308e-04 - mse: 6.5308e-04 - My_MSE: 6.2488 - val_loss: 6.7635e-04 - val_mse: 6.7635e-04 - val_My_MSE: 6.2492\n",
            "\n",
            "Epoch 00386: loss improved from 0.00070 to 0.00068, saving model to poids_train.hdf5\n",
            "Epoch 387/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.8050e-04 - mse: 6.8050e-04 - My_MSE: 6.2493 - val_loss: 6.7930e-04 - val_mse: 6.7930e-04 - val_My_MSE: 6.2492\n",
            "\n",
            "Epoch 00387: loss improved from 0.00068 to 0.00068, saving model to poids_train.hdf5\n",
            "Epoch 388/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.8191e-04 - mse: 6.8191e-04 - My_MSE: 6.2493 - val_loss: 6.7589e-04 - val_mse: 6.7589e-04 - val_My_MSE: 6.2492\n",
            "\n",
            "Epoch 00388: loss did not improve from 0.00068\n",
            "Epoch 389/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.5674e-04 - mse: 6.5674e-04 - My_MSE: 6.2489 - val_loss: 6.7756e-04 - val_mse: 6.7756e-04 - val_My_MSE: 6.2492\n",
            "\n",
            "Epoch 00389: loss improved from 0.00068 to 0.00068, saving model to poids_train.hdf5\n",
            "Epoch 390/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.7826e-04 - mse: 6.7826e-04 - My_MSE: 6.2492 - val_loss: 6.9025e-04 - val_mse: 6.9025e-04 - val_My_MSE: 6.2494\n",
            "\n",
            "Epoch 00390: loss did not improve from 0.00068\n",
            "Epoch 391/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.7947e-04 - mse: 6.7947e-04 - My_MSE: 6.2493 - val_loss: 7.0048e-04 - val_mse: 7.0048e-04 - val_My_MSE: 6.2496\n",
            "\n",
            "Epoch 00391: loss did not improve from 0.00068\n",
            "Epoch 392/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 7.1036e-04 - mse: 7.1036e-04 - My_MSE: 6.2497 - val_loss: 6.7919e-04 - val_mse: 6.7919e-04 - val_My_MSE: 6.2492\n",
            "\n",
            "Epoch 00392: loss did not improve from 0.00068\n",
            "Epoch 393/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.5770e-04 - mse: 6.5770e-04 - My_MSE: 6.2489 - val_loss: 6.5027e-04 - val_mse: 6.5027e-04 - val_My_MSE: 6.2488\n",
            "\n",
            "Epoch 00393: loss did not improve from 0.00068\n",
            "Epoch 394/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.8208e-04 - mse: 6.8208e-04 - My_MSE: 6.2493 - val_loss: 6.9415e-04 - val_mse: 6.9415e-04 - val_My_MSE: 6.2495\n",
            "\n",
            "Epoch 00394: loss improved from 0.00068 to 0.00067, saving model to poids_train.hdf5\n",
            "Epoch 395/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.4764e-04 - mse: 6.4764e-04 - My_MSE: 6.2488 - val_loss: 6.6555e-04 - val_mse: 6.6555e-04 - val_My_MSE: 6.2490\n",
            "\n",
            "Epoch 00395: loss improved from 0.00067 to 0.00067, saving model to poids_train.hdf5\n",
            "Epoch 396/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.8162e-04 - mse: 6.8162e-04 - My_MSE: 6.2493 - val_loss: 6.5920e-04 - val_mse: 6.5920e-04 - val_My_MSE: 6.2489\n",
            "\n",
            "Epoch 00396: loss improved from 0.00067 to 0.00067, saving model to poids_train.hdf5\n",
            "Epoch 397/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.5223e-04 - mse: 6.5223e-04 - My_MSE: 6.2488 - val_loss: 6.4601e-04 - val_mse: 6.4601e-04 - val_My_MSE: 6.2487\n",
            "\n",
            "Epoch 00397: loss improved from 0.00067 to 0.00066, saving model to poids_train.hdf5\n",
            "Epoch 398/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.5379e-04 - mse: 6.5379e-04 - My_MSE: 6.2489 - val_loss: 6.6076e-04 - val_mse: 6.6076e-04 - val_My_MSE: 6.2490\n",
            "\n",
            "Epoch 00398: loss did not improve from 0.00066\n",
            "Epoch 399/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.8501e-04 - mse: 6.8501e-04 - My_MSE: 6.2493 - val_loss: 6.6994e-04 - val_mse: 6.6994e-04 - val_My_MSE: 6.2491\n",
            "\n",
            "Epoch 00399: loss did not improve from 0.00066\n",
            "Epoch 400/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.1777e-04 - mse: 6.1777e-04 - My_MSE: 6.2483 - val_loss: 6.8841e-04 - val_mse: 6.8841e-04 - val_My_MSE: 6.2494\n",
            "\n",
            "Epoch 00400: loss did not improve from 0.00066\n",
            "Epoch 401/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.5983e-04 - mse: 6.5983e-04 - My_MSE: 6.2489 - val_loss: 6.4919e-04 - val_mse: 6.4919e-04 - val_My_MSE: 6.2488\n",
            "\n",
            "Epoch 00401: loss did not improve from 0.00066\n",
            "Epoch 402/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.6613e-04 - mse: 6.6613e-04 - My_MSE: 6.2490 - val_loss: 6.3926e-04 - val_mse: 6.3926e-04 - val_My_MSE: 6.2486\n",
            "\n",
            "Epoch 00402: loss did not improve from 0.00066\n",
            "Epoch 403/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.9112e-04 - mse: 6.9112e-04 - My_MSE: 6.2494 - val_loss: 6.8592e-04 - val_mse: 6.8592e-04 - val_My_MSE: 6.2494\n",
            "\n",
            "Epoch 00403: loss did not improve from 0.00066\n",
            "Epoch 404/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 6.9309e-04 - mse: 6.9309e-04 - My_MSE: 6.2495 - val_loss: 7.3938e-04 - val_mse: 7.3938e-04 - val_My_MSE: 6.2502\n",
            "\n",
            "Epoch 00404: loss did not improve from 0.00066\n",
            "Epoch 405/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.8766e-04 - mse: 6.8766e-04 - My_MSE: 6.2494 - val_loss: 6.5960e-04 - val_mse: 6.5960e-04 - val_My_MSE: 6.2489\n",
            "\n",
            "Epoch 00405: loss did not improve from 0.00066\n",
            "Epoch 406/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.5932e-04 - mse: 6.5932e-04 - My_MSE: 6.2489 - val_loss: 6.5023e-04 - val_mse: 6.5023e-04 - val_My_MSE: 6.2488\n",
            "\n",
            "Epoch 00406: loss improved from 0.00066 to 0.00065, saving model to poids_train.hdf5\n",
            "Epoch 407/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.2049e-04 - mse: 6.2049e-04 - My_MSE: 6.2483 - val_loss: 6.3129e-04 - val_mse: 6.3129e-04 - val_My_MSE: 6.2485\n",
            "\n",
            "Epoch 00407: loss improved from 0.00065 to 0.00064, saving model to poids_train.hdf5\n",
            "Epoch 408/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.6038e-04 - mse: 6.6038e-04 - My_MSE: 6.2490 - val_loss: 6.3775e-04 - val_mse: 6.3775e-04 - val_My_MSE: 6.2486\n",
            "\n",
            "Epoch 00408: loss did not improve from 0.00064\n",
            "Epoch 409/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.2176e-04 - mse: 6.2176e-04 - My_MSE: 6.2484 - val_loss: 6.3882e-04 - val_mse: 6.3882e-04 - val_My_MSE: 6.2486\n",
            "\n",
            "Epoch 00409: loss improved from 0.00064 to 0.00064, saving model to poids_train.hdf5\n",
            "Epoch 410/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.1051e-04 - mse: 6.1051e-04 - My_MSE: 6.2482 - val_loss: 6.4527e-04 - val_mse: 6.4527e-04 - val_My_MSE: 6.2487\n",
            "\n",
            "Epoch 00410: loss did not improve from 0.00064\n",
            "Epoch 411/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.1995e-04 - mse: 6.1995e-04 - My_MSE: 6.2483 - val_loss: 6.2382e-04 - val_mse: 6.2382e-04 - val_My_MSE: 6.2484\n",
            "\n",
            "Epoch 00411: loss did not improve from 0.00064\n",
            "Epoch 412/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.8509e-04 - mse: 6.8509e-04 - My_MSE: 6.2493 - val_loss: 6.2677e-04 - val_mse: 6.2677e-04 - val_My_MSE: 6.2484\n",
            "\n",
            "Epoch 00412: loss did not improve from 0.00064\n",
            "Epoch 413/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.3698e-04 - mse: 6.3698e-04 - My_MSE: 6.2486 - val_loss: 6.5084e-04 - val_mse: 6.5084e-04 - val_My_MSE: 6.2488\n",
            "\n",
            "Epoch 00413: loss did not improve from 0.00064\n",
            "Epoch 414/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 7.1966e-04 - mse: 7.1966e-04 - My_MSE: 6.2499 - val_loss: 6.4046e-04 - val_mse: 6.4046e-04 - val_My_MSE: 6.2486\n",
            "\n",
            "Epoch 00414: loss did not improve from 0.00064\n",
            "Epoch 415/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.1541e-04 - mse: 6.1541e-04 - My_MSE: 6.2483 - val_loss: 6.2099e-04 - val_mse: 6.2099e-04 - val_My_MSE: 6.2483\n",
            "\n",
            "Epoch 00415: loss improved from 0.00064 to 0.00063, saving model to poids_train.hdf5\n",
            "Epoch 416/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.2930e-04 - mse: 6.2930e-04 - My_MSE: 6.2485 - val_loss: 6.1834e-04 - val_mse: 6.1834e-04 - val_My_MSE: 6.2483\n",
            "\n",
            "Epoch 00416: loss improved from 0.00063 to 0.00063, saving model to poids_train.hdf5\n",
            "Epoch 417/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.3201e-04 - mse: 6.3201e-04 - My_MSE: 6.2485 - val_loss: 6.2706e-04 - val_mse: 6.2706e-04 - val_My_MSE: 6.2484\n",
            "\n",
            "Epoch 00417: loss improved from 0.00063 to 0.00063, saving model to poids_train.hdf5\n",
            "Epoch 418/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 6.7138e-04 - mse: 6.7138e-04 - My_MSE: 6.2491 - val_loss: 6.9952e-04 - val_mse: 6.9952e-04 - val_My_MSE: 6.2496\n",
            "\n",
            "Epoch 00418: loss did not improve from 0.00063\n",
            "Epoch 419/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 7.2469e-04 - mse: 7.2469e-04 - My_MSE: 6.2500 - val_loss: 6.3724e-04 - val_mse: 6.3724e-04 - val_My_MSE: 6.2486\n",
            "\n",
            "Epoch 00419: loss did not improve from 0.00063\n",
            "Epoch 420/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.3474e-04 - mse: 6.3474e-04 - My_MSE: 6.2486 - val_loss: 6.1715e-04 - val_mse: 6.1715e-04 - val_My_MSE: 6.2483\n",
            "\n",
            "Epoch 00420: loss did not improve from 0.00063\n",
            "Epoch 421/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.3994e-04 - mse: 6.3994e-04 - My_MSE: 6.2486 - val_loss: 6.4446e-04 - val_mse: 6.4446e-04 - val_My_MSE: 6.2487\n",
            "\n",
            "Epoch 00421: loss did not improve from 0.00063\n",
            "Epoch 422/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 6.2968e-04 - mse: 6.2968e-04 - My_MSE: 6.2485 - val_loss: 6.2592e-04 - val_mse: 6.2592e-04 - val_My_MSE: 6.2484\n",
            "\n",
            "Epoch 00422: loss did not improve from 0.00063\n",
            "Epoch 423/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.2528e-04 - mse: 6.2528e-04 - My_MSE: 6.2484 - val_loss: 6.2925e-04 - val_mse: 6.2925e-04 - val_My_MSE: 6.2485\n",
            "\n",
            "Epoch 00423: loss improved from 0.00063 to 0.00063, saving model to poids_train.hdf5\n",
            "Epoch 424/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.1761e-04 - mse: 6.1761e-04 - My_MSE: 6.2483 - val_loss: 6.0934e-04 - val_mse: 6.0934e-04 - val_My_MSE: 6.2482\n",
            "\n",
            "Epoch 00424: loss did not improve from 0.00063\n",
            "Epoch 425/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.2950e-04 - mse: 6.2950e-04 - My_MSE: 6.2485 - val_loss: 6.1289e-04 - val_mse: 6.1289e-04 - val_My_MSE: 6.2482\n",
            "\n",
            "Epoch 00425: loss improved from 0.00063 to 0.00062, saving model to poids_train.hdf5\n",
            "Epoch 426/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.2212e-04 - mse: 6.2212e-04 - My_MSE: 6.2484 - val_loss: 6.5871e-04 - val_mse: 6.5871e-04 - val_My_MSE: 6.2489\n",
            "\n",
            "Epoch 00426: loss did not improve from 0.00062\n",
            "Epoch 427/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.4042e-04 - mse: 6.4042e-04 - My_MSE: 6.2486 - val_loss: 6.0913e-04 - val_mse: 6.0913e-04 - val_My_MSE: 6.2482\n",
            "\n",
            "Epoch 00427: loss improved from 0.00062 to 0.00062, saving model to poids_train.hdf5\n",
            "Epoch 428/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.9035e-04 - mse: 5.9035e-04 - My_MSE: 6.2479 - val_loss: 6.0431e-04 - val_mse: 6.0431e-04 - val_My_MSE: 6.2481\n",
            "\n",
            "Epoch 00428: loss did not improve from 0.00062\n",
            "Epoch 429/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.2108e-04 - mse: 6.2108e-04 - My_MSE: 6.2483 - val_loss: 6.0088e-04 - val_mse: 6.0088e-04 - val_My_MSE: 6.2480\n",
            "\n",
            "Epoch 00429: loss improved from 0.00062 to 0.00061, saving model to poids_train.hdf5\n",
            "Epoch 430/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.0614e-04 - mse: 6.0614e-04 - My_MSE: 6.2481 - val_loss: 6.3086e-04 - val_mse: 6.3086e-04 - val_My_MSE: 6.2485\n",
            "\n",
            "Epoch 00430: loss did not improve from 0.00061\n",
            "Epoch 431/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.2372e-04 - mse: 6.2372e-04 - My_MSE: 6.2484 - val_loss: 5.9993e-04 - val_mse: 5.9993e-04 - val_My_MSE: 6.2480\n",
            "\n",
            "Epoch 00431: loss improved from 0.00061 to 0.00061, saving model to poids_train.hdf5\n",
            "Epoch 432/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.1874e-04 - mse: 6.1874e-04 - My_MSE: 6.2483 - val_loss: 6.1179e-04 - val_mse: 6.1179e-04 - val_My_MSE: 6.2482\n",
            "\n",
            "Epoch 00432: loss improved from 0.00061 to 0.00061, saving model to poids_train.hdf5\n",
            "Epoch 433/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.1111e-04 - mse: 6.1111e-04 - My_MSE: 6.2482 - val_loss: 6.0131e-04 - val_mse: 6.0131e-04 - val_My_MSE: 6.2480\n",
            "\n",
            "Epoch 00433: loss improved from 0.00061 to 0.00061, saving model to poids_train.hdf5\n",
            "Epoch 434/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.2213e-04 - mse: 6.2213e-04 - My_MSE: 6.2484 - val_loss: 6.0606e-04 - val_mse: 6.0606e-04 - val_My_MSE: 6.2481\n",
            "\n",
            "Epoch 00434: loss improved from 0.00061 to 0.00061, saving model to poids_train.hdf5\n",
            "Epoch 435/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.8000e-04 - mse: 5.8000e-04 - My_MSE: 6.2477 - val_loss: 5.9855e-04 - val_mse: 5.9855e-04 - val_My_MSE: 6.2480\n",
            "\n",
            "Epoch 00435: loss improved from 0.00061 to 0.00060, saving model to poids_train.hdf5\n",
            "Epoch 436/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.0477e-04 - mse: 6.0477e-04 - My_MSE: 6.2481 - val_loss: 5.9329e-04 - val_mse: 5.9329e-04 - val_My_MSE: 6.2479\n",
            "\n",
            "Epoch 00436: loss did not improve from 0.00060\n",
            "Epoch 437/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.3222e-04 - mse: 6.3222e-04 - My_MSE: 6.2485 - val_loss: 6.0064e-04 - val_mse: 6.0064e-04 - val_My_MSE: 6.2480\n",
            "\n",
            "Epoch 00437: loss improved from 0.00060 to 0.00060, saving model to poids_train.hdf5\n",
            "Epoch 438/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.9972e-04 - mse: 5.9972e-04 - My_MSE: 6.2480 - val_loss: 5.9836e-04 - val_mse: 5.9836e-04 - val_My_MSE: 6.2480\n",
            "\n",
            "Epoch 00438: loss improved from 0.00060 to 0.00060, saving model to poids_train.hdf5\n",
            "Epoch 439/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 5.8054e-04 - mse: 5.8054e-04 - My_MSE: 6.2477 - val_loss: 5.8595e-04 - val_mse: 5.8595e-04 - val_My_MSE: 6.2478\n",
            "\n",
            "Epoch 00439: loss improved from 0.00060 to 0.00059, saving model to poids_train.hdf5\n",
            "Epoch 440/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.7635e-04 - mse: 5.7635e-04 - My_MSE: 6.2476 - val_loss: 6.1585e-04 - val_mse: 6.1585e-04 - val_My_MSE: 6.2483\n",
            "\n",
            "Epoch 00440: loss did not improve from 0.00059\n",
            "Epoch 441/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.9268e-04 - mse: 5.9268e-04 - My_MSE: 6.2479 - val_loss: 5.9168e-04 - val_mse: 5.9168e-04 - val_My_MSE: 6.2479\n",
            "\n",
            "Epoch 00441: loss did not improve from 0.00059\n",
            "Epoch 442/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.1134e-04 - mse: 6.1134e-04 - My_MSE: 6.2482 - val_loss: 6.1638e-04 - val_mse: 6.1638e-04 - val_My_MSE: 6.2483\n",
            "\n",
            "Epoch 00442: loss did not improve from 0.00059\n",
            "Epoch 443/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.6332e-04 - mse: 6.6332e-04 - My_MSE: 6.2490 - val_loss: 5.9033e-04 - val_mse: 5.9033e-04 - val_My_MSE: 6.2479\n",
            "\n",
            "Epoch 00443: loss did not improve from 0.00059\n",
            "Epoch 444/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.0321e-04 - mse: 6.0321e-04 - My_MSE: 6.2481 - val_loss: 6.0445e-04 - val_mse: 6.0445e-04 - val_My_MSE: 6.2481\n",
            "\n",
            "Epoch 00444: loss improved from 0.00059 to 0.00059, saving model to poids_train.hdf5\n",
            "Epoch 445/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.9199e-04 - mse: 5.9199e-04 - My_MSE: 6.2479 - val_loss: 6.1021e-04 - val_mse: 6.1021e-04 - val_My_MSE: 6.2482\n",
            "\n",
            "Epoch 00445: loss did not improve from 0.00059\n",
            "Epoch 446/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 6.1983e-04 - mse: 6.1983e-04 - My_MSE: 6.2483 - val_loss: 5.7950e-04 - val_mse: 5.7950e-04 - val_My_MSE: 6.2477\n",
            "\n",
            "Epoch 00446: loss improved from 0.00059 to 0.00059, saving model to poids_train.hdf5\n",
            "Epoch 447/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.5033e-04 - mse: 5.5033e-04 - My_MSE: 6.2472 - val_loss: 5.8197e-04 - val_mse: 5.8197e-04 - val_My_MSE: 6.2477\n",
            "\n",
            "Epoch 00447: loss improved from 0.00059 to 0.00059, saving model to poids_train.hdf5\n",
            "Epoch 448/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.9177e-04 - mse: 5.9177e-04 - My_MSE: 6.2479 - val_loss: 5.9003e-04 - val_mse: 5.9003e-04 - val_My_MSE: 6.2479\n",
            "\n",
            "Epoch 00448: loss improved from 0.00059 to 0.00059, saving model to poids_train.hdf5\n",
            "Epoch 449/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.6157e-04 - mse: 5.6157e-04 - My_MSE: 6.2474 - val_loss: 5.7411e-04 - val_mse: 5.7411e-04 - val_My_MSE: 6.2476\n",
            "\n",
            "Epoch 00449: loss did not improve from 0.00059\n",
            "Epoch 450/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.6128e-04 - mse: 5.6128e-04 - My_MSE: 6.2474 - val_loss: 5.9723e-04 - val_mse: 5.9723e-04 - val_My_MSE: 6.2480\n",
            "\n",
            "Epoch 00450: loss did not improve from 0.00059\n",
            "Epoch 451/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.6489e-04 - mse: 5.6489e-04 - My_MSE: 6.2475 - val_loss: 5.8290e-04 - val_mse: 5.8290e-04 - val_My_MSE: 6.2477\n",
            "\n",
            "Epoch 00451: loss did not improve from 0.00059\n",
            "Epoch 452/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 5.5292e-04 - mse: 5.5292e-04 - My_MSE: 6.2473 - val_loss: 5.9679e-04 - val_mse: 5.9679e-04 - val_My_MSE: 6.2480\n",
            "\n",
            "Epoch 00452: loss improved from 0.00059 to 0.00058, saving model to poids_train.hdf5\n",
            "Epoch 453/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.1387e-04 - mse: 6.1387e-04 - My_MSE: 6.2482 - val_loss: 5.7661e-04 - val_mse: 5.7661e-04 - val_My_MSE: 6.2476\n",
            "\n",
            "Epoch 00453: loss did not improve from 0.00058\n",
            "Epoch 454/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.9753e-04 - mse: 5.9753e-04 - My_MSE: 6.2480 - val_loss: 5.8586e-04 - val_mse: 5.8586e-04 - val_My_MSE: 6.2478\n",
            "\n",
            "Epoch 00454: loss did not improve from 0.00058\n",
            "Epoch 455/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.9802e-04 - mse: 5.9802e-04 - My_MSE: 6.2480 - val_loss: 5.8475e-04 - val_mse: 5.8475e-04 - val_My_MSE: 6.2478\n",
            "\n",
            "Epoch 00455: loss improved from 0.00058 to 0.00058, saving model to poids_train.hdf5\n",
            "Epoch 456/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.5828e-04 - mse: 5.5828e-04 - My_MSE: 6.2474 - val_loss: 5.8696e-04 - val_mse: 5.8696e-04 - val_My_MSE: 6.2478\n",
            "\n",
            "Epoch 00456: loss did not improve from 0.00058\n",
            "Epoch 457/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.4760e-04 - mse: 5.4760e-04 - My_MSE: 6.2472 - val_loss: 5.6712e-04 - val_mse: 5.6712e-04 - val_My_MSE: 6.2475\n",
            "\n",
            "Epoch 00457: loss improved from 0.00058 to 0.00057, saving model to poids_train.hdf5\n",
            "Epoch 458/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.8533e-04 - mse: 5.8533e-04 - My_MSE: 6.2478 - val_loss: 5.8084e-04 - val_mse: 5.8084e-04 - val_My_MSE: 6.2477\n",
            "\n",
            "Epoch 00458: loss did not improve from 0.00057\n",
            "Epoch 459/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.5219e-04 - mse: 5.5219e-04 - My_MSE: 6.2473 - val_loss: 5.7306e-04 - val_mse: 5.7306e-04 - val_My_MSE: 6.2476\n",
            "\n",
            "Epoch 00459: loss improved from 0.00057 to 0.00057, saving model to poids_train.hdf5\n",
            "Epoch 460/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.5317e-04 - mse: 5.5317e-04 - My_MSE: 6.2473 - val_loss: 5.7848e-04 - val_mse: 5.7848e-04 - val_My_MSE: 6.2477\n",
            "\n",
            "Epoch 00460: loss did not improve from 0.00057\n",
            "Epoch 461/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.5551e-04 - mse: 5.5551e-04 - My_MSE: 6.2473 - val_loss: 5.8140e-04 - val_mse: 5.8140e-04 - val_My_MSE: 6.2477\n",
            "\n",
            "Epoch 00461: loss did not improve from 0.00057\n",
            "Epoch 462/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.8361e-04 - mse: 5.8361e-04 - My_MSE: 6.2478 - val_loss: 5.7231e-04 - val_mse: 5.7231e-04 - val_My_MSE: 6.2476\n",
            "\n",
            "Epoch 00462: loss did not improve from 0.00057\n",
            "Epoch 463/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.1016e-04 - mse: 6.1016e-04 - My_MSE: 6.2482 - val_loss: 5.8291e-04 - val_mse: 5.8291e-04 - val_My_MSE: 6.2477\n",
            "\n",
            "Epoch 00463: loss improved from 0.00057 to 0.00057, saving model to poids_train.hdf5\n",
            "Epoch 464/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.5776e-04 - mse: 5.5776e-04 - My_MSE: 6.2474 - val_loss: 5.7416e-04 - val_mse: 5.7416e-04 - val_My_MSE: 6.2476\n",
            "\n",
            "Epoch 00464: loss improved from 0.00057 to 0.00057, saving model to poids_train.hdf5\n",
            "Epoch 465/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.6481e-04 - mse: 5.6481e-04 - My_MSE: 6.2475 - val_loss: 5.6390e-04 - val_mse: 5.6390e-04 - val_My_MSE: 6.2474\n",
            "\n",
            "Epoch 00465: loss did not improve from 0.00057\n",
            "Epoch 466/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.5976e-04 - mse: 5.5976e-04 - My_MSE: 6.2474 - val_loss: 5.7682e-04 - val_mse: 5.7682e-04 - val_My_MSE: 6.2476\n",
            "\n",
            "Epoch 00466: loss improved from 0.00057 to 0.00056, saving model to poids_train.hdf5\n",
            "Epoch 467/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.6729e-04 - mse: 5.6729e-04 - My_MSE: 6.2475 - val_loss: 5.5996e-04 - val_mse: 5.5996e-04 - val_My_MSE: 6.2474\n",
            "\n",
            "Epoch 00467: loss did not improve from 0.00056\n",
            "Epoch 468/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.5962e-04 - mse: 5.5962e-04 - My_MSE: 6.2474 - val_loss: 5.5984e-04 - val_mse: 5.5984e-04 - val_My_MSE: 6.2474\n",
            "\n",
            "Epoch 00468: loss did not improve from 0.00056\n",
            "Epoch 469/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.9243e-04 - mse: 5.9243e-04 - My_MSE: 6.2479 - val_loss: 5.7256e-04 - val_mse: 5.7256e-04 - val_My_MSE: 6.2476\n",
            "\n",
            "Epoch 00469: loss did not improve from 0.00056\n",
            "Epoch 470/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.4766e-04 - mse: 5.4766e-04 - My_MSE: 6.2472 - val_loss: 5.5389e-04 - val_mse: 5.5389e-04 - val_My_MSE: 6.2473\n",
            "\n",
            "Epoch 00470: loss did not improve from 0.00056\n",
            "Epoch 471/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.5617e-04 - mse: 5.5617e-04 - My_MSE: 6.2473 - val_loss: 5.7844e-04 - val_mse: 5.7844e-04 - val_My_MSE: 6.2477\n",
            "\n",
            "Epoch 00471: loss did not improve from 0.00056\n",
            "Epoch 472/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.5194e-04 - mse: 5.5194e-04 - My_MSE: 6.2473 - val_loss: 5.5692e-04 - val_mse: 5.5692e-04 - val_My_MSE: 6.2473\n",
            "\n",
            "Epoch 00472: loss improved from 0.00056 to 0.00056, saving model to poids_train.hdf5\n",
            "Epoch 473/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.8648e-04 - mse: 5.8648e-04 - My_MSE: 6.2478 - val_loss: 5.6015e-04 - val_mse: 5.6015e-04 - val_My_MSE: 6.2474\n",
            "\n",
            "Epoch 00473: loss did not improve from 0.00056\n",
            "Epoch 474/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.2235e-04 - mse: 5.2235e-04 - My_MSE: 6.2468 - val_loss: 5.5755e-04 - val_mse: 5.5755e-04 - val_My_MSE: 6.2473\n",
            "\n",
            "Epoch 00474: loss improved from 0.00056 to 0.00055, saving model to poids_train.hdf5\n",
            "Epoch 475/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.1225e-04 - mse: 5.1225e-04 - My_MSE: 6.2466 - val_loss: 5.5704e-04 - val_mse: 5.5704e-04 - val_My_MSE: 6.2473\n",
            "\n",
            "Epoch 00475: loss did not improve from 0.00055\n",
            "Epoch 476/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.4921e-04 - mse: 5.4921e-04 - My_MSE: 6.2472 - val_loss: 5.5993e-04 - val_mse: 5.5993e-04 - val_My_MSE: 6.2474\n",
            "\n",
            "Epoch 00476: loss did not improve from 0.00055\n",
            "Epoch 477/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.0609e-04 - mse: 6.0609e-04 - My_MSE: 6.2481 - val_loss: 5.5516e-04 - val_mse: 5.5516e-04 - val_My_MSE: 6.2473\n",
            "\n",
            "Epoch 00477: loss did not improve from 0.00055\n",
            "Epoch 478/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 6.0317e-04 - mse: 6.0317e-04 - My_MSE: 6.2481 - val_loss: 5.7772e-04 - val_mse: 5.7772e-04 - val_My_MSE: 6.2477\n",
            "\n",
            "Epoch 00478: loss improved from 0.00055 to 0.00055, saving model to poids_train.hdf5\n",
            "Epoch 479/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.2245e-04 - mse: 5.2245e-04 - My_MSE: 6.2468 - val_loss: 5.4425e-04 - val_mse: 5.4425e-04 - val_My_MSE: 6.2471\n",
            "\n",
            "Epoch 00479: loss improved from 0.00055 to 0.00055, saving model to poids_train.hdf5\n",
            "Epoch 480/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.2088e-04 - mse: 5.2088e-04 - My_MSE: 6.2468 - val_loss: 5.8573e-04 - val_mse: 5.8573e-04 - val_My_MSE: 6.2478\n",
            "\n",
            "Epoch 00480: loss did not improve from 0.00055\n",
            "Epoch 481/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.7331e-04 - mse: 5.7331e-04 - My_MSE: 6.2476 - val_loss: 5.7062e-04 - val_mse: 5.7062e-04 - val_My_MSE: 6.2476\n",
            "\n",
            "Epoch 00481: loss did not improve from 0.00055\n",
            "Epoch 482/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.8338e-04 - mse: 5.8338e-04 - My_MSE: 6.2478 - val_loss: 5.4299e-04 - val_mse: 5.4299e-04 - val_My_MSE: 6.2471\n",
            "\n",
            "Epoch 00482: loss did not improve from 0.00055\n",
            "Epoch 483/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.3716e-04 - mse: 5.3716e-04 - My_MSE: 6.2470 - val_loss: 5.3914e-04 - val_mse: 5.3914e-04 - val_My_MSE: 6.2471\n",
            "\n",
            "Epoch 00483: loss did not improve from 0.00055\n",
            "Epoch 484/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.5333e-04 - mse: 5.5333e-04 - My_MSE: 6.2473 - val_loss: 5.6020e-04 - val_mse: 5.6020e-04 - val_My_MSE: 6.2474\n",
            "\n",
            "Epoch 00484: loss improved from 0.00055 to 0.00055, saving model to poids_train.hdf5\n",
            "Epoch 485/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 5.5734e-04 - mse: 5.5734e-04 - My_MSE: 6.2473 - val_loss: 5.4310e-04 - val_mse: 5.4310e-04 - val_My_MSE: 6.2471\n",
            "\n",
            "Epoch 00485: loss improved from 0.00055 to 0.00054, saving model to poids_train.hdf5\n",
            "Epoch 486/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.5848e-04 - mse: 5.5848e-04 - My_MSE: 6.2474 - val_loss: 5.4933e-04 - val_mse: 5.4933e-04 - val_My_MSE: 6.2472\n",
            "\n",
            "Epoch 00486: loss did not improve from 0.00054\n",
            "Epoch 487/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.6484e-04 - mse: 5.6484e-04 - My_MSE: 6.2475 - val_loss: 5.4644e-04 - val_mse: 5.4644e-04 - val_My_MSE: 6.2472\n",
            "\n",
            "Epoch 00487: loss improved from 0.00054 to 0.00054, saving model to poids_train.hdf5\n",
            "Epoch 488/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.5831e-04 - mse: 5.5831e-04 - My_MSE: 6.2474 - val_loss: 5.4852e-04 - val_mse: 5.4852e-04 - val_My_MSE: 6.2472\n",
            "\n",
            "Epoch 00488: loss improved from 0.00054 to 0.00054, saving model to poids_train.hdf5\n",
            "Epoch 489/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.2146e-04 - mse: 5.2146e-04 - My_MSE: 6.2468 - val_loss: 5.4946e-04 - val_mse: 5.4946e-04 - val_My_MSE: 6.2472\n",
            "\n",
            "Epoch 00489: loss did not improve from 0.00054\n",
            "Epoch 490/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.1808e-04 - mse: 5.1808e-04 - My_MSE: 6.2467 - val_loss: 5.4670e-04 - val_mse: 5.4670e-04 - val_My_MSE: 6.2472\n",
            "\n",
            "Epoch 00490: loss did not improve from 0.00054\n",
            "Epoch 491/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.3941e-04 - mse: 5.3941e-04 - My_MSE: 6.2471 - val_loss: 5.4328e-04 - val_mse: 5.4328e-04 - val_My_MSE: 6.2471\n",
            "\n",
            "Epoch 00491: loss did not improve from 0.00054\n",
            "Epoch 492/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.5437e-04 - mse: 5.5437e-04 - My_MSE: 6.2473 - val_loss: 5.5587e-04 - val_mse: 5.5587e-04 - val_My_MSE: 6.2473\n",
            "\n",
            "Epoch 00492: loss did not improve from 0.00054\n",
            "Epoch 493/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.5519e-04 - mse: 5.5519e-04 - My_MSE: 6.2473 - val_loss: 5.9282e-04 - val_mse: 5.9282e-04 - val_My_MSE: 6.2479\n",
            "\n",
            "Epoch 00493: loss did not improve from 0.00054\n",
            "Epoch 494/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.7434e-04 - mse: 5.7434e-04 - My_MSE: 6.2476 - val_loss: 5.3630e-04 - val_mse: 5.3630e-04 - val_My_MSE: 6.2470\n",
            "\n",
            "Epoch 00494: loss did not improve from 0.00054\n",
            "Epoch 495/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.4315e-04 - mse: 5.4315e-04 - My_MSE: 6.2471 - val_loss: 5.3673e-04 - val_mse: 5.3673e-04 - val_My_MSE: 6.2470\n",
            "\n",
            "Epoch 00495: loss improved from 0.00054 to 0.00054, saving model to poids_train.hdf5\n",
            "Epoch 496/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.5485e-04 - mse: 5.5485e-04 - My_MSE: 6.2473 - val_loss: 5.6135e-04 - val_mse: 5.6135e-04 - val_My_MSE: 6.2474\n",
            "\n",
            "Epoch 00496: loss improved from 0.00054 to 0.00054, saving model to poids_train.hdf5\n",
            "Epoch 497/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.2435e-04 - mse: 5.2435e-04 - My_MSE: 6.2468 - val_loss: 5.3658e-04 - val_mse: 5.3658e-04 - val_My_MSE: 6.2470\n",
            "\n",
            "Epoch 00497: loss improved from 0.00054 to 0.00053, saving model to poids_train.hdf5\n",
            "Epoch 498/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.3928e-04 - mse: 5.3928e-04 - My_MSE: 6.2471 - val_loss: 5.3523e-04 - val_mse: 5.3523e-04 - val_My_MSE: 6.2470\n",
            "\n",
            "Epoch 00498: loss did not improve from 0.00053\n",
            "Epoch 499/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.6953e-04 - mse: 5.6953e-04 - My_MSE: 6.2475 - val_loss: 5.2671e-04 - val_mse: 5.2671e-04 - val_My_MSE: 6.2469\n",
            "\n",
            "Epoch 00499: loss did not improve from 0.00053\n",
            "Epoch 500/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.2958e-04 - mse: 5.2958e-04 - My_MSE: 6.2469 - val_loss: 5.4250e-04 - val_mse: 5.4250e-04 - val_My_MSE: 6.2471\n",
            "\n",
            "Epoch 00500: loss improved from 0.00053 to 0.00053, saving model to poids_train.hdf5\n",
            "Epoch 501/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.0461e-04 - mse: 5.0461e-04 - My_MSE: 6.2465 - val_loss: 5.3029e-04 - val_mse: 5.3029e-04 - val_My_MSE: 6.2469\n",
            "\n",
            "Epoch 00501: loss did not improve from 0.00053\n",
            "Epoch 502/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.1979e-04 - mse: 5.1979e-04 - My_MSE: 6.2468 - val_loss: 5.3437e-04 - val_mse: 5.3437e-04 - val_My_MSE: 6.2470\n",
            "\n",
            "Epoch 00502: loss did not improve from 0.00053\n",
            "Epoch 503/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.5517e-04 - mse: 5.5517e-04 - My_MSE: 6.2473 - val_loss: 5.4458e-04 - val_mse: 5.4458e-04 - val_My_MSE: 6.2471\n",
            "\n",
            "Epoch 00503: loss did not improve from 0.00053\n",
            "Epoch 504/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.4486e-04 - mse: 5.4486e-04 - My_MSE: 6.2472 - val_loss: 5.6592e-04 - val_mse: 5.6592e-04 - val_My_MSE: 6.2475\n",
            "\n",
            "Epoch 00504: loss did not improve from 0.00053\n",
            "Epoch 505/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.2161e-04 - mse: 5.2161e-04 - My_MSE: 6.2468 - val_loss: 5.2982e-04 - val_mse: 5.2982e-04 - val_My_MSE: 6.2469\n",
            "\n",
            "Epoch 00505: loss did not improve from 0.00053\n",
            "Epoch 506/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.2364e-04 - mse: 5.2364e-04 - My_MSE: 6.2468 - val_loss: 5.1807e-04 - val_mse: 5.1807e-04 - val_My_MSE: 6.2467\n",
            "\n",
            "Epoch 00506: loss did not improve from 0.00053\n",
            "Epoch 507/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.4361e-04 - mse: 5.4361e-04 - My_MSE: 6.2471 - val_loss: 5.2592e-04 - val_mse: 5.2592e-04 - val_My_MSE: 6.2469\n",
            "\n",
            "Epoch 00507: loss improved from 0.00053 to 0.00053, saving model to poids_train.hdf5\n",
            "Epoch 508/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.1763e-04 - mse: 5.1763e-04 - My_MSE: 6.2467 - val_loss: 5.3200e-04 - val_mse: 5.3200e-04 - val_My_MSE: 6.2470\n",
            "\n",
            "Epoch 00508: loss did not improve from 0.00053\n",
            "Epoch 509/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.6618e-04 - mse: 5.6618e-04 - My_MSE: 6.2475 - val_loss: 5.2413e-04 - val_mse: 5.2413e-04 - val_My_MSE: 6.2468\n",
            "\n",
            "Epoch 00509: loss did not improve from 0.00053\n",
            "Epoch 510/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.2795e-04 - mse: 5.2795e-04 - My_MSE: 6.2469 - val_loss: 5.2762e-04 - val_mse: 5.2762e-04 - val_My_MSE: 6.2469\n",
            "\n",
            "Epoch 00510: loss improved from 0.00053 to 0.00052, saving model to poids_train.hdf5\n",
            "Epoch 511/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.1723e-04 - mse: 5.1723e-04 - My_MSE: 6.2467 - val_loss: 5.2939e-04 - val_mse: 5.2939e-04 - val_My_MSE: 6.2469\n",
            "\n",
            "Epoch 00511: loss improved from 0.00052 to 0.00052, saving model to poids_train.hdf5\n",
            "Epoch 512/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.1576e-04 - mse: 5.1576e-04 - My_MSE: 6.2467 - val_loss: 5.3660e-04 - val_mse: 5.3660e-04 - val_My_MSE: 6.2470\n",
            "\n",
            "Epoch 00512: loss did not improve from 0.00052\n",
            "Epoch 513/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.1401e-04 - mse: 5.1401e-04 - My_MSE: 6.2467 - val_loss: 5.8656e-04 - val_mse: 5.8656e-04 - val_My_MSE: 6.2478\n",
            "\n",
            "Epoch 00513: loss did not improve from 0.00052\n",
            "Epoch 514/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.6325e-04 - mse: 5.6325e-04 - My_MSE: 6.2474 - val_loss: 5.7148e-04 - val_mse: 5.7148e-04 - val_My_MSE: 6.2476\n",
            "\n",
            "Epoch 00514: loss did not improve from 0.00052\n",
            "Epoch 515/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.2221e-04 - mse: 5.2221e-04 - My_MSE: 6.2468 - val_loss: 5.2812e-04 - val_mse: 5.2812e-04 - val_My_MSE: 6.2469\n",
            "\n",
            "Epoch 00515: loss did not improve from 0.00052\n",
            "Epoch 516/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.4471e-04 - mse: 5.4471e-04 - My_MSE: 6.2471 - val_loss: 5.4994e-04 - val_mse: 5.4994e-04 - val_My_MSE: 6.2472\n",
            "\n",
            "Epoch 00516: loss improved from 0.00052 to 0.00052, saving model to poids_train.hdf5\n",
            "Epoch 517/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.1193e-04 - mse: 5.1193e-04 - My_MSE: 6.2466 - val_loss: 5.3394e-04 - val_mse: 5.3394e-04 - val_My_MSE: 6.2470\n",
            "\n",
            "Epoch 00517: loss did not improve from 0.00052\n",
            "Epoch 518/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.0054e-04 - mse: 5.0054e-04 - My_MSE: 6.2465 - val_loss: 5.0951e-04 - val_mse: 5.0951e-04 - val_My_MSE: 6.2466\n",
            "\n",
            "Epoch 00518: loss did not improve from 0.00052\n",
            "Epoch 519/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.0941e-04 - mse: 5.0941e-04 - My_MSE: 6.2466 - val_loss: 5.4286e-04 - val_mse: 5.4286e-04 - val_My_MSE: 6.2471\n",
            "\n",
            "Epoch 00519: loss improved from 0.00052 to 0.00051, saving model to poids_train.hdf5\n",
            "Epoch 520/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.0498e-04 - mse: 5.0498e-04 - My_MSE: 6.2465 - val_loss: 5.2547e-04 - val_mse: 5.2547e-04 - val_My_MSE: 6.2468\n",
            "\n",
            "Epoch 00520: loss did not improve from 0.00051\n",
            "Epoch 521/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.3456e-04 - mse: 5.3456e-04 - My_MSE: 6.2470 - val_loss: 5.1926e-04 - val_mse: 5.1926e-04 - val_My_MSE: 6.2468\n",
            "\n",
            "Epoch 00521: loss did not improve from 0.00051\n",
            "Epoch 522/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.2725e-04 - mse: 5.2725e-04 - My_MSE: 6.2469 - val_loss: 5.4873e-04 - val_mse: 5.4873e-04 - val_My_MSE: 6.2472\n",
            "\n",
            "Epoch 00522: loss did not improve from 0.00051\n",
            "Epoch 523/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.8202e-04 - mse: 4.8202e-04 - My_MSE: 6.2462 - val_loss: 5.2154e-04 - val_mse: 5.2154e-04 - val_My_MSE: 6.2468\n",
            "\n",
            "Epoch 00523: loss did not improve from 0.00051\n",
            "Epoch 524/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.2549e-04 - mse: 5.2549e-04 - My_MSE: 6.2468 - val_loss: 5.3885e-04 - val_mse: 5.3885e-04 - val_My_MSE: 6.2471\n",
            "\n",
            "Epoch 00524: loss did not improve from 0.00051\n",
            "Epoch 525/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.7100e-04 - mse: 5.7100e-04 - My_MSE: 6.2476 - val_loss: 5.1535e-04 - val_mse: 5.1535e-04 - val_My_MSE: 6.2467\n",
            "\n",
            "Epoch 00525: loss improved from 0.00051 to 0.00051, saving model to poids_train.hdf5\n",
            "Epoch 526/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.9163e-04 - mse: 4.9163e-04 - My_MSE: 6.2463 - val_loss: 5.1516e-04 - val_mse: 5.1516e-04 - val_My_MSE: 6.2467\n",
            "\n",
            "Epoch 00526: loss improved from 0.00051 to 0.00050, saving model to poids_train.hdf5\n",
            "Epoch 527/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.1187e-04 - mse: 5.1187e-04 - My_MSE: 6.2466 - val_loss: 5.1803e-04 - val_mse: 5.1803e-04 - val_My_MSE: 6.2467\n",
            "\n",
            "Epoch 00527: loss did not improve from 0.00050\n",
            "Epoch 528/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.8557e-04 - mse: 4.8557e-04 - My_MSE: 6.2462 - val_loss: 5.0915e-04 - val_mse: 5.0915e-04 - val_My_MSE: 6.2466\n",
            "\n",
            "Epoch 00528: loss did not improve from 0.00050\n",
            "Epoch 529/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.2023e-04 - mse: 5.2023e-04 - My_MSE: 6.2468 - val_loss: 5.0621e-04 - val_mse: 5.0621e-04 - val_My_MSE: 6.2465\n",
            "\n",
            "Epoch 00529: loss did not improve from 0.00050\n",
            "Epoch 530/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 4.8006e-04 - mse: 4.8006e-04 - My_MSE: 6.2461 - val_loss: 5.2759e-04 - val_mse: 5.2759e-04 - val_My_MSE: 6.2469\n",
            "\n",
            "Epoch 00530: loss did not improve from 0.00050\n",
            "Epoch 531/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.2790e-04 - mse: 5.2790e-04 - My_MSE: 6.2469 - val_loss: 5.3808e-04 - val_mse: 5.3808e-04 - val_My_MSE: 6.2470\n",
            "\n",
            "Epoch 00531: loss did not improve from 0.00050\n",
            "Epoch 532/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.1697e-04 - mse: 5.1697e-04 - My_MSE: 6.2467 - val_loss: 5.2999e-04 - val_mse: 5.2999e-04 - val_My_MSE: 6.2469\n",
            "\n",
            "Epoch 00532: loss did not improve from 0.00050\n",
            "Epoch 533/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.0204e-04 - mse: 5.0204e-04 - My_MSE: 6.2465 - val_loss: 5.0483e-04 - val_mse: 5.0483e-04 - val_My_MSE: 6.2465\n",
            "\n",
            "Epoch 00533: loss did not improve from 0.00050\n",
            "Epoch 534/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.0560e-04 - mse: 5.0560e-04 - My_MSE: 6.2465 - val_loss: 4.9082e-04 - val_mse: 4.9082e-04 - val_My_MSE: 6.2463\n",
            "\n",
            "Epoch 00534: loss improved from 0.00050 to 0.00050, saving model to poids_train.hdf5\n",
            "Epoch 535/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.9539e-04 - mse: 4.9539e-04 - My_MSE: 6.2464 - val_loss: 4.9668e-04 - val_mse: 4.9668e-04 - val_My_MSE: 6.2464\n",
            "\n",
            "Epoch 00535: loss improved from 0.00050 to 0.00050, saving model to poids_train.hdf5\n",
            "Epoch 536/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.2238e-04 - mse: 5.2238e-04 - My_MSE: 6.2468 - val_loss: 5.0559e-04 - val_mse: 5.0559e-04 - val_My_MSE: 6.2465\n",
            "\n",
            "Epoch 00536: loss did not improve from 0.00050\n",
            "Epoch 537/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.1797e-04 - mse: 5.1797e-04 - My_MSE: 6.2467 - val_loss: 5.0995e-04 - val_mse: 5.0995e-04 - val_My_MSE: 6.2466\n",
            "\n",
            "Epoch 00537: loss did not improve from 0.00050\n",
            "Epoch 538/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.0098e-04 - mse: 5.0098e-04 - My_MSE: 6.2465 - val_loss: 5.1000e-04 - val_mse: 5.1000e-04 - val_My_MSE: 6.2466\n",
            "\n",
            "Epoch 00538: loss did not improve from 0.00050\n",
            "Epoch 539/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.7594e-04 - mse: 4.7594e-04 - My_MSE: 6.2461 - val_loss: 4.9291e-04 - val_mse: 4.9291e-04 - val_My_MSE: 6.2463\n",
            "\n",
            "Epoch 00539: loss improved from 0.00050 to 0.00050, saving model to poids_train.hdf5\n",
            "Epoch 540/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.8992e-04 - mse: 4.8992e-04 - My_MSE: 6.2463 - val_loss: 5.0818e-04 - val_mse: 5.0818e-04 - val_My_MSE: 6.2466\n",
            "\n",
            "Epoch 00540: loss improved from 0.00050 to 0.00049, saving model to poids_train.hdf5\n",
            "Epoch 541/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.0764e-04 - mse: 5.0764e-04 - My_MSE: 6.2466 - val_loss: 5.1178e-04 - val_mse: 5.1178e-04 - val_My_MSE: 6.2466\n",
            "\n",
            "Epoch 00541: loss did not improve from 0.00049\n",
            "Epoch 542/1000\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 4.7144e-04 - mse: 4.7144e-04 - My_MSE: 6.2460 - val_loss: 4.8949e-04 - val_mse: 4.8949e-04 - val_My_MSE: 6.2463\n",
            "\n",
            "Epoch 00542: loss improved from 0.00049 to 0.00049, saving model to poids_train.hdf5\n",
            "Epoch 543/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.8457e-04 - mse: 4.8457e-04 - My_MSE: 6.2462 - val_loss: 4.8679e-04 - val_mse: 4.8679e-04 - val_My_MSE: 6.2462\n",
            "\n",
            "Epoch 00543: loss did not improve from 0.00049\n",
            "Epoch 544/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.7737e-04 - mse: 4.7737e-04 - My_MSE: 6.2461 - val_loss: 5.2689e-04 - val_mse: 5.2689e-04 - val_My_MSE: 6.2469\n",
            "\n",
            "Epoch 00544: loss did not improve from 0.00049\n",
            "Epoch 545/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 4.9871e-04 - mse: 4.9871e-04 - My_MSE: 6.2464 - val_loss: 5.0344e-04 - val_mse: 5.0344e-04 - val_My_MSE: 6.2465\n",
            "\n",
            "Epoch 00545: loss did not improve from 0.00049\n",
            "Epoch 546/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.9801e-04 - mse: 4.9801e-04 - My_MSE: 6.2464 - val_loss: 4.9561e-04 - val_mse: 4.9561e-04 - val_My_MSE: 6.2464\n",
            "\n",
            "Epoch 00546: loss did not improve from 0.00049\n",
            "Epoch 547/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.0748e-04 - mse: 5.0748e-04 - My_MSE: 6.2466 - val_loss: 4.9151e-04 - val_mse: 4.9151e-04 - val_My_MSE: 6.2463\n",
            "\n",
            "Epoch 00547: loss did not improve from 0.00049\n",
            "Epoch 548/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.8326e-04 - mse: 4.8326e-04 - My_MSE: 6.2462 - val_loss: 5.0282e-04 - val_mse: 5.0282e-04 - val_My_MSE: 6.2465\n",
            "\n",
            "Epoch 00548: loss did not improve from 0.00049\n",
            "Epoch 549/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.1648e-04 - mse: 5.1648e-04 - My_MSE: 6.2467 - val_loss: 5.1686e-04 - val_mse: 5.1686e-04 - val_My_MSE: 6.2467\n",
            "\n",
            "Epoch 00549: loss did not improve from 0.00049\n",
            "Epoch 550/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.1408e-04 - mse: 5.1408e-04 - My_MSE: 6.2467 - val_loss: 4.9540e-04 - val_mse: 4.9540e-04 - val_My_MSE: 6.2464\n",
            "\n",
            "Epoch 00550: loss did not improve from 0.00049\n",
            "Epoch 551/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.5488e-04 - mse: 5.5488e-04 - My_MSE: 6.2473 - val_loss: 5.2666e-04 - val_mse: 5.2666e-04 - val_My_MSE: 6.2469\n",
            "\n",
            "Epoch 00551: loss did not improve from 0.00049\n",
            "Epoch 552/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.8251e-04 - mse: 4.8251e-04 - My_MSE: 6.2462 - val_loss: 4.9617e-04 - val_mse: 4.9617e-04 - val_My_MSE: 6.2464\n",
            "\n",
            "Epoch 00552: loss did not improve from 0.00049\n",
            "Epoch 553/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.6165e-04 - mse: 4.6165e-04 - My_MSE: 6.2459 - val_loss: 4.8481e-04 - val_mse: 4.8481e-04 - val_My_MSE: 6.2462\n",
            "\n",
            "Epoch 00553: loss improved from 0.00049 to 0.00048, saving model to poids_train.hdf5\n",
            "Epoch 554/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.9356e-04 - mse: 4.9356e-04 - My_MSE: 6.2464 - val_loss: 4.7115e-04 - val_mse: 4.7115e-04 - val_My_MSE: 6.2460\n",
            "\n",
            "Epoch 00554: loss did not improve from 0.00048\n",
            "Epoch 555/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.5838e-04 - mse: 4.5838e-04 - My_MSE: 6.2458 - val_loss: 4.8371e-04 - val_mse: 4.8371e-04 - val_My_MSE: 6.2462\n",
            "\n",
            "Epoch 00555: loss did not improve from 0.00048\n",
            "Epoch 556/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 4.8192e-04 - mse: 4.8192e-04 - My_MSE: 6.2462 - val_loss: 4.8095e-04 - val_mse: 4.8095e-04 - val_My_MSE: 6.2462\n",
            "\n",
            "Epoch 00556: loss improved from 0.00048 to 0.00048, saving model to poids_train.hdf5\n",
            "Epoch 557/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.9197e-04 - mse: 4.9197e-04 - My_MSE: 6.2463 - val_loss: 5.5366e-04 - val_mse: 5.5366e-04 - val_My_MSE: 6.2473\n",
            "\n",
            "Epoch 00557: loss did not improve from 0.00048\n",
            "Epoch 558/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.0866e-04 - mse: 5.0866e-04 - My_MSE: 6.2466 - val_loss: 4.9815e-04 - val_mse: 4.9815e-04 - val_My_MSE: 6.2464\n",
            "\n",
            "Epoch 00558: loss did not improve from 0.00048\n",
            "Epoch 559/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.7947e-04 - mse: 4.7947e-04 - My_MSE: 6.2461 - val_loss: 4.9860e-04 - val_mse: 4.9860e-04 - val_My_MSE: 6.2464\n",
            "\n",
            "Epoch 00559: loss improved from 0.00048 to 0.00048, saving model to poids_train.hdf5\n",
            "Epoch 560/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.9358e-04 - mse: 4.9358e-04 - My_MSE: 6.2464 - val_loss: 4.9302e-04 - val_mse: 4.9302e-04 - val_My_MSE: 6.2463\n",
            "\n",
            "Epoch 00560: loss improved from 0.00048 to 0.00047, saving model to poids_train.hdf5\n",
            "Epoch 561/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.8695e-04 - mse: 4.8695e-04 - My_MSE: 6.2462 - val_loss: 4.9158e-04 - val_mse: 4.9158e-04 - val_My_MSE: 6.2463\n",
            "\n",
            "Epoch 00561: loss did not improve from 0.00047\n",
            "Epoch 562/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.8886e-04 - mse: 4.8886e-04 - My_MSE: 6.2463 - val_loss: 4.7140e-04 - val_mse: 4.7140e-04 - val_My_MSE: 6.2460\n",
            "\n",
            "Epoch 00562: loss improved from 0.00047 to 0.00047, saving model to poids_train.hdf5\n",
            "Epoch 563/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.8924e-04 - mse: 4.8924e-04 - My_MSE: 6.2463 - val_loss: 4.7893e-04 - val_mse: 4.7893e-04 - val_My_MSE: 6.2461\n",
            "\n",
            "Epoch 00563: loss improved from 0.00047 to 0.00047, saving model to poids_train.hdf5\n",
            "Epoch 564/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.8368e-04 - mse: 4.8368e-04 - My_MSE: 6.2462 - val_loss: 4.6152e-04 - val_mse: 4.6152e-04 - val_My_MSE: 6.2458\n",
            "\n",
            "Epoch 00564: loss improved from 0.00047 to 0.00047, saving model to poids_train.hdf5\n",
            "Epoch 565/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.5677e-04 - mse: 4.5677e-04 - My_MSE: 6.2458 - val_loss: 4.7781e-04 - val_mse: 4.7781e-04 - val_My_MSE: 6.2461\n",
            "\n",
            "Epoch 00565: loss improved from 0.00047 to 0.00046, saving model to poids_train.hdf5\n",
            "Epoch 566/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.6326e-04 - mse: 4.6326e-04 - My_MSE: 6.2459 - val_loss: 4.6569e-04 - val_mse: 4.6569e-04 - val_My_MSE: 6.2459\n",
            "\n",
            "Epoch 00566: loss did not improve from 0.00046\n",
            "Epoch 567/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.6761e-04 - mse: 4.6761e-04 - My_MSE: 6.2459 - val_loss: 4.6119e-04 - val_mse: 4.6119e-04 - val_My_MSE: 6.2458\n",
            "\n",
            "Epoch 00567: loss did not improve from 0.00046\n",
            "Epoch 568/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.6457e-04 - mse: 4.6457e-04 - My_MSE: 6.2459 - val_loss: 4.8419e-04 - val_mse: 4.8419e-04 - val_My_MSE: 6.2462\n",
            "\n",
            "Epoch 00568: loss did not improve from 0.00046\n",
            "Epoch 569/1000\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 4.8107e-04 - mse: 4.8107e-04 - My_MSE: 6.2462 - val_loss: 4.7882e-04 - val_mse: 4.7882e-04 - val_My_MSE: 6.2461\n",
            "\n",
            "Epoch 00569: loss did not improve from 0.00046\n",
            "Epoch 570/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.7152e-04 - mse: 4.7152e-04 - My_MSE: 6.2460 - val_loss: 4.6662e-04 - val_mse: 4.6662e-04 - val_My_MSE: 6.2459\n",
            "\n",
            "Epoch 00570: loss did not improve from 0.00046\n",
            "Epoch 571/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.5342e-04 - mse: 4.5342e-04 - My_MSE: 6.2457 - val_loss: 4.6534e-04 - val_mse: 4.6534e-04 - val_My_MSE: 6.2459\n",
            "\n",
            "Epoch 00571: loss improved from 0.00046 to 0.00046, saving model to poids_train.hdf5\n",
            "Epoch 572/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 4.4695e-04 - mse: 4.4695e-04 - My_MSE: 6.2456 - val_loss: 4.8961e-04 - val_mse: 4.8961e-04 - val_My_MSE: 6.2463\n",
            "\n",
            "Epoch 00572: loss did not improve from 0.00046\n",
            "Epoch 573/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.9317e-04 - mse: 4.9317e-04 - My_MSE: 6.2463 - val_loss: 4.6937e-04 - val_mse: 4.6937e-04 - val_My_MSE: 6.2460\n",
            "\n",
            "Epoch 00573: loss did not improve from 0.00046\n",
            "Epoch 574/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.7390e-04 - mse: 4.7390e-04 - My_MSE: 6.2460 - val_loss: 4.5977e-04 - val_mse: 4.5977e-04 - val_My_MSE: 6.2458\n",
            "\n",
            "Epoch 00574: loss did not improve from 0.00046\n",
            "Epoch 575/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.4690e-04 - mse: 4.4690e-04 - My_MSE: 6.2456 - val_loss: 4.5850e-04 - val_mse: 4.5850e-04 - val_My_MSE: 6.2458\n",
            "\n",
            "Epoch 00575: loss improved from 0.00046 to 0.00046, saving model to poids_train.hdf5\n",
            "Epoch 576/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.4194e-04 - mse: 4.4194e-04 - My_MSE: 6.2455 - val_loss: 4.7435e-04 - val_mse: 4.7435e-04 - val_My_MSE: 6.2461\n",
            "\n",
            "Epoch 00576: loss did not improve from 0.00046\n",
            "Epoch 577/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.7645e-04 - mse: 4.7645e-04 - My_MSE: 6.2461 - val_loss: 4.9730e-04 - val_mse: 4.9730e-04 - val_My_MSE: 6.2464\n",
            "\n",
            "Epoch 00577: loss did not improve from 0.00046\n",
            "Epoch 578/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.7254e-04 - mse: 4.7254e-04 - My_MSE: 6.2460 - val_loss: 4.6013e-04 - val_mse: 4.6013e-04 - val_My_MSE: 6.2458\n",
            "\n",
            "Epoch 00578: loss did not improve from 0.00046\n",
            "Epoch 579/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.3100e-04 - mse: 4.3100e-04 - My_MSE: 6.2454 - val_loss: 4.7372e-04 - val_mse: 4.7372e-04 - val_My_MSE: 6.2460\n",
            "\n",
            "Epoch 00579: loss improved from 0.00046 to 0.00046, saving model to poids_train.hdf5\n",
            "Epoch 580/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.5186e-04 - mse: 4.5186e-04 - My_MSE: 6.2457 - val_loss: 4.5861e-04 - val_mse: 4.5861e-04 - val_My_MSE: 6.2458\n",
            "\n",
            "Epoch 00580: loss improved from 0.00046 to 0.00046, saving model to poids_train.hdf5\n",
            "Epoch 581/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 4.5290e-04 - mse: 4.5290e-04 - My_MSE: 6.2457 - val_loss: 4.7485e-04 - val_mse: 4.7485e-04 - val_My_MSE: 6.2461\n",
            "\n",
            "Epoch 00581: loss improved from 0.00046 to 0.00045, saving model to poids_train.hdf5\n",
            "Epoch 582/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.5385e-04 - mse: 4.5385e-04 - My_MSE: 6.2457 - val_loss: 4.5030e-04 - val_mse: 4.5030e-04 - val_My_MSE: 6.2457\n",
            "\n",
            "Epoch 00582: loss improved from 0.00045 to 0.00045, saving model to poids_train.hdf5\n",
            "Epoch 583/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.6047e-04 - mse: 4.6047e-04 - My_MSE: 6.2458 - val_loss: 4.7701e-04 - val_mse: 4.7701e-04 - val_My_MSE: 6.2461\n",
            "\n",
            "Epoch 00583: loss did not improve from 0.00045\n",
            "Epoch 584/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 5.1318e-04 - mse: 5.1318e-04 - My_MSE: 6.2467 - val_loss: 4.4738e-04 - val_mse: 4.4738e-04 - val_My_MSE: 6.2456\n",
            "\n",
            "Epoch 00584: loss did not improve from 0.00045\n",
            "Epoch 585/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.2208e-04 - mse: 4.2208e-04 - My_MSE: 6.2452 - val_loss: 4.7443e-04 - val_mse: 4.7443e-04 - val_My_MSE: 6.2461\n",
            "\n",
            "Epoch 00585: loss improved from 0.00045 to 0.00045, saving model to poids_train.hdf5\n",
            "Epoch 586/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.7328e-04 - mse: 4.7328e-04 - My_MSE: 6.2460 - val_loss: 4.9743e-04 - val_mse: 4.9743e-04 - val_My_MSE: 6.2464\n",
            "\n",
            "Epoch 00586: loss did not improve from 0.00045\n",
            "Epoch 587/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.5483e-04 - mse: 4.5483e-04 - My_MSE: 6.2457 - val_loss: 4.5256e-04 - val_mse: 4.5256e-04 - val_My_MSE: 6.2457\n",
            "\n",
            "Epoch 00587: loss did not improve from 0.00045\n",
            "Epoch 588/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.4141e-04 - mse: 4.4141e-04 - My_MSE: 6.2455 - val_loss: 4.5195e-04 - val_mse: 4.5195e-04 - val_My_MSE: 6.2457\n",
            "\n",
            "Epoch 00588: loss improved from 0.00045 to 0.00045, saving model to poids_train.hdf5\n",
            "Epoch 589/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.2860e-04 - mse: 4.2860e-04 - My_MSE: 6.2453 - val_loss: 4.4904e-04 - val_mse: 4.4904e-04 - val_My_MSE: 6.2457\n",
            "\n",
            "Epoch 00589: loss did not improve from 0.00045\n",
            "Epoch 590/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.4717e-04 - mse: 4.4717e-04 - My_MSE: 6.2456 - val_loss: 4.4092e-04 - val_mse: 4.4092e-04 - val_My_MSE: 6.2455\n",
            "\n",
            "Epoch 00590: loss did not improve from 0.00045\n",
            "Epoch 591/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.5853e-04 - mse: 4.5853e-04 - My_MSE: 6.2458 - val_loss: 4.4767e-04 - val_mse: 4.4767e-04 - val_My_MSE: 6.2456\n",
            "\n",
            "Epoch 00591: loss did not improve from 0.00045\n",
            "Epoch 592/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.3700e-04 - mse: 4.3700e-04 - My_MSE: 6.2455 - val_loss: 4.4977e-04 - val_mse: 4.4977e-04 - val_My_MSE: 6.2457\n",
            "\n",
            "Epoch 00592: loss did not improve from 0.00045\n",
            "Epoch 593/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.6543e-04 - mse: 4.6543e-04 - My_MSE: 6.2459 - val_loss: 4.4612e-04 - val_mse: 4.4612e-04 - val_My_MSE: 6.2456\n",
            "\n",
            "Epoch 00593: loss did not improve from 0.00045\n",
            "Epoch 594/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.3358e-04 - mse: 4.3358e-04 - My_MSE: 6.2454 - val_loss: 4.3578e-04 - val_mse: 4.3578e-04 - val_My_MSE: 6.2454\n",
            "\n",
            "Epoch 00594: loss improved from 0.00045 to 0.00044, saving model to poids_train.hdf5\n",
            "Epoch 595/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.3706e-04 - mse: 4.3706e-04 - My_MSE: 6.2455 - val_loss: 4.3944e-04 - val_mse: 4.3944e-04 - val_My_MSE: 6.2455\n",
            "\n",
            "Epoch 00595: loss did not improve from 0.00044\n",
            "Epoch 596/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.6037e-04 - mse: 4.6037e-04 - My_MSE: 6.2458 - val_loss: 4.3925e-04 - val_mse: 4.3925e-04 - val_My_MSE: 6.2455\n",
            "\n",
            "Epoch 00596: loss did not improve from 0.00044\n",
            "Epoch 597/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.3540e-04 - mse: 4.3540e-04 - My_MSE: 6.2454 - val_loss: 4.4649e-04 - val_mse: 4.4649e-04 - val_My_MSE: 6.2456\n",
            "\n",
            "Epoch 00597: loss improved from 0.00044 to 0.00044, saving model to poids_train.hdf5\n",
            "Epoch 598/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.5687e-04 - mse: 4.5687e-04 - My_MSE: 6.2458 - val_loss: 4.6108e-04 - val_mse: 4.6108e-04 - val_My_MSE: 6.2458\n",
            "\n",
            "Epoch 00598: loss improved from 0.00044 to 0.00044, saving model to poids_train.hdf5\n",
            "Epoch 599/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.3184e-04 - mse: 4.3184e-04 - My_MSE: 6.2454 - val_loss: 4.6177e-04 - val_mse: 4.6177e-04 - val_My_MSE: 6.2459\n",
            "\n",
            "Epoch 00599: loss did not improve from 0.00044\n",
            "Epoch 600/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.3532e-04 - mse: 4.3532e-04 - My_MSE: 6.2454 - val_loss: 4.4372e-04 - val_mse: 4.4372e-04 - val_My_MSE: 6.2456\n",
            "\n",
            "Epoch 00600: loss improved from 0.00044 to 0.00044, saving model to poids_train.hdf5\n",
            "Epoch 601/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.3317e-04 - mse: 4.3317e-04 - My_MSE: 6.2454 - val_loss: 4.3448e-04 - val_mse: 4.3448e-04 - val_My_MSE: 6.2454\n",
            "\n",
            "Epoch 00601: loss did not improve from 0.00044\n",
            "Epoch 602/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.4142e-04 - mse: 4.4142e-04 - My_MSE: 6.2455 - val_loss: 4.4814e-04 - val_mse: 4.4814e-04 - val_My_MSE: 6.2456\n",
            "\n",
            "Epoch 00602: loss did not improve from 0.00044\n",
            "Epoch 603/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.3368e-04 - mse: 4.3368e-04 - My_MSE: 6.2454 - val_loss: 4.3562e-04 - val_mse: 4.3562e-04 - val_My_MSE: 6.2454\n",
            "\n",
            "Epoch 00603: loss did not improve from 0.00044\n",
            "Epoch 604/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.1213e-04 - mse: 4.1213e-04 - My_MSE: 6.2451 - val_loss: 4.3032e-04 - val_mse: 4.3032e-04 - val_My_MSE: 6.2454\n",
            "\n",
            "Epoch 00604: loss did not improve from 0.00044\n",
            "Epoch 605/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.8180e-04 - mse: 4.8180e-04 - My_MSE: 6.2462 - val_loss: 4.4092e-04 - val_mse: 4.4092e-04 - val_My_MSE: 6.2455\n",
            "\n",
            "Epoch 00605: loss did not improve from 0.00044\n",
            "Epoch 606/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 4.3502e-04 - mse: 4.3502e-04 - My_MSE: 6.2454 - val_loss: 4.3779e-04 - val_mse: 4.3779e-04 - val_My_MSE: 6.2455\n",
            "\n",
            "Epoch 00606: loss improved from 0.00044 to 0.00043, saving model to poids_train.hdf5\n",
            "Epoch 607/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.1346e-04 - mse: 4.1346e-04 - My_MSE: 6.2451 - val_loss: 4.3122e-04 - val_mse: 4.3122e-04 - val_My_MSE: 6.2454\n",
            "\n",
            "Epoch 00607: loss improved from 0.00043 to 0.00043, saving model to poids_train.hdf5\n",
            "Epoch 608/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.3488e-04 - mse: 4.3488e-04 - My_MSE: 6.2454 - val_loss: 4.2830e-04 - val_mse: 4.2830e-04 - val_My_MSE: 6.2453\n",
            "\n",
            "Epoch 00608: loss did not improve from 0.00043\n",
            "Epoch 609/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.4773e-04 - mse: 4.4773e-04 - My_MSE: 6.2456 - val_loss: 4.4742e-04 - val_mse: 4.4742e-04 - val_My_MSE: 6.2456\n",
            "\n",
            "Epoch 00609: loss did not improve from 0.00043\n",
            "Epoch 610/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.6052e-04 - mse: 4.6052e-04 - My_MSE: 6.2458 - val_loss: 4.3495e-04 - val_mse: 4.3495e-04 - val_My_MSE: 6.2454\n",
            "\n",
            "Epoch 00610: loss did not improve from 0.00043\n",
            "Epoch 611/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.2259e-04 - mse: 4.2259e-04 - My_MSE: 6.2452 - val_loss: 4.5742e-04 - val_mse: 4.5742e-04 - val_My_MSE: 6.2458\n",
            "\n",
            "Epoch 00611: loss did not improve from 0.00043\n",
            "Epoch 612/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.3137e-04 - mse: 4.3137e-04 - My_MSE: 6.2454 - val_loss: 4.4724e-04 - val_mse: 4.4724e-04 - val_My_MSE: 6.2456\n",
            "\n",
            "Epoch 00612: loss did not improve from 0.00043\n",
            "Epoch 613/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.3351e-04 - mse: 4.3351e-04 - My_MSE: 6.2454 - val_loss: 4.3060e-04 - val_mse: 4.3060e-04 - val_My_MSE: 6.2454\n",
            "\n",
            "Epoch 00613: loss improved from 0.00043 to 0.00043, saving model to poids_train.hdf5\n",
            "Epoch 614/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.1571e-04 - mse: 4.1571e-04 - My_MSE: 6.2451 - val_loss: 4.4383e-04 - val_mse: 4.4383e-04 - val_My_MSE: 6.2456\n",
            "\n",
            "Epoch 00614: loss did not improve from 0.00043\n",
            "Epoch 615/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.2531e-04 - mse: 4.2531e-04 - My_MSE: 6.2453 - val_loss: 4.5399e-04 - val_mse: 4.5399e-04 - val_My_MSE: 6.2457\n",
            "\n",
            "Epoch 00615: loss did not improve from 0.00043\n",
            "Epoch 616/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.5372e-04 - mse: 4.5372e-04 - My_MSE: 6.2457 - val_loss: 4.4344e-04 - val_mse: 4.4344e-04 - val_My_MSE: 6.2456\n",
            "\n",
            "Epoch 00616: loss did not improve from 0.00043\n",
            "Epoch 617/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.3342e-04 - mse: 4.3342e-04 - My_MSE: 6.2454 - val_loss: 4.4153e-04 - val_mse: 4.4153e-04 - val_My_MSE: 6.2455\n",
            "\n",
            "Epoch 00617: loss did not improve from 0.00043\n",
            "Epoch 618/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.3654e-04 - mse: 4.3654e-04 - My_MSE: 6.2455 - val_loss: 4.4436e-04 - val_mse: 4.4436e-04 - val_My_MSE: 6.2456\n",
            "\n",
            "Epoch 00618: loss did not improve from 0.00043\n",
            "Epoch 619/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.2189e-04 - mse: 4.2189e-04 - My_MSE: 6.2452 - val_loss: 4.2844e-04 - val_mse: 4.2844e-04 - val_My_MSE: 6.2453\n",
            "\n",
            "Epoch 00619: loss did not improve from 0.00043\n",
            "Epoch 620/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.2211e-04 - mse: 4.2211e-04 - My_MSE: 6.2452 - val_loss: 4.2887e-04 - val_mse: 4.2887e-04 - val_My_MSE: 6.2453\n",
            "\n",
            "Epoch 00620: loss improved from 0.00043 to 0.00043, saving model to poids_train.hdf5\n",
            "Epoch 621/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.2699e-04 - mse: 4.2699e-04 - My_MSE: 6.2453 - val_loss: 4.3539e-04 - val_mse: 4.3539e-04 - val_My_MSE: 6.2454\n",
            "\n",
            "Epoch 00621: loss did not improve from 0.00043\n",
            "Epoch 622/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.2403e-04 - mse: 4.2403e-04 - My_MSE: 6.2453 - val_loss: 4.3181e-04 - val_mse: 4.3181e-04 - val_My_MSE: 6.2454\n",
            "\n",
            "Epoch 00622: loss did not improve from 0.00043\n",
            "Epoch 623/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.3872e-04 - mse: 4.3872e-04 - My_MSE: 6.2455 - val_loss: 4.2833e-04 - val_mse: 4.2833e-04 - val_My_MSE: 6.2453\n",
            "\n",
            "Epoch 00623: loss did not improve from 0.00043\n",
            "Epoch 624/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.2010e-04 - mse: 4.2010e-04 - My_MSE: 6.2452 - val_loss: 4.2478e-04 - val_mse: 4.2478e-04 - val_My_MSE: 6.2453\n",
            "\n",
            "Epoch 00624: loss improved from 0.00043 to 0.00042, saving model to poids_train.hdf5\n",
            "Epoch 625/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.9929e-04 - mse: 3.9929e-04 - My_MSE: 6.2449 - val_loss: 4.2098e-04 - val_mse: 4.2098e-04 - val_My_MSE: 6.2452\n",
            "\n",
            "Epoch 00625: loss improved from 0.00042 to 0.00042, saving model to poids_train.hdf5\n",
            "Epoch 626/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.4139e-04 - mse: 4.4139e-04 - My_MSE: 6.2455 - val_loss: 4.4810e-04 - val_mse: 4.4810e-04 - val_My_MSE: 6.2456\n",
            "\n",
            "Epoch 00626: loss did not improve from 0.00042\n",
            "Epoch 627/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.3327e-04 - mse: 4.3327e-04 - My_MSE: 6.2454 - val_loss: 4.2588e-04 - val_mse: 4.2588e-04 - val_My_MSE: 6.2453\n",
            "\n",
            "Epoch 00627: loss did not improve from 0.00042\n",
            "Epoch 628/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.1941e-04 - mse: 4.1941e-04 - My_MSE: 6.2452 - val_loss: 4.2739e-04 - val_mse: 4.2739e-04 - val_My_MSE: 6.2453\n",
            "\n",
            "Epoch 00628: loss did not improve from 0.00042\n",
            "Epoch 629/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.2222e-04 - mse: 4.2222e-04 - My_MSE: 6.2452 - val_loss: 4.3670e-04 - val_mse: 4.3670e-04 - val_My_MSE: 6.2455\n",
            "\n",
            "Epoch 00629: loss did not improve from 0.00042\n",
            "Epoch 630/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.6326e-04 - mse: 4.6326e-04 - My_MSE: 6.2459 - val_loss: 4.4516e-04 - val_mse: 4.4516e-04 - val_My_MSE: 6.2456\n",
            "\n",
            "Epoch 00630: loss did not improve from 0.00042\n",
            "Epoch 631/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.6098e-04 - mse: 4.6098e-04 - My_MSE: 6.2458 - val_loss: 4.1968e-04 - val_mse: 4.1968e-04 - val_My_MSE: 6.2452\n",
            "\n",
            "Epoch 00631: loss did not improve from 0.00042\n",
            "Epoch 632/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.2522e-04 - mse: 4.2522e-04 - My_MSE: 6.2453 - val_loss: 4.1623e-04 - val_mse: 4.1623e-04 - val_My_MSE: 6.2451\n",
            "\n",
            "Epoch 00632: loss did not improve from 0.00042\n",
            "Epoch 633/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.2730e-04 - mse: 4.2730e-04 - My_MSE: 6.2453 - val_loss: 4.1237e-04 - val_mse: 4.1237e-04 - val_My_MSE: 6.2451\n",
            "\n",
            "Epoch 00633: loss did not improve from 0.00042\n",
            "Epoch 634/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.2317e-04 - mse: 4.2317e-04 - My_MSE: 6.2453 - val_loss: 4.1798e-04 - val_mse: 4.1798e-04 - val_My_MSE: 6.2452\n",
            "\n",
            "Epoch 00634: loss improved from 0.00042 to 0.00042, saving model to poids_train.hdf5\n",
            "Epoch 635/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.2740e-04 - mse: 4.2740e-04 - My_MSE: 6.2453 - val_loss: 4.2779e-04 - val_mse: 4.2779e-04 - val_My_MSE: 6.2453\n",
            "\n",
            "Epoch 00635: loss did not improve from 0.00042\n",
            "Epoch 636/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.1197e-04 - mse: 4.1197e-04 - My_MSE: 6.2451 - val_loss: 4.1304e-04 - val_mse: 4.1304e-04 - val_My_MSE: 6.2451\n",
            "\n",
            "Epoch 00636: loss did not improve from 0.00042\n",
            "Epoch 637/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.2320e-04 - mse: 4.2320e-04 - My_MSE: 6.2453 - val_loss: 4.2932e-04 - val_mse: 4.2932e-04 - val_My_MSE: 6.2453\n",
            "\n",
            "Epoch 00637: loss did not improve from 0.00042\n",
            "Epoch 638/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.4439e-04 - mse: 4.4439e-04 - My_MSE: 6.2456 - val_loss: 4.6279e-04 - val_mse: 4.6279e-04 - val_My_MSE: 6.2459\n",
            "\n",
            "Epoch 00638: loss did not improve from 0.00042\n",
            "Epoch 639/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.2337e-04 - mse: 4.2337e-04 - My_MSE: 6.2453 - val_loss: 4.3742e-04 - val_mse: 4.3742e-04 - val_My_MSE: 6.2455\n",
            "\n",
            "Epoch 00639: loss did not improve from 0.00042\n",
            "Epoch 640/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.2809e-04 - mse: 4.2809e-04 - My_MSE: 6.2453 - val_loss: 4.1192e-04 - val_mse: 4.1192e-04 - val_My_MSE: 6.2451\n",
            "\n",
            "Epoch 00640: loss did not improve from 0.00042\n",
            "Epoch 641/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.3236e-04 - mse: 4.3236e-04 - My_MSE: 6.2454 - val_loss: 4.3697e-04 - val_mse: 4.3697e-04 - val_My_MSE: 6.2455\n",
            "\n",
            "Epoch 00641: loss did not improve from 0.00042\n",
            "Epoch 642/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.2011e-04 - mse: 4.2011e-04 - My_MSE: 6.2452 - val_loss: 4.0828e-04 - val_mse: 4.0828e-04 - val_My_MSE: 6.2450\n",
            "\n",
            "Epoch 00642: loss did not improve from 0.00042\n",
            "Epoch 643/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.3026e-04 - mse: 4.3026e-04 - My_MSE: 6.2454 - val_loss: 4.0757e-04 - val_mse: 4.0757e-04 - val_My_MSE: 6.2450\n",
            "\n",
            "Epoch 00643: loss did not improve from 0.00042\n",
            "Epoch 644/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.4219e-04 - mse: 4.4219e-04 - My_MSE: 6.2455 - val_loss: 4.4769e-04 - val_mse: 4.4769e-04 - val_My_MSE: 6.2456\n",
            "\n",
            "Epoch 00644: loss did not improve from 0.00042\n",
            "Epoch 645/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.2474e-04 - mse: 4.2474e-04 - My_MSE: 6.2453 - val_loss: 4.3458e-04 - val_mse: 4.3458e-04 - val_My_MSE: 6.2454\n",
            "\n",
            "Epoch 00645: loss did not improve from 0.00042\n",
            "Epoch 646/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.3084e-04 - mse: 4.3084e-04 - My_MSE: 6.2454 - val_loss: 4.1754e-04 - val_mse: 4.1754e-04 - val_My_MSE: 6.2452\n",
            "\n",
            "Epoch 00646: loss did not improve from 0.00042\n",
            "Epoch 647/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.2459e-04 - mse: 4.2459e-04 - My_MSE: 6.2453 - val_loss: 4.0656e-04 - val_mse: 4.0656e-04 - val_My_MSE: 6.2450\n",
            "\n",
            "Epoch 00647: loss improved from 0.00042 to 0.00041, saving model to poids_train.hdf5\n",
            "Epoch 648/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.9227e-04 - mse: 3.9227e-04 - My_MSE: 6.2448 - val_loss: 4.0482e-04 - val_mse: 4.0482e-04 - val_My_MSE: 6.2450\n",
            "\n",
            "Epoch 00648: loss did not improve from 0.00041\n",
            "Epoch 649/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.2630e-04 - mse: 4.2630e-04 - My_MSE: 6.2453 - val_loss: 4.1484e-04 - val_mse: 4.1484e-04 - val_My_MSE: 6.2451\n",
            "\n",
            "Epoch 00649: loss did not improve from 0.00041\n",
            "Epoch 650/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.3679e-04 - mse: 4.3679e-04 - My_MSE: 6.2455 - val_loss: 4.0420e-04 - val_mse: 4.0420e-04 - val_My_MSE: 6.2450\n",
            "\n",
            "Epoch 00650: loss did not improve from 0.00041\n",
            "Epoch 651/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.0387e-04 - mse: 4.0387e-04 - My_MSE: 6.2450 - val_loss: 4.0400e-04 - val_mse: 4.0400e-04 - val_My_MSE: 6.2450\n",
            "\n",
            "Epoch 00651: loss improved from 0.00041 to 0.00041, saving model to poids_train.hdf5\n",
            "Epoch 652/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.0538e-04 - mse: 4.0538e-04 - My_MSE: 6.2450 - val_loss: 4.3097e-04 - val_mse: 4.3097e-04 - val_My_MSE: 6.2454\n",
            "\n",
            "Epoch 00652: loss did not improve from 0.00041\n",
            "Epoch 653/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.0896e-04 - mse: 4.0896e-04 - My_MSE: 6.2450 - val_loss: 4.2752e-04 - val_mse: 4.2752e-04 - val_My_MSE: 6.2453\n",
            "\n",
            "Epoch 00653: loss did not improve from 0.00041\n",
            "Epoch 654/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.0651e-04 - mse: 4.0651e-04 - My_MSE: 6.2450 - val_loss: 4.0504e-04 - val_mse: 4.0504e-04 - val_My_MSE: 6.2450\n",
            "\n",
            "Epoch 00654: loss did not improve from 0.00041\n",
            "Epoch 655/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.0682e-04 - mse: 4.0682e-04 - My_MSE: 6.2450 - val_loss: 4.0155e-04 - val_mse: 4.0155e-04 - val_My_MSE: 6.2449\n",
            "\n",
            "Epoch 00655: loss improved from 0.00041 to 0.00041, saving model to poids_train.hdf5\n",
            "Epoch 656/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.2379e-04 - mse: 4.2379e-04 - My_MSE: 6.2453 - val_loss: 4.2386e-04 - val_mse: 4.2386e-04 - val_My_MSE: 6.2453\n",
            "\n",
            "Epoch 00656: loss did not improve from 0.00041\n",
            "Epoch 657/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.0664e-04 - mse: 4.0664e-04 - My_MSE: 6.2450 - val_loss: 4.0929e-04 - val_mse: 4.0929e-04 - val_My_MSE: 6.2450\n",
            "\n",
            "Epoch 00657: loss improved from 0.00041 to 0.00041, saving model to poids_train.hdf5\n",
            "Epoch 658/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.1835e-04 - mse: 4.1835e-04 - My_MSE: 6.2452 - val_loss: 4.2569e-04 - val_mse: 4.2569e-04 - val_My_MSE: 6.2453\n",
            "\n",
            "Epoch 00658: loss did not improve from 0.00041\n",
            "Epoch 659/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.1240e-04 - mse: 4.1240e-04 - My_MSE: 6.2451 - val_loss: 4.2630e-04 - val_mse: 4.2630e-04 - val_My_MSE: 6.2453\n",
            "\n",
            "Epoch 00659: loss improved from 0.00041 to 0.00040, saving model to poids_train.hdf5\n",
            "Epoch 660/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.2182e-04 - mse: 4.2182e-04 - My_MSE: 6.2452 - val_loss: 4.2732e-04 - val_mse: 4.2732e-04 - val_My_MSE: 6.2453\n",
            "\n",
            "Epoch 00660: loss did not improve from 0.00040\n",
            "Epoch 661/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.0832e-04 - mse: 4.0832e-04 - My_MSE: 6.2450 - val_loss: 4.0260e-04 - val_mse: 4.0260e-04 - val_My_MSE: 6.2449\n",
            "\n",
            "Epoch 00661: loss did not improve from 0.00040\n",
            "Epoch 662/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.9166e-04 - mse: 3.9166e-04 - My_MSE: 6.2448 - val_loss: 3.9417e-04 - val_mse: 3.9417e-04 - val_My_MSE: 6.2448\n",
            "\n",
            "Epoch 00662: loss improved from 0.00040 to 0.00040, saving model to poids_train.hdf5\n",
            "Epoch 663/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.9075e-04 - mse: 3.9075e-04 - My_MSE: 6.2447 - val_loss: 4.0082e-04 - val_mse: 4.0082e-04 - val_My_MSE: 6.2449\n",
            "\n",
            "Epoch 00663: loss did not improve from 0.00040\n",
            "Epoch 664/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.8094e-04 - mse: 3.8094e-04 - My_MSE: 6.2446 - val_loss: 4.0390e-04 - val_mse: 4.0390e-04 - val_My_MSE: 6.2450\n",
            "\n",
            "Epoch 00664: loss did not improve from 0.00040\n",
            "Epoch 665/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.9888e-04 - mse: 3.9888e-04 - My_MSE: 6.2449 - val_loss: 4.1970e-04 - val_mse: 4.1970e-04 - val_My_MSE: 6.2452\n",
            "\n",
            "Epoch 00665: loss improved from 0.00040 to 0.00039, saving model to poids_train.hdf5\n",
            "Epoch 666/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.0593e-04 - mse: 4.0593e-04 - My_MSE: 6.2450 - val_loss: 4.2060e-04 - val_mse: 4.2060e-04 - val_My_MSE: 6.2452\n",
            "\n",
            "Epoch 00666: loss did not improve from 0.00039\n",
            "Epoch 667/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.9456e-04 - mse: 3.9456e-04 - My_MSE: 6.2448 - val_loss: 3.9815e-04 - val_mse: 3.9815e-04 - val_My_MSE: 6.2449\n",
            "\n",
            "Epoch 00667: loss did not improve from 0.00039\n",
            "Epoch 668/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.8423e-04 - mse: 3.8423e-04 - My_MSE: 6.2446 - val_loss: 3.8495e-04 - val_mse: 3.8495e-04 - val_My_MSE: 6.2447\n",
            "\n",
            "Epoch 00668: loss did not improve from 0.00039\n",
            "Epoch 669/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.9194e-04 - mse: 3.9194e-04 - My_MSE: 6.2448 - val_loss: 3.8280e-04 - val_mse: 3.8280e-04 - val_My_MSE: 6.2446\n",
            "\n",
            "Epoch 00669: loss did not improve from 0.00039\n",
            "Epoch 670/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.8052e-04 - mse: 3.8052e-04 - My_MSE: 6.2446 - val_loss: 4.0179e-04 - val_mse: 4.0179e-04 - val_My_MSE: 6.2449\n",
            "\n",
            "Epoch 00670: loss improved from 0.00039 to 0.00039, saving model to poids_train.hdf5\n",
            "Epoch 671/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.9683e-04 - mse: 3.9683e-04 - My_MSE: 6.2448 - val_loss: 4.0850e-04 - val_mse: 4.0850e-04 - val_My_MSE: 6.2450\n",
            "\n",
            "Epoch 00671: loss improved from 0.00039 to 0.00039, saving model to poids_train.hdf5\n",
            "Epoch 672/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.0290e-04 - mse: 4.0290e-04 - My_MSE: 6.2449 - val_loss: 4.0320e-04 - val_mse: 4.0320e-04 - val_My_MSE: 6.2449\n",
            "\n",
            "Epoch 00672: loss improved from 0.00039 to 0.00039, saving model to poids_train.hdf5\n",
            "Epoch 673/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.5521e-04 - mse: 3.5521e-04 - My_MSE: 6.2442 - val_loss: 3.8129e-04 - val_mse: 3.8129e-04 - val_My_MSE: 6.2446\n",
            "\n",
            "Epoch 00673: loss improved from 0.00039 to 0.00038, saving model to poids_train.hdf5\n",
            "Epoch 674/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.8557e-04 - mse: 3.8557e-04 - My_MSE: 6.2447 - val_loss: 3.7754e-04 - val_mse: 3.7754e-04 - val_My_MSE: 6.2445\n",
            "\n",
            "Epoch 00674: loss improved from 0.00038 to 0.00038, saving model to poids_train.hdf5\n",
            "Epoch 675/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.0408e-04 - mse: 4.0408e-04 - My_MSE: 6.2450 - val_loss: 3.8686e-04 - val_mse: 3.8686e-04 - val_My_MSE: 6.2447\n",
            "\n",
            "Epoch 00675: loss did not improve from 0.00038\n",
            "Epoch 676/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 4.0702e-04 - mse: 4.0702e-04 - My_MSE: 6.2450 - val_loss: 3.8687e-04 - val_mse: 3.8687e-04 - val_My_MSE: 6.2447\n",
            "\n",
            "Epoch 00676: loss did not improve from 0.00038\n",
            "Epoch 677/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 3.7967e-04 - mse: 3.7967e-04 - My_MSE: 6.2446 - val_loss: 3.7103e-04 - val_mse: 3.7103e-04 - val_My_MSE: 6.2444\n",
            "\n",
            "Epoch 00677: loss did not improve from 0.00038\n",
            "Epoch 678/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.7820e-04 - mse: 3.7820e-04 - My_MSE: 6.2446 - val_loss: 3.6637e-04 - val_mse: 3.6637e-04 - val_My_MSE: 6.2444\n",
            "\n",
            "Epoch 00678: loss did not improve from 0.00038\n",
            "Epoch 679/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.7380e-04 - mse: 3.7380e-04 - My_MSE: 6.2445 - val_loss: 3.7469e-04 - val_mse: 3.7469e-04 - val_My_MSE: 6.2445\n",
            "\n",
            "Epoch 00679: loss improved from 0.00038 to 0.00038, saving model to poids_train.hdf5\n",
            "Epoch 680/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.8125e-04 - mse: 3.8125e-04 - My_MSE: 6.2446 - val_loss: 3.6757e-04 - val_mse: 3.6757e-04 - val_My_MSE: 6.2444\n",
            "\n",
            "Epoch 00680: loss did not improve from 0.00038\n",
            "Epoch 681/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.8149e-04 - mse: 3.8149e-04 - My_MSE: 6.2446 - val_loss: 3.7212e-04 - val_mse: 3.7212e-04 - val_My_MSE: 6.2445\n",
            "\n",
            "Epoch 00681: loss improved from 0.00038 to 0.00038, saving model to poids_train.hdf5\n",
            "Epoch 682/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.5245e-04 - mse: 3.5245e-04 - My_MSE: 6.2441 - val_loss: 3.6402e-04 - val_mse: 3.6402e-04 - val_My_MSE: 6.2443\n",
            "\n",
            "Epoch 00682: loss improved from 0.00038 to 0.00037, saving model to poids_train.hdf5\n",
            "Epoch 683/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.7308e-04 - mse: 3.7308e-04 - My_MSE: 6.2445 - val_loss: 3.6325e-04 - val_mse: 3.6325e-04 - val_My_MSE: 6.2443\n",
            "\n",
            "Epoch 00683: loss improved from 0.00037 to 0.00037, saving model to poids_train.hdf5\n",
            "Epoch 684/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.6769e-04 - mse: 3.6769e-04 - My_MSE: 6.2444 - val_loss: 3.8858e-04 - val_mse: 3.8858e-04 - val_My_MSE: 6.2447\n",
            "\n",
            "Epoch 00684: loss did not improve from 0.00037\n",
            "Epoch 685/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.7676e-04 - mse: 3.7676e-04 - My_MSE: 6.2445 - val_loss: 3.5622e-04 - val_mse: 3.5622e-04 - val_My_MSE: 6.2442\n",
            "\n",
            "Epoch 00685: loss improved from 0.00037 to 0.00037, saving model to poids_train.hdf5\n",
            "Epoch 686/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.6694e-04 - mse: 3.6694e-04 - My_MSE: 6.2444 - val_loss: 3.5685e-04 - val_mse: 3.5685e-04 - val_My_MSE: 6.2442\n",
            "\n",
            "Epoch 00686: loss did not improve from 0.00037\n",
            "Epoch 687/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.5889e-04 - mse: 3.5889e-04 - My_MSE: 6.2442 - val_loss: 3.5583e-04 - val_mse: 3.5583e-04 - val_My_MSE: 6.2442\n",
            "\n",
            "Epoch 00687: loss improved from 0.00037 to 0.00036, saving model to poids_train.hdf5\n",
            "Epoch 688/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.8831e-04 - mse: 3.8831e-04 - My_MSE: 6.2447 - val_loss: 3.4345e-04 - val_mse: 3.4345e-04 - val_My_MSE: 6.2440\n",
            "\n",
            "Epoch 00688: loss improved from 0.00036 to 0.00036, saving model to poids_train.hdf5\n",
            "Epoch 689/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.8260e-04 - mse: 3.8260e-04 - My_MSE: 6.2446 - val_loss: 3.7158e-04 - val_mse: 3.7158e-04 - val_My_MSE: 6.2444\n",
            "\n",
            "Epoch 00689: loss did not improve from 0.00036\n",
            "Epoch 690/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.6925e-04 - mse: 3.6925e-04 - My_MSE: 6.2444 - val_loss: 3.4974e-04 - val_mse: 3.4974e-04 - val_My_MSE: 6.2441\n",
            "\n",
            "Epoch 00690: loss did not improve from 0.00036\n",
            "Epoch 691/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.4128e-04 - mse: 3.4128e-04 - My_MSE: 6.2440 - val_loss: 3.4889e-04 - val_mse: 3.4889e-04 - val_My_MSE: 6.2441\n",
            "\n",
            "Epoch 00691: loss improved from 0.00036 to 0.00035, saving model to poids_train.hdf5\n",
            "Epoch 692/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.3069e-04 - mse: 3.3069e-04 - My_MSE: 6.2438 - val_loss: 3.3524e-04 - val_mse: 3.3524e-04 - val_My_MSE: 6.2439\n",
            "\n",
            "Epoch 00692: loss improved from 0.00035 to 0.00035, saving model to poids_train.hdf5\n",
            "Epoch 693/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.2451e-04 - mse: 3.2451e-04 - My_MSE: 6.2437 - val_loss: 3.3325e-04 - val_mse: 3.3325e-04 - val_My_MSE: 6.2438\n",
            "\n",
            "Epoch 00693: loss did not improve from 0.00035\n",
            "Epoch 694/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.2215e-04 - mse: 3.2215e-04 - My_MSE: 6.2437 - val_loss: 3.2974e-04 - val_mse: 3.2974e-04 - val_My_MSE: 6.2438\n",
            "\n",
            "Epoch 00694: loss improved from 0.00035 to 0.00034, saving model to poids_train.hdf5\n",
            "Epoch 695/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.4722e-04 - mse: 3.4722e-04 - My_MSE: 6.2441 - val_loss: 3.3334e-04 - val_mse: 3.3334e-04 - val_My_MSE: 6.2438\n",
            "\n",
            "Epoch 00695: loss did not improve from 0.00034\n",
            "Epoch 696/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.3680e-04 - mse: 3.3680e-04 - My_MSE: 6.2439 - val_loss: 3.5471e-04 - val_mse: 3.5471e-04 - val_My_MSE: 6.2442\n",
            "\n",
            "Epoch 00696: loss did not improve from 0.00034\n",
            "Epoch 697/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.4095e-04 - mse: 3.4095e-04 - My_MSE: 6.2440 - val_loss: 3.4550e-04 - val_mse: 3.4550e-04 - val_My_MSE: 6.2440\n",
            "\n",
            "Epoch 00697: loss improved from 0.00034 to 0.00034, saving model to poids_train.hdf5\n",
            "Epoch 698/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.3004e-04 - mse: 3.3004e-04 - My_MSE: 6.2438 - val_loss: 3.2849e-04 - val_mse: 3.2849e-04 - val_My_MSE: 6.2438\n",
            "\n",
            "Epoch 00698: loss did not improve from 0.00034\n",
            "Epoch 699/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.4027e-04 - mse: 3.4027e-04 - My_MSE: 6.2440 - val_loss: 3.4743e-04 - val_mse: 3.4743e-04 - val_My_MSE: 6.2441\n",
            "\n",
            "Epoch 00699: loss improved from 0.00034 to 0.00034, saving model to poids_train.hdf5\n",
            "Epoch 700/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.6276e-04 - mse: 3.6276e-04 - My_MSE: 6.2443 - val_loss: 3.4302e-04 - val_mse: 3.4302e-04 - val_My_MSE: 6.2440\n",
            "\n",
            "Epoch 00700: loss did not improve from 0.00034\n",
            "Epoch 701/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.3563e-04 - mse: 3.3563e-04 - My_MSE: 6.2439 - val_loss: 3.3557e-04 - val_mse: 3.3557e-04 - val_My_MSE: 6.2439\n",
            "\n",
            "Epoch 00701: loss did not improve from 0.00034\n",
            "Epoch 702/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.2837e-04 - mse: 3.2837e-04 - My_MSE: 6.2438 - val_loss: 3.2571e-04 - val_mse: 3.2571e-04 - val_My_MSE: 6.2437\n",
            "\n",
            "Epoch 00702: loss improved from 0.00034 to 0.00033, saving model to poids_train.hdf5\n",
            "Epoch 703/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.1843e-04 - mse: 3.1843e-04 - My_MSE: 6.2436 - val_loss: 3.3570e-04 - val_mse: 3.3570e-04 - val_My_MSE: 6.2439\n",
            "\n",
            "Epoch 00703: loss improved from 0.00033 to 0.00033, saving model to poids_train.hdf5\n",
            "Epoch 704/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.4925e-04 - mse: 3.4925e-04 - My_MSE: 6.2441 - val_loss: 3.3169e-04 - val_mse: 3.3169e-04 - val_My_MSE: 6.2438\n",
            "\n",
            "Epoch 00704: loss did not improve from 0.00033\n",
            "Epoch 705/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.3703e-04 - mse: 3.3703e-04 - My_MSE: 6.2439 - val_loss: 3.4188e-04 - val_mse: 3.4188e-04 - val_My_MSE: 6.2440\n",
            "\n",
            "Epoch 00705: loss did not improve from 0.00033\n",
            "Epoch 706/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.2616e-04 - mse: 3.2616e-04 - My_MSE: 6.2437 - val_loss: 3.2099e-04 - val_mse: 3.2099e-04 - val_My_MSE: 6.2437\n",
            "\n",
            "Epoch 00706: loss did not improve from 0.00033\n",
            "Epoch 707/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.2571e-04 - mse: 3.2571e-04 - My_MSE: 6.2437 - val_loss: 3.1201e-04 - val_mse: 3.1201e-04 - val_My_MSE: 6.2435\n",
            "\n",
            "Epoch 00707: loss improved from 0.00033 to 0.00032, saving model to poids_train.hdf5\n",
            "Epoch 708/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.1309e-04 - mse: 3.1309e-04 - My_MSE: 6.2435 - val_loss: 3.0700e-04 - val_mse: 3.0700e-04 - val_My_MSE: 6.2434\n",
            "\n",
            "Epoch 00708: loss improved from 0.00032 to 0.00032, saving model to poids_train.hdf5\n",
            "Epoch 709/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.0834e-04 - mse: 3.0834e-04 - My_MSE: 6.2435 - val_loss: 3.0738e-04 - val_mse: 3.0738e-04 - val_My_MSE: 6.2434\n",
            "\n",
            "Epoch 00709: loss did not improve from 0.00032\n",
            "Epoch 710/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.3252e-04 - mse: 3.3252e-04 - My_MSE: 6.2438 - val_loss: 3.0695e-04 - val_mse: 3.0695e-04 - val_My_MSE: 6.2434\n",
            "\n",
            "Epoch 00710: loss did not improve from 0.00032\n",
            "Epoch 711/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.1922e-04 - mse: 3.1922e-04 - My_MSE: 6.2436 - val_loss: 3.2878e-04 - val_mse: 3.2878e-04 - val_My_MSE: 6.2438\n",
            "\n",
            "Epoch 00711: loss did not improve from 0.00032\n",
            "Epoch 712/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.0456e-04 - mse: 3.0456e-04 - My_MSE: 6.2434 - val_loss: 3.0306e-04 - val_mse: 3.0306e-04 - val_My_MSE: 6.2434\n",
            "\n",
            "Epoch 00712: loss improved from 0.00032 to 0.00032, saving model to poids_train.hdf5\n",
            "Epoch 713/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.2907e-04 - mse: 3.2907e-04 - My_MSE: 6.2438 - val_loss: 3.1036e-04 - val_mse: 3.1036e-04 - val_My_MSE: 6.2435\n",
            "\n",
            "Epoch 00713: loss improved from 0.00032 to 0.00032, saving model to poids_train.hdf5\n",
            "Epoch 714/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.1753e-04 - mse: 3.1753e-04 - My_MSE: 6.2436 - val_loss: 3.0163e-04 - val_mse: 3.0163e-04 - val_My_MSE: 6.2434\n",
            "\n",
            "Epoch 00714: loss improved from 0.00032 to 0.00031, saving model to poids_train.hdf5\n",
            "Epoch 715/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.0757e-04 - mse: 3.0757e-04 - My_MSE: 6.2434 - val_loss: 2.9972e-04 - val_mse: 2.9972e-04 - val_My_MSE: 6.2433\n",
            "\n",
            "Epoch 00715: loss improved from 0.00031 to 0.00031, saving model to poids_train.hdf5\n",
            "Epoch 716/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.0162e-04 - mse: 3.0162e-04 - My_MSE: 6.2434 - val_loss: 3.0229e-04 - val_mse: 3.0229e-04 - val_My_MSE: 6.2434\n",
            "\n",
            "Epoch 00716: loss did not improve from 0.00031\n",
            "Epoch 717/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.0191e-04 - mse: 3.0191e-04 - My_MSE: 6.2434 - val_loss: 3.0286e-04 - val_mse: 3.0286e-04 - val_My_MSE: 6.2434\n",
            "\n",
            "Epoch 00717: loss did not improve from 0.00031\n",
            "Epoch 718/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.3380e-04 - mse: 3.3380e-04 - My_MSE: 6.2439 - val_loss: 3.0277e-04 - val_mse: 3.0277e-04 - val_My_MSE: 6.2434\n",
            "\n",
            "Epoch 00718: loss did not improve from 0.00031\n",
            "Epoch 719/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.1121e-04 - mse: 3.1121e-04 - My_MSE: 6.2435 - val_loss: 3.0410e-04 - val_mse: 3.0410e-04 - val_My_MSE: 6.2434\n",
            "\n",
            "Epoch 00719: loss improved from 0.00031 to 0.00031, saving model to poids_train.hdf5\n",
            "Epoch 720/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.1055e-04 - mse: 3.1055e-04 - My_MSE: 6.2435 - val_loss: 2.9530e-04 - val_mse: 2.9530e-04 - val_My_MSE: 6.2433\n",
            "\n",
            "Epoch 00720: loss improved from 0.00031 to 0.00031, saving model to poids_train.hdf5\n",
            "Epoch 721/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.0609e-04 - mse: 3.0609e-04 - My_MSE: 6.2434 - val_loss: 3.0351e-04 - val_mse: 3.0351e-04 - val_My_MSE: 6.2434\n",
            "\n",
            "Epoch 00721: loss improved from 0.00031 to 0.00031, saving model to poids_train.hdf5\n",
            "Epoch 722/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.3180e-04 - mse: 3.3180e-04 - My_MSE: 6.2438 - val_loss: 3.2357e-04 - val_mse: 3.2357e-04 - val_My_MSE: 6.2437\n",
            "\n",
            "Epoch 00722: loss did not improve from 0.00031\n",
            "Epoch 723/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.1211e-04 - mse: 3.1211e-04 - My_MSE: 6.2435 - val_loss: 2.8860e-04 - val_mse: 2.8860e-04 - val_My_MSE: 6.2432\n",
            "\n",
            "Epoch 00723: loss did not improve from 0.00031\n",
            "Epoch 724/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.0301e-04 - mse: 3.0301e-04 - My_MSE: 6.2434 - val_loss: 2.8289e-04 - val_mse: 2.8289e-04 - val_My_MSE: 6.2431\n",
            "\n",
            "Epoch 00724: loss improved from 0.00031 to 0.00030, saving model to poids_train.hdf5\n",
            "Epoch 725/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.9517e-04 - mse: 2.9517e-04 - My_MSE: 6.2433 - val_loss: 2.9279e-04 - val_mse: 2.9279e-04 - val_My_MSE: 6.2432\n",
            "\n",
            "Epoch 00725: loss improved from 0.00030 to 0.00030, saving model to poids_train.hdf5\n",
            "Epoch 726/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.8567e-04 - mse: 2.8567e-04 - My_MSE: 6.2431 - val_loss: 2.8438e-04 - val_mse: 2.8438e-04 - val_My_MSE: 6.2431\n",
            "\n",
            "Epoch 00726: loss improved from 0.00030 to 0.00030, saving model to poids_train.hdf5\n",
            "Epoch 727/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.0104e-04 - mse: 3.0104e-04 - My_MSE: 6.2433 - val_loss: 2.7603e-04 - val_mse: 2.7603e-04 - val_My_MSE: 6.2430\n",
            "\n",
            "Epoch 00727: loss improved from 0.00030 to 0.00030, saving model to poids_train.hdf5\n",
            "Epoch 728/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.9441e-04 - mse: 2.9441e-04 - My_MSE: 6.2432 - val_loss: 2.8020e-04 - val_mse: 2.8020e-04 - val_My_MSE: 6.2430\n",
            "\n",
            "Epoch 00728: loss did not improve from 0.00030\n",
            "Epoch 729/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.0177e-04 - mse: 3.0177e-04 - My_MSE: 6.2434 - val_loss: 2.7327e-04 - val_mse: 2.7327e-04 - val_My_MSE: 6.2429\n",
            "\n",
            "Epoch 00729: loss did not improve from 0.00030\n",
            "Epoch 730/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.8408e-04 - mse: 2.8408e-04 - My_MSE: 6.2431 - val_loss: 2.7828e-04 - val_mse: 2.7828e-04 - val_My_MSE: 6.2430\n",
            "\n",
            "Epoch 00730: loss did not improve from 0.00030\n",
            "Epoch 731/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.0437e-04 - mse: 3.0437e-04 - My_MSE: 6.2434 - val_loss: 2.6927e-04 - val_mse: 2.6927e-04 - val_My_MSE: 6.2429\n",
            "\n",
            "Epoch 00731: loss did not improve from 0.00030\n",
            "Epoch 732/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.9366e-04 - mse: 2.9366e-04 - My_MSE: 6.2432 - val_loss: 2.8163e-04 - val_mse: 2.8163e-04 - val_My_MSE: 6.2430\n",
            "\n",
            "Epoch 00732: loss did not improve from 0.00030\n",
            "Epoch 733/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.8259e-04 - mse: 2.8259e-04 - My_MSE: 6.2431 - val_loss: 2.7192e-04 - val_mse: 2.7192e-04 - val_My_MSE: 6.2429\n",
            "\n",
            "Epoch 00733: loss improved from 0.00030 to 0.00029, saving model to poids_train.hdf5\n",
            "Epoch 734/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.8507e-04 - mse: 2.8507e-04 - My_MSE: 6.2431 - val_loss: 2.7503e-04 - val_mse: 2.7503e-04 - val_My_MSE: 6.2429\n",
            "\n",
            "Epoch 00734: loss did not improve from 0.00029\n",
            "Epoch 735/1000\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 3.1290e-04 - mse: 3.1290e-04 - My_MSE: 6.2435 - val_loss: 2.8185e-04 - val_mse: 2.8185e-04 - val_My_MSE: 6.2430\n",
            "\n",
            "Epoch 00735: loss did not improve from 0.00029\n",
            "Epoch 736/1000\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 3.1047e-04 - mse: 3.1047e-04 - My_MSE: 6.2435 - val_loss: 2.8427e-04 - val_mse: 2.8427e-04 - val_My_MSE: 6.2431\n",
            "\n",
            "Epoch 00736: loss did not improve from 0.00029\n",
            "Epoch 737/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.0266e-04 - mse: 3.0266e-04 - My_MSE: 6.2434 - val_loss: 2.8886e-04 - val_mse: 2.8886e-04 - val_My_MSE: 6.2432\n",
            "\n",
            "Epoch 00737: loss did not improve from 0.00029\n",
            "Epoch 738/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.8142e-04 - mse: 2.8142e-04 - My_MSE: 6.2430 - val_loss: 2.6285e-04 - val_mse: 2.6285e-04 - val_My_MSE: 6.2428\n",
            "\n",
            "Epoch 00738: loss did not improve from 0.00029\n",
            "Epoch 739/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.7790e-04 - mse: 2.7790e-04 - My_MSE: 6.2430 - val_loss: 2.6988e-04 - val_mse: 2.6988e-04 - val_My_MSE: 6.2429\n",
            "\n",
            "Epoch 00739: loss did not improve from 0.00029\n",
            "Epoch 740/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.7730e-04 - mse: 2.7730e-04 - My_MSE: 6.2430 - val_loss: 2.8807e-04 - val_mse: 2.8807e-04 - val_My_MSE: 6.2431\n",
            "\n",
            "Epoch 00740: loss did not improve from 0.00029\n",
            "Epoch 741/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.8928e-04 - mse: 2.8928e-04 - My_MSE: 6.2432 - val_loss: 2.7200e-04 - val_mse: 2.7200e-04 - val_My_MSE: 6.2429\n",
            "\n",
            "Epoch 00741: loss improved from 0.00029 to 0.00029, saving model to poids_train.hdf5\n",
            "Epoch 742/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.9349e-04 - mse: 2.9349e-04 - My_MSE: 6.2432 - val_loss: 2.9728e-04 - val_mse: 2.9728e-04 - val_My_MSE: 6.2433\n",
            "\n",
            "Epoch 00742: loss did not improve from 0.00029\n",
            "Epoch 743/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.9074e-04 - mse: 2.9074e-04 - My_MSE: 6.2432 - val_loss: 2.7253e-04 - val_mse: 2.7253e-04 - val_My_MSE: 6.2429\n",
            "\n",
            "Epoch 00743: loss did not improve from 0.00029\n",
            "Epoch 744/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.8517e-04 - mse: 2.8517e-04 - My_MSE: 6.2431 - val_loss: 2.6148e-04 - val_mse: 2.6148e-04 - val_My_MSE: 6.2427\n",
            "\n",
            "Epoch 00744: loss improved from 0.00029 to 0.00029, saving model to poids_train.hdf5\n",
            "Epoch 745/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.6228e-04 - mse: 2.6228e-04 - My_MSE: 6.2427 - val_loss: 2.6193e-04 - val_mse: 2.6193e-04 - val_My_MSE: 6.2427\n",
            "\n",
            "Epoch 00745: loss improved from 0.00029 to 0.00028, saving model to poids_train.hdf5\n",
            "Epoch 746/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.6264e-04 - mse: 2.6264e-04 - My_MSE: 6.2427 - val_loss: 2.5267e-04 - val_mse: 2.5267e-04 - val_My_MSE: 6.2426\n",
            "\n",
            "Epoch 00746: loss improved from 0.00028 to 0.00027, saving model to poids_train.hdf5\n",
            "Epoch 747/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.8850e-04 - mse: 2.8850e-04 - My_MSE: 6.2431 - val_loss: 2.6119e-04 - val_mse: 2.6119e-04 - val_My_MSE: 6.2427\n",
            "\n",
            "Epoch 00747: loss did not improve from 0.00027\n",
            "Epoch 748/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.8430e-04 - mse: 2.8430e-04 - My_MSE: 6.2431 - val_loss: 2.5050e-04 - val_mse: 2.5050e-04 - val_My_MSE: 6.2426\n",
            "\n",
            "Epoch 00748: loss did not improve from 0.00027\n",
            "Epoch 749/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.8843e-04 - mse: 2.8843e-04 - My_MSE: 6.2431 - val_loss: 2.6205e-04 - val_mse: 2.6205e-04 - val_My_MSE: 6.2427\n",
            "\n",
            "Epoch 00749: loss did not improve from 0.00027\n",
            "Epoch 750/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.8418e-04 - mse: 2.8418e-04 - My_MSE: 6.2431 - val_loss: 2.6555e-04 - val_mse: 2.6555e-04 - val_My_MSE: 6.2428\n",
            "\n",
            "Epoch 00750: loss did not improve from 0.00027\n",
            "Epoch 751/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.9003e-04 - mse: 2.9003e-04 - My_MSE: 6.2432 - val_loss: 2.6723e-04 - val_mse: 2.6723e-04 - val_My_MSE: 6.2428\n",
            "\n",
            "Epoch 00751: loss did not improve from 0.00027\n",
            "Epoch 752/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 3.0726e-04 - mse: 3.0726e-04 - My_MSE: 6.2434 - val_loss: 2.6027e-04 - val_mse: 2.6027e-04 - val_My_MSE: 6.2427\n",
            "\n",
            "Epoch 00752: loss did not improve from 0.00027\n",
            "Epoch 753/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.7410e-04 - mse: 2.7410e-04 - My_MSE: 6.2429 - val_loss: 2.4419e-04 - val_mse: 2.4419e-04 - val_My_MSE: 6.2425\n",
            "\n",
            "Epoch 00753: loss did not improve from 0.00027\n",
            "Epoch 754/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.8775e-04 - mse: 2.8775e-04 - My_MSE: 6.2431 - val_loss: 2.7146e-04 - val_mse: 2.7146e-04 - val_My_MSE: 6.2429\n",
            "\n",
            "Epoch 00754: loss did not improve from 0.00027\n",
            "Epoch 755/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.9457e-04 - mse: 2.9457e-04 - My_MSE: 6.2432 - val_loss: 2.4597e-04 - val_mse: 2.4597e-04 - val_My_MSE: 6.2425\n",
            "\n",
            "Epoch 00755: loss did not improve from 0.00027\n",
            "Epoch 756/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.6393e-04 - mse: 2.6393e-04 - My_MSE: 6.2428 - val_loss: 2.6203e-04 - val_mse: 2.6203e-04 - val_My_MSE: 6.2427\n",
            "\n",
            "Epoch 00756: loss did not improve from 0.00027\n",
            "Epoch 757/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.6978e-04 - mse: 2.6978e-04 - My_MSE: 6.2429 - val_loss: 2.4675e-04 - val_mse: 2.4675e-04 - val_My_MSE: 6.2425\n",
            "\n",
            "Epoch 00757: loss did not improve from 0.00027\n",
            "Epoch 758/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.7060e-04 - mse: 2.7060e-04 - My_MSE: 6.2429 - val_loss: 2.5403e-04 - val_mse: 2.5403e-04 - val_My_MSE: 6.2426\n",
            "\n",
            "Epoch 00758: loss improved from 0.00027 to 0.00027, saving model to poids_train.hdf5\n",
            "Epoch 759/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.8181e-04 - mse: 2.8181e-04 - My_MSE: 6.2430 - val_loss: 2.4910e-04 - val_mse: 2.4910e-04 - val_My_MSE: 6.2425\n",
            "\n",
            "Epoch 00759: loss improved from 0.00027 to 0.00027, saving model to poids_train.hdf5\n",
            "Epoch 760/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.5718e-04 - mse: 2.5718e-04 - My_MSE: 6.2427 - val_loss: 2.4930e-04 - val_mse: 2.4930e-04 - val_My_MSE: 6.2425\n",
            "\n",
            "Epoch 00760: loss improved from 0.00027 to 0.00026, saving model to poids_train.hdf5\n",
            "Epoch 761/1000\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 2.5979e-04 - mse: 2.5979e-04 - My_MSE: 6.2427 - val_loss: 2.4859e-04 - val_mse: 2.4859e-04 - val_My_MSE: 6.2425\n",
            "\n",
            "Epoch 00761: loss improved from 0.00026 to 0.00026, saving model to poids_train.hdf5\n",
            "Epoch 762/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.6338e-04 - mse: 2.6338e-04 - My_MSE: 6.2428 - val_loss: 2.4073e-04 - val_mse: 2.4073e-04 - val_My_MSE: 6.2424\n",
            "\n",
            "Epoch 00762: loss did not improve from 0.00026\n",
            "Epoch 763/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.5200e-04 - mse: 2.5200e-04 - My_MSE: 6.2426 - val_loss: 2.3565e-04 - val_mse: 2.3565e-04 - val_My_MSE: 6.2423\n",
            "\n",
            "Epoch 00763: loss improved from 0.00026 to 0.00026, saving model to poids_train.hdf5\n",
            "Epoch 764/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.7924e-04 - mse: 2.7924e-04 - My_MSE: 6.2430 - val_loss: 2.8296e-04 - val_mse: 2.8296e-04 - val_My_MSE: 6.2431\n",
            "\n",
            "Epoch 00764: loss did not improve from 0.00026\n",
            "Epoch 765/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.9337e-04 - mse: 2.9337e-04 - My_MSE: 6.2432 - val_loss: 2.6858e-04 - val_mse: 2.6858e-04 - val_My_MSE: 6.2428\n",
            "\n",
            "Epoch 00765: loss did not improve from 0.00026\n",
            "Epoch 766/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.6029e-04 - mse: 2.6029e-04 - My_MSE: 6.2427 - val_loss: 2.4941e-04 - val_mse: 2.4941e-04 - val_My_MSE: 6.2425\n",
            "\n",
            "Epoch 00766: loss did not improve from 0.00026\n",
            "Epoch 767/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.6998e-04 - mse: 2.6998e-04 - My_MSE: 6.2429 - val_loss: 2.3932e-04 - val_mse: 2.3932e-04 - val_My_MSE: 6.2424\n",
            "\n",
            "Epoch 00767: loss did not improve from 0.00026\n",
            "Epoch 768/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.6618e-04 - mse: 2.6618e-04 - My_MSE: 6.2428 - val_loss: 2.3952e-04 - val_mse: 2.3952e-04 - val_My_MSE: 6.2424\n",
            "\n",
            "Epoch 00768: loss did not improve from 0.00026\n",
            "Epoch 769/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.5787e-04 - mse: 2.5787e-04 - My_MSE: 6.2427 - val_loss: 2.3223e-04 - val_mse: 2.3223e-04 - val_My_MSE: 6.2423\n",
            "\n",
            "Epoch 00769: loss improved from 0.00026 to 0.00026, saving model to poids_train.hdf5\n",
            "Epoch 770/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.4783e-04 - mse: 2.4783e-04 - My_MSE: 6.2425 - val_loss: 2.2840e-04 - val_mse: 2.2840e-04 - val_My_MSE: 6.2422\n",
            "\n",
            "Epoch 00770: loss improved from 0.00026 to 0.00026, saving model to poids_train.hdf5\n",
            "Epoch 771/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.4664e-04 - mse: 2.4664e-04 - My_MSE: 6.2425 - val_loss: 2.2667e-04 - val_mse: 2.2667e-04 - val_My_MSE: 6.2422\n",
            "\n",
            "Epoch 00771: loss improved from 0.00026 to 0.00025, saving model to poids_train.hdf5\n",
            "Epoch 772/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.6419e-04 - mse: 2.6419e-04 - My_MSE: 6.2428 - val_loss: 2.3010e-04 - val_mse: 2.3010e-04 - val_My_MSE: 6.2422\n",
            "\n",
            "Epoch 00772: loss improved from 0.00025 to 0.00025, saving model to poids_train.hdf5\n",
            "Epoch 773/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.4447e-04 - mse: 2.4447e-04 - My_MSE: 6.2425 - val_loss: 2.4004e-04 - val_mse: 2.4004e-04 - val_My_MSE: 6.2424\n",
            "\n",
            "Epoch 00773: loss did not improve from 0.00025\n",
            "Epoch 774/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.5301e-04 - mse: 2.5301e-04 - My_MSE: 6.2426 - val_loss: 2.4407e-04 - val_mse: 2.4407e-04 - val_My_MSE: 6.2425\n",
            "\n",
            "Epoch 00774: loss improved from 0.00025 to 0.00025, saving model to poids_train.hdf5\n",
            "Epoch 775/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.6973e-04 - mse: 2.6973e-04 - My_MSE: 6.2429 - val_loss: 2.7792e-04 - val_mse: 2.7792e-04 - val_My_MSE: 6.2430\n",
            "\n",
            "Epoch 00775: loss did not improve from 0.00025\n",
            "Epoch 776/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.7718e-04 - mse: 2.7718e-04 - My_MSE: 6.2430 - val_loss: 2.6311e-04 - val_mse: 2.6311e-04 - val_My_MSE: 6.2428\n",
            "\n",
            "Epoch 00776: loss did not improve from 0.00025\n",
            "Epoch 777/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.6036e-04 - mse: 2.6036e-04 - My_MSE: 6.2427 - val_loss: 2.3490e-04 - val_mse: 2.3490e-04 - val_My_MSE: 6.2423\n",
            "\n",
            "Epoch 00777: loss did not improve from 0.00025\n",
            "Epoch 778/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.4198e-04 - mse: 2.4198e-04 - My_MSE: 6.2424 - val_loss: 2.3061e-04 - val_mse: 2.3061e-04 - val_My_MSE: 6.2422\n",
            "\n",
            "Epoch 00778: loss improved from 0.00025 to 0.00025, saving model to poids_train.hdf5\n",
            "Epoch 779/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.6063e-04 - mse: 2.6063e-04 - My_MSE: 6.2427 - val_loss: 2.5286e-04 - val_mse: 2.5286e-04 - val_My_MSE: 6.2426\n",
            "\n",
            "Epoch 00779: loss did not improve from 0.00025\n",
            "Epoch 780/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.4937e-04 - mse: 2.4937e-04 - My_MSE: 6.2425 - val_loss: 2.1925e-04 - val_mse: 2.1925e-04 - val_My_MSE: 6.2421\n",
            "\n",
            "Epoch 00780: loss did not improve from 0.00025\n",
            "Epoch 781/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.2851e-04 - mse: 2.2851e-04 - My_MSE: 6.2422 - val_loss: 2.2447e-04 - val_mse: 2.2447e-04 - val_My_MSE: 6.2422\n",
            "\n",
            "Epoch 00781: loss improved from 0.00025 to 0.00025, saving model to poids_train.hdf5\n",
            "Epoch 782/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.5321e-04 - mse: 2.5321e-04 - My_MSE: 6.2426 - val_loss: 2.2143e-04 - val_mse: 2.2143e-04 - val_My_MSE: 6.2421\n",
            "\n",
            "Epoch 00782: loss did not improve from 0.00025\n",
            "Epoch 783/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.4611e-04 - mse: 2.4611e-04 - My_MSE: 6.2425 - val_loss: 2.3025e-04 - val_mse: 2.3025e-04 - val_My_MSE: 6.2422\n",
            "\n",
            "Epoch 00783: loss did not improve from 0.00025\n",
            "Epoch 784/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.5675e-04 - mse: 2.5675e-04 - My_MSE: 6.2427 - val_loss: 2.2775e-04 - val_mse: 2.2775e-04 - val_My_MSE: 6.2422\n",
            "\n",
            "Epoch 00784: loss did not improve from 0.00025\n",
            "Epoch 785/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.4033e-04 - mse: 2.4033e-04 - My_MSE: 6.2424 - val_loss: 2.2009e-04 - val_mse: 2.2009e-04 - val_My_MSE: 6.2421\n",
            "\n",
            "Epoch 00785: loss improved from 0.00025 to 0.00024, saving model to poids_train.hdf5\n",
            "Epoch 786/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.4737e-04 - mse: 2.4737e-04 - My_MSE: 6.2425 - val_loss: 2.1394e-04 - val_mse: 2.1394e-04 - val_My_MSE: 6.2420\n",
            "\n",
            "Epoch 00786: loss improved from 0.00024 to 0.00024, saving model to poids_train.hdf5\n",
            "Epoch 787/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.3562e-04 - mse: 2.3562e-04 - My_MSE: 6.2423 - val_loss: 2.1514e-04 - val_mse: 2.1514e-04 - val_My_MSE: 6.2420\n",
            "\n",
            "Epoch 00787: loss improved from 0.00024 to 0.00024, saving model to poids_train.hdf5\n",
            "Epoch 788/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.3931e-04 - mse: 2.3931e-04 - My_MSE: 6.2424 - val_loss: 2.1028e-04 - val_mse: 2.1028e-04 - val_My_MSE: 6.2419\n",
            "\n",
            "Epoch 00788: loss improved from 0.00024 to 0.00024, saving model to poids_train.hdf5\n",
            "Epoch 789/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.5460e-04 - mse: 2.5460e-04 - My_MSE: 6.2426 - val_loss: 2.1455e-04 - val_mse: 2.1455e-04 - val_My_MSE: 6.2420\n",
            "\n",
            "Epoch 00789: loss did not improve from 0.00024\n",
            "Epoch 790/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.4057e-04 - mse: 2.4057e-04 - My_MSE: 6.2424 - val_loss: 2.1661e-04 - val_mse: 2.1661e-04 - val_My_MSE: 6.2420\n",
            "\n",
            "Epoch 00790: loss did not improve from 0.00024\n",
            "Epoch 791/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.3678e-04 - mse: 2.3678e-04 - My_MSE: 6.2423 - val_loss: 2.1921e-04 - val_mse: 2.1921e-04 - val_My_MSE: 6.2421\n",
            "\n",
            "Epoch 00791: loss improved from 0.00024 to 0.00024, saving model to poids_train.hdf5\n",
            "Epoch 792/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.2520e-04 - mse: 2.2520e-04 - My_MSE: 6.2422 - val_loss: 2.1370e-04 - val_mse: 2.1370e-04 - val_My_MSE: 6.2420\n",
            "\n",
            "Epoch 00792: loss improved from 0.00024 to 0.00023, saving model to poids_train.hdf5\n",
            "Epoch 793/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.3630e-04 - mse: 2.3630e-04 - My_MSE: 6.2423 - val_loss: 2.0618e-04 - val_mse: 2.0618e-04 - val_My_MSE: 6.2419\n",
            "\n",
            "Epoch 00793: loss did not improve from 0.00023\n",
            "Epoch 794/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.2655e-04 - mse: 2.2655e-04 - My_MSE: 6.2422 - val_loss: 2.0757e-04 - val_mse: 2.0757e-04 - val_My_MSE: 6.2419\n",
            "\n",
            "Epoch 00794: loss did not improve from 0.00023\n",
            "Epoch 795/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.3037e-04 - mse: 2.3037e-04 - My_MSE: 6.2422 - val_loss: 2.0924e-04 - val_mse: 2.0924e-04 - val_My_MSE: 6.2419\n",
            "\n",
            "Epoch 00795: loss did not improve from 0.00023\n",
            "Epoch 796/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.3671e-04 - mse: 2.3671e-04 - My_MSE: 6.2423 - val_loss: 2.0395e-04 - val_mse: 2.0395e-04 - val_My_MSE: 6.2418\n",
            "\n",
            "Epoch 00796: loss improved from 0.00023 to 0.00023, saving model to poids_train.hdf5\n",
            "Epoch 797/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.1197e-04 - mse: 2.1197e-04 - My_MSE: 6.2420 - val_loss: 2.2046e-04 - val_mse: 2.2046e-04 - val_My_MSE: 6.2421\n",
            "\n",
            "Epoch 00797: loss did not improve from 0.00023\n",
            "Epoch 798/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.1840e-04 - mse: 2.1840e-04 - My_MSE: 6.2421 - val_loss: 2.0201e-04 - val_mse: 2.0201e-04 - val_My_MSE: 6.2418\n",
            "\n",
            "Epoch 00798: loss improved from 0.00023 to 0.00023, saving model to poids_train.hdf5\n",
            "Epoch 799/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.3786e-04 - mse: 2.3786e-04 - My_MSE: 6.2424 - val_loss: 2.0332e-04 - val_mse: 2.0332e-04 - val_My_MSE: 6.2418\n",
            "\n",
            "Epoch 00799: loss did not improve from 0.00023\n",
            "Epoch 800/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.1897e-04 - mse: 2.1897e-04 - My_MSE: 6.2421 - val_loss: 2.0881e-04 - val_mse: 2.0881e-04 - val_My_MSE: 6.2419\n",
            "\n",
            "Epoch 00800: loss did not improve from 0.00023\n",
            "Epoch 801/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.4862e-04 - mse: 2.4862e-04 - My_MSE: 6.2425 - val_loss: 2.2147e-04 - val_mse: 2.2147e-04 - val_My_MSE: 6.2421\n",
            "\n",
            "Epoch 00801: loss did not improve from 0.00023\n",
            "Epoch 802/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.2521e-04 - mse: 2.2521e-04 - My_MSE: 6.2422 - val_loss: 2.2568e-04 - val_mse: 2.2568e-04 - val_My_MSE: 6.2422\n",
            "\n",
            "Epoch 00802: loss did not improve from 0.00023\n",
            "Epoch 803/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.5736e-04 - mse: 2.5736e-04 - My_MSE: 6.2427 - val_loss: 2.1598e-04 - val_mse: 2.1598e-04 - val_My_MSE: 6.2420\n",
            "\n",
            "Epoch 00803: loss did not improve from 0.00023\n",
            "Epoch 804/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.4474e-04 - mse: 2.4474e-04 - My_MSE: 6.2425 - val_loss: 2.4523e-04 - val_mse: 2.4523e-04 - val_My_MSE: 6.2425\n",
            "\n",
            "Epoch 00804: loss did not improve from 0.00023\n",
            "Epoch 805/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.6286e-04 - mse: 2.6286e-04 - My_MSE: 6.2428 - val_loss: 2.5710e-04 - val_mse: 2.5710e-04 - val_My_MSE: 6.2427\n",
            "\n",
            "Epoch 00805: loss did not improve from 0.00023\n",
            "Epoch 806/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.4032e-04 - mse: 2.4032e-04 - My_MSE: 6.2424 - val_loss: 2.2713e-04 - val_mse: 2.2713e-04 - val_My_MSE: 6.2422\n",
            "\n",
            "Epoch 00806: loss did not improve from 0.00023\n",
            "Epoch 807/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.4059e-04 - mse: 2.4059e-04 - My_MSE: 6.2424 - val_loss: 1.9962e-04 - val_mse: 1.9962e-04 - val_My_MSE: 6.2418\n",
            "\n",
            "Epoch 00807: loss did not improve from 0.00023\n",
            "Epoch 808/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.3105e-04 - mse: 2.3105e-04 - My_MSE: 6.2423 - val_loss: 1.9497e-04 - val_mse: 1.9497e-04 - val_My_MSE: 6.2417\n",
            "\n",
            "Epoch 00808: loss did not improve from 0.00023\n",
            "Epoch 809/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.1566e-04 - mse: 2.1566e-04 - My_MSE: 6.2420 - val_loss: 2.1634e-04 - val_mse: 2.1634e-04 - val_My_MSE: 6.2420\n",
            "\n",
            "Epoch 00809: loss did not improve from 0.00023\n",
            "Epoch 810/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.5636e-04 - mse: 2.5636e-04 - My_MSE: 6.2426 - val_loss: 2.4535e-04 - val_mse: 2.4535e-04 - val_My_MSE: 6.2425\n",
            "\n",
            "Epoch 00810: loss did not improve from 0.00023\n",
            "Epoch 811/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.4990e-04 - mse: 2.4990e-04 - My_MSE: 6.2425 - val_loss: 2.0641e-04 - val_mse: 2.0641e-04 - val_My_MSE: 6.2419\n",
            "\n",
            "Epoch 00811: loss did not improve from 0.00023\n",
            "Epoch 812/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.3235e-04 - mse: 2.3235e-04 - My_MSE: 6.2423 - val_loss: 2.0202e-04 - val_mse: 2.0202e-04 - val_My_MSE: 6.2418\n",
            "\n",
            "Epoch 00812: loss improved from 0.00023 to 0.00023, saving model to poids_train.hdf5\n",
            "Epoch 813/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.2174e-04 - mse: 2.2174e-04 - My_MSE: 6.2421 - val_loss: 1.9976e-04 - val_mse: 1.9976e-04 - val_My_MSE: 6.2418\n",
            "\n",
            "Epoch 00813: loss improved from 0.00023 to 0.00022, saving model to poids_train.hdf5\n",
            "Epoch 814/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.3492e-04 - mse: 2.3492e-04 - My_MSE: 6.2423 - val_loss: 1.9842e-04 - val_mse: 1.9842e-04 - val_My_MSE: 6.2417\n",
            "\n",
            "Epoch 00814: loss improved from 0.00022 to 0.00022, saving model to poids_train.hdf5\n",
            "Epoch 815/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.3208e-04 - mse: 2.3208e-04 - My_MSE: 6.2423 - val_loss: 1.9239e-04 - val_mse: 1.9239e-04 - val_My_MSE: 6.2417\n",
            "\n",
            "Epoch 00815: loss did not improve from 0.00022\n",
            "Epoch 816/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.3092e-04 - mse: 2.3092e-04 - My_MSE: 6.2423 - val_loss: 1.9500e-04 - val_mse: 1.9500e-04 - val_My_MSE: 6.2417\n",
            "\n",
            "Epoch 00816: loss did not improve from 0.00022\n",
            "Epoch 817/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.1137e-04 - mse: 2.1137e-04 - My_MSE: 6.2419 - val_loss: 1.9788e-04 - val_mse: 1.9788e-04 - val_My_MSE: 6.2417\n",
            "\n",
            "Epoch 00817: loss did not improve from 0.00022\n",
            "Epoch 818/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.2756e-04 - mse: 2.2756e-04 - My_MSE: 6.2422 - val_loss: 1.9611e-04 - val_mse: 1.9611e-04 - val_My_MSE: 6.2417\n",
            "\n",
            "Epoch 00818: loss improved from 0.00022 to 0.00022, saving model to poids_train.hdf5\n",
            "Epoch 819/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.0150e-04 - mse: 2.0150e-04 - My_MSE: 6.2418 - val_loss: 1.8552e-04 - val_mse: 1.8552e-04 - val_My_MSE: 6.2415\n",
            "\n",
            "Epoch 00819: loss improved from 0.00022 to 0.00022, saving model to poids_train.hdf5\n",
            "Epoch 820/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.1944e-04 - mse: 2.1944e-04 - My_MSE: 6.2421 - val_loss: 1.8528e-04 - val_mse: 1.8528e-04 - val_My_MSE: 6.2415\n",
            "\n",
            "Epoch 00820: loss did not improve from 0.00022\n",
            "Epoch 821/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.1797e-04 - mse: 2.1797e-04 - My_MSE: 6.2420 - val_loss: 1.9124e-04 - val_mse: 1.9124e-04 - val_My_MSE: 6.2416\n",
            "\n",
            "Epoch 00821: loss did not improve from 0.00022\n",
            "Epoch 822/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.0848e-04 - mse: 2.0848e-04 - My_MSE: 6.2419 - val_loss: 2.1660e-04 - val_mse: 2.1660e-04 - val_My_MSE: 6.2420\n",
            "\n",
            "Epoch 00822: loss did not improve from 0.00022\n",
            "Epoch 823/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.3007e-04 - mse: 2.3007e-04 - My_MSE: 6.2422 - val_loss: 2.0707e-04 - val_mse: 2.0707e-04 - val_My_MSE: 6.2419\n",
            "\n",
            "Epoch 00823: loss did not improve from 0.00022\n",
            "Epoch 824/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.3222e-04 - mse: 2.3222e-04 - My_MSE: 6.2423 - val_loss: 1.8548e-04 - val_mse: 1.8548e-04 - val_My_MSE: 6.2415\n",
            "\n",
            "Epoch 00824: loss did not improve from 0.00022\n",
            "Epoch 825/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.1261e-04 - mse: 2.1261e-04 - My_MSE: 6.2420 - val_loss: 1.8921e-04 - val_mse: 1.8921e-04 - val_My_MSE: 6.2416\n",
            "\n",
            "Epoch 00825: loss did not improve from 0.00022\n",
            "Epoch 826/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.1612e-04 - mse: 2.1612e-04 - My_MSE: 6.2420 - val_loss: 1.8992e-04 - val_mse: 1.8992e-04 - val_My_MSE: 6.2416\n",
            "\n",
            "Epoch 00826: loss improved from 0.00022 to 0.00021, saving model to poids_train.hdf5\n",
            "Epoch 827/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.1290e-04 - mse: 2.1290e-04 - My_MSE: 6.2420 - val_loss: 2.0229e-04 - val_mse: 2.0229e-04 - val_My_MSE: 6.2418\n",
            "\n",
            "Epoch 00827: loss improved from 0.00021 to 0.00021, saving model to poids_train.hdf5\n",
            "Epoch 828/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.9951e-04 - mse: 1.9951e-04 - My_MSE: 6.2418 - val_loss: 1.9016e-04 - val_mse: 1.9016e-04 - val_My_MSE: 6.2416\n",
            "\n",
            "Epoch 00828: loss improved from 0.00021 to 0.00021, saving model to poids_train.hdf5\n",
            "Epoch 829/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.9873e-04 - mse: 1.9873e-04 - My_MSE: 6.2417 - val_loss: 2.0336e-04 - val_mse: 2.0336e-04 - val_My_MSE: 6.2418\n",
            "\n",
            "Epoch 00829: loss did not improve from 0.00021\n",
            "Epoch 830/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.1182e-04 - mse: 2.1182e-04 - My_MSE: 6.2420 - val_loss: 1.8897e-04 - val_mse: 1.8897e-04 - val_My_MSE: 6.2416\n",
            "\n",
            "Epoch 00830: loss did not improve from 0.00021\n",
            "Epoch 831/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.1303e-04 - mse: 2.1303e-04 - My_MSE: 6.2420 - val_loss: 1.8985e-04 - val_mse: 1.8985e-04 - val_My_MSE: 6.2416\n",
            "\n",
            "Epoch 00831: loss improved from 0.00021 to 0.00021, saving model to poids_train.hdf5\n",
            "Epoch 832/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.0692e-04 - mse: 2.0692e-04 - My_MSE: 6.2419 - val_loss: 1.8020e-04 - val_mse: 1.8020e-04 - val_My_MSE: 6.2415\n",
            "\n",
            "Epoch 00832: loss improved from 0.00021 to 0.00021, saving model to poids_train.hdf5\n",
            "Epoch 833/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.0301e-04 - mse: 2.0301e-04 - My_MSE: 6.2418 - val_loss: 1.8427e-04 - val_mse: 1.8427e-04 - val_My_MSE: 6.2415\n",
            "\n",
            "Epoch 00833: loss improved from 0.00021 to 0.00020, saving model to poids_train.hdf5\n",
            "Epoch 834/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.1268e-04 - mse: 2.1268e-04 - My_MSE: 6.2420 - val_loss: 2.0305e-04 - val_mse: 2.0305e-04 - val_My_MSE: 6.2418\n",
            "\n",
            "Epoch 00834: loss did not improve from 0.00020\n",
            "Epoch 835/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.1384e-04 - mse: 2.1384e-04 - My_MSE: 6.2420 - val_loss: 1.8632e-04 - val_mse: 1.8632e-04 - val_My_MSE: 6.2416\n",
            "\n",
            "Epoch 00835: loss did not improve from 0.00020\n",
            "Epoch 836/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.1138e-04 - mse: 2.1138e-04 - My_MSE: 6.2419 - val_loss: 1.8665e-04 - val_mse: 1.8665e-04 - val_My_MSE: 6.2416\n",
            "\n",
            "Epoch 00836: loss did not improve from 0.00020\n",
            "Epoch 837/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.1098e-04 - mse: 2.1098e-04 - My_MSE: 6.2419 - val_loss: 1.8988e-04 - val_mse: 1.8988e-04 - val_My_MSE: 6.2416\n",
            "\n",
            "Epoch 00837: loss did not improve from 0.00020\n",
            "Epoch 838/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.1556e-04 - mse: 2.1556e-04 - My_MSE: 6.2420 - val_loss: 1.8461e-04 - val_mse: 1.8461e-04 - val_My_MSE: 6.2415\n",
            "\n",
            "Epoch 00838: loss did not improve from 0.00020\n",
            "Epoch 839/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.2306e-04 - mse: 2.2306e-04 - My_MSE: 6.2421 - val_loss: 1.7291e-04 - val_mse: 1.7291e-04 - val_My_MSE: 6.2413\n",
            "\n",
            "Epoch 00839: loss did not improve from 0.00020\n",
            "Epoch 840/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.9343e-04 - mse: 1.9343e-04 - My_MSE: 6.2417 - val_loss: 1.7672e-04 - val_mse: 1.7672e-04 - val_My_MSE: 6.2414\n",
            "\n",
            "Epoch 00840: loss improved from 0.00020 to 0.00020, saving model to poids_train.hdf5\n",
            "Epoch 841/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.9336e-04 - mse: 1.9336e-04 - My_MSE: 6.2417 - val_loss: 1.7252e-04 - val_mse: 1.7252e-04 - val_My_MSE: 6.2413\n",
            "\n",
            "Epoch 00841: loss improved from 0.00020 to 0.00020, saving model to poids_train.hdf5\n",
            "Epoch 842/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.1096e-04 - mse: 2.1096e-04 - My_MSE: 6.2419 - val_loss: 1.7537e-04 - val_mse: 1.7537e-04 - val_My_MSE: 6.2414\n",
            "\n",
            "Epoch 00842: loss improved from 0.00020 to 0.00019, saving model to poids_train.hdf5\n",
            "Epoch 843/1000\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 2.0567e-04 - mse: 2.0567e-04 - My_MSE: 6.2419 - val_loss: 1.8063e-04 - val_mse: 1.8063e-04 - val_My_MSE: 6.2415\n",
            "\n",
            "Epoch 00843: loss improved from 0.00019 to 0.00019, saving model to poids_train.hdf5\n",
            "Epoch 844/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.9089e-04 - mse: 1.9089e-04 - My_MSE: 6.2416 - val_loss: 1.6663e-04 - val_mse: 1.6663e-04 - val_My_MSE: 6.2412\n",
            "\n",
            "Epoch 00844: loss improved from 0.00019 to 0.00019, saving model to poids_train.hdf5\n",
            "Epoch 845/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.9272e-04 - mse: 1.9272e-04 - My_MSE: 6.2417 - val_loss: 1.6517e-04 - val_mse: 1.6517e-04 - val_My_MSE: 6.2412\n",
            "\n",
            "Epoch 00845: loss did not improve from 0.00019\n",
            "Epoch 846/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.9434e-04 - mse: 1.9434e-04 - My_MSE: 6.2417 - val_loss: 1.7024e-04 - val_mse: 1.7024e-04 - val_My_MSE: 6.2413\n",
            "\n",
            "Epoch 00846: loss did not improve from 0.00019\n",
            "Epoch 847/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.9276e-04 - mse: 1.9276e-04 - My_MSE: 6.2417 - val_loss: 1.7295e-04 - val_mse: 1.7295e-04 - val_My_MSE: 6.2413\n",
            "\n",
            "Epoch 00847: loss did not improve from 0.00019\n",
            "Epoch 848/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.9585e-04 - mse: 1.9585e-04 - My_MSE: 6.2417 - val_loss: 1.7957e-04 - val_mse: 1.7957e-04 - val_My_MSE: 6.2414\n",
            "\n",
            "Epoch 00848: loss did not improve from 0.00019\n",
            "Epoch 849/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.0037e-04 - mse: 2.0037e-04 - My_MSE: 6.2418 - val_loss: 1.9357e-04 - val_mse: 1.9357e-04 - val_My_MSE: 6.2417\n",
            "\n",
            "Epoch 00849: loss did not improve from 0.00019\n",
            "Epoch 850/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.0150e-04 - mse: 2.0150e-04 - My_MSE: 6.2418 - val_loss: 1.7124e-04 - val_mse: 1.7124e-04 - val_My_MSE: 6.2413\n",
            "\n",
            "Epoch 00850: loss did not improve from 0.00019\n",
            "Epoch 851/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.8895e-04 - mse: 1.8895e-04 - My_MSE: 6.2416 - val_loss: 1.6915e-04 - val_mse: 1.6915e-04 - val_My_MSE: 6.2413\n",
            "\n",
            "Epoch 00851: loss improved from 0.00019 to 0.00019, saving model to poids_train.hdf5\n",
            "Epoch 852/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.0162e-04 - mse: 2.0162e-04 - My_MSE: 6.2418 - val_loss: 1.8197e-04 - val_mse: 1.8197e-04 - val_My_MSE: 6.2415\n",
            "\n",
            "Epoch 00852: loss improved from 0.00019 to 0.00019, saving model to poids_train.hdf5\n",
            "Epoch 853/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.8749e-04 - mse: 1.8749e-04 - My_MSE: 6.2416 - val_loss: 1.8962e-04 - val_mse: 1.8962e-04 - val_My_MSE: 6.2416\n",
            "\n",
            "Epoch 00853: loss did not improve from 0.00019\n",
            "Epoch 854/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.9263e-04 - mse: 1.9263e-04 - My_MSE: 6.2417 - val_loss: 1.6326e-04 - val_mse: 1.6326e-04 - val_My_MSE: 6.2412\n",
            "\n",
            "Epoch 00854: loss did not improve from 0.00019\n",
            "Epoch 855/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.7647e-04 - mse: 1.7647e-04 - My_MSE: 6.2414 - val_loss: 1.6662e-04 - val_mse: 1.6662e-04 - val_My_MSE: 6.2412\n",
            "\n",
            "Epoch 00855: loss improved from 0.00019 to 0.00018, saving model to poids_train.hdf5\n",
            "Epoch 856/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.7577e-04 - mse: 1.7577e-04 - My_MSE: 6.2414 - val_loss: 1.6039e-04 - val_mse: 1.6039e-04 - val_My_MSE: 6.2412\n",
            "\n",
            "Epoch 00856: loss improved from 0.00018 to 0.00018, saving model to poids_train.hdf5\n",
            "Epoch 857/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.7839e-04 - mse: 1.7839e-04 - My_MSE: 6.2414 - val_loss: 1.9487e-04 - val_mse: 1.9487e-04 - val_My_MSE: 6.2417\n",
            "\n",
            "Epoch 00857: loss did not improve from 0.00018\n",
            "Epoch 858/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.9272e-04 - mse: 1.9272e-04 - My_MSE: 6.2417 - val_loss: 1.7410e-04 - val_mse: 1.7410e-04 - val_My_MSE: 6.2414\n",
            "\n",
            "Epoch 00858: loss did not improve from 0.00018\n",
            "Epoch 859/1000\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.8973e-04 - mse: 1.8973e-04 - My_MSE: 6.2416 - val_loss: 1.6560e-04 - val_mse: 1.6560e-04 - val_My_MSE: 6.2412\n",
            "\n",
            "Epoch 00859: loss did not improve from 0.00018\n",
            "Epoch 860/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.6880e-04 - mse: 1.6880e-04 - My_MSE: 6.2413 - val_loss: 1.5833e-04 - val_mse: 1.5833e-04 - val_My_MSE: 6.2411\n",
            "\n",
            "Epoch 00860: loss improved from 0.00018 to 0.00018, saving model to poids_train.hdf5\n",
            "Epoch 861/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.7075e-04 - mse: 1.7075e-04 - My_MSE: 6.2413 - val_loss: 1.5608e-04 - val_mse: 1.5608e-04 - val_My_MSE: 6.2411\n",
            "\n",
            "Epoch 00861: loss improved from 0.00018 to 0.00018, saving model to poids_train.hdf5\n",
            "Epoch 862/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.7970e-04 - mse: 1.7970e-04 - My_MSE: 6.2415 - val_loss: 1.5987e-04 - val_mse: 1.5987e-04 - val_My_MSE: 6.2411\n",
            "\n",
            "Epoch 00862: loss did not improve from 0.00018\n",
            "Epoch 863/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.7062e-04 - mse: 1.7062e-04 - My_MSE: 6.2413 - val_loss: 1.6535e-04 - val_mse: 1.6535e-04 - val_My_MSE: 6.2412\n",
            "\n",
            "Epoch 00863: loss did not improve from 0.00018\n",
            "Epoch 864/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.8704e-04 - mse: 1.8704e-04 - My_MSE: 6.2416 - val_loss: 1.6620e-04 - val_mse: 1.6620e-04 - val_My_MSE: 6.2412\n",
            "\n",
            "Epoch 00864: loss improved from 0.00018 to 0.00018, saving model to poids_train.hdf5\n",
            "Epoch 865/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.7620e-04 - mse: 1.7620e-04 - My_MSE: 6.2414 - val_loss: 1.5114e-04 - val_mse: 1.5114e-04 - val_My_MSE: 6.2410\n",
            "\n",
            "Epoch 00865: loss did not improve from 0.00018\n",
            "Epoch 866/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.7485e-04 - mse: 1.7485e-04 - My_MSE: 6.2414 - val_loss: 1.5436e-04 - val_mse: 1.5436e-04 - val_My_MSE: 6.2411\n",
            "\n",
            "Epoch 00866: loss improved from 0.00018 to 0.00017, saving model to poids_train.hdf5\n",
            "Epoch 867/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.7905e-04 - mse: 1.7905e-04 - My_MSE: 6.2414 - val_loss: 1.7256e-04 - val_mse: 1.7256e-04 - val_My_MSE: 6.2413\n",
            "\n",
            "Epoch 00867: loss did not improve from 0.00017\n",
            "Epoch 868/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.7470e-04 - mse: 1.7470e-04 - My_MSE: 6.2414 - val_loss: 1.5549e-04 - val_mse: 1.5549e-04 - val_My_MSE: 6.2411\n",
            "\n",
            "Epoch 00868: loss improved from 0.00017 to 0.00017, saving model to poids_train.hdf5\n",
            "Epoch 869/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.7190e-04 - mse: 1.7190e-04 - My_MSE: 6.2413 - val_loss: 1.4798e-04 - val_mse: 1.4798e-04 - val_My_MSE: 6.2410\n",
            "\n",
            "Epoch 00869: loss improved from 0.00017 to 0.00017, saving model to poids_train.hdf5\n",
            "Epoch 870/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.5990e-04 - mse: 1.5990e-04 - My_MSE: 6.2411 - val_loss: 1.4734e-04 - val_mse: 1.4734e-04 - val_My_MSE: 6.2409\n",
            "\n",
            "Epoch 00870: loss improved from 0.00017 to 0.00017, saving model to poids_train.hdf5\n",
            "Epoch 871/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.5844e-04 - mse: 1.5844e-04 - My_MSE: 6.2411 - val_loss: 1.5125e-04 - val_mse: 1.5125e-04 - val_My_MSE: 6.2410\n",
            "\n",
            "Epoch 00871: loss did not improve from 0.00017\n",
            "Epoch 872/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.6711e-04 - mse: 1.6711e-04 - My_MSE: 6.2413 - val_loss: 1.5151e-04 - val_mse: 1.5151e-04 - val_My_MSE: 6.2410\n",
            "\n",
            "Epoch 00872: loss did not improve from 0.00017\n",
            "Epoch 873/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.7194e-04 - mse: 1.7194e-04 - My_MSE: 6.2413 - val_loss: 1.5031e-04 - val_mse: 1.5031e-04 - val_My_MSE: 6.2410\n",
            "\n",
            "Epoch 00873: loss improved from 0.00017 to 0.00017, saving model to poids_train.hdf5\n",
            "Epoch 874/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.6009e-04 - mse: 1.6009e-04 - My_MSE: 6.2411 - val_loss: 1.5453e-04 - val_mse: 1.5453e-04 - val_My_MSE: 6.2411\n",
            "\n",
            "Epoch 00874: loss improved from 0.00017 to 0.00016, saving model to poids_train.hdf5\n",
            "Epoch 875/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.7313e-04 - mse: 1.7313e-04 - My_MSE: 6.2413 - val_loss: 1.4579e-04 - val_mse: 1.4579e-04 - val_My_MSE: 6.2409\n",
            "\n",
            "Epoch 00875: loss did not improve from 0.00016\n",
            "Epoch 876/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.6776e-04 - mse: 1.6776e-04 - My_MSE: 6.2413 - val_loss: 1.4840e-04 - val_mse: 1.4840e-04 - val_My_MSE: 6.2410\n",
            "\n",
            "Epoch 00876: loss did not improve from 0.00016\n",
            "Epoch 877/1000\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.7159e-04 - mse: 1.7159e-04 - My_MSE: 6.2413 - val_loss: 1.5335e-04 - val_mse: 1.5335e-04 - val_My_MSE: 6.2410\n",
            "\n",
            "Epoch 00877: loss did not improve from 0.00016\n",
            "Epoch 878/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.6522e-04 - mse: 1.6522e-04 - My_MSE: 6.2412 - val_loss: 1.4233e-04 - val_mse: 1.4233e-04 - val_My_MSE: 6.2409\n",
            "\n",
            "Epoch 00878: loss improved from 0.00016 to 0.00016, saving model to poids_train.hdf5\n",
            "Epoch 879/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.6153e-04 - mse: 1.6153e-04 - My_MSE: 6.2412 - val_loss: 1.4142e-04 - val_mse: 1.4142e-04 - val_My_MSE: 6.2409\n",
            "\n",
            "Epoch 00879: loss did not improve from 0.00016\n",
            "Epoch 880/1000\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.5877e-04 - mse: 1.5877e-04 - My_MSE: 6.2411 - val_loss: 1.4724e-04 - val_mse: 1.4724e-04 - val_My_MSE: 6.2409\n",
            "\n",
            "Epoch 00880: loss did not improve from 0.00016\n",
            "Epoch 881/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.4802e-04 - mse: 1.4802e-04 - My_MSE: 6.2410 - val_loss: 1.4502e-04 - val_mse: 1.4502e-04 - val_My_MSE: 6.2409\n",
            "\n",
            "Epoch 00881: loss improved from 0.00016 to 0.00016, saving model to poids_train.hdf5\n",
            "Epoch 882/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.5238e-04 - mse: 1.5238e-04 - My_MSE: 6.2410 - val_loss: 1.5072e-04 - val_mse: 1.5072e-04 - val_My_MSE: 6.2410\n",
            "\n",
            "Epoch 00882: loss did not improve from 0.00016\n",
            "Epoch 883/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.6613e-04 - mse: 1.6613e-04 - My_MSE: 6.2412 - val_loss: 1.5394e-04 - val_mse: 1.5394e-04 - val_My_MSE: 6.2411\n",
            "\n",
            "Epoch 00883: loss did not improve from 0.00016\n",
            "Epoch 884/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.8314e-04 - mse: 1.8314e-04 - My_MSE: 6.2415 - val_loss: 1.5403e-04 - val_mse: 1.5403e-04 - val_My_MSE: 6.2411\n",
            "\n",
            "Epoch 00884: loss did not improve from 0.00016\n",
            "Epoch 885/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.5460e-04 - mse: 1.5460e-04 - My_MSE: 6.2411 - val_loss: 1.3942e-04 - val_mse: 1.3942e-04 - val_My_MSE: 6.2408\n",
            "\n",
            "Epoch 00885: loss did not improve from 0.00016\n",
            "Epoch 886/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.6262e-04 - mse: 1.6262e-04 - My_MSE: 6.2412 - val_loss: 1.4116e-04 - val_mse: 1.4116e-04 - val_My_MSE: 6.2409\n",
            "\n",
            "Epoch 00886: loss improved from 0.00016 to 0.00016, saving model to poids_train.hdf5\n",
            "Epoch 887/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.5715e-04 - mse: 1.5715e-04 - My_MSE: 6.2411 - val_loss: 1.3944e-04 - val_mse: 1.3944e-04 - val_My_MSE: 6.2408\n",
            "\n",
            "Epoch 00887: loss did not improve from 0.00016\n",
            "Epoch 888/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.5539e-04 - mse: 1.5539e-04 - My_MSE: 6.2411 - val_loss: 1.4545e-04 - val_mse: 1.4545e-04 - val_My_MSE: 6.2409\n",
            "\n",
            "Epoch 00888: loss did not improve from 0.00016\n",
            "Epoch 889/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1.6164e-04 - mse: 1.6164e-04 - My_MSE: 6.2412 - val_loss: 1.3642e-04 - val_mse: 1.3642e-04 - val_My_MSE: 6.2408\n",
            "\n",
            "Epoch 00889: loss did not improve from 0.00016\n",
            "Epoch 890/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.7198e-04 - mse: 1.7198e-04 - My_MSE: 6.2413 - val_loss: 1.4176e-04 - val_mse: 1.4176e-04 - val_My_MSE: 6.2409\n",
            "\n",
            "Epoch 00890: loss did not improve from 0.00016\n",
            "Epoch 891/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.6551e-04 - mse: 1.6551e-04 - My_MSE: 6.2412 - val_loss: 1.3766e-04 - val_mse: 1.3766e-04 - val_My_MSE: 6.2408\n",
            "\n",
            "Epoch 00891: loss did not improve from 0.00016\n",
            "Epoch 892/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.6368e-04 - mse: 1.6368e-04 - My_MSE: 6.2412 - val_loss: 1.3547e-04 - val_mse: 1.3547e-04 - val_My_MSE: 6.2408\n",
            "\n",
            "Epoch 00892: loss did not improve from 0.00016\n",
            "Epoch 893/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.5602e-04 - mse: 1.5602e-04 - My_MSE: 6.2411 - val_loss: 1.3630e-04 - val_mse: 1.3630e-04 - val_My_MSE: 6.2408\n",
            "\n",
            "Epoch 00893: loss did not improve from 0.00016\n",
            "Epoch 894/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.5438e-04 - mse: 1.5438e-04 - My_MSE: 6.2411 - val_loss: 1.3236e-04 - val_mse: 1.3236e-04 - val_My_MSE: 6.2407\n",
            "\n",
            "Epoch 00894: loss improved from 0.00016 to 0.00016, saving model to poids_train.hdf5\n",
            "Epoch 895/1000\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.6024e-04 - mse: 1.6024e-04 - My_MSE: 6.2411 - val_loss: 1.3691e-04 - val_mse: 1.3691e-04 - val_My_MSE: 6.2408\n",
            "\n",
            "Epoch 00895: loss improved from 0.00016 to 0.00015, saving model to poids_train.hdf5\n",
            "Epoch 896/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.5542e-04 - mse: 1.5542e-04 - My_MSE: 6.2411 - val_loss: 1.3075e-04 - val_mse: 1.3075e-04 - val_My_MSE: 6.2407\n",
            "\n",
            "Epoch 00896: loss did not improve from 0.00015\n",
            "Epoch 897/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.5181e-04 - mse: 1.5181e-04 - My_MSE: 6.2410 - val_loss: 1.4192e-04 - val_mse: 1.4192e-04 - val_My_MSE: 6.2409\n",
            "\n",
            "Epoch 00897: loss did not improve from 0.00015\n",
            "Epoch 898/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.5667e-04 - mse: 1.5667e-04 - My_MSE: 6.2411 - val_loss: 1.4049e-04 - val_mse: 1.4049e-04 - val_My_MSE: 6.2408\n",
            "\n",
            "Epoch 00898: loss did not improve from 0.00015\n",
            "Epoch 899/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.5909e-04 - mse: 1.5909e-04 - My_MSE: 6.2411 - val_loss: 1.3253e-04 - val_mse: 1.3253e-04 - val_My_MSE: 6.2407\n",
            "\n",
            "Epoch 00899: loss did not improve from 0.00015\n",
            "Epoch 900/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.5755e-04 - mse: 1.5755e-04 - My_MSE: 6.2411 - val_loss: 1.3669e-04 - val_mse: 1.3669e-04 - val_My_MSE: 6.2408\n",
            "\n",
            "Epoch 00900: loss improved from 0.00015 to 0.00015, saving model to poids_train.hdf5\n",
            "Epoch 901/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.5842e-04 - mse: 1.5842e-04 - My_MSE: 6.2411 - val_loss: 1.3803e-04 - val_mse: 1.3803e-04 - val_My_MSE: 6.2408\n",
            "\n",
            "Epoch 00901: loss did not improve from 0.00015\n",
            "Epoch 902/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.5249e-04 - mse: 1.5249e-04 - My_MSE: 6.2410 - val_loss: 1.3328e-04 - val_mse: 1.3328e-04 - val_My_MSE: 6.2407\n",
            "\n",
            "Epoch 00902: loss improved from 0.00015 to 0.00015, saving model to poids_train.hdf5\n",
            "Epoch 903/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.6196e-04 - mse: 1.6196e-04 - My_MSE: 6.2412 - val_loss: 1.3684e-04 - val_mse: 1.3684e-04 - val_My_MSE: 6.2408\n",
            "\n",
            "Epoch 00903: loss did not improve from 0.00015\n",
            "Epoch 904/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.5433e-04 - mse: 1.5433e-04 - My_MSE: 6.2411 - val_loss: 1.3008e-04 - val_mse: 1.3008e-04 - val_My_MSE: 6.2407\n",
            "\n",
            "Epoch 00904: loss did not improve from 0.00015\n",
            "Epoch 905/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.5068e-04 - mse: 1.5068e-04 - My_MSE: 6.2410 - val_loss: 1.3552e-04 - val_mse: 1.3552e-04 - val_My_MSE: 6.2408\n",
            "\n",
            "Epoch 00905: loss did not improve from 0.00015\n",
            "Epoch 906/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.4979e-04 - mse: 1.4979e-04 - My_MSE: 6.2410 - val_loss: 1.2551e-04 - val_mse: 1.2551e-04 - val_My_MSE: 6.2406\n",
            "\n",
            "Epoch 00906: loss did not improve from 0.00015\n",
            "Epoch 907/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.5194e-04 - mse: 1.5194e-04 - My_MSE: 6.2410 - val_loss: 1.2761e-04 - val_mse: 1.2761e-04 - val_My_MSE: 6.2406\n",
            "\n",
            "Epoch 00907: loss did not improve from 0.00015\n",
            "Epoch 908/1000\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.5096e-04 - mse: 1.5096e-04 - My_MSE: 6.2410 - val_loss: 1.2983e-04 - val_mse: 1.2983e-04 - val_My_MSE: 6.2407\n",
            "\n",
            "Epoch 00908: loss improved from 0.00015 to 0.00015, saving model to poids_train.hdf5\n",
            "Epoch 909/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.4181e-04 - mse: 1.4181e-04 - My_MSE: 6.2409 - val_loss: 1.2875e-04 - val_mse: 1.2875e-04 - val_My_MSE: 6.2407\n",
            "\n",
            "Epoch 00909: loss improved from 0.00015 to 0.00015, saving model to poids_train.hdf5\n",
            "Epoch 910/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.4918e-04 - mse: 1.4918e-04 - My_MSE: 6.2410 - val_loss: 1.2619e-04 - val_mse: 1.2619e-04 - val_My_MSE: 6.2406\n",
            "\n",
            "Epoch 00910: loss improved from 0.00015 to 0.00015, saving model to poids_train.hdf5\n",
            "Epoch 911/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.4517e-04 - mse: 1.4517e-04 - My_MSE: 6.2409 - val_loss: 1.2979e-04 - val_mse: 1.2979e-04 - val_My_MSE: 6.2407\n",
            "\n",
            "Epoch 00911: loss improved from 0.00015 to 0.00015, saving model to poids_train.hdf5\n",
            "Epoch 912/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.5394e-04 - mse: 1.5394e-04 - My_MSE: 6.2411 - val_loss: 1.2808e-04 - val_mse: 1.2808e-04 - val_My_MSE: 6.2406\n",
            "\n",
            "Epoch 00912: loss improved from 0.00015 to 0.00015, saving model to poids_train.hdf5\n",
            "Epoch 913/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.4191e-04 - mse: 1.4191e-04 - My_MSE: 6.2409 - val_loss: 1.2685e-04 - val_mse: 1.2685e-04 - val_My_MSE: 6.2406\n",
            "\n",
            "Epoch 00913: loss did not improve from 0.00015\n",
            "Epoch 914/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.5295e-04 - mse: 1.5295e-04 - My_MSE: 6.2410 - val_loss: 1.2352e-04 - val_mse: 1.2352e-04 - val_My_MSE: 6.2406\n",
            "\n",
            "Epoch 00914: loss improved from 0.00015 to 0.00015, saving model to poids_train.hdf5\n",
            "Epoch 915/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.5333e-04 - mse: 1.5333e-04 - My_MSE: 6.2410 - val_loss: 1.2786e-04 - val_mse: 1.2786e-04 - val_My_MSE: 6.2406\n",
            "\n",
            "Epoch 00915: loss did not improve from 0.00015\n",
            "Epoch 916/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.4920e-04 - mse: 1.4920e-04 - My_MSE: 6.2410 - val_loss: 1.4076e-04 - val_mse: 1.4076e-04 - val_My_MSE: 6.2408\n",
            "\n",
            "Epoch 00916: loss did not improve from 0.00015\n",
            "Epoch 917/1000\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.6647e-04 - mse: 1.6647e-04 - My_MSE: 6.2412 - val_loss: 1.2594e-04 - val_mse: 1.2594e-04 - val_My_MSE: 6.2406\n",
            "\n",
            "Epoch 00917: loss did not improve from 0.00015\n",
            "Epoch 918/1000\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.4430e-04 - mse: 1.4430e-04 - My_MSE: 6.2409 - val_loss: 1.2963e-04 - val_mse: 1.2963e-04 - val_My_MSE: 6.2407\n",
            "\n",
            "Epoch 00918: loss improved from 0.00015 to 0.00014, saving model to poids_train.hdf5\n",
            "Epoch 919/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.5126e-04 - mse: 1.5126e-04 - My_MSE: 6.2410 - val_loss: 1.2352e-04 - val_mse: 1.2352e-04 - val_My_MSE: 6.2406\n",
            "\n",
            "Epoch 00919: loss did not improve from 0.00014\n",
            "Epoch 920/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.4460e-04 - mse: 1.4460e-04 - My_MSE: 6.2409 - val_loss: 1.2209e-04 - val_mse: 1.2209e-04 - val_My_MSE: 6.2406\n",
            "\n",
            "Epoch 00920: loss did not improve from 0.00014\n",
            "Epoch 921/1000\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.5112e-04 - mse: 1.5112e-04 - My_MSE: 6.2410 - val_loss: 1.2643e-04 - val_mse: 1.2643e-04 - val_My_MSE: 6.2406\n",
            "\n",
            "Epoch 00921: loss improved from 0.00014 to 0.00014, saving model to poids_train.hdf5\n",
            "Epoch 922/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.4141e-04 - mse: 1.4141e-04 - My_MSE: 6.2409 - val_loss: 1.2698e-04 - val_mse: 1.2698e-04 - val_My_MSE: 6.2406\n",
            "\n",
            "Epoch 00922: loss did not improve from 0.00014\n",
            "Epoch 923/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.5336e-04 - mse: 1.5336e-04 - My_MSE: 6.2410 - val_loss: 1.2636e-04 - val_mse: 1.2636e-04 - val_My_MSE: 6.2406\n",
            "\n",
            "Epoch 00923: loss did not improve from 0.00014\n",
            "Epoch 924/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.5807e-04 - mse: 1.5807e-04 - My_MSE: 6.2411 - val_loss: 1.3624e-04 - val_mse: 1.3624e-04 - val_My_MSE: 6.2408\n",
            "\n",
            "Epoch 00924: loss did not improve from 0.00014\n",
            "Epoch 925/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.4322e-04 - mse: 1.4322e-04 - My_MSE: 6.2409 - val_loss: 1.1918e-04 - val_mse: 1.1918e-04 - val_My_MSE: 6.2405\n",
            "\n",
            "Epoch 00925: loss did not improve from 0.00014\n",
            "Epoch 926/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.4986e-04 - mse: 1.4986e-04 - My_MSE: 6.2410 - val_loss: 1.1907e-04 - val_mse: 1.1907e-04 - val_My_MSE: 6.2405\n",
            "\n",
            "Epoch 00926: loss did not improve from 0.00014\n",
            "Epoch 927/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.3319e-04 - mse: 1.3319e-04 - My_MSE: 6.2407 - val_loss: 1.2118e-04 - val_mse: 1.2118e-04 - val_My_MSE: 6.2405\n",
            "\n",
            "Epoch 00927: loss improved from 0.00014 to 0.00014, saving model to poids_train.hdf5\n",
            "Epoch 928/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.4198e-04 - mse: 1.4198e-04 - My_MSE: 6.2409 - val_loss: 1.3037e-04 - val_mse: 1.3037e-04 - val_My_MSE: 6.2407\n",
            "\n",
            "Epoch 00928: loss did not improve from 0.00014\n",
            "Epoch 929/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.4416e-04 - mse: 1.4416e-04 - My_MSE: 6.2409 - val_loss: 1.2965e-04 - val_mse: 1.2965e-04 - val_My_MSE: 6.2407\n",
            "\n",
            "Epoch 00929: loss did not improve from 0.00014\n",
            "Epoch 930/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.3253e-04 - mse: 1.3253e-04 - My_MSE: 6.2407 - val_loss: 1.2085e-04 - val_mse: 1.2085e-04 - val_My_MSE: 6.2405\n",
            "\n",
            "Epoch 00930: loss improved from 0.00014 to 0.00014, saving model to poids_train.hdf5\n",
            "Epoch 931/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.4699e-04 - mse: 1.4699e-04 - My_MSE: 6.2409 - val_loss: 1.2026e-04 - val_mse: 1.2026e-04 - val_My_MSE: 6.2405\n",
            "\n",
            "Epoch 00931: loss did not improve from 0.00014\n",
            "Epoch 932/1000\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.4624e-04 - mse: 1.4624e-04 - My_MSE: 6.2409 - val_loss: 1.1733e-04 - val_mse: 1.1733e-04 - val_My_MSE: 6.2405\n",
            "\n",
            "Epoch 00932: loss improved from 0.00014 to 0.00014, saving model to poids_train.hdf5\n",
            "Epoch 933/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.4126e-04 - mse: 1.4126e-04 - My_MSE: 6.2409 - val_loss: 1.1628e-04 - val_mse: 1.1628e-04 - val_My_MSE: 6.2405\n",
            "\n",
            "Epoch 00933: loss improved from 0.00014 to 0.00014, saving model to poids_train.hdf5\n",
            "Epoch 934/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.3719e-04 - mse: 1.3719e-04 - My_MSE: 6.2408 - val_loss: 1.1605e-04 - val_mse: 1.1605e-04 - val_My_MSE: 6.2405\n",
            "\n",
            "Epoch 00934: loss improved from 0.00014 to 0.00014, saving model to poids_train.hdf5\n",
            "Epoch 935/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.3279e-04 - mse: 1.3279e-04 - My_MSE: 6.2407 - val_loss: 1.1724e-04 - val_mse: 1.1724e-04 - val_My_MSE: 6.2405\n",
            "\n",
            "Epoch 00935: loss did not improve from 0.00014\n",
            "Epoch 936/1000\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.3414e-04 - mse: 1.3414e-04 - My_MSE: 6.2407 - val_loss: 1.1636e-04 - val_mse: 1.1636e-04 - val_My_MSE: 6.2405\n",
            "\n",
            "Epoch 00936: loss improved from 0.00014 to 0.00014, saving model to poids_train.hdf5\n",
            "Epoch 937/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.3718e-04 - mse: 1.3718e-04 - My_MSE: 6.2408 - val_loss: 1.2792e-04 - val_mse: 1.2792e-04 - val_My_MSE: 6.2406\n",
            "\n",
            "Epoch 00937: loss did not improve from 0.00014\n",
            "Epoch 938/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.3383e-04 - mse: 1.3383e-04 - My_MSE: 6.2407 - val_loss: 1.1820e-04 - val_mse: 1.1820e-04 - val_My_MSE: 6.2405\n",
            "\n",
            "Epoch 00938: loss did not improve from 0.00014\n",
            "Epoch 939/1000\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.3953e-04 - mse: 1.3953e-04 - My_MSE: 6.2408 - val_loss: 1.1908e-04 - val_mse: 1.1908e-04 - val_My_MSE: 6.2405\n",
            "\n",
            "Epoch 00939: loss did not improve from 0.00014\n",
            "Epoch 940/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.4300e-04 - mse: 1.4300e-04 - My_MSE: 6.2409 - val_loss: 1.1864e-04 - val_mse: 1.1864e-04 - val_My_MSE: 6.2405\n",
            "\n",
            "Epoch 00940: loss improved from 0.00014 to 0.00014, saving model to poids_train.hdf5\n",
            "Epoch 941/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.3948e-04 - mse: 1.3948e-04 - My_MSE: 6.2408 - val_loss: 1.2369e-04 - val_mse: 1.2369e-04 - val_My_MSE: 6.2406\n",
            "\n",
            "Epoch 00941: loss did not improve from 0.00014\n",
            "Epoch 942/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.3533e-04 - mse: 1.3533e-04 - My_MSE: 6.2408 - val_loss: 1.1510e-04 - val_mse: 1.1510e-04 - val_My_MSE: 6.2404\n",
            "\n",
            "Epoch 00942: loss improved from 0.00014 to 0.00014, saving model to poids_train.hdf5\n",
            "Epoch 943/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.3636e-04 - mse: 1.3636e-04 - My_MSE: 6.2408 - val_loss: 1.1760e-04 - val_mse: 1.1760e-04 - val_My_MSE: 6.2405\n",
            "\n",
            "Epoch 00943: loss did not improve from 0.00014\n",
            "Epoch 944/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.3262e-04 - mse: 1.3262e-04 - My_MSE: 6.2407 - val_loss: 1.1656e-04 - val_mse: 1.1656e-04 - val_My_MSE: 6.2405\n",
            "\n",
            "Epoch 00944: loss did not improve from 0.00014\n",
            "Epoch 945/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.3676e-04 - mse: 1.3676e-04 - My_MSE: 6.2408 - val_loss: 1.1384e-04 - val_mse: 1.1384e-04 - val_My_MSE: 6.2404\n",
            "\n",
            "Epoch 00945: loss improved from 0.00014 to 0.00014, saving model to poids_train.hdf5\n",
            "Epoch 946/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.3059e-04 - mse: 1.3059e-04 - My_MSE: 6.2407 - val_loss: 1.2020e-04 - val_mse: 1.2020e-04 - val_My_MSE: 6.2405\n",
            "\n",
            "Epoch 00946: loss improved from 0.00014 to 0.00014, saving model to poids_train.hdf5\n",
            "Epoch 947/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.4330e-04 - mse: 1.4330e-04 - My_MSE: 6.2409 - val_loss: 1.2196e-04 - val_mse: 1.2196e-04 - val_My_MSE: 6.2406\n",
            "\n",
            "Epoch 00947: loss did not improve from 0.00014\n",
            "Epoch 948/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.3143e-04 - mse: 1.3143e-04 - My_MSE: 6.2407 - val_loss: 1.1477e-04 - val_mse: 1.1477e-04 - val_My_MSE: 6.2404\n",
            "\n",
            "Epoch 00948: loss did not improve from 0.00014\n",
            "Epoch 949/1000\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.3208e-04 - mse: 1.3208e-04 - My_MSE: 6.2407 - val_loss: 1.1231e-04 - val_mse: 1.1231e-04 - val_My_MSE: 6.2404\n",
            "\n",
            "Epoch 00949: loss did not improve from 0.00014\n",
            "Epoch 950/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.3994e-04 - mse: 1.3994e-04 - My_MSE: 6.2408 - val_loss: 1.1177e-04 - val_mse: 1.1177e-04 - val_My_MSE: 6.2404\n",
            "\n",
            "Epoch 00950: loss improved from 0.00014 to 0.00013, saving model to poids_train.hdf5\n",
            "Epoch 951/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.4143e-04 - mse: 1.4143e-04 - My_MSE: 6.2409 - val_loss: 1.1043e-04 - val_mse: 1.1043e-04 - val_My_MSE: 6.2404\n",
            "\n",
            "Epoch 00951: loss improved from 0.00013 to 0.00013, saving model to poids_train.hdf5\n",
            "Epoch 952/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.2994e-04 - mse: 1.2994e-04 - My_MSE: 6.2407 - val_loss: 1.1208e-04 - val_mse: 1.1208e-04 - val_My_MSE: 6.2404\n",
            "\n",
            "Epoch 00952: loss did not improve from 0.00013\n",
            "Epoch 953/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.4083e-04 - mse: 1.4083e-04 - My_MSE: 6.2408 - val_loss: 1.1069e-04 - val_mse: 1.1069e-04 - val_My_MSE: 6.2404\n",
            "\n",
            "Epoch 00953: loss did not improve from 0.00013\n",
            "Epoch 954/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.2226e-04 - mse: 1.2226e-04 - My_MSE: 6.2406 - val_loss: 1.0929e-04 - val_mse: 1.0929e-04 - val_My_MSE: 6.2404\n",
            "\n",
            "Epoch 00954: loss improved from 0.00013 to 0.00013, saving model to poids_train.hdf5\n",
            "Epoch 955/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.3427e-04 - mse: 1.3427e-04 - My_MSE: 6.2407 - val_loss: 1.1225e-04 - val_mse: 1.1225e-04 - val_My_MSE: 6.2404\n",
            "\n",
            "Epoch 00955: loss did not improve from 0.00013\n",
            "Epoch 956/1000\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.2819e-04 - mse: 1.2819e-04 - My_MSE: 6.2406 - val_loss: 1.1239e-04 - val_mse: 1.1239e-04 - val_My_MSE: 6.2404\n",
            "\n",
            "Epoch 00956: loss improved from 0.00013 to 0.00013, saving model to poids_train.hdf5\n",
            "Epoch 957/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.2738e-04 - mse: 1.2738e-04 - My_MSE: 6.2406 - val_loss: 1.1240e-04 - val_mse: 1.1240e-04 - val_My_MSE: 6.2404\n",
            "\n",
            "Epoch 00957: loss improved from 0.00013 to 0.00013, saving model to poids_train.hdf5\n",
            "Epoch 958/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.3424e-04 - mse: 1.3424e-04 - My_MSE: 6.2407 - val_loss: 1.1229e-04 - val_mse: 1.1229e-04 - val_My_MSE: 6.2404\n",
            "\n",
            "Epoch 00958: loss did not improve from 0.00013\n",
            "Epoch 959/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.3028e-04 - mse: 1.3028e-04 - My_MSE: 6.2407 - val_loss: 1.1103e-04 - val_mse: 1.1103e-04 - val_My_MSE: 6.2404\n",
            "\n",
            "Epoch 00959: loss improved from 0.00013 to 0.00013, saving model to poids_train.hdf5\n",
            "Epoch 960/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.4469e-04 - mse: 1.4469e-04 - My_MSE: 6.2409 - val_loss: 1.1178e-04 - val_mse: 1.1178e-04 - val_My_MSE: 6.2404\n",
            "\n",
            "Epoch 00960: loss did not improve from 0.00013\n",
            "Epoch 961/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.2599e-04 - mse: 1.2599e-04 - My_MSE: 6.2406 - val_loss: 1.1082e-04 - val_mse: 1.1082e-04 - val_My_MSE: 6.2404\n",
            "\n",
            "Epoch 00961: loss did not improve from 0.00013\n",
            "Epoch 962/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.2864e-04 - mse: 1.2864e-04 - My_MSE: 6.2407 - val_loss: 1.1301e-04 - val_mse: 1.1301e-04 - val_My_MSE: 6.2404\n",
            "\n",
            "Epoch 00962: loss did not improve from 0.00013\n",
            "Epoch 963/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.2078e-04 - mse: 1.2078e-04 - My_MSE: 6.2405 - val_loss: 1.1230e-04 - val_mse: 1.1230e-04 - val_My_MSE: 6.2404\n",
            "\n",
            "Epoch 00963: loss improved from 0.00013 to 0.00013, saving model to poids_train.hdf5\n",
            "Epoch 964/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.2989e-04 - mse: 1.2989e-04 - My_MSE: 6.2407 - val_loss: 1.1192e-04 - val_mse: 1.1192e-04 - val_My_MSE: 6.2404\n",
            "\n",
            "Epoch 00964: loss did not improve from 0.00013\n",
            "Epoch 965/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.2375e-04 - mse: 1.2375e-04 - My_MSE: 6.2406 - val_loss: 1.0836e-04 - val_mse: 1.0836e-04 - val_My_MSE: 6.2403\n",
            "\n",
            "Epoch 00965: loss improved from 0.00013 to 0.00013, saving model to poids_train.hdf5\n",
            "Epoch 966/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.2595e-04 - mse: 1.2595e-04 - My_MSE: 6.2406 - val_loss: 1.1066e-04 - val_mse: 1.1066e-04 - val_My_MSE: 6.2404\n",
            "\n",
            "Epoch 00966: loss improved from 0.00013 to 0.00013, saving model to poids_train.hdf5\n",
            "Epoch 967/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.2297e-04 - mse: 1.2297e-04 - My_MSE: 6.2406 - val_loss: 1.0653e-04 - val_mse: 1.0653e-04 - val_My_MSE: 6.2403\n",
            "\n",
            "Epoch 00967: loss did not improve from 0.00013\n",
            "Epoch 968/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.3945e-04 - mse: 1.3945e-04 - My_MSE: 6.2408 - val_loss: 1.0565e-04 - val_mse: 1.0565e-04 - val_My_MSE: 6.2403\n",
            "\n",
            "Epoch 00968: loss did not improve from 0.00013\n",
            "Epoch 969/1000\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.2249e-04 - mse: 1.2249e-04 - My_MSE: 6.2406 - val_loss: 1.0908e-04 - val_mse: 1.0908e-04 - val_My_MSE: 6.2404\n",
            "\n",
            "Epoch 00969: loss improved from 0.00013 to 0.00013, saving model to poids_train.hdf5\n",
            "Epoch 970/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.1929e-04 - mse: 1.1929e-04 - My_MSE: 6.2405 - val_loss: 1.0603e-04 - val_mse: 1.0603e-04 - val_My_MSE: 6.2403\n",
            "\n",
            "Epoch 00970: loss did not improve from 0.00013\n",
            "Epoch 971/1000\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.3333e-04 - mse: 1.3333e-04 - My_MSE: 6.2407 - val_loss: 1.1305e-04 - val_mse: 1.1305e-04 - val_My_MSE: 6.2404\n",
            "\n",
            "Epoch 00971: loss did not improve from 0.00013\n",
            "Epoch 972/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.2634e-04 - mse: 1.2634e-04 - My_MSE: 6.2406 - val_loss: 1.0666e-04 - val_mse: 1.0666e-04 - val_My_MSE: 6.2403\n",
            "\n",
            "Epoch 00972: loss did not improve from 0.00013\n",
            "Epoch 973/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.4529e-04 - mse: 1.4529e-04 - My_MSE: 6.2409 - val_loss: 1.1751e-04 - val_mse: 1.1751e-04 - val_My_MSE: 6.2405\n",
            "\n",
            "Epoch 00973: loss did not improve from 0.00013\n",
            "Epoch 974/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.3351e-04 - mse: 1.3351e-04 - My_MSE: 6.2407 - val_loss: 1.1737e-04 - val_mse: 1.1737e-04 - val_My_MSE: 6.2405\n",
            "\n",
            "Epoch 00974: loss did not improve from 0.00013\n",
            "Epoch 975/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.3208e-04 - mse: 1.3208e-04 - My_MSE: 6.2407 - val_loss: 1.0811e-04 - val_mse: 1.0811e-04 - val_My_MSE: 6.2403\n",
            "\n",
            "Epoch 00975: loss did not improve from 0.00013\n",
            "Epoch 976/1000\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.3794e-04 - mse: 1.3794e-04 - My_MSE: 6.2408 - val_loss: 1.1415e-04 - val_mse: 1.1415e-04 - val_My_MSE: 6.2404\n",
            "\n",
            "Epoch 00976: loss did not improve from 0.00013\n",
            "Epoch 977/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.3189e-04 - mse: 1.3189e-04 - My_MSE: 6.2407 - val_loss: 1.0549e-04 - val_mse: 1.0549e-04 - val_My_MSE: 6.2403\n",
            "\n",
            "Epoch 00977: loss did not improve from 0.00013\n",
            "Epoch 978/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.2737e-04 - mse: 1.2737e-04 - My_MSE: 6.2406 - val_loss: 1.0307e-04 - val_mse: 1.0307e-04 - val_My_MSE: 6.2403\n",
            "\n",
            "Epoch 00978: loss improved from 0.00013 to 0.00012, saving model to poids_train.hdf5\n",
            "Epoch 979/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.2954e-04 - mse: 1.2954e-04 - My_MSE: 6.2407 - val_loss: 1.0329e-04 - val_mse: 1.0329e-04 - val_My_MSE: 6.2403\n",
            "\n",
            "Epoch 00979: loss did not improve from 0.00012\n",
            "Epoch 980/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.2824e-04 - mse: 1.2824e-04 - My_MSE: 6.2406 - val_loss: 1.0613e-04 - val_mse: 1.0613e-04 - val_My_MSE: 6.2403\n",
            "\n",
            "Epoch 00980: loss improved from 0.00012 to 0.00012, saving model to poids_train.hdf5\n",
            "Epoch 981/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.2671e-04 - mse: 1.2671e-04 - My_MSE: 6.2406 - val_loss: 1.1451e-04 - val_mse: 1.1451e-04 - val_My_MSE: 6.2404\n",
            "\n",
            "Epoch 00981: loss did not improve from 0.00012\n",
            "Epoch 982/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.3905e-04 - mse: 1.3905e-04 - My_MSE: 6.2408 - val_loss: 1.0854e-04 - val_mse: 1.0854e-04 - val_My_MSE: 6.2403\n",
            "\n",
            "Epoch 00982: loss did not improve from 0.00012\n",
            "Epoch 983/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.2492e-04 - mse: 1.2492e-04 - My_MSE: 6.2406 - val_loss: 1.0790e-04 - val_mse: 1.0790e-04 - val_My_MSE: 6.2403\n",
            "\n",
            "Epoch 00983: loss did not improve from 0.00012\n",
            "Epoch 984/1000\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.3534e-04 - mse: 1.3534e-04 - My_MSE: 6.2408 - val_loss: 1.0402e-04 - val_mse: 1.0402e-04 - val_My_MSE: 6.2403\n",
            "\n",
            "Epoch 00984: loss did not improve from 0.00012\n",
            "Epoch 985/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.2605e-04 - mse: 1.2605e-04 - My_MSE: 6.2406 - val_loss: 1.0407e-04 - val_mse: 1.0407e-04 - val_My_MSE: 6.2403\n",
            "\n",
            "Epoch 00985: loss did not improve from 0.00012\n",
            "Epoch 986/1000\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.2064e-04 - mse: 1.2064e-04 - My_MSE: 6.2405 - val_loss: 1.0223e-04 - val_mse: 1.0223e-04 - val_My_MSE: 6.2402\n",
            "\n",
            "Epoch 00986: loss improved from 0.00012 to 0.00012, saving model to poids_train.hdf5\n",
            "Epoch 987/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.2809e-04 - mse: 1.2809e-04 - My_MSE: 6.2406 - val_loss: 1.0894e-04 - val_mse: 1.0894e-04 - val_My_MSE: 6.2403\n",
            "\n",
            "Epoch 00987: loss did not improve from 0.00012\n",
            "Epoch 988/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.2166e-04 - mse: 1.2166e-04 - My_MSE: 6.2405 - val_loss: 1.0367e-04 - val_mse: 1.0367e-04 - val_My_MSE: 6.2403\n",
            "\n",
            "Epoch 00988: loss improved from 0.00012 to 0.00012, saving model to poids_train.hdf5\n",
            "Epoch 989/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.2225e-04 - mse: 1.2225e-04 - My_MSE: 6.2406 - val_loss: 1.0093e-04 - val_mse: 1.0093e-04 - val_My_MSE: 6.2402\n",
            "\n",
            "Epoch 00989: loss did not improve from 0.00012\n",
            "Epoch 990/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.3026e-04 - mse: 1.3026e-04 - My_MSE: 6.2407 - val_loss: 1.0561e-04 - val_mse: 1.0561e-04 - val_My_MSE: 6.2403\n",
            "\n",
            "Epoch 00990: loss did not improve from 0.00012\n",
            "Epoch 991/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.3250e-04 - mse: 1.3250e-04 - My_MSE: 6.2407 - val_loss: 1.1050e-04 - val_mse: 1.1050e-04 - val_My_MSE: 6.2404\n",
            "\n",
            "Epoch 00991: loss did not improve from 0.00012\n",
            "Epoch 992/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.1784e-04 - mse: 1.1784e-04 - My_MSE: 6.2405 - val_loss: 1.0719e-04 - val_mse: 1.0719e-04 - val_My_MSE: 6.2403\n",
            "\n",
            "Epoch 00992: loss improved from 0.00012 to 0.00012, saving model to poids_train.hdf5\n",
            "Epoch 993/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.1817e-04 - mse: 1.1817e-04 - My_MSE: 6.2405 - val_loss: 1.1211e-04 - val_mse: 1.1211e-04 - val_My_MSE: 6.2404\n",
            "\n",
            "Epoch 00993: loss did not improve from 0.00012\n",
            "Epoch 994/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.3204e-04 - mse: 1.3204e-04 - My_MSE: 6.2407 - val_loss: 1.0047e-04 - val_mse: 1.0047e-04 - val_My_MSE: 6.2402\n",
            "\n",
            "Epoch 00994: loss did not improve from 0.00012\n",
            "Epoch 995/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.1200e-04 - mse: 1.1200e-04 - My_MSE: 6.2404 - val_loss: 1.0733e-04 - val_mse: 1.0733e-04 - val_My_MSE: 6.2403\n",
            "\n",
            "Epoch 00995: loss improved from 0.00012 to 0.00012, saving model to poids_train.hdf5\n",
            "Epoch 996/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.2272e-04 - mse: 1.2272e-04 - My_MSE: 6.2406 - val_loss: 1.0677e-04 - val_mse: 1.0677e-04 - val_My_MSE: 6.2403\n",
            "\n",
            "Epoch 00996: loss did not improve from 0.00012\n",
            "Epoch 997/1000\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.3012e-04 - mse: 1.3012e-04 - My_MSE: 6.2407 - val_loss: 1.0246e-04 - val_mse: 1.0246e-04 - val_My_MSE: 6.2402\n",
            "\n",
            "Epoch 00997: loss did not improve from 0.00012\n",
            "Epoch 998/1000\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.2651e-04 - mse: 1.2651e-04 - My_MSE: 6.2406 - val_loss: 1.1291e-04 - val_mse: 1.1291e-04 - val_My_MSE: 6.2404\n",
            "\n",
            "Epoch 00998: loss did not improve from 0.00012\n",
            "Epoch 999/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.2954e-04 - mse: 1.2954e-04 - My_MSE: 6.2407 - val_loss: 1.1179e-04 - val_mse: 1.1179e-04 - val_My_MSE: 6.2404\n",
            "\n",
            "Epoch 00999: loss did not improve from 0.00012\n",
            "Epoch 1000/1000\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.3153e-04 - mse: 1.3153e-04 - My_MSE: 6.2407 - val_loss: 1.0104e-04 - val_mse: 1.0104e-04 - val_My_MSE: 6.2402\n",
            "\n",
            "Epoch 01000: loss did not improve from 0.00012\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS8ZgVeRCN2M"
      },
      "source": [
        "model.load_weights(\"poids_train.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "Sa3Hrx64CN2M",
        "outputId": "eb6b88f7-2f30-478b-b58f-69b8ae90b24b"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, \"Evolution de l'erreur en fonction de la période\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4TfbnG6CN2N"
      },
      "source": [
        "**3. Prédictions single step**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRggLjz5CN2N",
        "outputId": "78575918-2648-4b0e-be08-b1c58155390d"
      },
      "source": [
        "# Création des instants d'entrainement et de validation\n",
        "y_train_timing = serie_etude.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_train)]\n",
        "y_val_timing = serie_etude.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_val)]\n",
        "\n",
        "# Calcul des prédictions\n",
        "pred_ent = model.predict(x_train, verbose=1)\n",
        "pred_val = model.predict(x_val, verbose=1)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 0s 2ms/step\n",
            "12/12 [==============================] - 0s 2ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "pV0yg4QXCN2O",
        "outputId": "8ce65389-d060-492c-90af-c0b7f3b717ae"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbes originales\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index,y=serie_entrainement[0],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index[temps_separation:],y=serie_test[0],line=dict(color='blue', width=1)))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index[taille_fenetre+horizon-1:],y=pred_ent[:,taille_fenetre-1,0],line=dict(color='green', width=1)))\n",
        "\n",
        "# Courbes des prédictions de validations\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index[temps_separation+taille_fenetre+horizon-1:],y=pred_val[:,taille_fenetre-1,0],line=dict(color='red', width=1)))\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"3dca988e-9350-4632-9182-de04ca66b734\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"3dca988e-9350-4632-9182-de04ca66b734\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '3dca988e-9350-4632-9182-de04ca66b734',\n",
              "                        [{\"line\": {\"color\": \"blue\", \"width\": 1}, \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500], \"y\": [-0.3997737259124131, -0.39663821834604174, -0.39365137681048884, -0.3907864798683186, -0.38801757504432344, -0.3821324507898519, -0.3763934293587709, -0.3706210862311272, -0.36463022967025455, -0.3577473769244706, -0.35024615038670454, -0.34194712553700274, -0.3326497313941316, -0.3199234065145797, -0.3051080675812845, -0.28778078536864177, -0.2674609584839191, -0.23571563448930655, -0.1966074971580348, -0.14853454184766277, -0.0896512592100983, -0.006225265450336413, 0.09729627454420649, 0.2225345894716806, 0.3675224176311685, 0.45838812095042686, 0.5508558289121261, 0.6407603294459197, 0.722847047324601, 0.7907720441641028, 0.8375505797233825, 0.8571591165468613, 0.8462014047925644, 0.8092912178307217, 0.7480946404894724, 0.6657516018680006, 0.5671962762650251, 0.47056335623297907, 0.36912442227360964, 0.26621164404319425, 0.1646829978238482, 0.06960722630425478, -0.020784283640840975, -0.10554314526311386, -0.18409904490985485, -0.30078906306526343, -0.4001453589493666, -0.4835585687120818, -0.5528548815255133, -0.6036192046351864, -0.6466106012197352, -0.6830658188631247, -0.7141831570379003, -0.7449865023027574, -0.7713362746616284, -0.7942833891599544, -0.8148146806574793, -0.8352498518764578, -0.8543521552329252, -0.8725509279710562, -0.8902306512050359, -0.9106978625168631, -0.9306203922502604, -0.949722695606728, -0.9676907796763472, -0.9869853235899075, -1.0028259454943649, -1.0141168742142899, -1.0201340036512985, -1.0203134281712518, -1.0140335699728829, -1.0017814384674935, -0.9847553331276295, -0.9601933979497229, -0.9339974180365264, -0.9090189616515849, -0.887699483869979, -0.8739030198892763, -0.8646498410745367, -0.8603308365585154, -0.8610421266197593, -0.8688406852191626, -0.8840661373409225, -0.9057765042552841, -0.9318699558713645, -0.9582069121930961, -0.983762090249316, -1.0056582897022008, -1.021018310213926, -1.0272469042637369, -1.0256384916027261, -1.0160841359152073, -0.9996539763023317, -0.9777385527937374, -0.9527985445202147, -0.9272818145754133, -0.9034247614401807, -0.884463434492248, -0.8696288715032435, -0.8596195464972716, -0.8548391646442274, -0.8556786150768666, -0.8629709402092584, -0.8763636990200658, -0.8949661769280917, -0.9198164729416379, -0.9476721296644034, -0.9760916920213083, -1.0017750304489237, -1.020063515447031, -1.0308994748485025, -1.0325399276023621, -1.0247926331515171, -1.010791112576582, -0.9912594719759403, -0.9679663244748469, -0.9429366039413475, -0.9169777207152321, -0.8934282524713483, -0.8739158359264159, -0.8595682823487135, -0.851346794523706, -0.8484119220187538, -0.8508149289824154, -0.858331534764749, -0.8755819207545547, -0.9002720163038566, -0.9307036964916674, -0.9635063435502911, -0.9922655308913934, -1.0165134731593815, -1.0328539205122806, -1.0388838659864288, -1.0348916704174658, -1.022620314856367, -1.0031655704785625, -0.9785523711520977, -0.9484346838742052, -0.9182529164106153, -0.890852229006303, -0.8684690201421161, -0.85476867643996, -0.8459384268508247, -0.8422089600432219, -0.8435930920542909, -0.8539164099701814, -0.8738389397035787, -0.9022264619676347, -0.9360672080345601, -0.9659990627739292, -0.9948159222821595, -1.0194996098128917, -1.0370447646569065, -1.0446126345877982, -1.04320287050245, -1.0326937200480366, -1.014161730344278, -0.9895997951663714, -0.961334025255141, -0.9319853002056202, -0.9039117708514828, -0.8806891115546568, -0.8615483600867706, -0.8472969267876147, -0.8384282290870608, -0.8349999391522369, -0.8381911323999797, -0.8478159762917657, -0.863329789249165, -0.8875328753871651, -0.9173301617365692, -0.9507351625407506, -0.9845438685148273, -1.0143731949570804, -1.0373779816225344, -1.050059450372098, -1.0505080116719814, -1.0413445451172185, -1.0241774633688199, -1.000487018716401, -0.9724839775665309, -0.9395723941922213, -0.9075323013433996, -0.87897176257796, -0.855832407522541, -0.8413502855548735, -0.831956130331599, -0.8278165503355313, -0.8289251375481004, -0.8369672008531547, -0.8520901246777984, -0.8737107793321833, -0.9006372733623331, -0.9313316823115042, -0.9639933529615932, -0.9961167500518218, -1.0245683525015754, -1.0454456770018676, -1.0573005113559315, -1.0580694735843033, -1.0477525636869827, -1.0304701376043282, -1.0068181410633281, -0.9788086918948882, -0.9486397404684378, -0.9174583221079644, -0.8885325262840482, -0.863522029806258, -0.8436251321471399, -0.8299632365564024, -0.8218635010842202, -0.8194092299720005, -0.8225171189783361, -0.8329301491542032, -0.8503407356082527, -0.8741401165763576, -0.9031107685302622, -0.9356955429575138, -0.970004074380032, -1.0034282992399228, -1.032738576178025, -1.0539683416996541, -1.06550277512523, -1.0655668553109277, -1.0539683416996541, -1.0353722718101979, -1.010246430998152, -0.9806990573729688, -0.9489985895083445, -0.9164778952667905, -0.8862448636546425, -0.8599399474257599, -0.83872299794127, -0.8236641543023239, -0.8142379589862007, -0.8105341242528769, -0.8124821618980852, -0.821568732230011, -0.8377297550629565, -0.8604846290041898, -0.8888016630639783, -0.9213543973983811, -0.9565984995320851, -0.9922334907985446, -1.0252219703956915, -1.0524304172429106, -1.0701165484954602, -1.0754992840940623, -1.0679378221817404, -1.0519177757573295, -1.0283106353463176, -0.9989939503896459, -0.9662938316281384, -0.9309600172344581, -0.8971192711675324, -0.8667708952211286, -0.8413887336662922, -0.8229977203710683, -0.8100214827672957, -0.8026202213192178, -0.8007747119711257, -0.8076376998593433, -0.8259902650431483, -0.8552108297212737, -0.893133483617139, -0.928755658846459, -0.966825697169429, -1.0047227189910153, -1.0389992103206847, -1.0654386949395322, -1.0814587413639432, -1.0844705100917325, -1.0740895200087142, -1.0555703463420951, -1.0295089348188635, -0.997987891474193, -0.9633717751603259, -0.9268973334612274, -0.8921402407388256, -0.8609844544526314, -0.8347756585022953, -0.8151478976231071, -0.8010566647881954, -0.7926429364060947, -0.7898810804025262, -0.795571400892477, -0.8130076194208058, -0.8416963185576408, -0.879676644620634, -0.9158370934098141, -0.9552912637438532, -0.9956553727147988, -1.0336100667035129, -1.0657590958680203, -1.0874822788195215, -1.0953000614746342, -1.088123080676498, -1.0714622323951108, -1.0462146392302392, -1.0143475628828011, -0.9783857626692838, -0.9392904413751516, -0.9012652591821702, -0.8664889424040589, -0.8365378636089804, -0.8139047420205727, -0.7966671720679068, -0.7849981702523658, -0.7789169606296595, -0.7793719299481129, -0.7902911935909912, -0.8113351265740973, -0.8413951416848618, -0.8744669255234155, -0.9128509567563039, -0.9549067826296673, -0.9979942994927627, -1.03314228134792, -1.0640930110398819, -1.0880590004908004, -1.1027333630155607, -1.1063218534146289, -1.0979273490882375, -1.0778702509648752, -1.0480729646154712, -1.0085162659843159, -0.9646661949114185, -0.92029707433437, -0.8785103852409369, -0.8467009810606267, -0.8195437983619654, -0.7976219668348016, -0.7812751114633327, -0.7693305648492921, -0.7647360155347711, -0.7672992229626767, -0.7766677461116722, -0.796442891417965, -0.8253046070561836, -0.8626377232436305, -0.9068338273192952, -0.9525037756660056, -0.9997693206365874, -1.0449330355162862, -1.0831889063777795, -1.1075393769428838, -1.118881569811367, -1.115100838855206, -1.096389424631494, -1.0703087890525531, -1.0365897953384533, -0.9978405070470883, -0.9566625797177828, -0.9157537891684072, -0.8771134371927282, -0.8423499364517567, -0.8125590581209223, -0.7883175238715038, -0.7699329185948499, -0.7574565064395188, -0.7507985751455336, -0.7509203274983591, -0.7629033222238184, -0.7864912385791211, -0.8204473289803021, -0.8547622684213902, -0.894767528352429, -0.9392455852451631, -0.9860882009901405, -1.0263433736454002, -1.0638366902970913, -1.09568454258882, -1.119201970739855, -1.1318898475079886, -1.1318898475079886, -1.1184330085114833, -1.0927368540467284, -1.0581335537700012, -1.0168466901250093, -0.9719713360809497, -0.9264487721613438, -0.8854630853891312, -0.8478992805331725, -0.8149108009360259, -0.7872345687332136, -0.7641785179192015, -0.7474471814335469, -0.7369508470162729, -0.7324844580731472, -0.7343043353469603, -0.7432691533260605, -0.7591610393790761, -0.7816339605032396, -0.8103290676586444, -0.8451053844367554, -0.8854246372777126, -0.930197463024656, -0.974598623694553, -1.0195700980171591, -1.0624910063974407, -1.1002342357733526, -1.1294548004514782, -1.1465001298470512, -1.1489351769035616, -1.136119139764033, -1.112922112541486, -1.080048977278595, -1.0398642928276027, -0.9951683633034966, -0.9483129315213797, -0.902380254413309, -0.8594593460330275, -0.8210624987629995, -0.789182606378422, -0.7627879778895628, -0.7420259977235263, -0.7268774418246033, -0.7151635838790742, -0.7121710392069943, -0.7175281427313173, -0.7306838048550434, -0.7507729430712546, -0.7780454701041716, -0.8123924496381085, -0.8533332802803328, -0.9045461646898894, -0.9611994568651758, -1.0203326522269613, -1.0770372085508058, -1.1131784332842767, -1.1421426772196115, -1.1611744923718115, -1.1685437137270405, -1.1629687375713456, -1.1444495639047265, -1.1140755558840436, -1.0740895200087142, -1.0237545341432155, -0.9697477536372415, -0.9158114613355349, -0.8648677137059085, -0.8250867344248116, -0.7899900167182123, -0.7600709780159826, -0.7355538989680642, -0.7137345957380168, -0.6989256648232913, -0.6907938892582605, -0.6889227478358892, -0.6935493372432591, -0.7049940584088581, -0.7231543830355702, -0.747799622454884, -0.7787759842211248, -0.8162244447428275, -0.8599079073329109, -0.9091278979672708, -0.9600716455968974, -1.0131620794473948, -1.0657590958680203, -1.1140755558840436, -1.1541897521307685, -1.181039349938081, -1.1907795381641229, -1.1818723923521501, -1.160021049029254, -1.1263148713522935, -1.0831889063777795, -1.0335844346292338, -0.9805132248344456, -0.9271728782597273, -0.8760497061101473, -0.8289956257523678, -0.7895799035297474, -0.7553674923857756, -0.7265954890075338, -0.7032959334878707, -0.682271224560474, -0.6684042723755038, -0.6612272915773677, -0.6602276406804846, -0.6664177866188769, -0.6805474675652072, -0.7026102755009058, -0.732381929776031, -0.7656908103016661, -0.8055422777870305, -0.8518530279907173, -0.9040976033900058, -0.9586875135858282, -1.0160585038409284, -1.0735127983374353, -1.1272119939520606, -1.1727089257973875, -1.2044926979034185, -1.2178854567142259, -1.2108366362874852, -1.1893056938930768, -1.1546383134306517, -1.1092695419567205, -1.0563393085704669, -0.9995963041352037, -0.941968993137313, -0.8861551513946657, -0.8341476726824584, -0.790541106315212, -0.7519904665995099, -0.718784114370991, -0.6909733137782138, -0.665219487146331, -0.6460659196413053, -0.6330448259075443, -0.6256051163480478, -0.6233174537186419, -0.6287706775215114, -0.6417276910695748, -0.6617463410815186, -0.6835592362929965, -0.7105369944717043, -0.7429295283418631, -0.7809226704419957, -0.8430420024572912, -0.916080598115465, -0.9979302193070649, -1.0829966658206867, -1.136311380321126, -1.1836666375516842, -1.2213457867418984, -1.2456321771213055, -1.2535140399621156, -1.243197130064795, -1.2145532870579485, -1.1700816381837837, -1.1134988342127647, -1.048970087215238, -0.9810835384871547, -0.9135814708732571, -0.8592735134945044, -0.8088744474433078, -0.7632429472080161, -0.7228980622927799, -0.6861352597580417, -0.655293466381766, -0.6300907293468828, -0.6101297515020669, -0.5933920069978426, -0.5820498141293596, -0.5755905314110371, -0.573475885283015, -0.5752573144454094, -0.5807938424896858, -0.5899829411187278, -0.6027477141096984, -0.6206004538450619, -0.6429708466721092, -0.6702177416307471, -0.7027256198351616, -0.7572450418267165, -0.824016595323661, -0.9032389289016575, -0.9931818775468697, -1.0533275398426776, -1.1139473955126484, -1.172068123940411, -1.2241012347268974, -1.2661378365445513, -1.2935641560231428, -1.302855782949301, -1.292410712680585, -1.2658815158017607, -1.2244216356553854, -1.1706583598550626, -1.1079238580570698, -1.0422416677169855, -0.9749895128273086, -0.9088587611873407, -0.8459768749622433, -0.7923609835890252, -0.7434485778460139, -0.6996113228102561, -0.660977378853147, -0.6262395101864544, -0.5966729125055619, -0.5719123287519925, -0.5515156056444325, -0.5343292998403245, -0.5209109089552382, -0.5108375037635685, -0.5036541149468627, -0.4989826694095046, -0.4965732544272732, -0.49620799736879656, -0.49770106569555167, -0.5011934358160732, -0.5066082115075241, -0.5139582088070438, -0.5232626517703416, -0.5368604671753815, -0.5536110277167455, -0.5739052225271892, -0.5982236529994448, -0.6466298252754447, -0.7095886077233793, -0.7897977761611193, -0.8896347054780477, -0.9659734306996502, -1.0494186485151213, -1.1368881019924046, -1.2228196310129444, -1.2819656424118693, -1.3317559466989382, -1.3678330912467114, -1.3866726658418185, -1.385519222499261, -1.3631552376907834, -1.3204137538304555, -1.2601783792746706, -1.1875755288792407, -1.1067063345288146, -1.0221525295007743, -0.9378422291783849, -0.8714743808513357, -0.8086437587747963, -0.7502090374371153, -0.6966828583238738, -0.6409907689340519, -0.5920975872467501, -0.54968291233348, -0.5132533267643697, -0.4740106210431329, -0.44230246355622505, -0.41682225931727107, -0.39617799445236124, -0.38159417722999167, -0.36901844078682916, -0.35795307432056006, -0.34788671794931736, -0.3396184515887504, -0.33156036823727175, -0.32351189691364773, -0.31525836899579124, -0.30513369965556364, -0.29420161997554567, -0.28220580921294686, -0.26883868247641846, -0.2509731267039155, -0.23032008285356498, -0.20635409340264635, -0.17844076451275295, -0.13944797151573693, -0.09287449255068975, -0.03763096446075142, 0.027282263650961296, 0.09890468720521729, 0.1796457211842479, 0.2678136486856354, 0.35932015386187005, 0.42718107051567444, 0.4893388506423885, 0.5413078812431772, 0.5782821483907176, 0.5960964400146623, 0.591482666644432, 0.5634796254945619, 0.5138815617645858, 0.4506984986667096, 0.3757246814004669, 0.29344572296469273, 0.20825111607967595, 0.13704521373245462, 0.06831921457173214, 0.003405986460019419, -0.05680375602148631, -0.11377744912526103, -0.164951885423399, -0.2103911451015979, -0.250332324846939, -0.2968160915520095, -0.3349822501535259, -0.3660880538948759, -0.3914593218181439, -0.40710770316550837, -0.42076895795438896, -0.43281603286554593, -0.4436141849574558, -0.4534972719976033, -0.4627228963324931, -0.4715307178566341, -0.48016872688867646, -0.4901011556718112, -0.5003603934020039, -0.5112091688406148, -0.5229102107490046, -0.5380075024993693, -0.5550207918020936, -0.5743601918456425, -0.5964998960041782, -0.626989248359117, -0.6629190084797855, -0.7051478508545326, -0.7545985301574039, -0.8264003782316133, -0.9112874002252814, -1.0083304334457925, -1.1132425134699744, -1.1796936660384305, -1.242812648950609, -1.298690570878954, -1.3429058990103282, -1.371293421274384, -1.3798801661578681, -1.3666796479041536, -1.331948187256031, -1.282734604640241, -1.2200641830279457, -1.1476535731896087, -1.0694757466384837, -0.9975329221557396, -0.9264615881984835, -0.8581777423190746, -0.7940398844543034, -0.7406931298610153, -0.6917422760065856, -0.6473218912809792, -0.6074191596470566, -0.5563664757017442, -0.5139325767327647, -0.4790793637318165, -0.4506745398176221, -0.43311592813461086, -0.4180391420436694, -0.405061750996554, -0.3938100393502762, -0.3839388712246684, -0.3751810322453715, -0.3672863533674219, -0.3600100482814545, -0.3525075401399744, -0.34526135274128483, -0.33806514788743947, -0.3307081017674931, -0.32230078140396223, -0.3132654752205945, -0.30338431058601795, -0.2924201908131512, -0.27758562782414675, -0.26045699418716667, -0.24057291256518792, -0.21738229336121082, -0.1851115118438776, -0.14641989571964054, -0.10019244975736065, -0.04522446646592216, 0.03494625386039945, 0.1310985724997133, 0.2419893338494852, 0.3613707198041947, 0.43083364110044015, 0.4954905484693622, 0.5507917487264287, 0.5920593883157109, 0.6150641749811647, 0.6160894579523272, 0.5934691524010589, 0.5486130224127086, 0.4892106902709932, 0.41673600024695867, 0.3354182445966491, 0.24955079576180703, 0.17526263648252907, 0.1026597860870992, 0.033369881292237454, -0.03150489870805665, -0.09292575669924787, -0.14860503005193018, -0.19847863858040599, -0.2427003747303497, -0.29475911759111517, -0.3380068349184547, -0.37367642948519086, -0.4031106375024322, -0.4226272833396206, -0.43966364150919607, -0.45470518349800393, -0.46823058829320546, -0.48068777639282734, -0.4924080423569263, -0.5037053790954209, -0.5149065955553689, -0.5280174015491067, -0.5417305612884025, -0.556398515794593, -0.572386522126155, -0.5930844221064939, -0.6165377700718313, -0.6432527994891787, -0.6738190480669546, -0.7162209069430853, -0.765799746617352, -0.8231899609281613, -0.8886991347668621, -0.9618274426850129, -1.0410882243744277, -1.1232390224388067, -1.2023780517753961, -1.2544111625618826, -1.2968963256794201, -1.3260528101718478, -1.338932927497074, -1.33361427208417, -1.309456042076158, -1.2674194402585042, -1.2105803155446946, -1.1425271583337973, -1.0677455816246475, -0.9904584696547198, -0.9141902326373847, -0.8528719029433097, -0.7953983843910934, -0.7425194151533981, -0.6946771485115376, -0.6523201457653953, -0.6149485814665298, -0.5823253589278595, -0.5541428932580359, -0.5272868874321536, -0.5049805747908039, -0.4866600496998477, -0.4717678145437154, -0.4597547021309782, -0.45021700729174097, -0.4427805017415295, -0.4370773652144392, -0.43290318191809474, -0.42987731554945197, -0.4278350800312681, -0.4266169157011559, -0.42608440935800845, -0.42625101784082237, -0.42705778737875566, -0.4284496090121085, -0.43069433791709694, -0.43367150334460947, -0.4374221166134925, -0.44200128668344607, -0.44872393896498586, -0.4570030989571214, -0.46713481711777577, -0.479502292957421, -0.502475039530026, -0.5329451678292555, -0.573283644725922, -0.6269956563776867, -0.685013856508333, -0.7578538035908441, -0.8478736484588936, -0.9563678108635736, -1.0444203940307053, -1.1395153896060082, -1.2377503142804953, -1.3325889891130074, -1.3966691748106508, -1.4490226865256257, -1.485035750887701, -1.5009276369407167, -1.494071057071069, -1.463504808493293, -1.4104464147356441, -1.338420286011493, -1.2737633786425708, -1.203147014003768, -1.128749918408804, -1.0528148983570966, -0.9770721188624822, -0.9031620326788202, -0.8322637152229476, -0.7652358409832127, -0.6817521750563229, -0.6069641903286034, -0.5409103349114726, -0.48324457580216335, -0.4269866583726313, -0.3794346749719811, -0.33930702188625983, -0.30526186002695893, -0.27491989209912476, -0.2485188555916957, -0.22500783545923034, -0.20329746854486877, -0.18354795531285512, -0.16402272273078317, -0.14419631327593233, -0.12354967744415163, -0.09800090740650122, -0.07025418699942167, -0.039892995015878195, -0.006532850341685067, 0.03804132682959564, 0.08728054151966479, 0.14059525602010411, 0.19672949869123965, 0.24807695149076123, 0.29767501522073725, 0.3425311452090877, 0.3793772519852326, 0.4049452460785922, 0.4159670380185869, 0.4105843024199848, 0.3888611194684838, 0.35566758327710457, 0.31202897681700925, 0.26089298863029, 0.20534828366757263, 0.15048923669182018, 0.09691820144859035, 0.0465255434159636, 0.0006313144193114399, -0.03970075445878526, -0.07445784718118706, -0.10356947554362643, -0.1271445758617894, -0.14866270221905806, -0.163157640223865, -0.1711676634360704, -0.17331434965694148, -0.16949517058936192, -0.1593128290820064, -0.14281858928343297, -0.12013420354646723, -0.09316285338632915, -0.06025767803058926, -0.021149540699317543, 0.024276902941741826, 0.09673877692863699, 0.17954960090570143, 0.2681340496141236, 0.3522713334351293, 0.39360305321010924, 0.42564314605893094, 0.4455080036252004, 0.4511470599665932, 0.44127871136915603, 0.4160311182042845, 0.3768781247430244, 0.3265110987846768, 0.2669165260858685, 0.2030670290567366, 0.13864721837489574, 0.07653429437817, 0.025776379287066713, -0.02057281902803873, -0.06194298691443728, -0.09806498759219884, -0.1315853327306361, -0.1591077724877739, -0.18100397194065868, -0.19773530842631334, -0.21015404841451663, -0.21804872729246627, -0.22181664221148772, -0.22184227428576678, -0.21843961642522192, -0.21171119692696935, -0.20170827993956722, -0.1884500895187248, -0.16848270365533916, -0.14353628736324658, -0.1131815033982729, -0.07699542253481374, -0.01661907157049413, 0.05698983774038873, 0.14342760022793993, 0.23930437406875393, 0.29825173689201595, 0.3554753427200115, 0.4078929346206839, 0.4517237816378719, 0.48318715281541486, 0.4986304775685468, 0.49542646828366466, 0.47338288440367526, 0.4371775794845068, 0.3879639968687167, 0.32888206565548944, 0.26332803568680035, 0.19864549624359917, 0.13413597330178156, 0.07199100921220708, 0.013812608617316693, -0.037079874863751666, -0.0829100236747062, -0.12348559725845396, -0.15881300363356476, -0.19114145731802584, -0.21810639945959417, -0.24017561541386254, -0.25786815468498187, -0.27445851476210176, -0.28599294818767756, -0.29313788889296477, -0.29659181090206777, -0.29705959625766054, -0.2954191435038009, -0.29179220499331426, -0.2862749010047472, -0.2780598211983093, -0.2674994065953376, -0.2544462727687277, -0.23868895510567723, -0.21339009779224763, -0.18207411104180932, -0.1437349359389093, -0.09717427301100162, -0.03645829706248455, 0.03672768302279394, 0.12283222854471733, 0.2204455754179375, 0.2887037892230673, 0.3579103897765221, 0.4250023442019545, 0.4856862800576228, 0.5354125041589941, 0.5687342007217686, 0.5813579973042043, 0.5711051675925813, 0.5423331642143396, 0.49632359088343175, 0.43576781539915876, 0.36451064890337925, 0.2910747560938799, 0.21509487991218423, 0.13962123719749991, 0.06690945048638397, 0.006469019336366747, -0.04974852757617577, -0.1013074449884996, -0.14799626828780257, -0.20083038139550952, -0.24609021655375501, -0.2843909435452364, -0.31650152459832553, -0.34036562655398495, -0.3606361116957204, -0.377832029527683, -0.39247691516702243, -0.40506059755321144, -0.4159603808196378, -0.425527552544296, -0.4341085302110674, -0.4419820626277368]}, {\"line\": {\"color\": \"blue\", \"width\": 1}, \"type\": \"scatter\", \"x\": [1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500], \"y\": [-0.4494108785556646, -0.45660964661693787, -0.46379752104664257, -0.471722958413727, -0.48006619859156024, -0.4890117925149512, -0.49879683687098136, -0.5118115225861728, -0.5266524935937469, -0.5436914149707502, -0.5633960720727756, -0.5906814151428321, -0.6231764773101072, -0.6618232373043559, -0.7077046502638684, -0.7730536236383252, -0.8517633157307406, -0.944307919915277, -1.0489060070295404, -1.1197786924111337, -1.191292179649704, -1.2599861387175775, -1.3215671971730127, -1.371293421274384, -1.4037820754230892, -1.4146757069916884, -1.4019237500378576, -1.3702681383032218, -1.3207341547589435, -1.2563976483185095, -1.181039349938081, -1.1045276082150945, -1.0255680033984587, -0.9470056957331479, -0.8711411638857078, -0.8089961997961334, -0.7508498392940918, -0.6971442356608969, -0.6481100775650601, -0.5848052621143583, -0.5309714981097682, -0.4857308870072319, -0.44805045621330364, -0.423295639676447, -0.4018375564531771, -0.3831820842315793, -0.36683779206753836, -0.35234221326087445, -0.3393224011308273, -0.3274271962597737, -0.3163156920598024, -0.30460824213284293, -0.29313788889296477, -0.28160345546738896, -0.26969094894619705, -0.25512552273712275, -0.2392977168698048, -0.22182945824862724, -0.20230422566655532, -0.1761979580133354, -0.14606104667973374, -0.11127191386448314, -0.07111286148777007, -0.014119944328286059, 0.0526003450201002, 0.1290480065573887, 0.2138389082725104, 0.27928400192551367, 0.34509435263699334, 0.4081492553634743, 0.4643475782203076, 0.5091396280229604, 0.5374630701013187, 0.5454090131278264, 0.5314395326457402, 0.500745123696569, 0.4542869890657776, 0.39507689748115526, 0.32663925915607195, 0.25672777655994306, 0.18527196148850097, 0.1149439576853375, 0.047813555148486245, -0.009128097862439665, -0.06154568976311191, -0.10905473943934467, -0.15152708651974273, -0.19250636527338563, -0.22783377164849644, -0.2579450509078191, -0.2833464365183649, -0.3039161761273084, -0.32107684985713725, -0.3352834270263048, -0.3469601184641294, -0.3565176781609329, -0.3642611278006361, -0.37046985699288076, -0.3754149249231679, -0.3794269853496974, -0.3825226991207505, -0.38485457707828774, -0.38656808124384273, -0.38786570500422, -0.38866798892915455, -0.38904734362848453, -0.3890761797120485, -0.38875577878356027, -0.38807652881516524, -0.3870480418347181, -0.38568121147378737, -0.38363833515374646, -0.38105205885898963, -0.37783651514068184, -0.37388469008870817, -0.36780988848457163, -0.3600600308262986, -0.3501743805787232, -0.3375172622997247, -0.32127549843279996, -0.3004814781739147, -0.2738497529979741, -0.2396245258168628, -0.19713295468075548, -0.1431582142676305, -0.07506020092674487, 0.00996138945688831, 0.08719082925968812, 0.17611490295230778, 0.27601591245493373, 0.3839269451697652, 0.46582142249135333, 0.5457934942420123, 0.619357547422907, 0.6811949266211327, 0.72585881605239, 0.7477742395609842, 0.7431604661907537, 0.7112485337133275, 0.6601766257123057, 0.5906496242303626, 0.5065764205950546, 0.41250670799091405, 0.3253576554421191, 0.23688214304938296, 0.1495792980549137, 0.06529462980680337, -0.018099323860109713, -0.09571965279566509, -0.16699604334715382, -0.2317042148646341, -0.312464472899374, -0.38118790885266857, -0.4391817585127498, -0.48794806143237035, -0.532060861266628, -0.5691632887855635, -0.6008317165573389, -0.6286873732801044, -0.6518908085212212, -0.6738190480669546, -0.6951385258485606, -0.7164900437230154, -0.741487724163666, -0.7677029281325719, -0.7955073207067794, -0.8252405268704859, -0.8634835816948395, -0.9044628604484823, -0.9475375612744382, -0.991624729034417, -1.0358144250915118, -1.0769090481794106, -1.1119609097560215, -1.1381697057063576, -1.1528440682311178, -1.15386935120228, -1.140412512205775, -1.1136269945841601, -1.0776139302220846, -1.0340906680962452, -0.9863573377700707, -0.9374000758970712, -0.8926785142986858, -0.8511032898180547, -0.8139175580577124, -0.7819415453945883, -0.7544575537488691, -0.733170116060112, -0.717970296012631, -0.7086530370121936, -0.7047569617217769, -0.7079225228952404, -0.7179062158269333, -0.7343555994955184, -0.7570079451396353, -0.785946557000691, -0.8210496827258601, -0.8619392492195262, -0.9178299871850107, -0.9791034607490973, -1.0419853469741949, -1.1002983159590503, -1.1336840927075225, -1.1586112849439056, -1.1728370861687827, -1.174759491739712, -1.1637376997997173, -1.1401561914629845, -1.105552891186257, -1.062426926211743, -1.0112717139693144, -0.9576814546703752, -0.9049050137297961, -0.8554479264083551, -0.815666947127258, -0.7807047978106239, -0.7509715916469173, -0.7266339371189524, -0.7047633697403467, -0.6899480308070515, -0.6818162552420206, -0.6799130737268005, -0.6861801158880302, -0.7024885231480804, -0.7288511115440908, -0.7647488315719105, -0.8008772402682419, -0.8433880354600585, -0.8918454718846164, -0.945205042515044, -0.9954631321577058, -1.0462787194159369, -1.094979660546146, -1.1382978660777527, -1.1725166852402944, -1.1937272267062145, -1.1989818019334213, -1.1872551279507524, -1.1629687375713456, -1.1272119939520606, -1.0823558639637103, -1.0313928922783744, -0.9776232084594818, -0.9238150765291706, -0.8723586874139632, -0.8249842061276954, -0.7846905853610172, -0.7496515398215459, -0.720091350159223, -0.6960100163740485, -0.6741458570140126, -0.6593177020435781, -0.6510449500700122, -0.6487829195148854, -0.6532749405322903, -0.6654950319448308, -0.6853983376225189, -0.7127669849339824, -0.7433076014374792, -0.7801344841579148, -0.8233181212995567, -0.8726534562681724, -0.9305947601759814, -0.9931946935840092, -1.0576209122844198, -1.1194582914826456, -1.1617512140430901, -1.1959059530199343, -1.218910739685388, -1.228458687354337, -1.223075951755735, -1.2025062121467915, -1.1677747514986687, -1.12150885742497, -1.0643493317826724, -1.002281263915935, -0.9392968493937214, -0.8785103852409369, -0.8298991563707045, -0.7856902362579005, -0.746492386666652, -0.7125875604140289, -0.6811049651807768, -0.6559086361644634, -0.6366140922509029, -0.6227663641216422, -0.6135836735111699, -0.6093351571994161, -0.6096747821836137, -0.6142565154609951, -0.6228368523259097, -0.635415792778357, -0.6520510089854653, -0.6728193971700714, -0.7043019924033236, -0.7428141840076072, -0.7887853092270966, -0.8423819765446056, -0.9060584570723538, -0.9767389018968543, -1.0517255352002366, -1.1259303902381077, -1.1754002935966883, -1.217308735042947, -1.2481313043635136, -1.2647921526449009, -1.2651766337590866, -1.248195384549211, -1.2144892068722508, -1.1666213081561112, -1.105552891186257, -1.0375894462353366, -0.9671845462093357, -0.8981061060272761, -0.8437853326113839, -0.7935977311729897, -0.7482609997919069, -0.7081788436380311, -0.6715698335489674, -0.6407280401726917, -0.6153330625807156, -0.5949747875845742, -0.577634689334792, -0.5653441097179841, -0.5575904072485692, -0.5538353083666873, -0.5536302517724548, -0.5567701808716393, -0.563114119255706, -0.5725531306089688, -0.5859715214940554, -0.6030617070196169, -0.6240864159470136, -0.649366049204734, -0.6907041769982837, -0.7416543326464801, -0.8032033510090665, -0.8758959136644731, -0.9470697759188456, -1.0247605930586683, -1.1058092119290475, -1.184435599780056, -1.2361483096380543, -1.278761633126987, -1.3086229996620888, -1.3227206405155703, -1.3190039897451071, -1.2966400049366296, -1.2567180492469976, -1.201865410289815, -1.1353501775356611, -1.0620424450975574, -0.9861394651386985, -0.9112745841881419, -0.8513083464122873, -0.7952381839268493, -0.7438202429230604, -0.6974902686636641, -0.6572927681755325, -0.6219397297261426, -0.5912196887026924, -0.5648250602138332, -0.5400132123117056, -0.5195972651484364, -0.5030645772384444, -0.4898896910590089, -0.4795791891802581, -0.47177422256228513, -0.4661351662208925, -0.462343541633163, -0.4601295712173094, -0.45933177290537375, -0.45981814151481887, -0.46146884709839014, -0.46434092102135854, -0.46837797272031007, -0.4736069158732377, -0.48004697453585093, -0.4897423066319044, -0.5016291810788173, -0.5160087747493683, -0.5332719767763135, -0.5664270648562743, -0.609610701997916, -0.6653925036477146, -0.7371430875733659, -0.8201077039961048, -0.9214761497512068, -1.040402566387463, -1.1700816381837837, -1.241402884865261, -1.308110358176508, -1.3657825253043867, -1.4096774525072724, -1.4353736069720273, -1.4388980171853978, -1.4183923577621518, -1.3749459918591498, -1.316312621945806, -1.243709771550376, -1.161430813114602, -1.0738331992659236, -0.9971484410415538, -0.921770918605416, -0.8494115729156371, -0.7812815194819025, -0.7152340720833416, -0.6551524899732312, -0.6011585255043969, -0.5531496503797225, -0.4972460963770984, -0.4504816584586722, -0.41160164658848414, -0.3792520464427428, -0.3563094175574156, -0.33629717556404154, -0.318609762707778, -0.3026345724133555, -0.28781923348006033, -0.27375363271942765, -0.26003406496156223, -0.24626323305513867, -0.22997404985079772, -0.21271725584242238, -0.1940763298229779, -0.17361552652972037, -0.14693253720522167, -0.11664824144451541, -0.08222436568774143, -0.04310982033789991, 0.011928651157805985, 0.07511812227425207, 0.146010031711555, 0.22286139841873862, 0.28325697343876755, 0.3427233857661805, 0.3980886662089443, 0.44557208381089813, 0.4812006670587878, 0.5008092038822667, 0.5013859255535454, 0.4823541104013453, 0.4487120129100826, 0.4014208358652218, 0.34355642818024984, 0.27845095951144433, 0.21314043424840612, 0.14748387598260077, 0.08384584356627113, 0.023956502013253615]}, {\"line\": {\"color\": \"green\", \"width\": 1}, \"type\": \"scatter\", \"x\": [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500], \"y\": [-0.26159730553627014, -0.23700262606143951, -0.19730144739151, -0.14975056052207947, -0.09150362014770508, -0.02056172490119934, 0.0809640884399414, 0.20338398218154907, 0.33312052488327026, 0.47255587577819824, 0.5598880648612976, 0.6313762068748474, 0.7009878754615784, 0.7645959258079529, 0.8173654675483704, 0.8538437485694885, 0.8528475165367126, 0.8043437600135803, 0.731461226940155, 0.6398293375968933, 0.5333289504051208, 0.41925686597824097, 0.32690906524658203, 0.22445201873779297, 0.12467342615127563, 0.029433518648147583, -0.057285696268081665, -0.138452410697937, -0.21607451140880585, -0.2870873808860779, -0.3911479115486145, -0.47727739810943604, -0.5491788983345032, -0.608893632888794, -0.6536507606506348, -0.6916457414627075, -0.7240598201751709, -0.7519688606262207, -0.7797514200210571, -0.8039509654045105, -0.8246824741363525, -0.8431302309036255, -0.8619871139526367, -0.8799393177032471, -0.8973202705383301, -0.9143481254577637, -0.9343883395195007, -0.953468918800354, -0.9714349508285522, -0.9878516793251038, -1.004849910736084, -1.01741623878479, -1.0248064994812012, -1.0265333652496338, -1.021993637084961, -1.011317491531372, -0.9953176975250244, -0.9756106734275818, -0.9488638639450073, -0.9229356050491333, -0.8998766541481018, -0.8817238807678223, -0.8715102672576904, -0.8659940958023071, -0.8655414581298828, -0.8700289726257324, -0.882718563079834, -0.9019088745117188, -0.9266561269760132, -0.9545084238052368, -0.9805876016616821, -1.0040889978408813, -1.0221033096313477, -1.0320730209350586, -1.0328936576843262, -1.02584707736969, -1.0114808082580566, -0.9910051226615906, -0.9663967490196228, -0.9405338168144226, -0.915743350982666, -0.8940340280532837, -0.8781411051750183, -0.8668551445007324, -0.8607722520828247, -0.8600226640701294, -0.8653295040130615, -0.8767582178115845, -0.8938270211219788, -0.9154320359230042, -0.9428552985191345, -0.9713596701622009, -0.9983133673667908, -1.0206258296966553, -1.0340237617492676, -1.0390443801879883, -1.034487247467041, -1.020865559577942, -1.0028843879699707, -0.9800926446914673, -0.9550802707672119, -0.9299353957176208, -0.9051679372787476, -0.8842206001281738, -0.8682667016983032, -0.858020544052124, -0.8538615703582764, -0.8550412654876709, -0.8614292144775391, -0.8726544976234436, -0.8951138257980347, -0.9233105182647705, -0.9557344913482666, -0.9882343411445618, -1.0142297744750977, -1.033940076828003, -1.043973684310913, -1.042709469795227, -1.0329344272613525, -1.0154728889465332, -0.9918980598449707, -0.9647482633590698, -0.9335448145866394, -0.9046598672866821, -0.8802316188812256, -0.8619434833526611, -0.8523315191268921, -0.8476753830909729, -0.8481486439704895, -0.8535910844802856, -0.8700752854347229, -0.8948230743408203, -0.9268589019775391, -0.962631106376648, -0.9913631677627563, -1.0176570415496826, -1.0377743244171143, -1.049048900604248, -1.0502681732177734, -1.0424376726150513, -1.0261774063110352, -1.0028570890426636, -0.9750525951385498, -0.9454513788223267, -0.9166147708892822, -0.8906572461128235, -0.8706377744674683, -0.8552711009979248, -0.8452515602111816, -0.8407852649688721, -0.8421597480773926, -0.8499670028686523, -0.8638980388641357, -0.8832588195800781, -0.9116846919059753, -0.944008469581604, -0.9778794050216675, -1.0100336074829102, -1.0358303785324097, -1.0527619123458862, -1.0578200817108154, -1.050038456916809, -1.0351619720458984, -1.0129761695861816, -0.9854853749275208, -0.955604076385498, -0.922292947769165, -0.8922012448310852, -0.8671482801437378, -0.8484906554222107, -0.8383959531784058, -0.833452045917511, -0.8337495923042297, -0.8391309976577759, -0.8523715734481812, -0.8719304800033569, -0.8973420858383179, -0.9271857142448425, -0.9593604207038879, -0.9916581511497498, -1.0214412212371826, -1.045491337776184, -1.060210108757019, -1.064765214920044, -1.0578800439834595, -1.0403409004211426, -1.0183653831481934, -0.9907641410827637, -0.960641622543335, -0.930234432220459, -0.9003033638000488, -0.8741649389266968, -0.8529976606369019, -0.8375629186630249, -0.8284593820571899, -0.8250020742416382, -0.8271217346191406, -0.8346081972122192, -0.8502160310745239, -0.8720253705978394, -0.8995495438575745, -0.9313330054283142, -0.9651194214820862, -0.998823881149292, -1.0295615196228027, -1.0540529489517212, -1.0686309337615967, -1.0723958015441895, -1.0643479824066162, -1.04519784450531, -1.0217456817626953, -0.9925543069839478, -0.9608671069145203, -0.9289777278900146, -0.8978110551834106, -0.8704650402069092, -0.8481041789054871, -0.831452488899231, -0.8210883140563965, -0.8164430856704712, -0.8174526691436768, -0.8239277005195618, -0.8383343815803528, -0.8591034412384033, -0.8858837485313416, -0.9174516201019287, -0.951879620552063, -0.9872888326644897, -1.021140456199646, -1.0501749515533447, -1.0711495876312256, -1.0808265209197998, -1.0773112773895264, -1.060957670211792, -1.0393105745315552, -1.0106871128082275, -0.9781702160835266, -0.9443657398223877, -0.9095712900161743, -0.8781945109367371, -0.8516314029693604, -0.8308522701263428, -0.8172074556350708, -0.8090812563896179, -0.8065125942230225, -0.809350848197937, -0.8236029148101807, -0.8481850028038025, -0.8828394412994385, -0.9250805377960205, -0.9614416360855103, -0.9992827773094177, -1.0346521139144897, -1.063920259475708, -1.0832573175430298, -1.090531349182129, -1.0841176509857178, -1.0646345615386963, -1.0405118465423584, -1.0094475746154785, -0.9749265909194946, -0.9394876956939697, -0.9038972854614258, -0.8718377351760864, -0.8446367979049683, -0.8231545686721802, -0.8084237575531006, -0.799340546131134, -0.7958971261978149, -0.7979485988616943, -0.8112281560897827, -0.8351407647132874, -0.8696045875549316, -0.9124090671539307, -0.9498704671859741, -0.989764928817749, -1.0283550024032593, -1.0620100498199463, -1.0873584747314453, -1.0999910831451416, -1.097476840019226, -1.0798988342285156, -1.0567736625671387, -1.025749921798706, -0.9899381995201111, -0.9523769021034241, -0.9135164022445679, -0.8778517246246338, -0.8469215631484985, -0.8217549324035645, -0.8041162490844727, -0.791982114315033, -0.7854107618331909, -0.7842882871627808, -0.7918725609779358, -0.8091207146644592, -0.8360339403152466, -0.8714328408241272, -0.907889723777771, -0.9485548734664917, -0.9911641478538513, -1.0326091051101685, -1.063704252243042, -1.0891344547271729, -1.10563063621521, -1.1113646030426025, -1.1054282188415527, -1.0879319906234741, -1.0601688623428345, -1.0240658521652222, -0.9801344871520996, -0.9352661967277527, -0.8925094604492188, -0.8543785810470581, -0.8273398280143738, -0.805133581161499, -0.7884923219680786, -0.7774857878684998, -0.7714389562606812, -0.7725280523300171, -0.7805200219154358, -0.7950462102890015, -0.8213735818862915, -0.8557322025299072, -0.8977883458137512, -0.9449884295463562, -0.9913508296012878, -1.0371534824371338, -1.0777159929275513, -1.1080615520477295, -1.122544765472412, -1.1227879524230957, -1.1077990531921387, -1.0789254903793335, -1.0470261573791504, -1.0082652568817139, -0.9669828414916992, -0.9257302284240723, -0.8866519927978516, -0.8514035940170288, -0.8211642503738403, -0.7965604066848755, -0.777722954750061, -0.7648299932479858, -0.7577401399612427, -0.756261944770813, -0.7642936706542969, -0.7832273840904236, -0.8133339285850525, -0.8534277081489563, -0.8910449743270874, -0.9338829517364502, -0.9797142148017883, -1.0259051322937012, -1.0630223751068115, -1.0957932472229004, -1.1205995082855225, -1.135033130645752, -1.1372685432434082, -1.1264485120773315, -1.102968454360962, -1.0687952041625977, -1.027984857559204, -0.9830899834632874, -0.9373591542243958, -0.893418550491333, -0.8558921813964844, -0.8227109909057617, -0.7948899865150452, -0.7727597951889038, -0.7554618120193481, -0.7444677352905273, -0.7395015358924866, -0.7402967810630798, -0.7479890584945679, -0.762328028678894, -0.7833307981491089, -0.8106937408447266, -0.8440115451812744, -0.8827952742576599, -0.9260439872741699, -0.9726173281669617, -1.0167806148529053, -1.0598771572113037, -1.0984498262405396, -1.1291553974151611, -1.1487622261047363, -1.1543166637420654, -1.144498586654663, -1.1198585033416748, -1.088566780090332, -1.0486268997192383, -1.0039114952087402, -0.9574630260467529, -0.9111752510070801, -0.8679419159889221, -0.8292899131774902, -0.7961530685424805, -0.769879937171936, -0.7492244243621826, -0.734157919883728, -0.7245028018951416, -0.7195875644683838, -0.7228331565856934, -0.7340786457061768, -0.7528958320617676, -0.7783807516098022, -0.8108829259872437, -0.8501332998275757, -0.8950197100639343, -0.9501339793205261, -1.0082430839538574, -1.0658867359161377, -1.11716890335083, -1.1451417207717896, -1.1649081707000732, -1.1729152202606201, -1.1682100296020508, -1.1509044170379639, -1.1218194961547852, -1.0827170610427856, -1.036129117012024, -0.9815537929534912, -0.9272018074989319, -0.8757408857345581, -0.8293238878250122, -0.7951674461364746, -0.7657642364501953, -0.7417806386947632, -0.7231671810150146, -0.7076730728149414, -0.6989083290100098, -0.6964479684829712, -0.6998934745788574, -0.7102389931678772, -0.7266210317611694, -0.7497353553771973, -0.7795318365097046, -0.8157860040664673, -0.8576846718788147, -0.905089259147644, -0.9571197628974915, -1.0090746879577637, -1.061370611190796, -1.1103909015655518, -1.1518278121948242, -1.1814961433410645, -1.1949058771133423, -1.189452886581421, -1.1651530265808105, -1.1330244541168213, -1.0902327299118042, -1.0403945446014404, -0.9873726963996887, -0.9337553977966309, -0.8824864625930786, -0.8354573845863342, -0.7938472032546997, -0.7603871822357178, -0.7322818636894226, -0.7096272110939026, -0.6922476291656494, -0.6776953935623169, -0.6698436141014099, -0.6682307720184326, -0.6717824935913086, -0.6835692524909973, -0.7030054330825806, -0.7305669784545898, -0.7661950588226318, -0.8046104311943054, -0.8498835563659668, -0.9003666639328003, -0.9557711482048035, -1.011852502822876, -1.068861484527588, -1.1230251789093018, -1.1697721481323242, -1.2042677402496338, -1.2214303016662598, -1.217919111251831, -1.19337797164917, -1.1601457595825195, -1.1150795221328735, -1.0617096424102783, -1.004399061203003, -0.9465591311454773, -0.8906747102737427, -0.8388901948928833, -0.7924677133560181, -0.7550603151321411, -0.7228633165359497, -0.6960576772689819, -0.6744554042816162, -0.6551896333694458, -0.6420856714248657, -0.6341928839683533, -0.6307514905929565, -0.6338211297988892, -0.6441351771354675, -0.6619848012924194, -0.6871943473815918, -0.7129426002502441, -0.7449327707290649, -0.7827419638633728, -0.8260287642478943, -0.897560715675354, -0.9759521484375, -1.0604097843170166, -1.1429754495620728, -1.1870639324188232, -1.224642276763916, -1.2489439249038696, -1.256960153579712, -1.2476272583007812, -1.2203527688980103, -1.17686128616333, -1.1202822923660278, -1.0554126501083374, -0.9870766997337341, -0.9195021390914917, -0.8556249141693115, -0.8072298169136047, -0.7629387378692627, -0.7240534424781799, -0.6906740665435791, -0.6607240438461304, -0.6364979147911072, -0.6174580454826355, -0.6024982333183289, -0.5899072885513306, -0.5823307037353516, -0.5793536305427551, -0.5805351734161377, -0.5855268239974976, -0.5942699313163757, -0.6067458391189575, -0.6229848861694336, -0.6451466679573059, -0.6720041036605835, -0.7041951417922974, -0.7421466708183289, -0.8066398501396179, -0.8821663856506348, -0.969819962978363, -1.0642129182815552, -1.119499683380127, -1.176676869392395, -1.2277681827545166, -1.2686893939971924, -1.2965360879898071, -1.306335687637329, -1.2961124181747437, -1.2659335136413574, -1.2240897417068481, -1.1693280935287476, -1.105478048324585, -1.0369668006896973, -0.9698007106781006, -0.9040576219558716, -0.842095673084259, -0.7852503061294556, -0.7385144233703613, -0.6966712474822998, -0.6600360870361328, -0.6284480690956116, -0.6004140377044678, -0.5768059492111206, -0.5559684038162231, -0.5391358733177185, -0.5252740979194641, -0.5148116946220398, -0.5073696374893188, -0.5025462508201599, -0.5000249147415161, -0.49959760904312134, -0.5011085271835327, -0.5044135451316833, -0.5099228620529175, -0.517314076423645, -0.5267297625541687, -0.5382538437843323, -0.5551084280014038, -0.5751885771751404, -0.5992965698242188, -0.6280175447463989, -0.6874547004699707, -0.7608820796012878, -0.8534160256385803, -0.9660201668739319, -1.0450607538223267, -1.1315712928771973, -1.216968297958374, -1.2942445278167725, -1.340687870979309, -1.3747878074645996, -1.390932321548462, -1.3868328332901, -1.361975908279419, -1.3171956539154053, -1.2554327249526978, -1.1797912120819092, -1.097450613975525, -1.0122597217559814, -0.9283494353294373, -0.8485869765281677, -0.7895505428314209, -0.7339508533477783, -0.6835019588470459, -0.6382737159729004, -0.5910701155662537, -0.5506628751754761, -0.5152397751808167, -0.483720600605011, -0.44972795248031616, -0.42236509919166565, -0.40026870369911194, -0.38214394450187683, -0.3690316379070282, -0.3574915826320648, -0.34706711769104004, -0.3372949957847595, -0.32919764518737793, -0.32097843289375305, -0.3125152885913849, -0.30361324548721313, -0.29229384660720825, -0.2800107002258301, -0.26635050773620605, -0.25099092721939087, -0.2299133688211441, -0.2058403491973877, -0.17784279584884644, -0.14523032307624817, -0.09883695840835571, -0.04451301693916321, 0.019514024257659912, 0.09393131732940674, 0.17450153827667236, 0.25952398777008057, 0.3414958715438843, 0.4270176887512207, 0.49088579416275024, 0.5436972379684448, 0.5823258757591248, 0.602085292339325, 0.5995245575904846, 0.5731298327445984, 0.5242445468902588, 0.45667606592178345, 0.3797844648361206, 0.29845893383026123, 0.21276164054870605, 0.12913405895233154, 0.06315204501152039, -0.000270158052444458, -0.05887800455093384, -0.11395314335823059, -0.165777325630188, -0.21170561015605927, -0.2520984411239624, -0.28734341263771057, -0.32795190811157227, -0.361039936542511, -0.3880736827850342, -0.41031891107559204, -0.4245184659957886, -0.4368512034416199, -0.44784998893737793, -0.45789581537246704, -0.4671362340450287, -0.47603532671928406, -0.48480796813964844, -0.49367427825927734, -0.504315197467804, -0.5154753923416138, -0.5275152921676636, -0.540698766708374, -0.5582133531570435, -0.5778264999389648, -0.6002426147460938, -0.6259729862213135, -0.6621416211128235, -0.7038750648498535, -0.7525429725646973, -0.8091849088668823, -0.8925325870513916, -0.987362802028656, -1.0902750492095947, -1.1958301067352295, -1.2531098127365112, -1.3073148727416992, -1.3494677543640137, -1.3752892017364502, -1.3824117183685303, -1.3676565885543823, -1.331319808959961, -1.2756571769714355, -1.2115395069122314, -1.1370315551757812, -1.0578911304473877, -0.9778446555137634, -0.9086573123931885, -0.8419607877731323, -0.7800125479698181, -0.7234824299812317, -0.6778091192245483, -0.6364102363586426, -0.5994598865509033, -0.5667418241500854, -0.5229461193084717, -0.4863848090171814, -0.45653235912323, -0.4322302043437958, -0.4168083369731903, -0.4035792052745819, -0.392134428024292, -0.3821302354335785, -0.37345024943351746, -0.36557820439338684, -0.35828542709350586, -0.3513605296611786, -0.34391364455223083, -0.33651772141456604, -0.32895129919052124, -0.32101401686668396, -0.3116605877876282, -0.301501989364624, -0.29023951292037964, -0.2776251435279846, -0.2600938677787781, -0.2401028722524643, -0.21682044863700867, -0.18963798880577087, -0.15107357501983643, -0.10563379526138306, -0.051529258489608765, 0.012384027242660522, 0.10668647289276123, 0.215451180934906, 0.3232850432395935, 0.4301220178604126, 0.4983493685722351, 0.5540202856063843, 0.5963122248649597, 0.6204866766929626, 0.6229811310768127, 0.6015881896018982, 0.5569141507148743, 0.49231648445129395, 0.41706305742263794, 0.3352278470993042, 0.25055116415023804, 0.1644904613494873, 0.09455567598342896, 0.026767760515213013, -0.03639817237854004, -0.09512984752655029, -0.15134912729263306, -0.20160678029060364, -0.24616803228855133, -0.2853740453720093, -0.33108994364738464, -0.3687279522418976, -0.3998376429080963, -0.42573949694633484, -0.443439245223999, -0.45909014344215393, -0.47305479645729065, -0.4857693016529083, -0.4975969195365906, -0.5090816020965576, -0.5205076336860657, -0.5321699380874634, -0.5463882088661194, -0.5614458918571472, -0.5778347253799438, -0.5959218144416809, -0.6199592351913452, -0.6469546556472778, -0.6777893304824829, -0.7129525542259216, -0.7623758316040039, -0.8188258409500122, -0.8835437297821045, -0.9559391140937805, -1.0333904027938843, -1.1144416332244873, -1.1941149234771729, -1.264775037765503, -1.30422842502594, -1.3318564891815186, -1.3427854776382446, -1.3352720737457275, -1.3094596862792969, -1.266322135925293, -1.2085496187210083, -1.1389787197113037, -1.0631788969039917, -0.9855290651321411, -0.9096376895904541, -0.8380472660064697, -0.7836297750473022, -0.7331096529960632, -0.6877763271331787, -0.6476625800132751, -0.6125607490539551, -0.5821450352668762, -0.5553144216537476, -0.531122088432312, -0.5082622170448303, -0.4894930422306061, -0.47424250841140747, -0.4619821310043335, -0.4521714746952057, -0.444504052400589, -0.4386512339115143, -0.4343002438545227, -0.43125784397125244, -0.42921239137649536, -0.42801767587661743, -0.4275488257408142, -0.4277524948120117, -0.4285814166069031, -0.4300174117088318, -0.4320327341556549, -0.4350677728652954, -0.43882521986961365, -0.4434302747249603, -0.448971688747406, -0.4572315812110901, -0.4671497941017151, -0.4792550206184387, -0.4940168857574463, -0.5224900245666504, -0.5591763257980347, -0.6078022122383118, -0.6719692945480347, -0.7389838099479675, -0.8244616389274597, -0.9288371205329895, -1.0517914295196533, -1.1421936750411987, -1.239186406135559, -1.3333842754364014, -1.4158086776733398, -1.4630796909332275, -1.4946069717407227, -1.5100419521331787, -1.5083301067352295, -1.4766521453857422, -1.4157958030700684, -1.32904851436615, -1.2304282188415527, -1.1604863405227661, -1.0846108198165894, -1.008939266204834, -0.934943675994873, -0.862938404083252, -0.794957160949707, -0.7315114736557007, -0.6728445291519165, -0.5991829633712769, -0.5355806350708008, -0.48026415705680847, -0.4310750961303711, -0.38154134154319763, -0.3397928774356842, -0.304383248090744, -0.27400314807891846, -0.24650052189826965, -0.22199486196041107, -0.1995120346546173, -0.17807956039905548, -0.1581002175807953, -0.13771581649780273, -0.11651259660720825, -0.09403499960899353, -0.06549233198165894, -0.03464224934577942, -0.0007828772068023682, 0.03634923696517944, 0.08668076992034912, 0.14056414365768433, 0.19764339923858643, 0.253059983253479, 0.29751741886138916, 0.34116894006729126, 0.3795701265335083, 0.40774035453796387, 0.42262423038482666, 0.42162156105041504, 0.40397173166275024, 0.37083762884140015, 0.3281949758529663, 0.27793991565704346, 0.2224217653274536, 0.1654396653175354, 0.11179196834564209, 0.06106358766555786, 0.01501232385635376, -0.02566954493522644, -0.061901092529296875, -0.09264549612998962, -0.11771580576896667, -0.13720452785491943, -0.15384075045585632, -0.1637706756591797, -0.16749021410942078, -0.16552865505218506, -0.1569821536540985, -0.14222219586372375, -0.12097617983818054, -0.09320032596588135, -0.06128266453742981, -0.02277013659477234, 0.02229619026184082, 0.07386422157287598, 0.15707725286483765, 0.244401216506958, 0.3226984739303589, 0.39346468448638916, 0.42848265171051025, 0.45028066635131836, 0.45814353227615356, 0.4506433606147766, 0.42767006158828735, 0.39047014713287354, 0.3417065143585205, 0.28435373306274414, 0.22014641761779785, 0.15538740158081055, 0.09294503927230835, 0.035251766443252563, -0.010048210620880127, -0.051353782415390015, -0.08889701962471008, -0.12111029028892517, -0.15026450157165527, -0.17358621954917908, -0.19128559529781342, -0.2041212022304535, -0.21287082135677338, -0.2173565924167633, -0.21789875626564026, -0.21481838822364807, -0.20832538604736328, -0.1984839290380478, -0.18527139723300934, -0.16862547397613525, -0.14379268884658813, -0.11402493715286255, -0.07834824919700623, -0.03630399703979492, 0.03539913892745972, 0.1187828779220581, 0.21403300762176514, 0.3048890233039856, 0.3555036187171936, 0.4077228903770447, 0.4524977207183838, 0.4857795238494873, 0.5040313601493835, 0.5042185187339783, 0.4850364923477173, 0.4475937485694885, 0.3979499936103821, 0.33907419443130493, 0.2744501233100891, 0.205732524394989, 0.14151698350906372, 0.07973206043243408, 0.022254496812820435, -0.03023400902748108, -0.07605975866317749, -0.11766114830970764, -0.15396982431411743, -0.18515561521053314, -0.2132183015346527, -0.2362401932477951, -0.25454533100128174, -0.2688061594963074, -0.2815297544002533, -0.28978705406188965, -0.29406434297561646, -0.29493382573127747, -0.29356908798217773, -0.289969265460968, -0.284379780292511, -0.2768417000770569, -0.2661198675632477, -0.2530234456062317, -0.23719164729118347, -0.21833883225917816, -0.18737712502479553, -0.1503426730632782, -0.1052732765674591, -0.05089566111564636, 0.020157068967819214, 0.10416156053543091, 0.20103693008422852, 0.29763317108154297, 0.358675479888916, 0.42464572191238403, 0.4855204224586487, 0.5363579988479614, 0.5722622871398926, 0.5881199240684509, 0.58072429895401, 0.5494454503059387, 0.5011022090911865, 0.4394427537918091, 0.36726903915405273, 0.29101109504699707, 0.21507078409194946, 0.13986974954605103, 0.0682523250579834, 0.0012324750423431396, -0.05278301239013672, -0.10405638813972473, -0.15120205283164978, -0.19340334832668304, -0.24043749272823334, -0.2800281345844269]}, {\"line\": {\"color\": \"red\", \"width\": 1}, \"type\": \"scatter\", \"x\": [1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500], \"y\": [-0.8348081707954407, -0.9244580268859863, -1.0275323390960693, -1.1375486850738525, -1.2028822898864746, -1.270150899887085, -1.3299046754837036, -1.37721848487854, -1.4083534479141235, -1.4176697731018066, -1.4027633666992188, -1.3638356924057007, -1.312050700187683, -1.244895339012146, -1.1669902801513672, -1.0835554599761963, -1.0049371719360352, -0.9270232915878296, -0.8527694940567017, -0.7835868000984192, -0.7292052507400513, -0.6788220405578613, -0.6331756711006165, -0.5921918153762817, -0.5383234024047852, -0.4927266836166382, -0.4535711109638214, -0.42100659012794495, -0.3991425633430481, -0.3801329433917999, -0.3634965121746063, -0.34877467155456543, -0.33577418327331543, -0.3238322138786316, -0.3126338720321655, -0.3018663823604584, -0.29005324840545654, -0.2781883180141449, -0.26593053340911865, -0.2529774010181427, -0.23659269511699677, -0.21873719990253448, -0.19881992042064667, -0.17641064524650574, -0.1457671821117401, -0.11090117692947388, -0.07068949937820435, -0.024445921182632446, 0.04220706224441528, 0.11782944202423096, 0.20277822017669678, 0.2862153649330139, 0.34454643726348877, 0.4067803621292114, 0.46339988708496094, 0.5097425580024719, 0.5411032438278198, 0.5528267025947571, 0.5421920418739319, 0.5091512203216553, 0.4611549973487854, 0.4014051556587219, 0.3327549695968628, 0.26022398471832275, 0.18883484601974487, 0.1187969446182251, 0.052609801292419434, -0.008848398923873901, -0.05956244468688965, -0.10760936141014099, -0.15076249837875366, -0.1888841688632965, -0.22513487935066223, -0.25599226355552673, -0.28206565976142883, -0.30389684438705444, -0.3213688135147095, -0.3357551395893097, -0.3475613594055176, -0.35717928409576416, -0.36494365334510803, -0.3711768686771393, -0.3761236369609833, -0.38001418113708496, -0.38311874866485596, -0.38546809554100037, -0.38718172907829285, -0.3883771002292633, -0.38918811082839966, -0.3895893096923828, -0.3896208107471466, -0.38933509588241577, -0.3886558711528778, -0.3876493573188782, -0.3862870931625366, -0.38456299901008606, -0.381991982460022, -0.37885046005249023, -0.3749794363975525, -0.37023839354515076, -0.36277997493743896, -0.35345032811164856, -0.34150397777557373, -0.32608968019485474, -0.3063545823097229, -0.2811071276664734, -0.24878355860710144, -0.207330584526062, -0.15628015995025635, -0.0913865864276886, -0.010032445192337036, 0.09033071994781494, 0.17638951539993286, 0.2713237404823303, 0.3681262731552124, 0.47428399324417114, 0.5558820366859436, 0.6276447176933289, 0.6847285628318787, 0.732815682888031, 0.756107747554779, 0.7502816319465637, 0.7156805396080017, 0.6541029810905457, 0.5768755078315735, 0.4876443147659302, 0.39009881019592285, 0.295712947845459, 0.20958924293518066, 0.1236388087272644, 0.041462868452072144, -0.035847872495651245, -0.11107540130615234, -0.18215778470039368, -0.2465713918209076, -0.3044549822807312, -0.3760405480861664, -0.43602290749549866, -0.4866062104701996, -0.5293769836425781, -0.568832278251648, -0.6025439500808716, -0.6319919228553772, -0.6586357355117798, -0.6814432144165039, -0.7036488056182861, -0.725760817527771, -0.7483357191085815, -0.7753674387931824, -0.8037198781967163, -0.8331226706504822, -0.864366888999939, -0.9051076173782349, -0.9474475979804993, -0.9910337328910828, -1.0343364477157593, -1.0758090019226074, -1.1118180751800537, -1.1393322944641113, -1.1557526588439941, -1.1588799953460693, -1.1478993892669678, -1.1231951713562012, -1.086918830871582, -1.0439107418060303, -0.9960711598396301, -0.9469769597053528, -0.8993058204650879, -0.8579481244087219, -0.8207852840423584, -0.7889001369476318, -0.7626701593399048, -0.7411249279975891, -0.7257674932479858, -0.7162705659866333, -0.7123515605926514, -0.7144626379013062, -0.723120391368866, -0.7383195161819458, -0.7598111629486084, -0.7873969078063965, -0.8210885524749756, -0.8604907989501953, -0.9048481583595276, -0.965249240398407, -1.0275137424468994, -1.0877562761306763, -1.13875412940979, -1.162635087966919, -1.1772485971450806, -1.1796302795410156, -1.169126272201538, -1.146549105644226, -1.1129833459854126, -1.0702219009399414, -1.0214929580688477, -0.9674118757247925, -0.9143081307411194, -0.8645264506340027, -0.8198386430740356, -0.785730242729187, -0.7565693855285645, -0.7328122854232788, -0.7143746614456177, -0.698879599571228, -0.6901094317436218, -0.6876254081726074, -0.6910057663917542, -0.7043092250823975, -0.7266601920127869, -0.7592118978500366, -0.8016297817230225, -0.8421319723129272, -0.8887431621551514, -0.9403559565544128, -0.9955349564552307, -1.0451215505599976, -1.0937275886535645, -1.137426733970642, -1.17263662815094, -1.1957478523254395, -1.2034618854522705, -1.1940354108810425, -1.1678296327590942, -1.1331524848937988, -1.0883432626724243, -1.037146806716919, -0.983119010925293, -0.9291893839836121, -0.8777048587799072, -0.8305078744888306, -0.7886701822280884, -0.7544155120849609, -0.7255822420120239, -0.7022138833999634, -0.684105396270752, -0.6687191724777222, -0.6598901748657227, -0.657014012336731, -0.6589803695678711, -0.6689374446868896, -0.6863075494766235, -0.7115402221679688, -0.7446054220199585, -0.7800519466400146, -0.8224259614944458, -0.8707756996154785, -0.923823356628418, -0.9852071404457092, -1.0488505363464355, -1.111217737197876, -1.1668241024017334, -1.1997618675231934, -1.2228416204452515, -1.2324180603027344, -1.2270984649658203, -1.2069458961486816, -1.1729044914245605, -1.1270320415496826, -1.0720717906951904, -1.0094043016433716, -0.9461219906806946, -0.8852546215057373, -0.8290693759918213, -0.7865557670593262, -0.7485336065292358, -0.7158671617507935, -0.6885104775428772, -0.6636345982551575, -0.644767165184021, -0.6313177347183228, -0.6220438480377197, -0.6167287826538086, -0.6161372065544128, -0.620018482208252, -0.6281176805496216, -0.64031583070755, -0.6566454172134399, -0.6772398948669434, -0.7022562026977539, -0.7403482794761658, -0.7849884033203125, -0.8371518850326538, -0.8970226049423218, -0.9657109379768372, -1.039121389389038, -1.1135549545288086, -1.1823030710220337, -1.221947431564331, -1.2524116039276123, -1.2686575651168823, -1.2685413360595703, -1.2516734600067139, -1.2183802127838135, -1.1708518266677856, -1.1117440462112427, -1.0429046154022217, -0.9721781611442566, -0.9030711650848389, -0.8384072780609131, -0.7905491590499878, -0.7468099594116211, -0.7084195613861084, -0.6753954887390137, -0.6456322073936462, -0.6213756203651428, -0.6020785570144653, -0.585903525352478, -0.5725433230400085, -0.5638383626937866, -0.5593605041503906, -0.5586563348770142, -0.5613702535629272, -0.567372739315033, -0.5766010284423828, -0.5890487432479858, -0.6060584187507629, -0.6268937587738037, -0.6520427465438843, -0.6819065809249878, -0.731749415397644, -0.790370762348175, -0.860277533531189, -0.9413425922393799, -1.016539216041565, -1.0962151288986206, -1.1751389503479004, -1.2457611560821533, -1.2855799198150635, -1.314159631729126, -1.326646089553833, -1.3211281299591064, -1.2976038455963135, -1.2568029165267944, -1.2014827728271484, -1.1341547966003418, -1.0597976446151733, -0.9835736751556396, -0.9089932441711426, -0.8387210369110107, -0.7855523228645325, -0.7363212704658508, -0.6923196315765381, -0.6535767912864685, -0.6204286813735962, -0.5918093323707581, -0.5668843984603882, -0.5443600416183472, -0.5234344601631165, -0.5064806342124939, -0.4929874539375305, -0.48246142268180847, -0.47442004084587097, -0.46859902143478394, -0.46470463275909424, -0.4624781012535095, -0.4616987407207489, -0.4622211754322052, -0.46394968032836914, -0.4668081998825073, -0.4709760546684265, -0.47631269693374634, -0.48291343450546265, -0.4908355176448822, -0.5028799772262573, -0.5171387791633606, -0.5342716574668884, -0.5547611713409424, -0.5957330465316772, -0.6471894979476929, -0.7129539847373962, -0.7966216802597046, -0.8913151025772095, -1.005955696105957, -1.1354031562805176, -1.2647299766540527, -1.3228459358215332, -1.3778901100158691, -1.4182476997375488, -1.4395654201507568, -1.4401321411132812, -1.4166984558105469, -1.3709856271743774, -1.3037104606628418]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"rangeslider\": {\"visible\": true}}, \"yaxis\": {\"autorange\": true, \"fixedrange\": false}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('3dca988e-9350-4632-9182-de04ca66b734');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67HaeJa6CN2O"
      },
      "source": [
        "**Prédictions multi-step ahead (Prédiction de Xt+1 avec Xt, Yt+1, Zt+1)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0q8VJ6oGC_XQ"
      },
      "source": [
        "predictions = []\n",
        "\n",
        "data_to_predict = x_val[0,:,:]                          # [[X0,Y0,Z0],[X1,Y1,Z1], ... , [X15,Y15,Z15]] : (16,3)\n",
        "\n",
        "data_to_predict = tf.expand_dims(data_to_predict,0)     # (1,16,3)\n",
        "prediction = model.predict(data_to_predict)             # [[[X^1][X^2][X^3]...[X1^6]]] : (1,16,1)\n",
        "predictions.append(prediction[0,taille_fenetre-1,0])    # [X^16] : (1,)\n",
        "\n",
        "data_to_predict = x_val[1,0:taille_fenetre-1,:]         # [[X1,Y1,Z1],[X2,Y2,Z2], ... , [X15,Y15,Z15]] : (15,3)\n",
        "\n",
        "data_to_predict = np.insert(data_to_predict,taille_fenetre-1,     # [[X1,Y1,Z1],[X2,Y2,Z2], ... , [X15,Y15,Z15], [X^16,Y16,Z16]] : (16,3)\n",
        "                            [predictions[0],\n",
        "                             x_val[1,taille_fenetre-1,1],\n",
        "                             x_val[1,taille_fenetre-1,2]],axis=0)    \n",
        "\n",
        "\n",
        "for i in range(1,300):\n",
        "  data_to_predict = tf.expand_dims(data_to_predict,0)   # (16,3) => (1,16,3)\n",
        "  prediction = model.predict(data_to_predict)           # [[[X^2][X^3][X^4]...[X^17]]] : (1,16,1)\n",
        "\n",
        "  predictions.append(prediction[0,taille_fenetre-1,0])  # [X^17] : (1,)\n",
        "  data_to_predict = x_val[i+1,0:taille_fenetre-1,:]     # [[X2,Y2,Z2],[X3,Y3,Z3], ... , [X16,Y16,Z16]] : (15,3)\n",
        "\n",
        "  data_to_predict = np.insert(data_to_predict,taille_fenetre-1,       # [[X2,Y2,Z2],[X3,Y3,Z3], ... , [X16,Y16,Z16], [X^17,Y17,Z17]] : (16,3)\n",
        "                            [predictions[i],\n",
        "                             x_val[i+1,taille_fenetre-1,1],\n",
        "                             x_val[i+1,taille_fenetre-1,2]],axis=0)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "unzkoI-aCN2P",
        "outputId": "ed25fa5d-92b2-4da8-9efb-7155876c5cb7"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "n_max = len(predictions)\n",
        "\n",
        "# Courbes originales\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index,y=serie_entrainement[0],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index[temps_separation:],y=serie_test[0],line=dict(color='blue', width=1)))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index[taille_fenetre+horizon-1:],y=pred_ent[:,taille_fenetre-1,0],line=dict(color='green', width=1)))\n",
        "\n",
        "# Courbes des prédictions de validations\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index[temps_separation+taille_fenetre+horizon-1:temps_separation+taille_fenetre+horizon-1+n_max],y=predictions[0:n_max],line=dict(color='red', width=1)))\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"6ce38ca1-94e6-4b8f-b1cd-1626a298b2d6\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"6ce38ca1-94e6-4b8f-b1cd-1626a298b2d6\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '6ce38ca1-94e6-4b8f-b1cd-1626a298b2d6',\n",
              "                        [{\"line\": {\"color\": \"blue\", \"width\": 1}, \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500], \"y\": [-0.3997737259124131, -0.39663821834604174, -0.39365137681048884, -0.3907864798683186, -0.38801757504432344, -0.3821324507898519, -0.3763934293587709, -0.3706210862311272, -0.36463022967025455, -0.3577473769244706, -0.35024615038670454, -0.34194712553700274, -0.3326497313941316, -0.3199234065145797, -0.3051080675812845, -0.28778078536864177, -0.2674609584839191, -0.23571563448930655, -0.1966074971580348, -0.14853454184766277, -0.0896512592100983, -0.006225265450336413, 0.09729627454420649, 0.2225345894716806, 0.3675224176311685, 0.45838812095042686, 0.5508558289121261, 0.6407603294459197, 0.722847047324601, 0.7907720441641028, 0.8375505797233825, 0.8571591165468613, 0.8462014047925644, 0.8092912178307217, 0.7480946404894724, 0.6657516018680006, 0.5671962762650251, 0.47056335623297907, 0.36912442227360964, 0.26621164404319425, 0.1646829978238482, 0.06960722630425478, -0.020784283640840975, -0.10554314526311386, -0.18409904490985485, -0.30078906306526343, -0.4001453589493666, -0.4835585687120818, -0.5528548815255133, -0.6036192046351864, -0.6466106012197352, -0.6830658188631247, -0.7141831570379003, -0.7449865023027574, -0.7713362746616284, -0.7942833891599544, -0.8148146806574793, -0.8352498518764578, -0.8543521552329252, -0.8725509279710562, -0.8902306512050359, -0.9106978625168631, -0.9306203922502604, -0.949722695606728, -0.9676907796763472, -0.9869853235899075, -1.0028259454943649, -1.0141168742142899, -1.0201340036512985, -1.0203134281712518, -1.0140335699728829, -1.0017814384674935, -0.9847553331276295, -0.9601933979497229, -0.9339974180365264, -0.9090189616515849, -0.887699483869979, -0.8739030198892763, -0.8646498410745367, -0.8603308365585154, -0.8610421266197593, -0.8688406852191626, -0.8840661373409225, -0.9057765042552841, -0.9318699558713645, -0.9582069121930961, -0.983762090249316, -1.0056582897022008, -1.021018310213926, -1.0272469042637369, -1.0256384916027261, -1.0160841359152073, -0.9996539763023317, -0.9777385527937374, -0.9527985445202147, -0.9272818145754133, -0.9034247614401807, -0.884463434492248, -0.8696288715032435, -0.8596195464972716, -0.8548391646442274, -0.8556786150768666, -0.8629709402092584, -0.8763636990200658, -0.8949661769280917, -0.9198164729416379, -0.9476721296644034, -0.9760916920213083, -1.0017750304489237, -1.020063515447031, -1.0308994748485025, -1.0325399276023621, -1.0247926331515171, -1.010791112576582, -0.9912594719759403, -0.9679663244748469, -0.9429366039413475, -0.9169777207152321, -0.8934282524713483, -0.8739158359264159, -0.8595682823487135, -0.851346794523706, -0.8484119220187538, -0.8508149289824154, -0.858331534764749, -0.8755819207545547, -0.9002720163038566, -0.9307036964916674, -0.9635063435502911, -0.9922655308913934, -1.0165134731593815, -1.0328539205122806, -1.0388838659864288, -1.0348916704174658, -1.022620314856367, -1.0031655704785625, -0.9785523711520977, -0.9484346838742052, -0.9182529164106153, -0.890852229006303, -0.8684690201421161, -0.85476867643996, -0.8459384268508247, -0.8422089600432219, -0.8435930920542909, -0.8539164099701814, -0.8738389397035787, -0.9022264619676347, -0.9360672080345601, -0.9659990627739292, -0.9948159222821595, -1.0194996098128917, -1.0370447646569065, -1.0446126345877982, -1.04320287050245, -1.0326937200480366, -1.014161730344278, -0.9895997951663714, -0.961334025255141, -0.9319853002056202, -0.9039117708514828, -0.8806891115546568, -0.8615483600867706, -0.8472969267876147, -0.8384282290870608, -0.8349999391522369, -0.8381911323999797, -0.8478159762917657, -0.863329789249165, -0.8875328753871651, -0.9173301617365692, -0.9507351625407506, -0.9845438685148273, -1.0143731949570804, -1.0373779816225344, -1.050059450372098, -1.0505080116719814, -1.0413445451172185, -1.0241774633688199, -1.000487018716401, -0.9724839775665309, -0.9395723941922213, -0.9075323013433996, -0.87897176257796, -0.855832407522541, -0.8413502855548735, -0.831956130331599, -0.8278165503355313, -0.8289251375481004, -0.8369672008531547, -0.8520901246777984, -0.8737107793321833, -0.9006372733623331, -0.9313316823115042, -0.9639933529615932, -0.9961167500518218, -1.0245683525015754, -1.0454456770018676, -1.0573005113559315, -1.0580694735843033, -1.0477525636869827, -1.0304701376043282, -1.0068181410633281, -0.9788086918948882, -0.9486397404684378, -0.9174583221079644, -0.8885325262840482, -0.863522029806258, -0.8436251321471399, -0.8299632365564024, -0.8218635010842202, -0.8194092299720005, -0.8225171189783361, -0.8329301491542032, -0.8503407356082527, -0.8741401165763576, -0.9031107685302622, -0.9356955429575138, -0.970004074380032, -1.0034282992399228, -1.032738576178025, -1.0539683416996541, -1.06550277512523, -1.0655668553109277, -1.0539683416996541, -1.0353722718101979, -1.010246430998152, -0.9806990573729688, -0.9489985895083445, -0.9164778952667905, -0.8862448636546425, -0.8599399474257599, -0.83872299794127, -0.8236641543023239, -0.8142379589862007, -0.8105341242528769, -0.8124821618980852, -0.821568732230011, -0.8377297550629565, -0.8604846290041898, -0.8888016630639783, -0.9213543973983811, -0.9565984995320851, -0.9922334907985446, -1.0252219703956915, -1.0524304172429106, -1.0701165484954602, -1.0754992840940623, -1.0679378221817404, -1.0519177757573295, -1.0283106353463176, -0.9989939503896459, -0.9662938316281384, -0.9309600172344581, -0.8971192711675324, -0.8667708952211286, -0.8413887336662922, -0.8229977203710683, -0.8100214827672957, -0.8026202213192178, -0.8007747119711257, -0.8076376998593433, -0.8259902650431483, -0.8552108297212737, -0.893133483617139, -0.928755658846459, -0.966825697169429, -1.0047227189910153, -1.0389992103206847, -1.0654386949395322, -1.0814587413639432, -1.0844705100917325, -1.0740895200087142, -1.0555703463420951, -1.0295089348188635, -0.997987891474193, -0.9633717751603259, -0.9268973334612274, -0.8921402407388256, -0.8609844544526314, -0.8347756585022953, -0.8151478976231071, -0.8010566647881954, -0.7926429364060947, -0.7898810804025262, -0.795571400892477, -0.8130076194208058, -0.8416963185576408, -0.879676644620634, -0.9158370934098141, -0.9552912637438532, -0.9956553727147988, -1.0336100667035129, -1.0657590958680203, -1.0874822788195215, -1.0953000614746342, -1.088123080676498, -1.0714622323951108, -1.0462146392302392, -1.0143475628828011, -0.9783857626692838, -0.9392904413751516, -0.9012652591821702, -0.8664889424040589, -0.8365378636089804, -0.8139047420205727, -0.7966671720679068, -0.7849981702523658, -0.7789169606296595, -0.7793719299481129, -0.7902911935909912, -0.8113351265740973, -0.8413951416848618, -0.8744669255234155, -0.9128509567563039, -0.9549067826296673, -0.9979942994927627, -1.03314228134792, -1.0640930110398819, -1.0880590004908004, -1.1027333630155607, -1.1063218534146289, -1.0979273490882375, -1.0778702509648752, -1.0480729646154712, -1.0085162659843159, -0.9646661949114185, -0.92029707433437, -0.8785103852409369, -0.8467009810606267, -0.8195437983619654, -0.7976219668348016, -0.7812751114633327, -0.7693305648492921, -0.7647360155347711, -0.7672992229626767, -0.7766677461116722, -0.796442891417965, -0.8253046070561836, -0.8626377232436305, -0.9068338273192952, -0.9525037756660056, -0.9997693206365874, -1.0449330355162862, -1.0831889063777795, -1.1075393769428838, -1.118881569811367, -1.115100838855206, -1.096389424631494, -1.0703087890525531, -1.0365897953384533, -0.9978405070470883, -0.9566625797177828, -0.9157537891684072, -0.8771134371927282, -0.8423499364517567, -0.8125590581209223, -0.7883175238715038, -0.7699329185948499, -0.7574565064395188, -0.7507985751455336, -0.7509203274983591, -0.7629033222238184, -0.7864912385791211, -0.8204473289803021, -0.8547622684213902, -0.894767528352429, -0.9392455852451631, -0.9860882009901405, -1.0263433736454002, -1.0638366902970913, -1.09568454258882, -1.119201970739855, -1.1318898475079886, -1.1318898475079886, -1.1184330085114833, -1.0927368540467284, -1.0581335537700012, -1.0168466901250093, -0.9719713360809497, -0.9264487721613438, -0.8854630853891312, -0.8478992805331725, -0.8149108009360259, -0.7872345687332136, -0.7641785179192015, -0.7474471814335469, -0.7369508470162729, -0.7324844580731472, -0.7343043353469603, -0.7432691533260605, -0.7591610393790761, -0.7816339605032396, -0.8103290676586444, -0.8451053844367554, -0.8854246372777126, -0.930197463024656, -0.974598623694553, -1.0195700980171591, -1.0624910063974407, -1.1002342357733526, -1.1294548004514782, -1.1465001298470512, -1.1489351769035616, -1.136119139764033, -1.112922112541486, -1.080048977278595, -1.0398642928276027, -0.9951683633034966, -0.9483129315213797, -0.902380254413309, -0.8594593460330275, -0.8210624987629995, -0.789182606378422, -0.7627879778895628, -0.7420259977235263, -0.7268774418246033, -0.7151635838790742, -0.7121710392069943, -0.7175281427313173, -0.7306838048550434, -0.7507729430712546, -0.7780454701041716, -0.8123924496381085, -0.8533332802803328, -0.9045461646898894, -0.9611994568651758, -1.0203326522269613, -1.0770372085508058, -1.1131784332842767, -1.1421426772196115, -1.1611744923718115, -1.1685437137270405, -1.1629687375713456, -1.1444495639047265, -1.1140755558840436, -1.0740895200087142, -1.0237545341432155, -0.9697477536372415, -0.9158114613355349, -0.8648677137059085, -0.8250867344248116, -0.7899900167182123, -0.7600709780159826, -0.7355538989680642, -0.7137345957380168, -0.6989256648232913, -0.6907938892582605, -0.6889227478358892, -0.6935493372432591, -0.7049940584088581, -0.7231543830355702, -0.747799622454884, -0.7787759842211248, -0.8162244447428275, -0.8599079073329109, -0.9091278979672708, -0.9600716455968974, -1.0131620794473948, -1.0657590958680203, -1.1140755558840436, -1.1541897521307685, -1.181039349938081, -1.1907795381641229, -1.1818723923521501, -1.160021049029254, -1.1263148713522935, -1.0831889063777795, -1.0335844346292338, -0.9805132248344456, -0.9271728782597273, -0.8760497061101473, -0.8289956257523678, -0.7895799035297474, -0.7553674923857756, -0.7265954890075338, -0.7032959334878707, -0.682271224560474, -0.6684042723755038, -0.6612272915773677, -0.6602276406804846, -0.6664177866188769, -0.6805474675652072, -0.7026102755009058, -0.732381929776031, -0.7656908103016661, -0.8055422777870305, -0.8518530279907173, -0.9040976033900058, -0.9586875135858282, -1.0160585038409284, -1.0735127983374353, -1.1272119939520606, -1.1727089257973875, -1.2044926979034185, -1.2178854567142259, -1.2108366362874852, -1.1893056938930768, -1.1546383134306517, -1.1092695419567205, -1.0563393085704669, -0.9995963041352037, -0.941968993137313, -0.8861551513946657, -0.8341476726824584, -0.790541106315212, -0.7519904665995099, -0.718784114370991, -0.6909733137782138, -0.665219487146331, -0.6460659196413053, -0.6330448259075443, -0.6256051163480478, -0.6233174537186419, -0.6287706775215114, -0.6417276910695748, -0.6617463410815186, -0.6835592362929965, -0.7105369944717043, -0.7429295283418631, -0.7809226704419957, -0.8430420024572912, -0.916080598115465, -0.9979302193070649, -1.0829966658206867, -1.136311380321126, -1.1836666375516842, -1.2213457867418984, -1.2456321771213055, -1.2535140399621156, -1.243197130064795, -1.2145532870579485, -1.1700816381837837, -1.1134988342127647, -1.048970087215238, -0.9810835384871547, -0.9135814708732571, -0.8592735134945044, -0.8088744474433078, -0.7632429472080161, -0.7228980622927799, -0.6861352597580417, -0.655293466381766, -0.6300907293468828, -0.6101297515020669, -0.5933920069978426, -0.5820498141293596, -0.5755905314110371, -0.573475885283015, -0.5752573144454094, -0.5807938424896858, -0.5899829411187278, -0.6027477141096984, -0.6206004538450619, -0.6429708466721092, -0.6702177416307471, -0.7027256198351616, -0.7572450418267165, -0.824016595323661, -0.9032389289016575, -0.9931818775468697, -1.0533275398426776, -1.1139473955126484, -1.172068123940411, -1.2241012347268974, -1.2661378365445513, -1.2935641560231428, -1.302855782949301, -1.292410712680585, -1.2658815158017607, -1.2244216356553854, -1.1706583598550626, -1.1079238580570698, -1.0422416677169855, -0.9749895128273086, -0.9088587611873407, -0.8459768749622433, -0.7923609835890252, -0.7434485778460139, -0.6996113228102561, -0.660977378853147, -0.6262395101864544, -0.5966729125055619, -0.5719123287519925, -0.5515156056444325, -0.5343292998403245, -0.5209109089552382, -0.5108375037635685, -0.5036541149468627, -0.4989826694095046, -0.4965732544272732, -0.49620799736879656, -0.49770106569555167, -0.5011934358160732, -0.5066082115075241, -0.5139582088070438, -0.5232626517703416, -0.5368604671753815, -0.5536110277167455, -0.5739052225271892, -0.5982236529994448, -0.6466298252754447, -0.7095886077233793, -0.7897977761611193, -0.8896347054780477, -0.9659734306996502, -1.0494186485151213, -1.1368881019924046, -1.2228196310129444, -1.2819656424118693, -1.3317559466989382, -1.3678330912467114, -1.3866726658418185, -1.385519222499261, -1.3631552376907834, -1.3204137538304555, -1.2601783792746706, -1.1875755288792407, -1.1067063345288146, -1.0221525295007743, -0.9378422291783849, -0.8714743808513357, -0.8086437587747963, -0.7502090374371153, -0.6966828583238738, -0.6409907689340519, -0.5920975872467501, -0.54968291233348, -0.5132533267643697, -0.4740106210431329, -0.44230246355622505, -0.41682225931727107, -0.39617799445236124, -0.38159417722999167, -0.36901844078682916, -0.35795307432056006, -0.34788671794931736, -0.3396184515887504, -0.33156036823727175, -0.32351189691364773, -0.31525836899579124, -0.30513369965556364, -0.29420161997554567, -0.28220580921294686, -0.26883868247641846, -0.2509731267039155, -0.23032008285356498, -0.20635409340264635, -0.17844076451275295, -0.13944797151573693, -0.09287449255068975, -0.03763096446075142, 0.027282263650961296, 0.09890468720521729, 0.1796457211842479, 0.2678136486856354, 0.35932015386187005, 0.42718107051567444, 0.4893388506423885, 0.5413078812431772, 0.5782821483907176, 0.5960964400146623, 0.591482666644432, 0.5634796254945619, 0.5138815617645858, 0.4506984986667096, 0.3757246814004669, 0.29344572296469273, 0.20825111607967595, 0.13704521373245462, 0.06831921457173214, 0.003405986460019419, -0.05680375602148631, -0.11377744912526103, -0.164951885423399, -0.2103911451015979, -0.250332324846939, -0.2968160915520095, -0.3349822501535259, -0.3660880538948759, -0.3914593218181439, -0.40710770316550837, -0.42076895795438896, -0.43281603286554593, -0.4436141849574558, -0.4534972719976033, -0.4627228963324931, -0.4715307178566341, -0.48016872688867646, -0.4901011556718112, -0.5003603934020039, -0.5112091688406148, -0.5229102107490046, -0.5380075024993693, -0.5550207918020936, -0.5743601918456425, -0.5964998960041782, -0.626989248359117, -0.6629190084797855, -0.7051478508545326, -0.7545985301574039, -0.8264003782316133, -0.9112874002252814, -1.0083304334457925, -1.1132425134699744, -1.1796936660384305, -1.242812648950609, -1.298690570878954, -1.3429058990103282, -1.371293421274384, -1.3798801661578681, -1.3666796479041536, -1.331948187256031, -1.282734604640241, -1.2200641830279457, -1.1476535731896087, -1.0694757466384837, -0.9975329221557396, -0.9264615881984835, -0.8581777423190746, -0.7940398844543034, -0.7406931298610153, -0.6917422760065856, -0.6473218912809792, -0.6074191596470566, -0.5563664757017442, -0.5139325767327647, -0.4790793637318165, -0.4506745398176221, -0.43311592813461086, -0.4180391420436694, -0.405061750996554, -0.3938100393502762, -0.3839388712246684, -0.3751810322453715, -0.3672863533674219, -0.3600100482814545, -0.3525075401399744, -0.34526135274128483, -0.33806514788743947, -0.3307081017674931, -0.32230078140396223, -0.3132654752205945, -0.30338431058601795, -0.2924201908131512, -0.27758562782414675, -0.26045699418716667, -0.24057291256518792, -0.21738229336121082, -0.1851115118438776, -0.14641989571964054, -0.10019244975736065, -0.04522446646592216, 0.03494625386039945, 0.1310985724997133, 0.2419893338494852, 0.3613707198041947, 0.43083364110044015, 0.4954905484693622, 0.5507917487264287, 0.5920593883157109, 0.6150641749811647, 0.6160894579523272, 0.5934691524010589, 0.5486130224127086, 0.4892106902709932, 0.41673600024695867, 0.3354182445966491, 0.24955079576180703, 0.17526263648252907, 0.1026597860870992, 0.033369881292237454, -0.03150489870805665, -0.09292575669924787, -0.14860503005193018, -0.19847863858040599, -0.2427003747303497, -0.29475911759111517, -0.3380068349184547, -0.37367642948519086, -0.4031106375024322, -0.4226272833396206, -0.43966364150919607, -0.45470518349800393, -0.46823058829320546, -0.48068777639282734, -0.4924080423569263, -0.5037053790954209, -0.5149065955553689, -0.5280174015491067, -0.5417305612884025, -0.556398515794593, -0.572386522126155, -0.5930844221064939, -0.6165377700718313, -0.6432527994891787, -0.6738190480669546, -0.7162209069430853, -0.765799746617352, -0.8231899609281613, -0.8886991347668621, -0.9618274426850129, -1.0410882243744277, -1.1232390224388067, -1.2023780517753961, -1.2544111625618826, -1.2968963256794201, -1.3260528101718478, -1.338932927497074, -1.33361427208417, -1.309456042076158, -1.2674194402585042, -1.2105803155446946, -1.1425271583337973, -1.0677455816246475, -0.9904584696547198, -0.9141902326373847, -0.8528719029433097, -0.7953983843910934, -0.7425194151533981, -0.6946771485115376, -0.6523201457653953, -0.6149485814665298, -0.5823253589278595, -0.5541428932580359, -0.5272868874321536, -0.5049805747908039, -0.4866600496998477, -0.4717678145437154, -0.4597547021309782, -0.45021700729174097, -0.4427805017415295, -0.4370773652144392, -0.43290318191809474, -0.42987731554945197, -0.4278350800312681, -0.4266169157011559, -0.42608440935800845, -0.42625101784082237, -0.42705778737875566, -0.4284496090121085, -0.43069433791709694, -0.43367150334460947, -0.4374221166134925, -0.44200128668344607, -0.44872393896498586, -0.4570030989571214, -0.46713481711777577, -0.479502292957421, -0.502475039530026, -0.5329451678292555, -0.573283644725922, -0.6269956563776867, -0.685013856508333, -0.7578538035908441, -0.8478736484588936, -0.9563678108635736, -1.0444203940307053, -1.1395153896060082, -1.2377503142804953, -1.3325889891130074, -1.3966691748106508, -1.4490226865256257, -1.485035750887701, -1.5009276369407167, -1.494071057071069, -1.463504808493293, -1.4104464147356441, -1.338420286011493, -1.2737633786425708, -1.203147014003768, -1.128749918408804, -1.0528148983570966, -0.9770721188624822, -0.9031620326788202, -0.8322637152229476, -0.7652358409832127, -0.6817521750563229, -0.6069641903286034, -0.5409103349114726, -0.48324457580216335, -0.4269866583726313, -0.3794346749719811, -0.33930702188625983, -0.30526186002695893, -0.27491989209912476, -0.2485188555916957, -0.22500783545923034, -0.20329746854486877, -0.18354795531285512, -0.16402272273078317, -0.14419631327593233, -0.12354967744415163, -0.09800090740650122, -0.07025418699942167, -0.039892995015878195, -0.006532850341685067, 0.03804132682959564, 0.08728054151966479, 0.14059525602010411, 0.19672949869123965, 0.24807695149076123, 0.29767501522073725, 0.3425311452090877, 0.3793772519852326, 0.4049452460785922, 0.4159670380185869, 0.4105843024199848, 0.3888611194684838, 0.35566758327710457, 0.31202897681700925, 0.26089298863029, 0.20534828366757263, 0.15048923669182018, 0.09691820144859035, 0.0465255434159636, 0.0006313144193114399, -0.03970075445878526, -0.07445784718118706, -0.10356947554362643, -0.1271445758617894, -0.14866270221905806, -0.163157640223865, -0.1711676634360704, -0.17331434965694148, -0.16949517058936192, -0.1593128290820064, -0.14281858928343297, -0.12013420354646723, -0.09316285338632915, -0.06025767803058926, -0.021149540699317543, 0.024276902941741826, 0.09673877692863699, 0.17954960090570143, 0.2681340496141236, 0.3522713334351293, 0.39360305321010924, 0.42564314605893094, 0.4455080036252004, 0.4511470599665932, 0.44127871136915603, 0.4160311182042845, 0.3768781247430244, 0.3265110987846768, 0.2669165260858685, 0.2030670290567366, 0.13864721837489574, 0.07653429437817, 0.025776379287066713, -0.02057281902803873, -0.06194298691443728, -0.09806498759219884, -0.1315853327306361, -0.1591077724877739, -0.18100397194065868, -0.19773530842631334, -0.21015404841451663, -0.21804872729246627, -0.22181664221148772, -0.22184227428576678, -0.21843961642522192, -0.21171119692696935, -0.20170827993956722, -0.1884500895187248, -0.16848270365533916, -0.14353628736324658, -0.1131815033982729, -0.07699542253481374, -0.01661907157049413, 0.05698983774038873, 0.14342760022793993, 0.23930437406875393, 0.29825173689201595, 0.3554753427200115, 0.4078929346206839, 0.4517237816378719, 0.48318715281541486, 0.4986304775685468, 0.49542646828366466, 0.47338288440367526, 0.4371775794845068, 0.3879639968687167, 0.32888206565548944, 0.26332803568680035, 0.19864549624359917, 0.13413597330178156, 0.07199100921220708, 0.013812608617316693, -0.037079874863751666, -0.0829100236747062, -0.12348559725845396, -0.15881300363356476, -0.19114145731802584, -0.21810639945959417, -0.24017561541386254, -0.25786815468498187, -0.27445851476210176, -0.28599294818767756, -0.29313788889296477, -0.29659181090206777, -0.29705959625766054, -0.2954191435038009, -0.29179220499331426, -0.2862749010047472, -0.2780598211983093, -0.2674994065953376, -0.2544462727687277, -0.23868895510567723, -0.21339009779224763, -0.18207411104180932, -0.1437349359389093, -0.09717427301100162, -0.03645829706248455, 0.03672768302279394, 0.12283222854471733, 0.2204455754179375, 0.2887037892230673, 0.3579103897765221, 0.4250023442019545, 0.4856862800576228, 0.5354125041589941, 0.5687342007217686, 0.5813579973042043, 0.5711051675925813, 0.5423331642143396, 0.49632359088343175, 0.43576781539915876, 0.36451064890337925, 0.2910747560938799, 0.21509487991218423, 0.13962123719749991, 0.06690945048638397, 0.006469019336366747, -0.04974852757617577, -0.1013074449884996, -0.14799626828780257, -0.20083038139550952, -0.24609021655375501, -0.2843909435452364, -0.31650152459832553, -0.34036562655398495, -0.3606361116957204, -0.377832029527683, -0.39247691516702243, -0.40506059755321144, -0.4159603808196378, -0.425527552544296, -0.4341085302110674, -0.4419820626277368]}, {\"line\": {\"color\": \"blue\", \"width\": 1}, \"type\": \"scatter\", \"x\": [1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500], \"y\": [-0.4494108785556646, -0.45660964661693787, -0.46379752104664257, -0.471722958413727, -0.48006619859156024, -0.4890117925149512, -0.49879683687098136, -0.5118115225861728, -0.5266524935937469, -0.5436914149707502, -0.5633960720727756, -0.5906814151428321, -0.6231764773101072, -0.6618232373043559, -0.7077046502638684, -0.7730536236383252, -0.8517633157307406, -0.944307919915277, -1.0489060070295404, -1.1197786924111337, -1.191292179649704, -1.2599861387175775, -1.3215671971730127, -1.371293421274384, -1.4037820754230892, -1.4146757069916884, -1.4019237500378576, -1.3702681383032218, -1.3207341547589435, -1.2563976483185095, -1.181039349938081, -1.1045276082150945, -1.0255680033984587, -0.9470056957331479, -0.8711411638857078, -0.8089961997961334, -0.7508498392940918, -0.6971442356608969, -0.6481100775650601, -0.5848052621143583, -0.5309714981097682, -0.4857308870072319, -0.44805045621330364, -0.423295639676447, -0.4018375564531771, -0.3831820842315793, -0.36683779206753836, -0.35234221326087445, -0.3393224011308273, -0.3274271962597737, -0.3163156920598024, -0.30460824213284293, -0.29313788889296477, -0.28160345546738896, -0.26969094894619705, -0.25512552273712275, -0.2392977168698048, -0.22182945824862724, -0.20230422566655532, -0.1761979580133354, -0.14606104667973374, -0.11127191386448314, -0.07111286148777007, -0.014119944328286059, 0.0526003450201002, 0.1290480065573887, 0.2138389082725104, 0.27928400192551367, 0.34509435263699334, 0.4081492553634743, 0.4643475782203076, 0.5091396280229604, 0.5374630701013187, 0.5454090131278264, 0.5314395326457402, 0.500745123696569, 0.4542869890657776, 0.39507689748115526, 0.32663925915607195, 0.25672777655994306, 0.18527196148850097, 0.1149439576853375, 0.047813555148486245, -0.009128097862439665, -0.06154568976311191, -0.10905473943934467, -0.15152708651974273, -0.19250636527338563, -0.22783377164849644, -0.2579450509078191, -0.2833464365183649, -0.3039161761273084, -0.32107684985713725, -0.3352834270263048, -0.3469601184641294, -0.3565176781609329, -0.3642611278006361, -0.37046985699288076, -0.3754149249231679, -0.3794269853496974, -0.3825226991207505, -0.38485457707828774, -0.38656808124384273, -0.38786570500422, -0.38866798892915455, -0.38904734362848453, -0.3890761797120485, -0.38875577878356027, -0.38807652881516524, -0.3870480418347181, -0.38568121147378737, -0.38363833515374646, -0.38105205885898963, -0.37783651514068184, -0.37388469008870817, -0.36780988848457163, -0.3600600308262986, -0.3501743805787232, -0.3375172622997247, -0.32127549843279996, -0.3004814781739147, -0.2738497529979741, -0.2396245258168628, -0.19713295468075548, -0.1431582142676305, -0.07506020092674487, 0.00996138945688831, 0.08719082925968812, 0.17611490295230778, 0.27601591245493373, 0.3839269451697652, 0.46582142249135333, 0.5457934942420123, 0.619357547422907, 0.6811949266211327, 0.72585881605239, 0.7477742395609842, 0.7431604661907537, 0.7112485337133275, 0.6601766257123057, 0.5906496242303626, 0.5065764205950546, 0.41250670799091405, 0.3253576554421191, 0.23688214304938296, 0.1495792980549137, 0.06529462980680337, -0.018099323860109713, -0.09571965279566509, -0.16699604334715382, -0.2317042148646341, -0.312464472899374, -0.38118790885266857, -0.4391817585127498, -0.48794806143237035, -0.532060861266628, -0.5691632887855635, -0.6008317165573389, -0.6286873732801044, -0.6518908085212212, -0.6738190480669546, -0.6951385258485606, -0.7164900437230154, -0.741487724163666, -0.7677029281325719, -0.7955073207067794, -0.8252405268704859, -0.8634835816948395, -0.9044628604484823, -0.9475375612744382, -0.991624729034417, -1.0358144250915118, -1.0769090481794106, -1.1119609097560215, -1.1381697057063576, -1.1528440682311178, -1.15386935120228, -1.140412512205775, -1.1136269945841601, -1.0776139302220846, -1.0340906680962452, -0.9863573377700707, -0.9374000758970712, -0.8926785142986858, -0.8511032898180547, -0.8139175580577124, -0.7819415453945883, -0.7544575537488691, -0.733170116060112, -0.717970296012631, -0.7086530370121936, -0.7047569617217769, -0.7079225228952404, -0.7179062158269333, -0.7343555994955184, -0.7570079451396353, -0.785946557000691, -0.8210496827258601, -0.8619392492195262, -0.9178299871850107, -0.9791034607490973, -1.0419853469741949, -1.1002983159590503, -1.1336840927075225, -1.1586112849439056, -1.1728370861687827, -1.174759491739712, -1.1637376997997173, -1.1401561914629845, -1.105552891186257, -1.062426926211743, -1.0112717139693144, -0.9576814546703752, -0.9049050137297961, -0.8554479264083551, -0.815666947127258, -0.7807047978106239, -0.7509715916469173, -0.7266339371189524, -0.7047633697403467, -0.6899480308070515, -0.6818162552420206, -0.6799130737268005, -0.6861801158880302, -0.7024885231480804, -0.7288511115440908, -0.7647488315719105, -0.8008772402682419, -0.8433880354600585, -0.8918454718846164, -0.945205042515044, -0.9954631321577058, -1.0462787194159369, -1.094979660546146, -1.1382978660777527, -1.1725166852402944, -1.1937272267062145, -1.1989818019334213, -1.1872551279507524, -1.1629687375713456, -1.1272119939520606, -1.0823558639637103, -1.0313928922783744, -0.9776232084594818, -0.9238150765291706, -0.8723586874139632, -0.8249842061276954, -0.7846905853610172, -0.7496515398215459, -0.720091350159223, -0.6960100163740485, -0.6741458570140126, -0.6593177020435781, -0.6510449500700122, -0.6487829195148854, -0.6532749405322903, -0.6654950319448308, -0.6853983376225189, -0.7127669849339824, -0.7433076014374792, -0.7801344841579148, -0.8233181212995567, -0.8726534562681724, -0.9305947601759814, -0.9931946935840092, -1.0576209122844198, -1.1194582914826456, -1.1617512140430901, -1.1959059530199343, -1.218910739685388, -1.228458687354337, -1.223075951755735, -1.2025062121467915, -1.1677747514986687, -1.12150885742497, -1.0643493317826724, -1.002281263915935, -0.9392968493937214, -0.8785103852409369, -0.8298991563707045, -0.7856902362579005, -0.746492386666652, -0.7125875604140289, -0.6811049651807768, -0.6559086361644634, -0.6366140922509029, -0.6227663641216422, -0.6135836735111699, -0.6093351571994161, -0.6096747821836137, -0.6142565154609951, -0.6228368523259097, -0.635415792778357, -0.6520510089854653, -0.6728193971700714, -0.7043019924033236, -0.7428141840076072, -0.7887853092270966, -0.8423819765446056, -0.9060584570723538, -0.9767389018968543, -1.0517255352002366, -1.1259303902381077, -1.1754002935966883, -1.217308735042947, -1.2481313043635136, -1.2647921526449009, -1.2651766337590866, -1.248195384549211, -1.2144892068722508, -1.1666213081561112, -1.105552891186257, -1.0375894462353366, -0.9671845462093357, -0.8981061060272761, -0.8437853326113839, -0.7935977311729897, -0.7482609997919069, -0.7081788436380311, -0.6715698335489674, -0.6407280401726917, -0.6153330625807156, -0.5949747875845742, -0.577634689334792, -0.5653441097179841, -0.5575904072485692, -0.5538353083666873, -0.5536302517724548, -0.5567701808716393, -0.563114119255706, -0.5725531306089688, -0.5859715214940554, -0.6030617070196169, -0.6240864159470136, -0.649366049204734, -0.6907041769982837, -0.7416543326464801, -0.8032033510090665, -0.8758959136644731, -0.9470697759188456, -1.0247605930586683, -1.1058092119290475, -1.184435599780056, -1.2361483096380543, -1.278761633126987, -1.3086229996620888, -1.3227206405155703, -1.3190039897451071, -1.2966400049366296, -1.2567180492469976, -1.201865410289815, -1.1353501775356611, -1.0620424450975574, -0.9861394651386985, -0.9112745841881419, -0.8513083464122873, -0.7952381839268493, -0.7438202429230604, -0.6974902686636641, -0.6572927681755325, -0.6219397297261426, -0.5912196887026924, -0.5648250602138332, -0.5400132123117056, -0.5195972651484364, -0.5030645772384444, -0.4898896910590089, -0.4795791891802581, -0.47177422256228513, -0.4661351662208925, -0.462343541633163, -0.4601295712173094, -0.45933177290537375, -0.45981814151481887, -0.46146884709839014, -0.46434092102135854, -0.46837797272031007, -0.4736069158732377, -0.48004697453585093, -0.4897423066319044, -0.5016291810788173, -0.5160087747493683, -0.5332719767763135, -0.5664270648562743, -0.609610701997916, -0.6653925036477146, -0.7371430875733659, -0.8201077039961048, -0.9214761497512068, -1.040402566387463, -1.1700816381837837, -1.241402884865261, -1.308110358176508, -1.3657825253043867, -1.4096774525072724, -1.4353736069720273, -1.4388980171853978, -1.4183923577621518, -1.3749459918591498, -1.316312621945806, -1.243709771550376, -1.161430813114602, -1.0738331992659236, -0.9971484410415538, -0.921770918605416, -0.8494115729156371, -0.7812815194819025, -0.7152340720833416, -0.6551524899732312, -0.6011585255043969, -0.5531496503797225, -0.4972460963770984, -0.4504816584586722, -0.41160164658848414, -0.3792520464427428, -0.3563094175574156, -0.33629717556404154, -0.318609762707778, -0.3026345724133555, -0.28781923348006033, -0.27375363271942765, -0.26003406496156223, -0.24626323305513867, -0.22997404985079772, -0.21271725584242238, -0.1940763298229779, -0.17361552652972037, -0.14693253720522167, -0.11664824144451541, -0.08222436568774143, -0.04310982033789991, 0.011928651157805985, 0.07511812227425207, 0.146010031711555, 0.22286139841873862, 0.28325697343876755, 0.3427233857661805, 0.3980886662089443, 0.44557208381089813, 0.4812006670587878, 0.5008092038822667, 0.5013859255535454, 0.4823541104013453, 0.4487120129100826, 0.4014208358652218, 0.34355642818024984, 0.27845095951144433, 0.21314043424840612, 0.14748387598260077, 0.08384584356627113, 0.023956502013253615]}, {\"line\": {\"color\": \"green\", \"width\": 1}, \"type\": \"scatter\", \"x\": [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500], \"y\": [-0.26159730553627014, -0.23700262606143951, -0.19730144739151, -0.14975056052207947, -0.09150362014770508, -0.02056172490119934, 0.0809640884399414, 0.20338398218154907, 0.33312052488327026, 0.47255587577819824, 0.5598880648612976, 0.6313762068748474, 0.7009878754615784, 0.7645959258079529, 0.8173654675483704, 0.8538437485694885, 0.8528475165367126, 0.8043437600135803, 0.731461226940155, 0.6398293375968933, 0.5333289504051208, 0.41925686597824097, 0.32690906524658203, 0.22445201873779297, 0.12467342615127563, 0.029433518648147583, -0.057285696268081665, -0.138452410697937, -0.21607451140880585, -0.2870873808860779, -0.3911479115486145, -0.47727739810943604, -0.5491788983345032, -0.608893632888794, -0.6536507606506348, -0.6916457414627075, -0.7240598201751709, -0.7519688606262207, -0.7797514200210571, -0.8039509654045105, -0.8246824741363525, -0.8431302309036255, -0.8619871139526367, -0.8799393177032471, -0.8973202705383301, -0.9143481254577637, -0.9343883395195007, -0.953468918800354, -0.9714349508285522, -0.9878516793251038, -1.004849910736084, -1.01741623878479, -1.0248064994812012, -1.0265333652496338, -1.021993637084961, -1.011317491531372, -0.9953176975250244, -0.9756106734275818, -0.9488638639450073, -0.9229356050491333, -0.8998766541481018, -0.8817238807678223, -0.8715102672576904, -0.8659940958023071, -0.8655414581298828, -0.8700289726257324, -0.882718563079834, -0.9019088745117188, -0.9266561269760132, -0.9545084238052368, -0.9805876016616821, -1.0040889978408813, -1.0221033096313477, -1.0320730209350586, -1.0328936576843262, -1.02584707736969, -1.0114808082580566, -0.9910051226615906, -0.9663967490196228, -0.9405338168144226, -0.915743350982666, -0.8940340280532837, -0.8781411051750183, -0.8668551445007324, -0.8607722520828247, -0.8600226640701294, -0.8653295040130615, -0.8767582178115845, -0.8938270211219788, -0.9154320359230042, -0.9428552985191345, -0.9713596701622009, -0.9983133673667908, -1.0206258296966553, -1.0340237617492676, -1.0390443801879883, -1.034487247467041, -1.020865559577942, -1.0028843879699707, -0.9800926446914673, -0.9550802707672119, -0.9299353957176208, -0.9051679372787476, -0.8842206001281738, -0.8682667016983032, -0.858020544052124, -0.8538615703582764, -0.8550412654876709, -0.8614292144775391, -0.8726544976234436, -0.8951138257980347, -0.9233105182647705, -0.9557344913482666, -0.9882343411445618, -1.0142297744750977, -1.033940076828003, -1.043973684310913, -1.042709469795227, -1.0329344272613525, -1.0154728889465332, -0.9918980598449707, -0.9647482633590698, -0.9335448145866394, -0.9046598672866821, -0.8802316188812256, -0.8619434833526611, -0.8523315191268921, -0.8476753830909729, -0.8481486439704895, -0.8535910844802856, -0.8700752854347229, -0.8948230743408203, -0.9268589019775391, -0.962631106376648, -0.9913631677627563, -1.0176570415496826, -1.0377743244171143, -1.049048900604248, -1.0502681732177734, -1.0424376726150513, -1.0261774063110352, -1.0028570890426636, -0.9750525951385498, -0.9454513788223267, -0.9166147708892822, -0.8906572461128235, -0.8706377744674683, -0.8552711009979248, -0.8452515602111816, -0.8407852649688721, -0.8421597480773926, -0.8499670028686523, -0.8638980388641357, -0.8832588195800781, -0.9116846919059753, -0.944008469581604, -0.9778794050216675, -1.0100336074829102, -1.0358303785324097, -1.0527619123458862, -1.0578200817108154, -1.050038456916809, -1.0351619720458984, -1.0129761695861816, -0.9854853749275208, -0.955604076385498, -0.922292947769165, -0.8922012448310852, -0.8671482801437378, -0.8484906554222107, -0.8383959531784058, -0.833452045917511, -0.8337495923042297, -0.8391309976577759, -0.8523715734481812, -0.8719304800033569, -0.8973420858383179, -0.9271857142448425, -0.9593604207038879, -0.9916581511497498, -1.0214412212371826, -1.045491337776184, -1.060210108757019, -1.064765214920044, -1.0578800439834595, -1.0403409004211426, -1.0183653831481934, -0.9907641410827637, -0.960641622543335, -0.930234432220459, -0.9003033638000488, -0.8741649389266968, -0.8529976606369019, -0.8375629186630249, -0.8284593820571899, -0.8250020742416382, -0.8271217346191406, -0.8346081972122192, -0.8502160310745239, -0.8720253705978394, -0.8995495438575745, -0.9313330054283142, -0.9651194214820862, -0.998823881149292, -1.0295615196228027, -1.0540529489517212, -1.0686309337615967, -1.0723958015441895, -1.0643479824066162, -1.04519784450531, -1.0217456817626953, -0.9925543069839478, -0.9608671069145203, -0.9289777278900146, -0.8978110551834106, -0.8704650402069092, -0.8481041789054871, -0.831452488899231, -0.8210883140563965, -0.8164430856704712, -0.8174526691436768, -0.8239277005195618, -0.8383343815803528, -0.8591034412384033, -0.8858837485313416, -0.9174516201019287, -0.951879620552063, -0.9872888326644897, -1.021140456199646, -1.0501749515533447, -1.0711495876312256, -1.0808265209197998, -1.0773112773895264, -1.060957670211792, -1.0393105745315552, -1.0106871128082275, -0.9781702160835266, -0.9443657398223877, -0.9095712900161743, -0.8781945109367371, -0.8516314029693604, -0.8308522701263428, -0.8172074556350708, -0.8090812563896179, -0.8065125942230225, -0.809350848197937, -0.8236029148101807, -0.8481850028038025, -0.8828394412994385, -0.9250805377960205, -0.9614416360855103, -0.9992827773094177, -1.0346521139144897, -1.063920259475708, -1.0832573175430298, -1.090531349182129, -1.0841176509857178, -1.0646345615386963, -1.0405118465423584, -1.0094475746154785, -0.9749265909194946, -0.9394876956939697, -0.9038972854614258, -0.8718377351760864, -0.8446367979049683, -0.8231545686721802, -0.8084237575531006, -0.799340546131134, -0.7958971261978149, -0.7979485988616943, -0.8112281560897827, -0.8351407647132874, -0.8696045875549316, -0.9124090671539307, -0.9498704671859741, -0.989764928817749, -1.0283550024032593, -1.0620100498199463, -1.0873584747314453, -1.0999910831451416, -1.097476840019226, -1.0798988342285156, -1.0567736625671387, -1.025749921798706, -0.9899381995201111, -0.9523769021034241, -0.9135164022445679, -0.8778517246246338, -0.8469215631484985, -0.8217549324035645, -0.8041162490844727, -0.791982114315033, -0.7854107618331909, -0.7842882871627808, -0.7918725609779358, -0.8091207146644592, -0.8360339403152466, -0.8714328408241272, -0.907889723777771, -0.9485548734664917, -0.9911641478538513, -1.0326091051101685, -1.063704252243042, -1.0891344547271729, -1.10563063621521, -1.1113646030426025, -1.1054282188415527, -1.0879319906234741, -1.0601688623428345, -1.0240658521652222, -0.9801344871520996, -0.9352661967277527, -0.8925094604492188, -0.8543785810470581, -0.8273398280143738, -0.805133581161499, -0.7884923219680786, -0.7774857878684998, -0.7714389562606812, -0.7725280523300171, -0.7805200219154358, -0.7950462102890015, -0.8213735818862915, -0.8557322025299072, -0.8977883458137512, -0.9449884295463562, -0.9913508296012878, -1.0371534824371338, -1.0777159929275513, -1.1080615520477295, -1.122544765472412, -1.1227879524230957, -1.1077990531921387, -1.0789254903793335, -1.0470261573791504, -1.0082652568817139, -0.9669828414916992, -0.9257302284240723, -0.8866519927978516, -0.8514035940170288, -0.8211642503738403, -0.7965604066848755, -0.777722954750061, -0.7648299932479858, -0.7577401399612427, -0.756261944770813, -0.7642936706542969, -0.7832273840904236, -0.8133339285850525, -0.8534277081489563, -0.8910449743270874, -0.9338829517364502, -0.9797142148017883, -1.0259051322937012, -1.0630223751068115, -1.0957932472229004, -1.1205995082855225, -1.135033130645752, -1.1372685432434082, -1.1264485120773315, -1.102968454360962, -1.0687952041625977, -1.027984857559204, -0.9830899834632874, -0.9373591542243958, -0.893418550491333, -0.8558921813964844, -0.8227109909057617, -0.7948899865150452, -0.7727597951889038, -0.7554618120193481, -0.7444677352905273, -0.7395015358924866, -0.7402967810630798, -0.7479890584945679, -0.762328028678894, -0.7833307981491089, -0.8106937408447266, -0.8440115451812744, -0.8827952742576599, -0.9260439872741699, -0.9726173281669617, -1.0167806148529053, -1.0598771572113037, -1.0984498262405396, -1.1291553974151611, -1.1487622261047363, -1.1543166637420654, -1.144498586654663, -1.1198585033416748, -1.088566780090332, -1.0486268997192383, -1.0039114952087402, -0.9574630260467529, -0.9111752510070801, -0.8679419159889221, -0.8292899131774902, -0.7961530685424805, -0.769879937171936, -0.7492244243621826, -0.734157919883728, -0.7245028018951416, -0.7195875644683838, -0.7228331565856934, -0.7340786457061768, -0.7528958320617676, -0.7783807516098022, -0.8108829259872437, -0.8501332998275757, -0.8950197100639343, -0.9501339793205261, -1.0082430839538574, -1.0658867359161377, -1.11716890335083, -1.1451417207717896, -1.1649081707000732, -1.1729152202606201, -1.1682100296020508, -1.1509044170379639, -1.1218194961547852, -1.0827170610427856, -1.036129117012024, -0.9815537929534912, -0.9272018074989319, -0.8757408857345581, -0.8293238878250122, -0.7951674461364746, -0.7657642364501953, -0.7417806386947632, -0.7231671810150146, -0.7076730728149414, -0.6989083290100098, -0.6964479684829712, -0.6998934745788574, -0.7102389931678772, -0.7266210317611694, -0.7497353553771973, -0.7795318365097046, -0.8157860040664673, -0.8576846718788147, -0.905089259147644, -0.9571197628974915, -1.0090746879577637, -1.061370611190796, -1.1103909015655518, -1.1518278121948242, -1.1814961433410645, -1.1949058771133423, -1.189452886581421, -1.1651530265808105, -1.1330244541168213, -1.0902327299118042, -1.0403945446014404, -0.9873726963996887, -0.9337553977966309, -0.8824864625930786, -0.8354573845863342, -0.7938472032546997, -0.7603871822357178, -0.7322818636894226, -0.7096272110939026, -0.6922476291656494, -0.6776953935623169, -0.6698436141014099, -0.6682307720184326, -0.6717824935913086, -0.6835692524909973, -0.7030054330825806, -0.7305669784545898, -0.7661950588226318, -0.8046104311943054, -0.8498835563659668, -0.9003666639328003, -0.9557711482048035, -1.011852502822876, -1.068861484527588, -1.1230251789093018, -1.1697721481323242, -1.2042677402496338, -1.2214303016662598, -1.217919111251831, -1.19337797164917, -1.1601457595825195, -1.1150795221328735, -1.0617096424102783, -1.004399061203003, -0.9465591311454773, -0.8906747102737427, -0.8388901948928833, -0.7924677133560181, -0.7550603151321411, -0.7228633165359497, -0.6960576772689819, -0.6744554042816162, -0.6551896333694458, -0.6420856714248657, -0.6341928839683533, -0.6307514905929565, -0.6338211297988892, -0.6441351771354675, -0.6619848012924194, -0.6871943473815918, -0.7129426002502441, -0.7449327707290649, -0.7827419638633728, -0.8260287642478943, -0.897560715675354, -0.9759521484375, -1.0604097843170166, -1.1429754495620728, -1.1870639324188232, -1.224642276763916, -1.2489439249038696, -1.256960153579712, -1.2476272583007812, -1.2203527688980103, -1.17686128616333, -1.1202822923660278, -1.0554126501083374, -0.9870766997337341, -0.9195021390914917, -0.8556249141693115, -0.8072298169136047, -0.7629387378692627, -0.7240534424781799, -0.6906740665435791, -0.6607240438461304, -0.6364979147911072, -0.6174580454826355, -0.6024982333183289, -0.5899072885513306, -0.5823307037353516, -0.5793536305427551, -0.5805351734161377, -0.5855268239974976, -0.5942699313163757, -0.6067458391189575, -0.6229848861694336, -0.6451466679573059, -0.6720041036605835, -0.7041951417922974, -0.7421466708183289, -0.8066398501396179, -0.8821663856506348, -0.969819962978363, -1.0642129182815552, -1.119499683380127, -1.176676869392395, -1.2277681827545166, -1.2686893939971924, -1.2965360879898071, -1.306335687637329, -1.2961124181747437, -1.2659335136413574, -1.2240897417068481, -1.1693280935287476, -1.105478048324585, -1.0369668006896973, -0.9698007106781006, -0.9040576219558716, -0.842095673084259, -0.7852503061294556, -0.7385144233703613, -0.6966712474822998, -0.6600360870361328, -0.6284480690956116, -0.6004140377044678, -0.5768059492111206, -0.5559684038162231, -0.5391358733177185, -0.5252740979194641, -0.5148116946220398, -0.5073696374893188, -0.5025462508201599, -0.5000249147415161, -0.49959760904312134, -0.5011085271835327, -0.5044135451316833, -0.5099228620529175, -0.517314076423645, -0.5267297625541687, -0.5382538437843323, -0.5551084280014038, -0.5751885771751404, -0.5992965698242188, -0.6280175447463989, -0.6874547004699707, -0.7608820796012878, -0.8534160256385803, -0.9660201668739319, -1.0450607538223267, -1.1315712928771973, -1.216968297958374, -1.2942445278167725, -1.340687870979309, -1.3747878074645996, -1.390932321548462, -1.3868328332901, -1.361975908279419, -1.3171956539154053, -1.2554327249526978, -1.1797912120819092, -1.097450613975525, -1.0122597217559814, -0.9283494353294373, -0.8485869765281677, -0.7895505428314209, -0.7339508533477783, -0.6835019588470459, -0.6382737159729004, -0.5910701155662537, -0.5506628751754761, -0.5152397751808167, -0.483720600605011, -0.44972795248031616, -0.42236509919166565, -0.40026870369911194, -0.38214394450187683, -0.3690316379070282, -0.3574915826320648, -0.34706711769104004, -0.3372949957847595, -0.32919764518737793, -0.32097843289375305, -0.3125152885913849, -0.30361324548721313, -0.29229384660720825, -0.2800107002258301, -0.26635050773620605, -0.25099092721939087, -0.2299133688211441, -0.2058403491973877, -0.17784279584884644, -0.14523032307624817, -0.09883695840835571, -0.04451301693916321, 0.019514024257659912, 0.09393131732940674, 0.17450153827667236, 0.25952398777008057, 0.3414958715438843, 0.4270176887512207, 0.49088579416275024, 0.5436972379684448, 0.5823258757591248, 0.602085292339325, 0.5995245575904846, 0.5731298327445984, 0.5242445468902588, 0.45667606592178345, 0.3797844648361206, 0.29845893383026123, 0.21276164054870605, 0.12913405895233154, 0.06315204501152039, -0.000270158052444458, -0.05887800455093384, -0.11395314335823059, -0.165777325630188, -0.21170561015605927, -0.2520984411239624, -0.28734341263771057, -0.32795190811157227, -0.361039936542511, -0.3880736827850342, -0.41031891107559204, -0.4245184659957886, -0.4368512034416199, -0.44784998893737793, -0.45789581537246704, -0.4671362340450287, -0.47603532671928406, -0.48480796813964844, -0.49367427825927734, -0.504315197467804, -0.5154753923416138, -0.5275152921676636, -0.540698766708374, -0.5582133531570435, -0.5778264999389648, -0.6002426147460938, -0.6259729862213135, -0.6621416211128235, -0.7038750648498535, -0.7525429725646973, -0.8091849088668823, -0.8925325870513916, -0.987362802028656, -1.0902750492095947, -1.1958301067352295, -1.2531098127365112, -1.3073148727416992, -1.3494677543640137, -1.3752892017364502, -1.3824117183685303, -1.3676565885543823, -1.331319808959961, -1.2756571769714355, -1.2115395069122314, -1.1370315551757812, -1.0578911304473877, -0.9778446555137634, -0.9086573123931885, -0.8419607877731323, -0.7800125479698181, -0.7234824299812317, -0.6778091192245483, -0.6364102363586426, -0.5994598865509033, -0.5667418241500854, -0.5229461193084717, -0.4863848090171814, -0.45653235912323, -0.4322302043437958, -0.4168083369731903, -0.4035792052745819, -0.392134428024292, -0.3821302354335785, -0.37345024943351746, -0.36557820439338684, -0.35828542709350586, -0.3513605296611786, -0.34391364455223083, -0.33651772141456604, -0.32895129919052124, -0.32101401686668396, -0.3116605877876282, -0.301501989364624, -0.29023951292037964, -0.2776251435279846, -0.2600938677787781, -0.2401028722524643, -0.21682044863700867, -0.18963798880577087, -0.15107357501983643, -0.10563379526138306, -0.051529258489608765, 0.012384027242660522, 0.10668647289276123, 0.215451180934906, 0.3232850432395935, 0.4301220178604126, 0.4983493685722351, 0.5540202856063843, 0.5963122248649597, 0.6204866766929626, 0.6229811310768127, 0.6015881896018982, 0.5569141507148743, 0.49231648445129395, 0.41706305742263794, 0.3352278470993042, 0.25055116415023804, 0.1644904613494873, 0.09455567598342896, 0.026767760515213013, -0.03639817237854004, -0.09512984752655029, -0.15134912729263306, -0.20160678029060364, -0.24616803228855133, -0.2853740453720093, -0.33108994364738464, -0.3687279522418976, -0.3998376429080963, -0.42573949694633484, -0.443439245223999, -0.45909014344215393, -0.47305479645729065, -0.4857693016529083, -0.4975969195365906, -0.5090816020965576, -0.5205076336860657, -0.5321699380874634, -0.5463882088661194, -0.5614458918571472, -0.5778347253799438, -0.5959218144416809, -0.6199592351913452, -0.6469546556472778, -0.6777893304824829, -0.7129525542259216, -0.7623758316040039, -0.8188258409500122, -0.8835437297821045, -0.9559391140937805, -1.0333904027938843, -1.1144416332244873, -1.1941149234771729, -1.264775037765503, -1.30422842502594, -1.3318564891815186, -1.3427854776382446, -1.3352720737457275, -1.3094596862792969, -1.266322135925293, -1.2085496187210083, -1.1389787197113037, -1.0631788969039917, -0.9855290651321411, -0.9096376895904541, -0.8380472660064697, -0.7836297750473022, -0.7331096529960632, -0.6877763271331787, -0.6476625800132751, -0.6125607490539551, -0.5821450352668762, -0.5553144216537476, -0.531122088432312, -0.5082622170448303, -0.4894930422306061, -0.47424250841140747, -0.4619821310043335, -0.4521714746952057, -0.444504052400589, -0.4386512339115143, -0.4343002438545227, -0.43125784397125244, -0.42921239137649536, -0.42801767587661743, -0.4275488257408142, -0.4277524948120117, -0.4285814166069031, -0.4300174117088318, -0.4320327341556549, -0.4350677728652954, -0.43882521986961365, -0.4434302747249603, -0.448971688747406, -0.4572315812110901, -0.4671497941017151, -0.4792550206184387, -0.4940168857574463, -0.5224900245666504, -0.5591763257980347, -0.6078022122383118, -0.6719692945480347, -0.7389838099479675, -0.8244616389274597, -0.9288371205329895, -1.0517914295196533, -1.1421936750411987, -1.239186406135559, -1.3333842754364014, -1.4158086776733398, -1.4630796909332275, -1.4946069717407227, -1.5100419521331787, -1.5083301067352295, -1.4766521453857422, -1.4157958030700684, -1.32904851436615, -1.2304282188415527, -1.1604863405227661, -1.0846108198165894, -1.008939266204834, -0.934943675994873, -0.862938404083252, -0.794957160949707, -0.7315114736557007, -0.6728445291519165, -0.5991829633712769, -0.5355806350708008, -0.48026415705680847, -0.4310750961303711, -0.38154134154319763, -0.3397928774356842, -0.304383248090744, -0.27400314807891846, -0.24650052189826965, -0.22199486196041107, -0.1995120346546173, -0.17807956039905548, -0.1581002175807953, -0.13771581649780273, -0.11651259660720825, -0.09403499960899353, -0.06549233198165894, -0.03464224934577942, -0.0007828772068023682, 0.03634923696517944, 0.08668076992034912, 0.14056414365768433, 0.19764339923858643, 0.253059983253479, 0.29751741886138916, 0.34116894006729126, 0.3795701265335083, 0.40774035453796387, 0.42262423038482666, 0.42162156105041504, 0.40397173166275024, 0.37083762884140015, 0.3281949758529663, 0.27793991565704346, 0.2224217653274536, 0.1654396653175354, 0.11179196834564209, 0.06106358766555786, 0.01501232385635376, -0.02566954493522644, -0.061901092529296875, -0.09264549612998962, -0.11771580576896667, -0.13720452785491943, -0.15384075045585632, -0.1637706756591797, -0.16749021410942078, -0.16552865505218506, -0.1569821536540985, -0.14222219586372375, -0.12097617983818054, -0.09320032596588135, -0.06128266453742981, -0.02277013659477234, 0.02229619026184082, 0.07386422157287598, 0.15707725286483765, 0.244401216506958, 0.3226984739303589, 0.39346468448638916, 0.42848265171051025, 0.45028066635131836, 0.45814353227615356, 0.4506433606147766, 0.42767006158828735, 0.39047014713287354, 0.3417065143585205, 0.28435373306274414, 0.22014641761779785, 0.15538740158081055, 0.09294503927230835, 0.035251766443252563, -0.010048210620880127, -0.051353782415390015, -0.08889701962471008, -0.12111029028892517, -0.15026450157165527, -0.17358621954917908, -0.19128559529781342, -0.2041212022304535, -0.21287082135677338, -0.2173565924167633, -0.21789875626564026, -0.21481838822364807, -0.20832538604736328, -0.1984839290380478, -0.18527139723300934, -0.16862547397613525, -0.14379268884658813, -0.11402493715286255, -0.07834824919700623, -0.03630399703979492, 0.03539913892745972, 0.1187828779220581, 0.21403300762176514, 0.3048890233039856, 0.3555036187171936, 0.4077228903770447, 0.4524977207183838, 0.4857795238494873, 0.5040313601493835, 0.5042185187339783, 0.4850364923477173, 0.4475937485694885, 0.3979499936103821, 0.33907419443130493, 0.2744501233100891, 0.205732524394989, 0.14151698350906372, 0.07973206043243408, 0.022254496812820435, -0.03023400902748108, -0.07605975866317749, -0.11766114830970764, -0.15396982431411743, -0.18515561521053314, -0.2132183015346527, -0.2362401932477951, -0.25454533100128174, -0.2688061594963074, -0.2815297544002533, -0.28978705406188965, -0.29406434297561646, -0.29493382573127747, -0.29356908798217773, -0.289969265460968, -0.284379780292511, -0.2768417000770569, -0.2661198675632477, -0.2530234456062317, -0.23719164729118347, -0.21833883225917816, -0.18737712502479553, -0.1503426730632782, -0.1052732765674591, -0.05089566111564636, 0.020157068967819214, 0.10416156053543091, 0.20103693008422852, 0.29763317108154297, 0.358675479888916, 0.42464572191238403, 0.4855204224586487, 0.5363579988479614, 0.5722622871398926, 0.5881199240684509, 0.58072429895401, 0.5494454503059387, 0.5011022090911865, 0.4394427537918091, 0.36726903915405273, 0.29101109504699707, 0.21507078409194946, 0.13986974954605103, 0.0682523250579834, 0.0012324750423431396, -0.05278301239013672, -0.10405638813972473, -0.15120205283164978, -0.19340334832668304, -0.24043749272823334, -0.2800281345844269]}, {\"line\": {\"color\": \"red\", \"width\": 1}, \"type\": \"scatter\", \"x\": [1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365], \"y\": [-0.8348081707954407, -0.9094305038452148, -0.9966443777084351, -1.0912649631500244, -1.1776299476623535, -1.2580516338348389, -1.3281915187835693, -1.3830851316452026, -1.418796420097351, -1.4309666156768799, -1.4171909093856812, -1.3773565292358398, -1.3183283805847168, -1.2427427768707275, -1.1547729969024658, -1.0600546598434448, -0.9651462435722351, -0.8729629516601562, -0.7865219116210938, -0.7078765630722046, -0.638731837272644, -0.5785079002380371, -0.5270298719406128, -0.48385924100875854, -0.4480052888393402, -0.41843709349632263, -0.3933144807815552, -0.3719949424266815, -0.3532070219516754, -0.3365880250930786, -0.32177507877349854, -0.3084249198436737, -0.29645028710365295, -0.2854437232017517, -0.27504080533981323, -0.2649081349372864, -0.2545050382614136, -0.2435961365699768, -0.23189757764339447, -0.2191365361213684, -0.20436762273311615, -0.18746022880077362, -0.16804537177085876, -0.14573484659194946, -0.11848980188369751, -0.08621340990066528, -0.04825162887573242, -0.0039751529693603516, 0.051291048526763916, 0.11665672063827515, 0.19168293476104736, 0.2688204050064087, 0.33633196353912354, 0.3999010920524597, 0.4569241404533386, 0.5039141178131104, 0.5370007753372192, 0.5524638295173645, 0.547730565071106, 0.5219410061836243, 0.47779613733291626, 0.41986268758773804, 0.35221409797668457, 0.28280001878738403, 0.21185100078582764, 0.14202123880386353, 0.07626932859420776, 0.01601549983024597, -0.0375923216342926, -0.08637940883636475, -0.1306653916835785, -0.17039397358894348, -0.20553651452064514, -0.23622994124889374, -0.26281893253326416, -0.28556370735168457, -0.3049352467060089, -0.32130166888237, -0.3350420594215393, -0.3465076684951782, -0.35598045587539673, -0.3637625575065613, -0.3701179027557373, -0.37527087330818176, -0.37939733266830444, -0.3826694190502167, -0.3852251470088959, -0.38717472553253174, -0.38856956362724304, -0.38950106501579285, -0.3900270164012909, -0.39018622040748596, -0.38993674516677856, -0.389314740896225, -0.3883170783519745, -0.38692325353622437, -0.3849335312843323, -0.38232627511024475, -0.37899965047836304, -0.37481820583343506, -0.36905521154403687, -0.36150485277175903, -0.3516496419906616, -0.33874428272247314, -0.32199594378471375, -0.300371915102005, -0.27253228425979614, -0.2367967665195465, -0.191796213388443, -0.13493797183036804, -0.06364887952804565, 0.024418532848358154, 0.12018179893493652, 0.22678101062774658, 0.3294713497161865, 0.43153077363967896, 0.5289604663848877, 0.6144292950630188, 0.6808598637580872, 0.7325524687767029, 0.7613632082939148, 0.7609503865242004, 0.729647696018219, 0.6685484051704407, 0.5834482312202454, 0.4819902777671814, 0.37079644203186035, 0.2592669129371643, 0.15184056758880615, 0.04933091998100281, -0.04613277316093445, -0.1332116723060608, -0.21310122311115265, -0.2861940264701843, -0.35221776366233826, -0.41126748919487, -0.46361061930656433, -0.5090749263763428, -0.5485532879829407, -0.5830920934677124, -0.6140618324279785, -0.642338216304779, -0.668779730796814, -0.6941697597503662, -0.7189154028892517, -0.7436184883117676, -0.7687291502952576, -0.7946360111236572, -0.8224732875823975, -0.852263331413269, -0.8833866119384766, -0.9158618450164795, -0.9514944553375244, -0.9890993237495422, -1.0278414487838745, -1.0664104223251343, -1.1029052734375, -1.1348408460617065, -1.159595012664795, -1.174727201461792, -1.178260087966919, -1.1695001125335693, -1.1489555835723877, -1.1182115077972412, -1.080234169960022, -1.0373566150665283, -0.9926069378852844, -0.9487004280090332, -0.9080722332000732, -0.8717564940452576, -0.8406496644020081, -0.8151973485946655, -0.7954699993133545, -0.781508207321167, -0.7731189727783203, -0.7700304388999939, -0.7728638648986816, -0.7812243103981018, -0.7949712872505188, -0.8140449523925781, -0.8384288549423218, -0.8678907752037048, -0.9019739627838135, -0.9403037428855896, -0.985152542591095, -1.032870888710022, -1.0796844959259033, -1.1204982995986938, -1.1509575843811035, -1.1704704761505127, -1.1775344610214233, -1.171583890914917, -1.1534978151321411, -1.1247990131378174, -1.0874418020248413, -1.0438741445541382, -0.9965818524360657, -0.949112594127655, -0.9040799140930176, -0.8633503913879395, -0.8283935785293579, -0.7992372512817383, -0.7759964466094971, -0.758540153503418, -0.7469943761825562, -0.7411496639251709, -0.7407121062278748, -0.7454037666320801, -0.7573391199111938, -0.775774359703064, -0.8012279272079468, -0.8342939615249634, -0.8717495203018188, -0.9138602018356323, -0.9598525166511536, -1.0085071325302124, -1.0566737651824951, -1.102933645248413, -1.1444709300994873, -1.1781035661697388, -1.2006957530975342, -1.2096333503723145, -1.2034685611724854, -1.1821885108947754, -1.1501737833023071, -1.1088874340057373, -1.060884714126587, -1.0095058679580688, -0.9577154517173767, -0.9080362319946289, -0.8624292612075806, -0.8221728801727295, -0.7879517078399658, -0.7598501443862915, -0.7377867698669434, -0.7214838266372681, -0.711073637008667, -0.7061973810195923, -0.7063987255096436, -0.7105705738067627, -0.7202411890029907, -0.735328197479248, -0.7562488317489624, -0.7835397720336914, -0.8160054683685303, -0.854218602180481, -0.8981416821479797, -0.9463962316513062, -0.9992010593414307, -1.0541698932647705, -1.1081615686416626, -1.1568195819854736, -1.1953942775726318, -1.222388505935669, -1.235498070716858, -1.233332633972168, -1.2160292863845825, -1.1848808526992798, -1.1423370838165283, -1.0907070636749268, -1.032987117767334, -0.973595142364502, -0.9159419536590576, -0.8625601530075073, -0.8157781362533569, -0.7754538059234619, -0.7417793273925781, -0.7146289944648743, -0.6936290264129639, -0.6785160899162292, -0.6688082218170166, -0.6632707715034485, -0.661219596862793, -0.6625956296920776, -0.6674048900604248, -0.6757079362869263, -0.6876579523086548, -0.7034239768981934, -0.7232405543327332, -0.7474043965339661, -0.7788386344909668, -0.8169170618057251, -0.8620854616165161, -0.9144861698150635, -0.9731748104095459, -1.0359649658203125, -1.0995972156524658, -1.1589820384979248, -1.2074072360992432, -1.2436425685882568, -1.2646822929382324, -1.2684439420700073, -1.25456702709198, -1.2240228652954102, -1.1792950630187988, -1.1230835914611816, -1.0585896968841553, -0.9909678101539612, -0.9243505001068115, -0.8618886470794678]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"rangeslider\": {\"visible\": true}}, \"yaxis\": {\"autorange\": true, \"fixedrange\": false}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('6ce38ca1-94e6-4b8f-b1cd-1626a298b2d6');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzsunU3GQmmt"
      },
      "source": [
        "**Prédictions multi-step ahead (Prédiction de Xt+1 avec Xt, Yt, Zt)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTZ9r4zuQls0"
      },
      "source": [
        "predictions_2 = []\n",
        "\n",
        "data_to_predict = x_val[0,:,:]                          # [[X0,Y0,Z0],[X1,Y1,Z1], ... , [X15,Y15,Z15]] : (16,3)\n",
        "\n",
        "data_to_predict = tf.expand_dims(data_to_predict,0)     # (1,16,3)\n",
        "prediction = model.predict(data_to_predict)             # [[[X^1][X^2][X^3]...[X1^6]]] : (1,16,1)\n",
        "predictions_2.append(prediction[0,taille_fenetre-1,0])  # [X^16] : (1,)\n",
        "\n",
        "data_to_predict = x_val[1,0:taille_fenetre-1,:]         # [[X1,Y1,Z1],[X2,Y2,Z2], ... , [X15,Y15,Z15]] : (15,3)\n",
        "\n",
        "data_to_predict = np.insert(data_to_predict,taille_fenetre-1,     # [[X1,Y1,Z1],[X2,Y2,Z2], ... , [X15,Y15,Z15], [X^16,Y15,Z15]] : (16,3)\n",
        "                            [predictions_2[0],\n",
        "                             x_val[1,taille_fenetre-2,1],\n",
        "                             x_val[1,taille_fenetre-2,2]],axis=0)    \n",
        "\n",
        "\n",
        "for i in range(1,300):\n",
        "  data_to_predict = tf.expand_dims(data_to_predict,0)   # (16,3) => (1,16,3)\n",
        "  prediction = model.predict(data_to_predict)           # [[[X^2][X^3][X^4]...[X^17]]] : (1,16,1)\n",
        "\n",
        "  predictions_2.append(prediction[0,taille_fenetre-1,0])  # [X^17] : (1,)\n",
        "  data_to_predict = x_val[i+1,0:taille_fenetre-1,:]     # [[X2,Y2,Z2],[X3,Y3,Z3], ... , [X16,Y16,Z16]] : (15,3)\n",
        "\n",
        "  data_to_predict = np.insert(data_to_predict,taille_fenetre-1,       # [[X2,Y2,Z2],[X3,Y3,Z3], ... , [X16,Y16,Z16], [X^17,Y16,Z16]] : (16,3)\n",
        "                            [predictions_2[i],\n",
        "                             x_val[i+1,taille_fenetre-2,1],\n",
        "                             x_val[i+1,taille_fenetre-2,2]],axis=0)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "6LjnJlWJV9O_",
        "outputId": "3fff1344-d471-461c-fb39-8ebb95dd868a"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "n_max = len(predictions_2)\n",
        "\n",
        "# Courbes originales\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index,y=serie_entrainement[0],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index[temps_separation:],y=serie_test[0],line=dict(color='blue', width=1)))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index[taille_fenetre+horizon-1:],y=pred_ent[:,taille_fenetre-1,0],line=dict(color='green', width=1)))\n",
        "\n",
        "# Courbes des prédictions de validations\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index[temps_separation+taille_fenetre+horizon-1:temps_separation+taille_fenetre+horizon-1+n_max],y=predictions_2[0:n_max],line=dict(color='red', width=1)))\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"7cce2839-df9c-48ad-b313-b03c3bea48c9\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"7cce2839-df9c-48ad-b313-b03c3bea48c9\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '7cce2839-df9c-48ad-b313-b03c3bea48c9',\n",
              "                        [{\"line\": {\"color\": \"blue\", \"width\": 1}, \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500], \"y\": [-0.3997737259124131, -0.39663821834604174, -0.39365137681048884, -0.3907864798683186, -0.38801757504432344, -0.3821324507898519, -0.3763934293587709, -0.3706210862311272, -0.36463022967025455, -0.3577473769244706, -0.35024615038670454, -0.34194712553700274, -0.3326497313941316, -0.3199234065145797, -0.3051080675812845, -0.28778078536864177, -0.2674609584839191, -0.23571563448930655, -0.1966074971580348, -0.14853454184766277, -0.0896512592100983, -0.006225265450336413, 0.09729627454420649, 0.2225345894716806, 0.3675224176311685, 0.45838812095042686, 0.5508558289121261, 0.6407603294459197, 0.722847047324601, 0.7907720441641028, 0.8375505797233825, 0.8571591165468613, 0.8462014047925644, 0.8092912178307217, 0.7480946404894724, 0.6657516018680006, 0.5671962762650251, 0.47056335623297907, 0.36912442227360964, 0.26621164404319425, 0.1646829978238482, 0.06960722630425478, -0.020784283640840975, -0.10554314526311386, -0.18409904490985485, -0.30078906306526343, -0.4001453589493666, -0.4835585687120818, -0.5528548815255133, -0.6036192046351864, -0.6466106012197352, -0.6830658188631247, -0.7141831570379003, -0.7449865023027574, -0.7713362746616284, -0.7942833891599544, -0.8148146806574793, -0.8352498518764578, -0.8543521552329252, -0.8725509279710562, -0.8902306512050359, -0.9106978625168631, -0.9306203922502604, -0.949722695606728, -0.9676907796763472, -0.9869853235899075, -1.0028259454943649, -1.0141168742142899, -1.0201340036512985, -1.0203134281712518, -1.0140335699728829, -1.0017814384674935, -0.9847553331276295, -0.9601933979497229, -0.9339974180365264, -0.9090189616515849, -0.887699483869979, -0.8739030198892763, -0.8646498410745367, -0.8603308365585154, -0.8610421266197593, -0.8688406852191626, -0.8840661373409225, -0.9057765042552841, -0.9318699558713645, -0.9582069121930961, -0.983762090249316, -1.0056582897022008, -1.021018310213926, -1.0272469042637369, -1.0256384916027261, -1.0160841359152073, -0.9996539763023317, -0.9777385527937374, -0.9527985445202147, -0.9272818145754133, -0.9034247614401807, -0.884463434492248, -0.8696288715032435, -0.8596195464972716, -0.8548391646442274, -0.8556786150768666, -0.8629709402092584, -0.8763636990200658, -0.8949661769280917, -0.9198164729416379, -0.9476721296644034, -0.9760916920213083, -1.0017750304489237, -1.020063515447031, -1.0308994748485025, -1.0325399276023621, -1.0247926331515171, -1.010791112576582, -0.9912594719759403, -0.9679663244748469, -0.9429366039413475, -0.9169777207152321, -0.8934282524713483, -0.8739158359264159, -0.8595682823487135, -0.851346794523706, -0.8484119220187538, -0.8508149289824154, -0.858331534764749, -0.8755819207545547, -0.9002720163038566, -0.9307036964916674, -0.9635063435502911, -0.9922655308913934, -1.0165134731593815, -1.0328539205122806, -1.0388838659864288, -1.0348916704174658, -1.022620314856367, -1.0031655704785625, -0.9785523711520977, -0.9484346838742052, -0.9182529164106153, -0.890852229006303, -0.8684690201421161, -0.85476867643996, -0.8459384268508247, -0.8422089600432219, -0.8435930920542909, -0.8539164099701814, -0.8738389397035787, -0.9022264619676347, -0.9360672080345601, -0.9659990627739292, -0.9948159222821595, -1.0194996098128917, -1.0370447646569065, -1.0446126345877982, -1.04320287050245, -1.0326937200480366, -1.014161730344278, -0.9895997951663714, -0.961334025255141, -0.9319853002056202, -0.9039117708514828, -0.8806891115546568, -0.8615483600867706, -0.8472969267876147, -0.8384282290870608, -0.8349999391522369, -0.8381911323999797, -0.8478159762917657, -0.863329789249165, -0.8875328753871651, -0.9173301617365692, -0.9507351625407506, -0.9845438685148273, -1.0143731949570804, -1.0373779816225344, -1.050059450372098, -1.0505080116719814, -1.0413445451172185, -1.0241774633688199, -1.000487018716401, -0.9724839775665309, -0.9395723941922213, -0.9075323013433996, -0.87897176257796, -0.855832407522541, -0.8413502855548735, -0.831956130331599, -0.8278165503355313, -0.8289251375481004, -0.8369672008531547, -0.8520901246777984, -0.8737107793321833, -0.9006372733623331, -0.9313316823115042, -0.9639933529615932, -0.9961167500518218, -1.0245683525015754, -1.0454456770018676, -1.0573005113559315, -1.0580694735843033, -1.0477525636869827, -1.0304701376043282, -1.0068181410633281, -0.9788086918948882, -0.9486397404684378, -0.9174583221079644, -0.8885325262840482, -0.863522029806258, -0.8436251321471399, -0.8299632365564024, -0.8218635010842202, -0.8194092299720005, -0.8225171189783361, -0.8329301491542032, -0.8503407356082527, -0.8741401165763576, -0.9031107685302622, -0.9356955429575138, -0.970004074380032, -1.0034282992399228, -1.032738576178025, -1.0539683416996541, -1.06550277512523, -1.0655668553109277, -1.0539683416996541, -1.0353722718101979, -1.010246430998152, -0.9806990573729688, -0.9489985895083445, -0.9164778952667905, -0.8862448636546425, -0.8599399474257599, -0.83872299794127, -0.8236641543023239, -0.8142379589862007, -0.8105341242528769, -0.8124821618980852, -0.821568732230011, -0.8377297550629565, -0.8604846290041898, -0.8888016630639783, -0.9213543973983811, -0.9565984995320851, -0.9922334907985446, -1.0252219703956915, -1.0524304172429106, -1.0701165484954602, -1.0754992840940623, -1.0679378221817404, -1.0519177757573295, -1.0283106353463176, -0.9989939503896459, -0.9662938316281384, -0.9309600172344581, -0.8971192711675324, -0.8667708952211286, -0.8413887336662922, -0.8229977203710683, -0.8100214827672957, -0.8026202213192178, -0.8007747119711257, -0.8076376998593433, -0.8259902650431483, -0.8552108297212737, -0.893133483617139, -0.928755658846459, -0.966825697169429, -1.0047227189910153, -1.0389992103206847, -1.0654386949395322, -1.0814587413639432, -1.0844705100917325, -1.0740895200087142, -1.0555703463420951, -1.0295089348188635, -0.997987891474193, -0.9633717751603259, -0.9268973334612274, -0.8921402407388256, -0.8609844544526314, -0.8347756585022953, -0.8151478976231071, -0.8010566647881954, -0.7926429364060947, -0.7898810804025262, -0.795571400892477, -0.8130076194208058, -0.8416963185576408, -0.879676644620634, -0.9158370934098141, -0.9552912637438532, -0.9956553727147988, -1.0336100667035129, -1.0657590958680203, -1.0874822788195215, -1.0953000614746342, -1.088123080676498, -1.0714622323951108, -1.0462146392302392, -1.0143475628828011, -0.9783857626692838, -0.9392904413751516, -0.9012652591821702, -0.8664889424040589, -0.8365378636089804, -0.8139047420205727, -0.7966671720679068, -0.7849981702523658, -0.7789169606296595, -0.7793719299481129, -0.7902911935909912, -0.8113351265740973, -0.8413951416848618, -0.8744669255234155, -0.9128509567563039, -0.9549067826296673, -0.9979942994927627, -1.03314228134792, -1.0640930110398819, -1.0880590004908004, -1.1027333630155607, -1.1063218534146289, -1.0979273490882375, -1.0778702509648752, -1.0480729646154712, -1.0085162659843159, -0.9646661949114185, -0.92029707433437, -0.8785103852409369, -0.8467009810606267, -0.8195437983619654, -0.7976219668348016, -0.7812751114633327, -0.7693305648492921, -0.7647360155347711, -0.7672992229626767, -0.7766677461116722, -0.796442891417965, -0.8253046070561836, -0.8626377232436305, -0.9068338273192952, -0.9525037756660056, -0.9997693206365874, -1.0449330355162862, -1.0831889063777795, -1.1075393769428838, -1.118881569811367, -1.115100838855206, -1.096389424631494, -1.0703087890525531, -1.0365897953384533, -0.9978405070470883, -0.9566625797177828, -0.9157537891684072, -0.8771134371927282, -0.8423499364517567, -0.8125590581209223, -0.7883175238715038, -0.7699329185948499, -0.7574565064395188, -0.7507985751455336, -0.7509203274983591, -0.7629033222238184, -0.7864912385791211, -0.8204473289803021, -0.8547622684213902, -0.894767528352429, -0.9392455852451631, -0.9860882009901405, -1.0263433736454002, -1.0638366902970913, -1.09568454258882, -1.119201970739855, -1.1318898475079886, -1.1318898475079886, -1.1184330085114833, -1.0927368540467284, -1.0581335537700012, -1.0168466901250093, -0.9719713360809497, -0.9264487721613438, -0.8854630853891312, -0.8478992805331725, -0.8149108009360259, -0.7872345687332136, -0.7641785179192015, -0.7474471814335469, -0.7369508470162729, -0.7324844580731472, -0.7343043353469603, -0.7432691533260605, -0.7591610393790761, -0.7816339605032396, -0.8103290676586444, -0.8451053844367554, -0.8854246372777126, -0.930197463024656, -0.974598623694553, -1.0195700980171591, -1.0624910063974407, -1.1002342357733526, -1.1294548004514782, -1.1465001298470512, -1.1489351769035616, -1.136119139764033, -1.112922112541486, -1.080048977278595, -1.0398642928276027, -0.9951683633034966, -0.9483129315213797, -0.902380254413309, -0.8594593460330275, -0.8210624987629995, -0.789182606378422, -0.7627879778895628, -0.7420259977235263, -0.7268774418246033, -0.7151635838790742, -0.7121710392069943, -0.7175281427313173, -0.7306838048550434, -0.7507729430712546, -0.7780454701041716, -0.8123924496381085, -0.8533332802803328, -0.9045461646898894, -0.9611994568651758, -1.0203326522269613, -1.0770372085508058, -1.1131784332842767, -1.1421426772196115, -1.1611744923718115, -1.1685437137270405, -1.1629687375713456, -1.1444495639047265, -1.1140755558840436, -1.0740895200087142, -1.0237545341432155, -0.9697477536372415, -0.9158114613355349, -0.8648677137059085, -0.8250867344248116, -0.7899900167182123, -0.7600709780159826, -0.7355538989680642, -0.7137345957380168, -0.6989256648232913, -0.6907938892582605, -0.6889227478358892, -0.6935493372432591, -0.7049940584088581, -0.7231543830355702, -0.747799622454884, -0.7787759842211248, -0.8162244447428275, -0.8599079073329109, -0.9091278979672708, -0.9600716455968974, -1.0131620794473948, -1.0657590958680203, -1.1140755558840436, -1.1541897521307685, -1.181039349938081, -1.1907795381641229, -1.1818723923521501, -1.160021049029254, -1.1263148713522935, -1.0831889063777795, -1.0335844346292338, -0.9805132248344456, -0.9271728782597273, -0.8760497061101473, -0.8289956257523678, -0.7895799035297474, -0.7553674923857756, -0.7265954890075338, -0.7032959334878707, -0.682271224560474, -0.6684042723755038, -0.6612272915773677, -0.6602276406804846, -0.6664177866188769, -0.6805474675652072, -0.7026102755009058, -0.732381929776031, -0.7656908103016661, -0.8055422777870305, -0.8518530279907173, -0.9040976033900058, -0.9586875135858282, -1.0160585038409284, -1.0735127983374353, -1.1272119939520606, -1.1727089257973875, -1.2044926979034185, -1.2178854567142259, -1.2108366362874852, -1.1893056938930768, -1.1546383134306517, -1.1092695419567205, -1.0563393085704669, -0.9995963041352037, -0.941968993137313, -0.8861551513946657, -0.8341476726824584, -0.790541106315212, -0.7519904665995099, -0.718784114370991, -0.6909733137782138, -0.665219487146331, -0.6460659196413053, -0.6330448259075443, -0.6256051163480478, -0.6233174537186419, -0.6287706775215114, -0.6417276910695748, -0.6617463410815186, -0.6835592362929965, -0.7105369944717043, -0.7429295283418631, -0.7809226704419957, -0.8430420024572912, -0.916080598115465, -0.9979302193070649, -1.0829966658206867, -1.136311380321126, -1.1836666375516842, -1.2213457867418984, -1.2456321771213055, -1.2535140399621156, -1.243197130064795, -1.2145532870579485, -1.1700816381837837, -1.1134988342127647, -1.048970087215238, -0.9810835384871547, -0.9135814708732571, -0.8592735134945044, -0.8088744474433078, -0.7632429472080161, -0.7228980622927799, -0.6861352597580417, -0.655293466381766, -0.6300907293468828, -0.6101297515020669, -0.5933920069978426, -0.5820498141293596, -0.5755905314110371, -0.573475885283015, -0.5752573144454094, -0.5807938424896858, -0.5899829411187278, -0.6027477141096984, -0.6206004538450619, -0.6429708466721092, -0.6702177416307471, -0.7027256198351616, -0.7572450418267165, -0.824016595323661, -0.9032389289016575, -0.9931818775468697, -1.0533275398426776, -1.1139473955126484, -1.172068123940411, -1.2241012347268974, -1.2661378365445513, -1.2935641560231428, -1.302855782949301, -1.292410712680585, -1.2658815158017607, -1.2244216356553854, -1.1706583598550626, -1.1079238580570698, -1.0422416677169855, -0.9749895128273086, -0.9088587611873407, -0.8459768749622433, -0.7923609835890252, -0.7434485778460139, -0.6996113228102561, -0.660977378853147, -0.6262395101864544, -0.5966729125055619, -0.5719123287519925, -0.5515156056444325, -0.5343292998403245, -0.5209109089552382, -0.5108375037635685, -0.5036541149468627, -0.4989826694095046, -0.4965732544272732, -0.49620799736879656, -0.49770106569555167, -0.5011934358160732, -0.5066082115075241, -0.5139582088070438, -0.5232626517703416, -0.5368604671753815, -0.5536110277167455, -0.5739052225271892, -0.5982236529994448, -0.6466298252754447, -0.7095886077233793, -0.7897977761611193, -0.8896347054780477, -0.9659734306996502, -1.0494186485151213, -1.1368881019924046, -1.2228196310129444, -1.2819656424118693, -1.3317559466989382, -1.3678330912467114, -1.3866726658418185, -1.385519222499261, -1.3631552376907834, -1.3204137538304555, -1.2601783792746706, -1.1875755288792407, -1.1067063345288146, -1.0221525295007743, -0.9378422291783849, -0.8714743808513357, -0.8086437587747963, -0.7502090374371153, -0.6966828583238738, -0.6409907689340519, -0.5920975872467501, -0.54968291233348, -0.5132533267643697, -0.4740106210431329, -0.44230246355622505, -0.41682225931727107, -0.39617799445236124, -0.38159417722999167, -0.36901844078682916, -0.35795307432056006, -0.34788671794931736, -0.3396184515887504, -0.33156036823727175, -0.32351189691364773, -0.31525836899579124, -0.30513369965556364, -0.29420161997554567, -0.28220580921294686, -0.26883868247641846, -0.2509731267039155, -0.23032008285356498, -0.20635409340264635, -0.17844076451275295, -0.13944797151573693, -0.09287449255068975, -0.03763096446075142, 0.027282263650961296, 0.09890468720521729, 0.1796457211842479, 0.2678136486856354, 0.35932015386187005, 0.42718107051567444, 0.4893388506423885, 0.5413078812431772, 0.5782821483907176, 0.5960964400146623, 0.591482666644432, 0.5634796254945619, 0.5138815617645858, 0.4506984986667096, 0.3757246814004669, 0.29344572296469273, 0.20825111607967595, 0.13704521373245462, 0.06831921457173214, 0.003405986460019419, -0.05680375602148631, -0.11377744912526103, -0.164951885423399, -0.2103911451015979, -0.250332324846939, -0.2968160915520095, -0.3349822501535259, -0.3660880538948759, -0.3914593218181439, -0.40710770316550837, -0.42076895795438896, -0.43281603286554593, -0.4436141849574558, -0.4534972719976033, -0.4627228963324931, -0.4715307178566341, -0.48016872688867646, -0.4901011556718112, -0.5003603934020039, -0.5112091688406148, -0.5229102107490046, -0.5380075024993693, -0.5550207918020936, -0.5743601918456425, -0.5964998960041782, -0.626989248359117, -0.6629190084797855, -0.7051478508545326, -0.7545985301574039, -0.8264003782316133, -0.9112874002252814, -1.0083304334457925, -1.1132425134699744, -1.1796936660384305, -1.242812648950609, -1.298690570878954, -1.3429058990103282, -1.371293421274384, -1.3798801661578681, -1.3666796479041536, -1.331948187256031, -1.282734604640241, -1.2200641830279457, -1.1476535731896087, -1.0694757466384837, -0.9975329221557396, -0.9264615881984835, -0.8581777423190746, -0.7940398844543034, -0.7406931298610153, -0.6917422760065856, -0.6473218912809792, -0.6074191596470566, -0.5563664757017442, -0.5139325767327647, -0.4790793637318165, -0.4506745398176221, -0.43311592813461086, -0.4180391420436694, -0.405061750996554, -0.3938100393502762, -0.3839388712246684, -0.3751810322453715, -0.3672863533674219, -0.3600100482814545, -0.3525075401399744, -0.34526135274128483, -0.33806514788743947, -0.3307081017674931, -0.32230078140396223, -0.3132654752205945, -0.30338431058601795, -0.2924201908131512, -0.27758562782414675, -0.26045699418716667, -0.24057291256518792, -0.21738229336121082, -0.1851115118438776, -0.14641989571964054, -0.10019244975736065, -0.04522446646592216, 0.03494625386039945, 0.1310985724997133, 0.2419893338494852, 0.3613707198041947, 0.43083364110044015, 0.4954905484693622, 0.5507917487264287, 0.5920593883157109, 0.6150641749811647, 0.6160894579523272, 0.5934691524010589, 0.5486130224127086, 0.4892106902709932, 0.41673600024695867, 0.3354182445966491, 0.24955079576180703, 0.17526263648252907, 0.1026597860870992, 0.033369881292237454, -0.03150489870805665, -0.09292575669924787, -0.14860503005193018, -0.19847863858040599, -0.2427003747303497, -0.29475911759111517, -0.3380068349184547, -0.37367642948519086, -0.4031106375024322, -0.4226272833396206, -0.43966364150919607, -0.45470518349800393, -0.46823058829320546, -0.48068777639282734, -0.4924080423569263, -0.5037053790954209, -0.5149065955553689, -0.5280174015491067, -0.5417305612884025, -0.556398515794593, -0.572386522126155, -0.5930844221064939, -0.6165377700718313, -0.6432527994891787, -0.6738190480669546, -0.7162209069430853, -0.765799746617352, -0.8231899609281613, -0.8886991347668621, -0.9618274426850129, -1.0410882243744277, -1.1232390224388067, -1.2023780517753961, -1.2544111625618826, -1.2968963256794201, -1.3260528101718478, -1.338932927497074, -1.33361427208417, -1.309456042076158, -1.2674194402585042, -1.2105803155446946, -1.1425271583337973, -1.0677455816246475, -0.9904584696547198, -0.9141902326373847, -0.8528719029433097, -0.7953983843910934, -0.7425194151533981, -0.6946771485115376, -0.6523201457653953, -0.6149485814665298, -0.5823253589278595, -0.5541428932580359, -0.5272868874321536, -0.5049805747908039, -0.4866600496998477, -0.4717678145437154, -0.4597547021309782, -0.45021700729174097, -0.4427805017415295, -0.4370773652144392, -0.43290318191809474, -0.42987731554945197, -0.4278350800312681, -0.4266169157011559, -0.42608440935800845, -0.42625101784082237, -0.42705778737875566, -0.4284496090121085, -0.43069433791709694, -0.43367150334460947, -0.4374221166134925, -0.44200128668344607, -0.44872393896498586, -0.4570030989571214, -0.46713481711777577, -0.479502292957421, -0.502475039530026, -0.5329451678292555, -0.573283644725922, -0.6269956563776867, -0.685013856508333, -0.7578538035908441, -0.8478736484588936, -0.9563678108635736, -1.0444203940307053, -1.1395153896060082, -1.2377503142804953, -1.3325889891130074, -1.3966691748106508, -1.4490226865256257, -1.485035750887701, -1.5009276369407167, -1.494071057071069, -1.463504808493293, -1.4104464147356441, -1.338420286011493, -1.2737633786425708, -1.203147014003768, -1.128749918408804, -1.0528148983570966, -0.9770721188624822, -0.9031620326788202, -0.8322637152229476, -0.7652358409832127, -0.6817521750563229, -0.6069641903286034, -0.5409103349114726, -0.48324457580216335, -0.4269866583726313, -0.3794346749719811, -0.33930702188625983, -0.30526186002695893, -0.27491989209912476, -0.2485188555916957, -0.22500783545923034, -0.20329746854486877, -0.18354795531285512, -0.16402272273078317, -0.14419631327593233, -0.12354967744415163, -0.09800090740650122, -0.07025418699942167, -0.039892995015878195, -0.006532850341685067, 0.03804132682959564, 0.08728054151966479, 0.14059525602010411, 0.19672949869123965, 0.24807695149076123, 0.29767501522073725, 0.3425311452090877, 0.3793772519852326, 0.4049452460785922, 0.4159670380185869, 0.4105843024199848, 0.3888611194684838, 0.35566758327710457, 0.31202897681700925, 0.26089298863029, 0.20534828366757263, 0.15048923669182018, 0.09691820144859035, 0.0465255434159636, 0.0006313144193114399, -0.03970075445878526, -0.07445784718118706, -0.10356947554362643, -0.1271445758617894, -0.14866270221905806, -0.163157640223865, -0.1711676634360704, -0.17331434965694148, -0.16949517058936192, -0.1593128290820064, -0.14281858928343297, -0.12013420354646723, -0.09316285338632915, -0.06025767803058926, -0.021149540699317543, 0.024276902941741826, 0.09673877692863699, 0.17954960090570143, 0.2681340496141236, 0.3522713334351293, 0.39360305321010924, 0.42564314605893094, 0.4455080036252004, 0.4511470599665932, 0.44127871136915603, 0.4160311182042845, 0.3768781247430244, 0.3265110987846768, 0.2669165260858685, 0.2030670290567366, 0.13864721837489574, 0.07653429437817, 0.025776379287066713, -0.02057281902803873, -0.06194298691443728, -0.09806498759219884, -0.1315853327306361, -0.1591077724877739, -0.18100397194065868, -0.19773530842631334, -0.21015404841451663, -0.21804872729246627, -0.22181664221148772, -0.22184227428576678, -0.21843961642522192, -0.21171119692696935, -0.20170827993956722, -0.1884500895187248, -0.16848270365533916, -0.14353628736324658, -0.1131815033982729, -0.07699542253481374, -0.01661907157049413, 0.05698983774038873, 0.14342760022793993, 0.23930437406875393, 0.29825173689201595, 0.3554753427200115, 0.4078929346206839, 0.4517237816378719, 0.48318715281541486, 0.4986304775685468, 0.49542646828366466, 0.47338288440367526, 0.4371775794845068, 0.3879639968687167, 0.32888206565548944, 0.26332803568680035, 0.19864549624359917, 0.13413597330178156, 0.07199100921220708, 0.013812608617316693, -0.037079874863751666, -0.0829100236747062, -0.12348559725845396, -0.15881300363356476, -0.19114145731802584, -0.21810639945959417, -0.24017561541386254, -0.25786815468498187, -0.27445851476210176, -0.28599294818767756, -0.29313788889296477, -0.29659181090206777, -0.29705959625766054, -0.2954191435038009, -0.29179220499331426, -0.2862749010047472, -0.2780598211983093, -0.2674994065953376, -0.2544462727687277, -0.23868895510567723, -0.21339009779224763, -0.18207411104180932, -0.1437349359389093, -0.09717427301100162, -0.03645829706248455, 0.03672768302279394, 0.12283222854471733, 0.2204455754179375, 0.2887037892230673, 0.3579103897765221, 0.4250023442019545, 0.4856862800576228, 0.5354125041589941, 0.5687342007217686, 0.5813579973042043, 0.5711051675925813, 0.5423331642143396, 0.49632359088343175, 0.43576781539915876, 0.36451064890337925, 0.2910747560938799, 0.21509487991218423, 0.13962123719749991, 0.06690945048638397, 0.006469019336366747, -0.04974852757617577, -0.1013074449884996, -0.14799626828780257, -0.20083038139550952, -0.24609021655375501, -0.2843909435452364, -0.31650152459832553, -0.34036562655398495, -0.3606361116957204, -0.377832029527683, -0.39247691516702243, -0.40506059755321144, -0.4159603808196378, -0.425527552544296, -0.4341085302110674, -0.4419820626277368]}, {\"line\": {\"color\": \"blue\", \"width\": 1}, \"type\": \"scatter\", \"x\": [1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500], \"y\": [-0.4494108785556646, -0.45660964661693787, -0.46379752104664257, -0.471722958413727, -0.48006619859156024, -0.4890117925149512, -0.49879683687098136, -0.5118115225861728, -0.5266524935937469, -0.5436914149707502, -0.5633960720727756, -0.5906814151428321, -0.6231764773101072, -0.6618232373043559, -0.7077046502638684, -0.7730536236383252, -0.8517633157307406, -0.944307919915277, -1.0489060070295404, -1.1197786924111337, -1.191292179649704, -1.2599861387175775, -1.3215671971730127, -1.371293421274384, -1.4037820754230892, -1.4146757069916884, -1.4019237500378576, -1.3702681383032218, -1.3207341547589435, -1.2563976483185095, -1.181039349938081, -1.1045276082150945, -1.0255680033984587, -0.9470056957331479, -0.8711411638857078, -0.8089961997961334, -0.7508498392940918, -0.6971442356608969, -0.6481100775650601, -0.5848052621143583, -0.5309714981097682, -0.4857308870072319, -0.44805045621330364, -0.423295639676447, -0.4018375564531771, -0.3831820842315793, -0.36683779206753836, -0.35234221326087445, -0.3393224011308273, -0.3274271962597737, -0.3163156920598024, -0.30460824213284293, -0.29313788889296477, -0.28160345546738896, -0.26969094894619705, -0.25512552273712275, -0.2392977168698048, -0.22182945824862724, -0.20230422566655532, -0.1761979580133354, -0.14606104667973374, -0.11127191386448314, -0.07111286148777007, -0.014119944328286059, 0.0526003450201002, 0.1290480065573887, 0.2138389082725104, 0.27928400192551367, 0.34509435263699334, 0.4081492553634743, 0.4643475782203076, 0.5091396280229604, 0.5374630701013187, 0.5454090131278264, 0.5314395326457402, 0.500745123696569, 0.4542869890657776, 0.39507689748115526, 0.32663925915607195, 0.25672777655994306, 0.18527196148850097, 0.1149439576853375, 0.047813555148486245, -0.009128097862439665, -0.06154568976311191, -0.10905473943934467, -0.15152708651974273, -0.19250636527338563, -0.22783377164849644, -0.2579450509078191, -0.2833464365183649, -0.3039161761273084, -0.32107684985713725, -0.3352834270263048, -0.3469601184641294, -0.3565176781609329, -0.3642611278006361, -0.37046985699288076, -0.3754149249231679, -0.3794269853496974, -0.3825226991207505, -0.38485457707828774, -0.38656808124384273, -0.38786570500422, -0.38866798892915455, -0.38904734362848453, -0.3890761797120485, -0.38875577878356027, -0.38807652881516524, -0.3870480418347181, -0.38568121147378737, -0.38363833515374646, -0.38105205885898963, -0.37783651514068184, -0.37388469008870817, -0.36780988848457163, -0.3600600308262986, -0.3501743805787232, -0.3375172622997247, -0.32127549843279996, -0.3004814781739147, -0.2738497529979741, -0.2396245258168628, -0.19713295468075548, -0.1431582142676305, -0.07506020092674487, 0.00996138945688831, 0.08719082925968812, 0.17611490295230778, 0.27601591245493373, 0.3839269451697652, 0.46582142249135333, 0.5457934942420123, 0.619357547422907, 0.6811949266211327, 0.72585881605239, 0.7477742395609842, 0.7431604661907537, 0.7112485337133275, 0.6601766257123057, 0.5906496242303626, 0.5065764205950546, 0.41250670799091405, 0.3253576554421191, 0.23688214304938296, 0.1495792980549137, 0.06529462980680337, -0.018099323860109713, -0.09571965279566509, -0.16699604334715382, -0.2317042148646341, -0.312464472899374, -0.38118790885266857, -0.4391817585127498, -0.48794806143237035, -0.532060861266628, -0.5691632887855635, -0.6008317165573389, -0.6286873732801044, -0.6518908085212212, -0.6738190480669546, -0.6951385258485606, -0.7164900437230154, -0.741487724163666, -0.7677029281325719, -0.7955073207067794, -0.8252405268704859, -0.8634835816948395, -0.9044628604484823, -0.9475375612744382, -0.991624729034417, -1.0358144250915118, -1.0769090481794106, -1.1119609097560215, -1.1381697057063576, -1.1528440682311178, -1.15386935120228, -1.140412512205775, -1.1136269945841601, -1.0776139302220846, -1.0340906680962452, -0.9863573377700707, -0.9374000758970712, -0.8926785142986858, -0.8511032898180547, -0.8139175580577124, -0.7819415453945883, -0.7544575537488691, -0.733170116060112, -0.717970296012631, -0.7086530370121936, -0.7047569617217769, -0.7079225228952404, -0.7179062158269333, -0.7343555994955184, -0.7570079451396353, -0.785946557000691, -0.8210496827258601, -0.8619392492195262, -0.9178299871850107, -0.9791034607490973, -1.0419853469741949, -1.1002983159590503, -1.1336840927075225, -1.1586112849439056, -1.1728370861687827, -1.174759491739712, -1.1637376997997173, -1.1401561914629845, -1.105552891186257, -1.062426926211743, -1.0112717139693144, -0.9576814546703752, -0.9049050137297961, -0.8554479264083551, -0.815666947127258, -0.7807047978106239, -0.7509715916469173, -0.7266339371189524, -0.7047633697403467, -0.6899480308070515, -0.6818162552420206, -0.6799130737268005, -0.6861801158880302, -0.7024885231480804, -0.7288511115440908, -0.7647488315719105, -0.8008772402682419, -0.8433880354600585, -0.8918454718846164, -0.945205042515044, -0.9954631321577058, -1.0462787194159369, -1.094979660546146, -1.1382978660777527, -1.1725166852402944, -1.1937272267062145, -1.1989818019334213, -1.1872551279507524, -1.1629687375713456, -1.1272119939520606, -1.0823558639637103, -1.0313928922783744, -0.9776232084594818, -0.9238150765291706, -0.8723586874139632, -0.8249842061276954, -0.7846905853610172, -0.7496515398215459, -0.720091350159223, -0.6960100163740485, -0.6741458570140126, -0.6593177020435781, -0.6510449500700122, -0.6487829195148854, -0.6532749405322903, -0.6654950319448308, -0.6853983376225189, -0.7127669849339824, -0.7433076014374792, -0.7801344841579148, -0.8233181212995567, -0.8726534562681724, -0.9305947601759814, -0.9931946935840092, -1.0576209122844198, -1.1194582914826456, -1.1617512140430901, -1.1959059530199343, -1.218910739685388, -1.228458687354337, -1.223075951755735, -1.2025062121467915, -1.1677747514986687, -1.12150885742497, -1.0643493317826724, -1.002281263915935, -0.9392968493937214, -0.8785103852409369, -0.8298991563707045, -0.7856902362579005, -0.746492386666652, -0.7125875604140289, -0.6811049651807768, -0.6559086361644634, -0.6366140922509029, -0.6227663641216422, -0.6135836735111699, -0.6093351571994161, -0.6096747821836137, -0.6142565154609951, -0.6228368523259097, -0.635415792778357, -0.6520510089854653, -0.6728193971700714, -0.7043019924033236, -0.7428141840076072, -0.7887853092270966, -0.8423819765446056, -0.9060584570723538, -0.9767389018968543, -1.0517255352002366, -1.1259303902381077, -1.1754002935966883, -1.217308735042947, -1.2481313043635136, -1.2647921526449009, -1.2651766337590866, -1.248195384549211, -1.2144892068722508, -1.1666213081561112, -1.105552891186257, -1.0375894462353366, -0.9671845462093357, -0.8981061060272761, -0.8437853326113839, -0.7935977311729897, -0.7482609997919069, -0.7081788436380311, -0.6715698335489674, -0.6407280401726917, -0.6153330625807156, -0.5949747875845742, -0.577634689334792, -0.5653441097179841, -0.5575904072485692, -0.5538353083666873, -0.5536302517724548, -0.5567701808716393, -0.563114119255706, -0.5725531306089688, -0.5859715214940554, -0.6030617070196169, -0.6240864159470136, -0.649366049204734, -0.6907041769982837, -0.7416543326464801, -0.8032033510090665, -0.8758959136644731, -0.9470697759188456, -1.0247605930586683, -1.1058092119290475, -1.184435599780056, -1.2361483096380543, -1.278761633126987, -1.3086229996620888, -1.3227206405155703, -1.3190039897451071, -1.2966400049366296, -1.2567180492469976, -1.201865410289815, -1.1353501775356611, -1.0620424450975574, -0.9861394651386985, -0.9112745841881419, -0.8513083464122873, -0.7952381839268493, -0.7438202429230604, -0.6974902686636641, -0.6572927681755325, -0.6219397297261426, -0.5912196887026924, -0.5648250602138332, -0.5400132123117056, -0.5195972651484364, -0.5030645772384444, -0.4898896910590089, -0.4795791891802581, -0.47177422256228513, -0.4661351662208925, -0.462343541633163, -0.4601295712173094, -0.45933177290537375, -0.45981814151481887, -0.46146884709839014, -0.46434092102135854, -0.46837797272031007, -0.4736069158732377, -0.48004697453585093, -0.4897423066319044, -0.5016291810788173, -0.5160087747493683, -0.5332719767763135, -0.5664270648562743, -0.609610701997916, -0.6653925036477146, -0.7371430875733659, -0.8201077039961048, -0.9214761497512068, -1.040402566387463, -1.1700816381837837, -1.241402884865261, -1.308110358176508, -1.3657825253043867, -1.4096774525072724, -1.4353736069720273, -1.4388980171853978, -1.4183923577621518, -1.3749459918591498, -1.316312621945806, -1.243709771550376, -1.161430813114602, -1.0738331992659236, -0.9971484410415538, -0.921770918605416, -0.8494115729156371, -0.7812815194819025, -0.7152340720833416, -0.6551524899732312, -0.6011585255043969, -0.5531496503797225, -0.4972460963770984, -0.4504816584586722, -0.41160164658848414, -0.3792520464427428, -0.3563094175574156, -0.33629717556404154, -0.318609762707778, -0.3026345724133555, -0.28781923348006033, -0.27375363271942765, -0.26003406496156223, -0.24626323305513867, -0.22997404985079772, -0.21271725584242238, -0.1940763298229779, -0.17361552652972037, -0.14693253720522167, -0.11664824144451541, -0.08222436568774143, -0.04310982033789991, 0.011928651157805985, 0.07511812227425207, 0.146010031711555, 0.22286139841873862, 0.28325697343876755, 0.3427233857661805, 0.3980886662089443, 0.44557208381089813, 0.4812006670587878, 0.5008092038822667, 0.5013859255535454, 0.4823541104013453, 0.4487120129100826, 0.4014208358652218, 0.34355642818024984, 0.27845095951144433, 0.21314043424840612, 0.14748387598260077, 0.08384584356627113, 0.023956502013253615]}, {\"line\": {\"color\": \"green\", \"width\": 1}, \"type\": \"scatter\", \"x\": [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500], \"y\": [-0.26159730553627014, -0.23700262606143951, -0.19730144739151, -0.14975056052207947, -0.09150362014770508, -0.02056172490119934, 0.0809640884399414, 0.20338398218154907, 0.33312052488327026, 0.47255587577819824, 0.5598880648612976, 0.6313762068748474, 0.7009878754615784, 0.7645959258079529, 0.8173654675483704, 0.8538437485694885, 0.8528475165367126, 0.8043437600135803, 0.731461226940155, 0.6398293375968933, 0.5333289504051208, 0.41925686597824097, 0.32690906524658203, 0.22445201873779297, 0.12467342615127563, 0.029433518648147583, -0.057285696268081665, -0.138452410697937, -0.21607451140880585, -0.2870873808860779, -0.3911479115486145, -0.47727739810943604, -0.5491788983345032, -0.608893632888794, -0.6536507606506348, -0.6916457414627075, -0.7240598201751709, -0.7519688606262207, -0.7797514200210571, -0.8039509654045105, -0.8246824741363525, -0.8431302309036255, -0.8619871139526367, -0.8799393177032471, -0.8973202705383301, -0.9143481254577637, -0.9343883395195007, -0.953468918800354, -0.9714349508285522, -0.9878516793251038, -1.004849910736084, -1.01741623878479, -1.0248064994812012, -1.0265333652496338, -1.021993637084961, -1.011317491531372, -0.9953176975250244, -0.9756106734275818, -0.9488638639450073, -0.9229356050491333, -0.8998766541481018, -0.8817238807678223, -0.8715102672576904, -0.8659940958023071, -0.8655414581298828, -0.8700289726257324, -0.882718563079834, -0.9019088745117188, -0.9266561269760132, -0.9545084238052368, -0.9805876016616821, -1.0040889978408813, -1.0221033096313477, -1.0320730209350586, -1.0328936576843262, -1.02584707736969, -1.0114808082580566, -0.9910051226615906, -0.9663967490196228, -0.9405338168144226, -0.915743350982666, -0.8940340280532837, -0.8781411051750183, -0.8668551445007324, -0.8607722520828247, -0.8600226640701294, -0.8653295040130615, -0.8767582178115845, -0.8938270211219788, -0.9154320359230042, -0.9428552985191345, -0.9713596701622009, -0.9983133673667908, -1.0206258296966553, -1.0340237617492676, -1.0390443801879883, -1.034487247467041, -1.020865559577942, -1.0028843879699707, -0.9800926446914673, -0.9550802707672119, -0.9299353957176208, -0.9051679372787476, -0.8842206001281738, -0.8682667016983032, -0.858020544052124, -0.8538615703582764, -0.8550412654876709, -0.8614292144775391, -0.8726544976234436, -0.8951138257980347, -0.9233105182647705, -0.9557344913482666, -0.9882343411445618, -1.0142297744750977, -1.033940076828003, -1.043973684310913, -1.042709469795227, -1.0329344272613525, -1.0154728889465332, -0.9918980598449707, -0.9647482633590698, -0.9335448145866394, -0.9046598672866821, -0.8802316188812256, -0.8619434833526611, -0.8523315191268921, -0.8476753830909729, -0.8481486439704895, -0.8535910844802856, -0.8700752854347229, -0.8948230743408203, -0.9268589019775391, -0.962631106376648, -0.9913631677627563, -1.0176570415496826, -1.0377743244171143, -1.049048900604248, -1.0502681732177734, -1.0424376726150513, -1.0261774063110352, -1.0028570890426636, -0.9750525951385498, -0.9454513788223267, -0.9166147708892822, -0.8906572461128235, -0.8706377744674683, -0.8552711009979248, -0.8452515602111816, -0.8407852649688721, -0.8421597480773926, -0.8499670028686523, -0.8638980388641357, -0.8832588195800781, -0.9116846919059753, -0.944008469581604, -0.9778794050216675, -1.0100336074829102, -1.0358303785324097, -1.0527619123458862, -1.0578200817108154, -1.050038456916809, -1.0351619720458984, -1.0129761695861816, -0.9854853749275208, -0.955604076385498, -0.922292947769165, -0.8922012448310852, -0.8671482801437378, -0.8484906554222107, -0.8383959531784058, -0.833452045917511, -0.8337495923042297, -0.8391309976577759, -0.8523715734481812, -0.8719304800033569, -0.8973420858383179, -0.9271857142448425, -0.9593604207038879, -0.9916581511497498, -1.0214412212371826, -1.045491337776184, -1.060210108757019, -1.064765214920044, -1.0578800439834595, -1.0403409004211426, -1.0183653831481934, -0.9907641410827637, -0.960641622543335, -0.930234432220459, -0.9003033638000488, -0.8741649389266968, -0.8529976606369019, -0.8375629186630249, -0.8284593820571899, -0.8250020742416382, -0.8271217346191406, -0.8346081972122192, -0.8502160310745239, -0.8720253705978394, -0.8995495438575745, -0.9313330054283142, -0.9651194214820862, -0.998823881149292, -1.0295615196228027, -1.0540529489517212, -1.0686309337615967, -1.0723958015441895, -1.0643479824066162, -1.04519784450531, -1.0217456817626953, -0.9925543069839478, -0.9608671069145203, -0.9289777278900146, -0.8978110551834106, -0.8704650402069092, -0.8481041789054871, -0.831452488899231, -0.8210883140563965, -0.8164430856704712, -0.8174526691436768, -0.8239277005195618, -0.8383343815803528, -0.8591034412384033, -0.8858837485313416, -0.9174516201019287, -0.951879620552063, -0.9872888326644897, -1.021140456199646, -1.0501749515533447, -1.0711495876312256, -1.0808265209197998, -1.0773112773895264, -1.060957670211792, -1.0393105745315552, -1.0106871128082275, -0.9781702160835266, -0.9443657398223877, -0.9095712900161743, -0.8781945109367371, -0.8516314029693604, -0.8308522701263428, -0.8172074556350708, -0.8090812563896179, -0.8065125942230225, -0.809350848197937, -0.8236029148101807, -0.8481850028038025, -0.8828394412994385, -0.9250805377960205, -0.9614416360855103, -0.9992827773094177, -1.0346521139144897, -1.063920259475708, -1.0832573175430298, -1.090531349182129, -1.0841176509857178, -1.0646345615386963, -1.0405118465423584, -1.0094475746154785, -0.9749265909194946, -0.9394876956939697, -0.9038972854614258, -0.8718377351760864, -0.8446367979049683, -0.8231545686721802, -0.8084237575531006, -0.799340546131134, -0.7958971261978149, -0.7979485988616943, -0.8112281560897827, -0.8351407647132874, -0.8696045875549316, -0.9124090671539307, -0.9498704671859741, -0.989764928817749, -1.0283550024032593, -1.0620100498199463, -1.0873584747314453, -1.0999910831451416, -1.097476840019226, -1.0798988342285156, -1.0567736625671387, -1.025749921798706, -0.9899381995201111, -0.9523769021034241, -0.9135164022445679, -0.8778517246246338, -0.8469215631484985, -0.8217549324035645, -0.8041162490844727, -0.791982114315033, -0.7854107618331909, -0.7842882871627808, -0.7918725609779358, -0.8091207146644592, -0.8360339403152466, -0.8714328408241272, -0.907889723777771, -0.9485548734664917, -0.9911641478538513, -1.0326091051101685, -1.063704252243042, -1.0891344547271729, -1.10563063621521, -1.1113646030426025, -1.1054282188415527, -1.0879319906234741, -1.0601688623428345, -1.0240658521652222, -0.9801344871520996, -0.9352661967277527, -0.8925094604492188, -0.8543785810470581, -0.8273398280143738, -0.805133581161499, -0.7884923219680786, -0.7774857878684998, -0.7714389562606812, -0.7725280523300171, -0.7805200219154358, -0.7950462102890015, -0.8213735818862915, -0.8557322025299072, -0.8977883458137512, -0.9449884295463562, -0.9913508296012878, -1.0371534824371338, -1.0777159929275513, -1.1080615520477295, -1.122544765472412, -1.1227879524230957, -1.1077990531921387, -1.0789254903793335, -1.0470261573791504, -1.0082652568817139, -0.9669828414916992, -0.9257302284240723, -0.8866519927978516, -0.8514035940170288, -0.8211642503738403, -0.7965604066848755, -0.777722954750061, -0.7648299932479858, -0.7577401399612427, -0.756261944770813, -0.7642936706542969, -0.7832273840904236, -0.8133339285850525, -0.8534277081489563, -0.8910449743270874, -0.9338829517364502, -0.9797142148017883, -1.0259051322937012, -1.0630223751068115, -1.0957932472229004, -1.1205995082855225, -1.135033130645752, -1.1372685432434082, -1.1264485120773315, -1.102968454360962, -1.0687952041625977, -1.027984857559204, -0.9830899834632874, -0.9373591542243958, -0.893418550491333, -0.8558921813964844, -0.8227109909057617, -0.7948899865150452, -0.7727597951889038, -0.7554618120193481, -0.7444677352905273, -0.7395015358924866, -0.7402967810630798, -0.7479890584945679, -0.762328028678894, -0.7833307981491089, -0.8106937408447266, -0.8440115451812744, -0.8827952742576599, -0.9260439872741699, -0.9726173281669617, -1.0167806148529053, -1.0598771572113037, -1.0984498262405396, -1.1291553974151611, -1.1487622261047363, -1.1543166637420654, -1.144498586654663, -1.1198585033416748, -1.088566780090332, -1.0486268997192383, -1.0039114952087402, -0.9574630260467529, -0.9111752510070801, -0.8679419159889221, -0.8292899131774902, -0.7961530685424805, -0.769879937171936, -0.7492244243621826, -0.734157919883728, -0.7245028018951416, -0.7195875644683838, -0.7228331565856934, -0.7340786457061768, -0.7528958320617676, -0.7783807516098022, -0.8108829259872437, -0.8501332998275757, -0.8950197100639343, -0.9501339793205261, -1.0082430839538574, -1.0658867359161377, -1.11716890335083, -1.1451417207717896, -1.1649081707000732, -1.1729152202606201, -1.1682100296020508, -1.1509044170379639, -1.1218194961547852, -1.0827170610427856, -1.036129117012024, -0.9815537929534912, -0.9272018074989319, -0.8757408857345581, -0.8293238878250122, -0.7951674461364746, -0.7657642364501953, -0.7417806386947632, -0.7231671810150146, -0.7076730728149414, -0.6989083290100098, -0.6964479684829712, -0.6998934745788574, -0.7102389931678772, -0.7266210317611694, -0.7497353553771973, -0.7795318365097046, -0.8157860040664673, -0.8576846718788147, -0.905089259147644, -0.9571197628974915, -1.0090746879577637, -1.061370611190796, -1.1103909015655518, -1.1518278121948242, -1.1814961433410645, -1.1949058771133423, -1.189452886581421, -1.1651530265808105, -1.1330244541168213, -1.0902327299118042, -1.0403945446014404, -0.9873726963996887, -0.9337553977966309, -0.8824864625930786, -0.8354573845863342, -0.7938472032546997, -0.7603871822357178, -0.7322818636894226, -0.7096272110939026, -0.6922476291656494, -0.6776953935623169, -0.6698436141014099, -0.6682307720184326, -0.6717824935913086, -0.6835692524909973, -0.7030054330825806, -0.7305669784545898, -0.7661950588226318, -0.8046104311943054, -0.8498835563659668, -0.9003666639328003, -0.9557711482048035, -1.011852502822876, -1.068861484527588, -1.1230251789093018, -1.1697721481323242, -1.2042677402496338, -1.2214303016662598, -1.217919111251831, -1.19337797164917, -1.1601457595825195, -1.1150795221328735, -1.0617096424102783, -1.004399061203003, -0.9465591311454773, -0.8906747102737427, -0.8388901948928833, -0.7924677133560181, -0.7550603151321411, -0.7228633165359497, -0.6960576772689819, -0.6744554042816162, -0.6551896333694458, -0.6420856714248657, -0.6341928839683533, -0.6307514905929565, -0.6338211297988892, -0.6441351771354675, -0.6619848012924194, -0.6871943473815918, -0.7129426002502441, -0.7449327707290649, -0.7827419638633728, -0.8260287642478943, -0.897560715675354, -0.9759521484375, -1.0604097843170166, -1.1429754495620728, -1.1870639324188232, -1.224642276763916, -1.2489439249038696, -1.256960153579712, -1.2476272583007812, -1.2203527688980103, -1.17686128616333, -1.1202822923660278, -1.0554126501083374, -0.9870766997337341, -0.9195021390914917, -0.8556249141693115, -0.8072298169136047, -0.7629387378692627, -0.7240534424781799, -0.6906740665435791, -0.6607240438461304, -0.6364979147911072, -0.6174580454826355, -0.6024982333183289, -0.5899072885513306, -0.5823307037353516, -0.5793536305427551, -0.5805351734161377, -0.5855268239974976, -0.5942699313163757, -0.6067458391189575, -0.6229848861694336, -0.6451466679573059, -0.6720041036605835, -0.7041951417922974, -0.7421466708183289, -0.8066398501396179, -0.8821663856506348, -0.969819962978363, -1.0642129182815552, -1.119499683380127, -1.176676869392395, -1.2277681827545166, -1.2686893939971924, -1.2965360879898071, -1.306335687637329, -1.2961124181747437, -1.2659335136413574, -1.2240897417068481, -1.1693280935287476, -1.105478048324585, -1.0369668006896973, -0.9698007106781006, -0.9040576219558716, -0.842095673084259, -0.7852503061294556, -0.7385144233703613, -0.6966712474822998, -0.6600360870361328, -0.6284480690956116, -0.6004140377044678, -0.5768059492111206, -0.5559684038162231, -0.5391358733177185, -0.5252740979194641, -0.5148116946220398, -0.5073696374893188, -0.5025462508201599, -0.5000249147415161, -0.49959760904312134, -0.5011085271835327, -0.5044135451316833, -0.5099228620529175, -0.517314076423645, -0.5267297625541687, -0.5382538437843323, -0.5551084280014038, -0.5751885771751404, -0.5992965698242188, -0.6280175447463989, -0.6874547004699707, -0.7608820796012878, -0.8534160256385803, -0.9660201668739319, -1.0450607538223267, -1.1315712928771973, -1.216968297958374, -1.2942445278167725, -1.340687870979309, -1.3747878074645996, -1.390932321548462, -1.3868328332901, -1.361975908279419, -1.3171956539154053, -1.2554327249526978, -1.1797912120819092, -1.097450613975525, -1.0122597217559814, -0.9283494353294373, -0.8485869765281677, -0.7895505428314209, -0.7339508533477783, -0.6835019588470459, -0.6382737159729004, -0.5910701155662537, -0.5506628751754761, -0.5152397751808167, -0.483720600605011, -0.44972795248031616, -0.42236509919166565, -0.40026870369911194, -0.38214394450187683, -0.3690316379070282, -0.3574915826320648, -0.34706711769104004, -0.3372949957847595, -0.32919764518737793, -0.32097843289375305, -0.3125152885913849, -0.30361324548721313, -0.29229384660720825, -0.2800107002258301, -0.26635050773620605, -0.25099092721939087, -0.2299133688211441, -0.2058403491973877, -0.17784279584884644, -0.14523032307624817, -0.09883695840835571, -0.04451301693916321, 0.019514024257659912, 0.09393131732940674, 0.17450153827667236, 0.25952398777008057, 0.3414958715438843, 0.4270176887512207, 0.49088579416275024, 0.5436972379684448, 0.5823258757591248, 0.602085292339325, 0.5995245575904846, 0.5731298327445984, 0.5242445468902588, 0.45667606592178345, 0.3797844648361206, 0.29845893383026123, 0.21276164054870605, 0.12913405895233154, 0.06315204501152039, -0.000270158052444458, -0.05887800455093384, -0.11395314335823059, -0.165777325630188, -0.21170561015605927, -0.2520984411239624, -0.28734341263771057, -0.32795190811157227, -0.361039936542511, -0.3880736827850342, -0.41031891107559204, -0.4245184659957886, -0.4368512034416199, -0.44784998893737793, -0.45789581537246704, -0.4671362340450287, -0.47603532671928406, -0.48480796813964844, -0.49367427825927734, -0.504315197467804, -0.5154753923416138, -0.5275152921676636, -0.540698766708374, -0.5582133531570435, -0.5778264999389648, -0.6002426147460938, -0.6259729862213135, -0.6621416211128235, -0.7038750648498535, -0.7525429725646973, -0.8091849088668823, -0.8925325870513916, -0.987362802028656, -1.0902750492095947, -1.1958301067352295, -1.2531098127365112, -1.3073148727416992, -1.3494677543640137, -1.3752892017364502, -1.3824117183685303, -1.3676565885543823, -1.331319808959961, -1.2756571769714355, -1.2115395069122314, -1.1370315551757812, -1.0578911304473877, -0.9778446555137634, -0.9086573123931885, -0.8419607877731323, -0.7800125479698181, -0.7234824299812317, -0.6778091192245483, -0.6364102363586426, -0.5994598865509033, -0.5667418241500854, -0.5229461193084717, -0.4863848090171814, -0.45653235912323, -0.4322302043437958, -0.4168083369731903, -0.4035792052745819, -0.392134428024292, -0.3821302354335785, -0.37345024943351746, -0.36557820439338684, -0.35828542709350586, -0.3513605296611786, -0.34391364455223083, -0.33651772141456604, -0.32895129919052124, -0.32101401686668396, -0.3116605877876282, -0.301501989364624, -0.29023951292037964, -0.2776251435279846, -0.2600938677787781, -0.2401028722524643, -0.21682044863700867, -0.18963798880577087, -0.15107357501983643, -0.10563379526138306, -0.051529258489608765, 0.012384027242660522, 0.10668647289276123, 0.215451180934906, 0.3232850432395935, 0.4301220178604126, 0.4983493685722351, 0.5540202856063843, 0.5963122248649597, 0.6204866766929626, 0.6229811310768127, 0.6015881896018982, 0.5569141507148743, 0.49231648445129395, 0.41706305742263794, 0.3352278470993042, 0.25055116415023804, 0.1644904613494873, 0.09455567598342896, 0.026767760515213013, -0.03639817237854004, -0.09512984752655029, -0.15134912729263306, -0.20160678029060364, -0.24616803228855133, -0.2853740453720093, -0.33108994364738464, -0.3687279522418976, -0.3998376429080963, -0.42573949694633484, -0.443439245223999, -0.45909014344215393, -0.47305479645729065, -0.4857693016529083, -0.4975969195365906, -0.5090816020965576, -0.5205076336860657, -0.5321699380874634, -0.5463882088661194, -0.5614458918571472, -0.5778347253799438, -0.5959218144416809, -0.6199592351913452, -0.6469546556472778, -0.6777893304824829, -0.7129525542259216, -0.7623758316040039, -0.8188258409500122, -0.8835437297821045, -0.9559391140937805, -1.0333904027938843, -1.1144416332244873, -1.1941149234771729, -1.264775037765503, -1.30422842502594, -1.3318564891815186, -1.3427854776382446, -1.3352720737457275, -1.3094596862792969, -1.266322135925293, -1.2085496187210083, -1.1389787197113037, -1.0631788969039917, -0.9855290651321411, -0.9096376895904541, -0.8380472660064697, -0.7836297750473022, -0.7331096529960632, -0.6877763271331787, -0.6476625800132751, -0.6125607490539551, -0.5821450352668762, -0.5553144216537476, -0.531122088432312, -0.5082622170448303, -0.4894930422306061, -0.47424250841140747, -0.4619821310043335, -0.4521714746952057, -0.444504052400589, -0.4386512339115143, -0.4343002438545227, -0.43125784397125244, -0.42921239137649536, -0.42801767587661743, -0.4275488257408142, -0.4277524948120117, -0.4285814166069031, -0.4300174117088318, -0.4320327341556549, -0.4350677728652954, -0.43882521986961365, -0.4434302747249603, -0.448971688747406, -0.4572315812110901, -0.4671497941017151, -0.4792550206184387, -0.4940168857574463, -0.5224900245666504, -0.5591763257980347, -0.6078022122383118, -0.6719692945480347, -0.7389838099479675, -0.8244616389274597, -0.9288371205329895, -1.0517914295196533, -1.1421936750411987, -1.239186406135559, -1.3333842754364014, -1.4158086776733398, -1.4630796909332275, -1.4946069717407227, -1.5100419521331787, -1.5083301067352295, -1.4766521453857422, -1.4157958030700684, -1.32904851436615, -1.2304282188415527, -1.1604863405227661, -1.0846108198165894, -1.008939266204834, -0.934943675994873, -0.862938404083252, -0.794957160949707, -0.7315114736557007, -0.6728445291519165, -0.5991829633712769, -0.5355806350708008, -0.48026415705680847, -0.4310750961303711, -0.38154134154319763, -0.3397928774356842, -0.304383248090744, -0.27400314807891846, -0.24650052189826965, -0.22199486196041107, -0.1995120346546173, -0.17807956039905548, -0.1581002175807953, -0.13771581649780273, -0.11651259660720825, -0.09403499960899353, -0.06549233198165894, -0.03464224934577942, -0.0007828772068023682, 0.03634923696517944, 0.08668076992034912, 0.14056414365768433, 0.19764339923858643, 0.253059983253479, 0.29751741886138916, 0.34116894006729126, 0.3795701265335083, 0.40774035453796387, 0.42262423038482666, 0.42162156105041504, 0.40397173166275024, 0.37083762884140015, 0.3281949758529663, 0.27793991565704346, 0.2224217653274536, 0.1654396653175354, 0.11179196834564209, 0.06106358766555786, 0.01501232385635376, -0.02566954493522644, -0.061901092529296875, -0.09264549612998962, -0.11771580576896667, -0.13720452785491943, -0.15384075045585632, -0.1637706756591797, -0.16749021410942078, -0.16552865505218506, -0.1569821536540985, -0.14222219586372375, -0.12097617983818054, -0.09320032596588135, -0.06128266453742981, -0.02277013659477234, 0.02229619026184082, 0.07386422157287598, 0.15707725286483765, 0.244401216506958, 0.3226984739303589, 0.39346468448638916, 0.42848265171051025, 0.45028066635131836, 0.45814353227615356, 0.4506433606147766, 0.42767006158828735, 0.39047014713287354, 0.3417065143585205, 0.28435373306274414, 0.22014641761779785, 0.15538740158081055, 0.09294503927230835, 0.035251766443252563, -0.010048210620880127, -0.051353782415390015, -0.08889701962471008, -0.12111029028892517, -0.15026450157165527, -0.17358621954917908, -0.19128559529781342, -0.2041212022304535, -0.21287082135677338, -0.2173565924167633, -0.21789875626564026, -0.21481838822364807, -0.20832538604736328, -0.1984839290380478, -0.18527139723300934, -0.16862547397613525, -0.14379268884658813, -0.11402493715286255, -0.07834824919700623, -0.03630399703979492, 0.03539913892745972, 0.1187828779220581, 0.21403300762176514, 0.3048890233039856, 0.3555036187171936, 0.4077228903770447, 0.4524977207183838, 0.4857795238494873, 0.5040313601493835, 0.5042185187339783, 0.4850364923477173, 0.4475937485694885, 0.3979499936103821, 0.33907419443130493, 0.2744501233100891, 0.205732524394989, 0.14151698350906372, 0.07973206043243408, 0.022254496812820435, -0.03023400902748108, -0.07605975866317749, -0.11766114830970764, -0.15396982431411743, -0.18515561521053314, -0.2132183015346527, -0.2362401932477951, -0.25454533100128174, -0.2688061594963074, -0.2815297544002533, -0.28978705406188965, -0.29406434297561646, -0.29493382573127747, -0.29356908798217773, -0.289969265460968, -0.284379780292511, -0.2768417000770569, -0.2661198675632477, -0.2530234456062317, -0.23719164729118347, -0.21833883225917816, -0.18737712502479553, -0.1503426730632782, -0.1052732765674591, -0.05089566111564636, 0.020157068967819214, 0.10416156053543091, 0.20103693008422852, 0.29763317108154297, 0.358675479888916, 0.42464572191238403, 0.4855204224586487, 0.5363579988479614, 0.5722622871398926, 0.5881199240684509, 0.58072429895401, 0.5494454503059387, 0.5011022090911865, 0.4394427537918091, 0.36726903915405273, 0.29101109504699707, 0.21507078409194946, 0.13986974954605103, 0.0682523250579834, 0.0012324750423431396, -0.05278301239013672, -0.10405638813972473, -0.15120205283164978, -0.19340334832668304, -0.24043749272823334, -0.2800281345844269]}, {\"line\": {\"color\": \"red\", \"width\": 1}, \"type\": \"scatter\", \"x\": [1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365], \"y\": [-0.8348081707954407, -0.8795527219772339, -0.9374568462371826, -1.0063316822052002, -1.0839921236038208, -1.1607604026794434, -1.233931541442871, -1.2998042106628418, -1.3552935123443604, -1.3954426050186157, -1.4159367084503174, -1.4137611389160156, -1.3877742290496826, -1.3417675495147705, -1.2785401344299316, -1.2016724348068237, -1.1159279346466064, -1.0271003246307373, -0.9390268325805664, -0.8548667430877686, -0.7766827344894409, -0.7063201665878296, -0.6441456079483032, -0.5901626348495483, -0.5437881946563721, -0.5037301778793335, -0.4686742424964905, -0.43834879994392395, -0.41185104846954346, -0.3885166645050049, -0.36794283986091614, -0.34972891211509705, -0.33372068405151367, -0.319444477558136, -0.3064754903316498, -0.2944166362285614, -0.2828708291053772, -0.27142730355262756, -0.2597641944885254, -0.24757829308509827, -0.2345590591430664, -0.22005318105220795, -0.20370501279830933, -0.18514449894428253, -0.1639510989189148, -0.13878947496414185, -0.10905233025550842, -0.07409170269966125, -0.033191800117492676, 0.01660454273223877, 0.07596731185913086, 0.15008842945098877, 0.23651587963104248, 0.316353440284729, 0.3883451819419861, 0.4500003457069397, 0.4981312155723572, 0.5290408730506897, 0.5396984815597534, 0.528477668762207, 0.49558693170547485, 0.453067421913147, 0.4032444953918457, 0.3490315079689026, 0.2904324531555176, 0.22716891765594482, 0.1626303791999817, 0.0998070240020752, 0.040767133235931396, -0.0136166512966156, -0.06434819102287292, -0.1109004020690918, -0.1529253125190735, -0.19024823606014252, -0.2229149490594864, -0.2511936128139496, -0.27554431557655334, -0.2963574528694153, -0.31401383876800537, -0.3288901746273041, -0.3413117527961731, -0.35162052512168884, -0.3601263165473938, -0.3671042025089264, -0.37279582023620605, -0.377388596534729, -0.38105806708335876, -0.38395318388938904, -0.38619664311408997, -0.3878602683544159, -0.3890279531478882, -0.3897651731967926, -0.3901197910308838, -0.39008158445358276, -0.38966885209083557, -0.3888852596282959, -0.3877166509628296, -0.38603901863098145, -0.383789986371994, -0.38088181614875793, -0.3771900534629822, -0.37223872542381287, -0.3657124638557434, -0.3570352792739868, -0.34566402435302734, -0.3309279680252075, -0.31192857027053833, -0.2874983847141266, -0.2561804950237274, -0.21655993163585663, -0.1665196418762207, -0.10369455814361572, -0.0245400071144104, 0.07298541069030762, 0.18935739994049072, 0.30929017066955566, 0.4258503317832947, 0.5303078293800354, 0.614815890789032, 0.6807004809379578, 0.7300556302070618, 0.7506032586097717, 0.7395753264427185, 0.696829617023468, 0.6253258585929871, 0.5436967015266418, 0.4555895924568176, 0.365257203578949, 0.26875054836273193, 0.1706535816192627, 0.07436144351959229, -0.01744675636291504, -0.10288676619529724, -0.18372637033462524, -0.2587377727031708, -0.3272649645805359, -0.389164000749588, -0.4441208243370056, -0.49230432510375977, -0.5343500375747681, -0.5714980959892273, -0.6044598817825317, -0.6341310739517212, -0.6613998413085938, -0.6871455907821655, -0.7119200229644775, -0.7362880706787109, -0.7607570886611938, -0.7857290506362915, -0.812077522277832, -0.8393706679344177, -0.8677863478660583, -0.8974649906158447, -0.9295758008956909, -0.9638747572898865, -0.9997622966766357, -1.036008596420288, -1.0712025165557861, -1.1033064126968384, -1.1299889087677002, -1.1489050388336182, -1.1580498218536377, -1.156305193901062, -1.1435977220535278, -1.1208746433258057, -1.089918851852417, -1.0532190799713135, -1.0134806632995605, -0.9732081890106201, -0.9347480535507202, -0.8995431661605835, -0.8686060309410095, -0.8424290418624878, -0.8213351368904114, -0.8054195642471313, -0.7945983409881592, -0.7886906862258911, -0.7880352735519409, -0.7925176620483398, -0.8020431399345398, -0.8165650367736816, -0.8360573053359985, -0.8603330254554749, -0.8891267776489258, -0.9221983551979065, -0.9607540965080261, -1.0031144618988037, -1.0463917255401611, -1.0861303806304932, -1.1186209917068481, -1.1419379711151123, -1.1545639038085938, -1.1559993028640747, -1.1460535526275635, -1.1256039142608643, -1.0957496166229248, -1.0590496063232422, -1.0177638530731201, -0.9748063087463379, -0.9327567219734192, -0.8936979174613953, -0.8590619564056396, -0.829416036605835, -0.8050081133842468, -0.7857376933097839, -0.7718247175216675, -0.7631556391716003, -0.7595231533050537, -0.7604553699493408, -0.7671787142753601, -0.7801684737205505, -0.7999184727668762, -0.8270910978317261, -0.8590041399002075, -0.8953157663345337, -0.9355801939964294, -0.9792031645774841, -1.0238473415374756, -1.0677556991577148, -1.1085721254348755, -1.1437084674835205, -1.1701204776763916, -1.1851627826690674, -1.1870297193527222, -1.1750918626785278, -1.151799201965332, -1.118607759475708, -1.077951192855835, -1.032618522644043, -0.9854904413223267, -0.9390741586685181, -0.8954062461853027, -0.855881929397583, -0.8213826417922974, -0.7922661304473877, -0.768596887588501, -0.7501774430274963, -0.7371405363082886, -0.7291892766952515, -0.725222647190094, -0.7252479791641235, -0.7303001880645752, -0.740680456161499, -0.7567498683929443, -0.7790391445159912, -0.8070023059844971, -0.8404965400695801, -0.8785109519958496, -0.9209991097450256, -0.9679155945777893, -1.017831802368164, -1.0683164596557617, -1.1157331466674805, -1.1561486721038818, -1.187058925628662, -1.2062063217163086, -1.2122702598571777, -1.2044353485107422, -1.1832497119903564, -1.1500400304794312, -1.1071293354034424, -1.0572013854980469, -1.0038630962371826, -0.9504251480102539, -0.8996396064758301, -0.8536503911018372, -0.8132057189941406, -0.7786576747894287, -0.7498525381088257, -0.7267745137214661, -0.709182858467102, -0.6959987878799438, -0.6863758563995361, -0.6804025173187256, -0.6780399680137634, -0.6792635321617126, -0.6841098070144653, -0.6926552057266235, -0.70502769947052, -0.7214200496673584, -0.742081344127655, -0.7688826322555542, -0.8020616173744202, -0.8421040177345276, -0.8879204988479614, -0.9395500421524048, -0.9958614110946655, -1.0544520616531372, -1.1112570762634277, -1.1610393524169922, -1.20090651512146, -1.2280452251434326, -1.24070143699646, -1.2373871803283691, -1.2182447910308838, -1.1846935749053955, -1.1388423442840576, -1.08372163772583, -1.0235264301300049, -0.9622224569320679]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"rangeslider\": {\"visible\": true}}, \"yaxis\": {\"autorange\": true, \"fixedrange\": false}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('7cce2839-df9c-48ad-b313-b03c3bea48c9');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}