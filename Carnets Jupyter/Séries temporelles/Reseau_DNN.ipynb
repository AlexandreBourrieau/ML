{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reseau_DNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPb4doHZxf95hNLbtaZY25l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/Reseau_DNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubCeIvtF6R4W"
      },
      "source": [
        "Dans ce carnet nous allons mettre en place un modèle à réseau de neurones profond (Deep Neural Network - DNN) pour réaliser des prédictions sur notre série temporelle.  \r\n",
        "Nous allons également utiliser une technique permettant d'optimiser l'entrainement en choisisant un taux d'apprentissage optimal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRhtHsNn5fc3"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFeah3y_6kif"
      },
      "source": [
        "# Création de la série temporelle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJfSLtub6sdc"
      },
      "source": [
        "# Fonction permettant d'afficher une série temporelle\r\n",
        "def affiche_serie(temps, serie, format=\"-\", debut=0, fin=None, label=None):\r\n",
        "    plt.plot(temps[debut:fin], serie[debut:fin], format, label=label)\r\n",
        "    plt.xlabel(\"Temps\")\r\n",
        "    plt.ylabel(\"Valeur\")\r\n",
        "    if label:\r\n",
        "        plt.legend(fontsize=14)\r\n",
        "    plt.grid(True)\r\n",
        "\r\n",
        "# Fonction permettant de créer une tendance\r\n",
        "def tendance(temps, pente=0):\r\n",
        "    return pente * temps\r\n",
        "\r\n",
        "# Fonction permettant de créer un motif\r\n",
        "def motif_periodique(instants):\r\n",
        "    return (np.where(instants < 0.4,                            # Si les instants sont < 0.4\r\n",
        "                    np.cos(instants * 2 * np.pi),               # Alors on retourne la fonction cos(2*pi*t)\r\n",
        "                    1 / np.exp(3 * instants)))                  # Sinon, on retourne la fonction exp(-3t)\r\n",
        "\r\n",
        "# Fonction permettant de créer une saisonnalité avec un motif\r\n",
        "def saisonnalite(temps, periode, amplitude=1, phase=0):\r\n",
        "    \"\"\"Répétition du motif sur la même période\"\"\"\r\n",
        "    instants = ((temps + phase) % periode) / periode            # Mapping du temps =[0 1 2 ... 1460] => instants = [0.0 ... 1.0]\r\n",
        "    return amplitude * motif_periodique(instants)\r\n",
        "\r\n",
        "# Fonction permettant de générer du bruit gaussien N(0,1)\r\n",
        "def bruit_blanc(temps, niveau_bruit=1, graine=None):\r\n",
        "    rnd = np.random.RandomState(graine)\r\n",
        "    return rnd.randn(len(temps)) * niveau_bruit\r\n",
        "\r\n",
        "# Création de la série temporelle\r\n",
        "temps = np.arange(4 * 365)                # temps = [0 1 2 .... 4*365] = [0 1 2 .... 1460]\r\n",
        "amplitude = 40                            # Amplitude de la la saisonnalité\r\n",
        "niveau_bruit = 5                          # Niveau du bruit\r\n",
        "offset = 10                               # Offset de la série\r\n",
        "\r\n",
        "serie = offset + tendance(temps, 0.1) + saisonnalite(temps, periode=365, amplitude=amplitude) + bruit_blanc(temps,niveau_bruit)\r\n",
        "\r\n",
        "plt.figure(figsize=(10, 6))\r\n",
        "affiche_serie(temps,serie)\r\n",
        "plt.title('Série temporelle expérimentale')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atOtbHUk7AYt"
      },
      "source": [
        "# Préparation des données X et Y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78vEUlpIFMp6"
      },
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\r\n",
        "# au format X(X1,X2,...Xn) / Y(Y1,Y2,...,Yn)\r\n",
        "# X sont les données d'entrées du réseau\r\n",
        "# Y sont les labels\r\n",
        "\r\n",
        "def prepare_dataset_XY(serie, taille_fenetre, batch_size, buffer_melange):\r\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie)\r\n",
        "  dataset = dataset.window(taille_fenetre+1, shift=1, drop_remainder=True)\r\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre + 1))\r\n",
        "  dataset = dataset.shuffle(buffer_melange).map(lambda x: (x[:-1], x[-1:]))\r\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\r\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHgcL7ZPJlws"
      },
      "source": [
        "**1. Séparation des données en données pour l'entrainement et la validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ggmBUlAKWH2"
      },
      "source": [
        "<img src=\"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/Images/Series/illustration1.png?raw=true\" width=\"600\">  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_14USRg5JvSu"
      },
      "source": [
        "temps_separation = 1000\r\n",
        "\r\n",
        "# Extraction des temps et des données d'entrainement\r\n",
        "temps_entrainement = temps[:temps_separation]\r\n",
        "x_entrainement = serie[:temps_separation]\r\n",
        "\r\n",
        "# Exctraction des temps et des données de valiadation\r\n",
        "temps_validation = temps[temps_separation:]\r\n",
        "x_validation = serie[temps_separation:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjh4mjMrKhA5"
      },
      "source": [
        "**2. Préparation des données X et des labels Y**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdqfYabAKoE2"
      },
      "source": [
        "On commence par créer notre dataset à partir de la série (remarque : les valeurs ci-dessous sont en réalité mélangées) :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2CDLaYDoDms"
      },
      "source": [
        "<img src=\"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/images/Split_XY_30elements.png?raw=true\" width=\"1200\"> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziajfOefKvsu"
      },
      "source": [
        "# Définition des caractéristiques du dataset que l'on souhaite créer\r\n",
        "taille_fenetre = 20\r\n",
        "batch_size = 32\r\n",
        "buffer_melange = 1000\r\n",
        "\r\n",
        "# Création du dataset X,Y\r\n",
        "dataset = prepare_dataset_XY(x_entrainement,taille_fenetre,batch_size,buffer_melange)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Yt-EgZ3sgPY"
      },
      "source": [
        "# Création et entrainement du modèle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyrcfKcgCsZ7"
      },
      "source": [
        "**1. Création du réseau**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ0WlZFCs3hQ"
      },
      "source": [
        "On met maintenant en place un réseau de neurones profond constitué de :\r\n",
        "-  **D'une couche d'entrée avec 10 neurones** avec une **fonction d'activation de type \"relu\"** qui prend en entrée des **vecteurs 1D de dimension égale à la taille de la fenêtre** avec un **batch_size** de 32.  \r\n",
        "On a donc ici : 20*10 poids + 10 offsets = 210 paramètres\r\n",
        "- **Une couche profonde à 10 neurones**, avec une **fonction d'activation de type \"relu\"**  \r\n",
        "On a donc ici : 10*10 poids + 10 offsets = 110 paramètres\r\n",
        "- D'une **couche de sortie à 1 neurone** sans fonction d'activation  \r\n",
        "On a donc ici : 10 poids + 1 offset = 11 paramètres"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwAjNSA3Xxiv"
      },
      "source": [
        "<img src=\"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/images/DNN_1.png?raw=true\" width=\"1200\"> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GP4mlHafXwY4"
      },
      "source": [
        "\r\n",
        "Pour les paramètre de compilation du modèle :\r\n",
        "- On utilise un **taux d'apprentissage de 1e-6**\r\n",
        "- On choisit un **optimiseur de type Descente de gradient stochastique et à moment** (MSGD - Momentum Stochastic Gradient Descent) avec un moment fixé à 0.9.  \r\n",
        "- On utilise une **fonction d'objectif** de type **erreur moyenne quadratique** (mse - mean squared error)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGl_t4BG3Jw2"
      },
      "source": [
        "# Création du modèle à réseau de neurones profonds\r\n",
        "\r\n",
        "model = tf.keras.models.Sequential()\r\n",
        "model.add(tf.keras.Input(shape=(taille_fenetre,), batch_size=batch_size))\r\n",
        "model.add(tf.keras.layers.Dense(10,activation=\"relu\"))\r\n",
        "model.add(tf.keras.layers.Dense(10,activation=\"relu\"))\r\n",
        "model.add(tf.keras.layers.Dense(1))\r\n",
        "\r\n",
        "model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9))\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7B0db8g0x0-"
      },
      "source": [
        "**2. Entrainement du réseau**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ynbm3GnF1OxV"
      },
      "source": [
        "On entraine le réseau sur 100 périodes. Comme le modèle prend 1 élément (vecteur d'entrée X de dimension 20) à chaque itération, et qu'il y a 45 éléments, il y aura 45 itérations par période.  \r\n",
        "Pour entrainer notre réseau, on lui rentre un Dataset au format tensorflow. La méthode [fit](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) de la classe Model de Keras permet de gérer ce genre d'entrée. Dans ce cas, le Dataset doit être au format `(entrées, labels)`, ce qui est notre cas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0viBqeL02-O"
      },
      "source": [
        "# Lance l'entrainement du modèle\r\n",
        "\r\n",
        "model.fit(dataset,epochs=100,verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRzsuUcVLPu7"
      },
      "source": [
        "# Prédictions avec le modèle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJR8ojSMSkP9"
      },
      "source": [
        "# Création d'une liste vide pour recevoir les prédictions\r\n",
        "predictions = []\r\n",
        "\r\n",
        "# Calcul des prédiction pour chaque groupe de 20 valeurs consécutives de la série\r\n",
        "# dans l'intervalle de validation\r\n",
        "for t in temps[temps_separation:-taille_fenetre]:\r\n",
        "    X = np.reshape(serie[t:t+taille_fenetre],(1,taille_fenetre))\r\n",
        "    predictions.append(model.predict(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOafTkl4Urx6"
      },
      "source": [
        "# Affiche la série et les prédictions\r\n",
        "plt.figure(figsize=(10, 6))\r\n",
        "affiche_serie(temps,serie,label=\"Série temporelle\")\r\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions)[:,0,0],label=\"Prédictions\")\r\n",
        "plt.title('Prédictions avec le modèle de régression linéaire')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# Zoom sur l'intervalle de validation\r\n",
        "plt.figure(figsize=(10, 6))\r\n",
        "affiche_serie(temps[temps_separation:],serie[temps_separation:],label=\"Série temporelle\")\r\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions)[:,0,0],label=\"Prédictions\")\r\n",
        "plt.title(\"Prédictions avec le modèle de régression linéaire (zoom sur l'intervalle de validation)\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbDPIn--Exij"
      },
      "source": [
        "**Erreurs de prédiction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK1TbpTwE1ir"
      },
      "source": [
        "On peut calculer l'erreur quadratique moyenne et l'erreur absolue moyenne. On otient des résultats identiques, voire inférieurs à ceux obtenus avec les méthodes statistiques vues précédemment :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5FGyrGbE6p3"
      },
      "source": [
        "# Calcule de l'erreur quadratique moyenne et de l'erreur absolue moyenne \r\n",
        "\r\n",
        "mae = tf.keras.metrics.mean_absolute_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions)[:,0,0]).numpy()\r\n",
        "mse = tf.keras.metrics.mean_squared_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions)[:,0,0]).numpy()\r\n",
        "\r\n",
        "print(mae)\r\n",
        "print(mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9ZuXlL7Y9to"
      },
      "source": [
        "# Optimisation du taux d'apprentissage "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kjgIQKYeN0q"
      },
      "source": [
        "**1. Evolution de l'erreur en fonction du taux d'apprentissage**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXmtd66dZH6p"
      },
      "source": [
        "Afin de trouver la valeur optimale pour le taux d'apprentissage, nous allons regarder comment évolue l'erreur en fonction du taux d'apprentissage. Pour mettre en place un taux d'apprentissage qui varie à chaque période, Keras nous propose [plusieurs méthodes](https://keras.io/api/optimizers/learning_rate_schedules/) qui peuvent être implantées dans la classe de l'optimiseur. C'est ce que nous avions fait lors de la mise en place du réseau VDSR.\r\n",
        "\r\n",
        "Cette fois, nous allons implanter notre propre algorithme de régulation. Pour cela, nous allons utiliser la classe [LearningRateScheduler](https://keras.io/api/callbacks/learning_rate_scheduler/) de Keras. Cela nous permet d'appeler une fonction spécifique qui reçoit en argument le numéro de la période d'entrainement et le taux d'apprentissage courant et retourne le nouveau taux d'apprentissage souhaité."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77TvBckNcPPf"
      },
      "source": [
        "La fonction que nous allons utiliser est la suivante : $\r\n",
        "Taux = {10^{ - 8}}{.10^{\\left( {\\frac{{Periode}}{{20}}} \\right)}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3606UBBHUV4"
      },
      "source": [
        "# Remise à zéro des états du modèle\r\n",
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4kTpNagZHTP"
      },
      "source": [
        "# Création du modèle à réseau de neurones profonds\r\n",
        "model = tf.keras.models.Sequential()\r\n",
        "model.add(tf.keras.Input(shape=(taille_fenetre,), batch_size=batch_size))\r\n",
        "model.add(tf.keras.layers.Dense(10,activation=\"relu\"))\r\n",
        "model.add(tf.keras.layers.Dense(10,activation=\"relu\"))\r\n",
        "model.add(tf.keras.layers.Dense(1))\r\n",
        "\r\n",
        "# Définition de la fonction de régulation du taux d'apprentissage\r\n",
        "def RegulationTauxApprentissage(periode, taux):\r\n",
        "  return 1e-8*10**(periode/20)\r\n",
        "\r\n",
        "# Définition de l'optimiseur à utiliser\r\n",
        "optimiseur=tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9)\r\n",
        "\r\n",
        "# Compile le modèle\r\n",
        "model.compile(loss=\"mse\", optimizer=optimiseur)\r\n",
        "\r\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\r\n",
        "historique = model.fit(dataset,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig6DK553eXnY"
      },
      "source": [
        "**2. Exploitation des résultats**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZBh9ssMfTiS"
      },
      "source": [
        "Nous pouvons maintenant regarder comment évolue l'erreur en fonction du taux d'apprentissage :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwRY1BA3eeO7"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \r\n",
        "taux = 1e-8*(10**(np.arange(100)/20))\r\n",
        "\r\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\r\n",
        "plt.figure(figsize=(10, 6))\r\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\r\n",
        "plt.axis([ 1e-8, 1e-4, 0, 300])\r\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pjY5UhYgLJV"
      },
      "source": [
        "On peut récupérer la valeur optimale du taux d'apprentissage et relancer l'entrainement avec cette valeur :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AA5-IHFsKWW9"
      },
      "source": [
        "# Remise à zéro des états du modèle\r\n",
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPQbOSNxgcsR"
      },
      "source": [
        "# Création du modèle à réseau de neurones profonds\r\n",
        "\r\n",
        "model = tf.keras.models.Sequential()\r\n",
        "model.add(tf.keras.Input(shape=(taille_fenetre,), batch_size=batch_size))\r\n",
        "model.add(tf.keras.layers.Dense(10,activation=\"relu\"))\r\n",
        "model.add(tf.keras.layers.Dense(10,activation=\"relu\"))\r\n",
        "model.add(tf.keras.layers.Dense(1))\r\n",
        "\r\n",
        "model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(lr=6e-6, momentum=0.9))\r\n",
        "historique = model.fit(dataset,epochs=500,verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTSV3CUijG2K"
      },
      "source": [
        "On affiche maintenant l'évolution de l'erreur en fonction des périodes :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZF485IvcjMlR"
      },
      "source": [
        "erreur = historique.history[\"loss\"]\r\n",
        "periodes = np.arange(0,500)\r\n",
        "\r\n",
        "# Affiche l'erreur en fonction de la période\r\n",
        "plt.figure(figsize=(10, 6))\r\n",
        "plt.plot(periodes,erreur)\r\n",
        "\r\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6xDkx5NIFjR"
      },
      "source": [
        "# Affiche l'erreur en fonction de la période\r\n",
        "plt.figure(figsize=(10, 6))\r\n",
        "plt.plot(periodes[400:500],erreur[400:500])\r\n",
        "\r\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE0ARCgco5HJ"
      },
      "source": [
        "**3. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4JAQ7MMo7Vx"
      },
      "source": [
        "# Création d'une liste vide pour recevoir les prédictions\r\n",
        "predictions = []\r\n",
        "\r\n",
        "# Calcul des prédiction pour chaque groupe de 20 valeurs consécutives de la série\r\n",
        "# dans l'intervalle de validation\r\n",
        "for t in temps[temps_separation:-taille_fenetre]:\r\n",
        "    X = np.reshape(serie[t:t+taille_fenetre],(1,taille_fenetre))\r\n",
        "    predictions.append(model.predict(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8sYGagYpANh"
      },
      "source": [
        "# Affiche la série et les prédictions\r\n",
        "plt.figure(figsize=(10, 6))\r\n",
        "affiche_serie(temps,serie,label=\"Série temporelle\")\r\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions)[:,0,0],label=\"Prédictions\")\r\n",
        "plt.title('Prédictions avec le modèle de régression linéaire')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# Zoom sur l'intervalle de validation\r\n",
        "plt.figure(figsize=(10, 6))\r\n",
        "affiche_serie(temps[temps_separation:],serie[temps_separation:],label=\"Série temporelle\")\r\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions)[:,0,0],label=\"Prédictions\")\r\n",
        "plt.title(\"Prédictions avec le modèle de régression linéaire (zoom sur l'intervalle de validation)\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfU1idC2pEZh"
      },
      "source": [
        "# Calcule de l'erreur quadratique moyenne et de l'erreur absolue moyenne \r\n",
        "\r\n",
        "mae = tf.keras.metrics.mean_absolute_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions)[:,0,0]).numpy()\r\n",
        "mse = tf.keras.metrics.mean_squared_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions)[:,0,0]).numpy()\r\n",
        "\r\n",
        "print(mae)\r\n",
        "print(mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iqe1feB-Mxck"
      },
      "source": [
        "# Optimisation du taux d'apprentissage avec sauvegarde des meilleurs paramètres"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ3fQYQAPjO6"
      },
      "source": [
        "**1. Entrainement du modèle**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj2uv8U8M8mU"
      },
      "source": [
        "Nous allons maintenant ajouter la possibilité de sauvegarder les meilleurs paramètres trouvés lors de la phase d'entrainement. Pour cela, on va utliser la méthode [ModelCheckpoint](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint) de la classe Callbacks de Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJSrhFjINZ2d"
      },
      "source": [
        "# Remise à zéro des états du modèle\r\n",
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6J4B9IaNi1S"
      },
      "source": [
        "# Création du modèle à réseau de neurones profonds\r\n",
        "model = tf.keras.models.Sequential()\r\n",
        "model.add(tf.keras.Input(shape=(taille_fenetre,), batch_size=batch_size))\r\n",
        "model.add(tf.keras.layers.Dense(10,activation=\"relu\"))\r\n",
        "model.add(tf.keras.layers.Dense(10,activation=\"relu\"))\r\n",
        "model.add(tf.keras.layers.Dense(1))\r\n",
        "\r\n",
        "# Utilisation de la méthode ModelCheckPoint\r\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"best_model.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', period=1)\r\n",
        "\r\n",
        "# Définition de l'optimiseur à utiliser\r\n",
        "optimiseur=tf.keras.optimizers.SGD(lr=6e-6, momentum=0.9)\r\n",
        "\r\n",
        "# Compile le modèle\r\n",
        "model.compile(loss=\"mse\", optimizer=optimiseur)\r\n",
        "\r\n",
        "# Entraine le modèle en utilisant la méthode ModelCheckpoint pour sauvegarder les meilleurs poids du modèle\r\n",
        "historique = model.fit(dataset,epochs=500,verbose=1, callbacks=[CheckPoint])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7-QWLTjPg_1"
      },
      "source": [
        "**2. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7RnQ63tPnhF"
      },
      "source": [
        "# Création d'une liste vide pour recevoir les prédictions\r\n",
        "predictions = []\r\n",
        "\r\n",
        "# Calcul des prédiction pour chaque groupe de 20 valeurs consécutives de la série\r\n",
        "# dans l'intervalle de validation\r\n",
        "for t in temps[temps_separation:-taille_fenetre]:\r\n",
        "    X = np.reshape(serie[t:t+taille_fenetre],(1,taille_fenetre))\r\n",
        "    predictions.append(model.predict(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0lwm22EPqUy"
      },
      "source": [
        "# Affiche la série et les prédictions\r\n",
        "plt.figure(figsize=(10, 6))\r\n",
        "affiche_serie(temps,serie,label=\"Série temporelle\")\r\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions)[:,0,0],label=\"Prédictions\")\r\n",
        "plt.title('Prédictions avec le modèle de régression linéaire')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# Zoom sur l'intervalle de validation\r\n",
        "plt.figure(figsize=(10, 6))\r\n",
        "affiche_serie(temps[temps_separation:],serie[temps_separation:],label=\"Série temporelle\")\r\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions)[:,0,0],label=\"Prédictions\")\r\n",
        "plt.title(\"Prédictions avec le modèle de régression linéaire (zoom sur l'intervalle de validation)\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmdroPksPuJ-"
      },
      "source": [
        "# Calcule de l'erreur quadratique moyenne et de l'erreur absolue moyenne \r\n",
        "\r\n",
        "mae = tf.keras.metrics.mean_absolute_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions)[:,0,0]).numpy()\r\n",
        "mse = tf.keras.metrics.mean_squared_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions)[:,0,0]).numpy()\r\n",
        "\r\n",
        "print(mae)\r\n",
        "print(mse)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}