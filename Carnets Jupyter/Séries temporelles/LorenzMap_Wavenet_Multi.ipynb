{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LorenzMap_Wavenet_Multi.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/LorenzMap_Wavenet_Multi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVuCzMYKDp3z"
      },
      "source": [
        "# Chargement des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3nLyKtnliCW"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eo2qVS1lrzuX"
      },
      "source": [
        "On télécharge un script depuis Github permettant de télécharger un fichier stocké sur GoogleDrive, puis on utilise ce script écrit en Python pour télécharger le fichier `bitcoin.zip`. Enfin, on décompresse les données pour obtenir le fichier `bitstampUSD_1-min_data_2012-01-01_to_2021-03-31.csv` :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sOdetggHqKE"
      },
      "source": [
        "# Récupération des données au format .csv\n",
        "\n",
        "!git clone https://github.com/chentinghao/download_google_drive.git\n",
        "!python download_google_drive/download_gdrive.py \"1FZsEdpBm-AQ2L9n_pMnm6336-O_IVo7z\" \"/content/bitcoin.zip\"\n",
        "!unzip bitcoin.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPJ6nnrRsUBm"
      },
      "source": [
        "Charge la série sous Pandas et affiche les informations du fichier :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEB_JjihEYTP"
      },
      "source": [
        "# Création de la série sous Pandas\n",
        "serie = pd.read_csv(\"bitstampUSD_1-min_data_2012-01-01_to_2021-03-31.csv\")\n",
        "serie"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF62FumBty9H"
      },
      "source": [
        "# Pré-traitement des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2oeGZgr1UUP"
      },
      "source": [
        "**1. Recherche des erreurs dans les données**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2d3HD1YwmIS"
      },
      "source": [
        "On commence par vérifier qu'il ne manque pas de dates. Pour cela, on vérifie qu'il y a bien 60 secondes entre deux Timestamp. Si on trouve un décalage non cohérent, on enregistre les informations dans une liste."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCXSGY9Swltw"
      },
      "source": [
        "# Fonction permettant de vérifier si chaque intervalle est bien de 60s\n",
        "def recherche_erreur(fenetre):\n",
        "  if fenetre.values[1] - fenetre.values[0] != 60:\n",
        "    Timestamp_Errors.append(fenetre.values)\n",
        "  return 0\n",
        "\n",
        "# Définit une liste pour sauvegarder le résultat des recherches\n",
        "Timestamp_Errors = []\n",
        "\n",
        "# Applique la fonction sur une fenêtre glissante des données\n",
        "#serie.Timestamp.rolling(2).apply(recherche_erreur)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l1fTgb_1O8b"
      },
      "source": [
        "On affiche les erreurs trouvées :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZERkJS40sHP"
      },
      "source": [
        "# Affiche les informations sur les erreurs trouvées\n",
        "\n",
        "for erreur in Timestamp_Errors:\n",
        "  print (pd.to_datetime(Timestamp_Errors[0],unit=\"s\"))\n",
        "  print((Timestamp_Errors[0][1] - Timestamp_Errors[0][0])/60 - 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHOSRv8_1aYa"
      },
      "source": [
        "On observe qu'il manque des données entre le 5 janvier 2015 à 9:12:00 et le 9 janvier 2015 à 21:05:00, soit 6472 données."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tjbmho3h2nB7"
      },
      "source": [
        "Recherchons maintenant le nombre de données manquantes :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUzjd-qN2s-T"
      },
      "source": [
        "# Affichage du nombre total de données manquantes\n",
        "\n",
        "data_manquantes = sum(np.isnan(serie['Open']))\n",
        "print (\"Nombre de données manquantes : %s\" %data_manquantes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D41XKoGE3O60"
      },
      "source": [
        "On a donc en tout : 6472 + 1243608 = 1250080 données manquantes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GefH2TaM3a0S"
      },
      "source": [
        "**2. Identification des erreurs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XivWgphwsdIm"
      },
      "source": [
        "On convertit maintenant les `Timestamp` (mesure de temps exprimé en seconde écoulé depuis le 01/01/1970 - 00:00:00 UTC) en format plus standard :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdLSr2IWSOnX"
      },
      "source": [
        "# Conversion des timestamp en date\n",
        "serie.Timestamp = pd.to_datetime(serie['Timestamp'], unit=\"s\")\n",
        "serie"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgHcZyd13gCa"
      },
      "source": [
        "On demande maintenant à échantillonner les données sur 60 secondes :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVNlzo9QWNmP"
      },
      "source": [
        "# Echantillonnage de la série sur 1min\n",
        "serie_minute = serie.set_index('Timestamp').resample('60s').asfreq()\n",
        "\n",
        "# Récupère le nombre de données sans valeurs numériques\n",
        "data_manquantes = sum(np.isnan(serie_minute['Open']))\n",
        "\n",
        "# Affiche le nombre de données manquantes et la série sur 1min \n",
        "print (\"Nombre de données manquantes : %s\" %data_manquantes)\n",
        "serie_minute"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqFOnDlU3tey"
      },
      "source": [
        "On obtient en tout 4863849 données après échantillonnage, soit (4863849-4857377) =  6472 données supplémentaires. Ceci est cohérent avec ce qu'on avait trouvé avant. Il manque 1250080 données. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tkbi7uuBnUD3"
      },
      "source": [
        "**3. Correction des données**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IQuSqAknduG"
      },
      "source": [
        "Pour corriger les données, on va tout simplement utiliser la fonction [fillna](https://pandas.pydata.org/docs/reference/api/pandas.Series.fillna.html) de Pandas avec la fonctionnalité de type `backfill` :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2Oav6gin5aP"
      },
      "source": [
        "# Applique la fonction de remplissage automatique des données non numérique avec l'option backfill\n",
        "serie_minute = serie_minute.interpolate(method=\"slinear\")\n",
        "serie_minute = serie_minute.fillna(method=\"backfill\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfOvQ-BKn80v"
      },
      "source": [
        "# Récupère le nombre de données non numériques et affiche les informations\n",
        "\n",
        "data_manquantes = sum(np.isnan(serie_minute['Open']))\n",
        "print (\"Nombre de données manquantes : %s\" %data_manquantes)\n",
        "serie_minute"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v05rWWccJI26"
      },
      "source": [
        "**4. Affichage des données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1QKMBThNQni"
      },
      "source": [
        "# Affiche la série\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(serie_minute.index, serie_minute.Open)\n",
        "plt.title(\"Evolution du prix du BTC\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU7u1mA1E6jk"
      },
      "source": [
        "# Préparation des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhIs2GS7vu8k"
      },
      "source": [
        "Nous allons réaliser des modélisations sur la série journalière, et pour une période allant du 1er avril 2013 au 31 mars 2021."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeKrafzHv9gd"
      },
      "source": [
        "**1. Création de la série horaire pour la modélisation globale**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUlNO0pswBpr"
      },
      "source": [
        "On va réaliser des prédictions à l'aide d'une série à fréquence journalière. On commence par tenter d'estimer les données manquantes à l'aide d'une interpolation linéaire à l'aide de la fonction [interpolate](https://pandas.pydata.org/docs/reference/api/pandas.Series.interpolate.html#pandas.Series.interpolate) de Pandas, puis on complète avec la méthode `backfill` si nécessaire."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir4BkaUtuiXX"
      },
      "source": [
        "# Echantillonne la série sur 1 heure\n",
        "serie_heure = serie.set_index('Timestamp').resample('1H').asfreq()\n",
        "\n",
        "# Remplissage des données non numériques par interpolation linéraire\n",
        "serie_heure = serie_heure.interpolate(method=\"slinear\")\n",
        "\n",
        "# Remplissage des données non numériques restantes par backfill\n",
        "serie_heure = serie_heure.fillna(method=\"backfill\")\n",
        "\n",
        "# Affiche les informations\n",
        "data_manquantes = sum(np.isnan(serie_heure['Open']))\n",
        "print (\"Nombre de données manquantes : %s\" %data_manquantes)\n",
        "serie_heure"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epIrgQTvca6p"
      },
      "source": [
        "# Affiche la série\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(serie_heure.index, serie_heure.Open)\n",
        "plt.title(\"Evolution du prix du BTC\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hsbej4XvckFD"
      },
      "source": [
        "On construit une nouvelle série avec les dates retenues pour le début et la fin :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMiDRe1ZcqMR"
      },
      "source": [
        "# Définition des dates de début et de fin\n",
        "\n",
        "date_debut = \"2013-04-01 00:00:00\"\n",
        "date_fin = \"2021-03-31 00:00:00\"\n",
        "\n",
        "serie_etude = serie_heure.loc[date_debut:date_fin].copy()\n",
        "serie_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUD1yl1wxkla"
      },
      "source": [
        "serie_etude['x'] = np.linspace(0,serie_etude.index.size-1,serie_etude.index.size)\n",
        "serie_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG5Fyx5O5oe5"
      },
      "source": [
        "# Prépartion des datasets multivariés"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tShkj2wRIp6a"
      },
      "source": [
        "serie_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7cGUeWb5oe7"
      },
      "source": [
        "**1. Séparation des données en données pour l'entrainement et la validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob-EAw_j5oe8"
      },
      "source": [
        "On réserve 20% des données pour l'entrainement et le reste pour la validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5AWeK_Z5oe8"
      },
      "source": [
        "# Sépare les données en entrainement et tests\n",
        "pourcentage = 0.8\n",
        "temps_separation = int(len(serie_etude) * pourcentage)\n",
        "date_separation = serie_etude.index[temps_separation]\n",
        "\n",
        "serie_entrainement = []\n",
        "serie_test = []\n",
        "\n",
        "serie_entrainement.append(serie_etude['Open'].iloc[:temps_separation])\n",
        "serie_test.append(serie_etude['Open'].iloc[temps_separation:])\n",
        "\n",
        "serie_entrainement.append(serie_etude['Close'].iloc[:temps_separation])\n",
        "serie_test.append(serie_etude['Close'].iloc[temps_separation:])\n",
        "\n",
        "serie_entrainement.append(serie_etude['Low'].iloc[:temps_separation])\n",
        "serie_test.append(serie_etude['Low'].iloc[temps_separation:])\n",
        "\n",
        "serie_entrainement.append(serie_etude['High'].iloc[:temps_separation])\n",
        "serie_test.append(serie_etude['High'].iloc[temps_separation:])\n",
        "\n",
        "serie_entrainement.append(serie_etude['Volume_(BTC)'].iloc[:temps_separation])\n",
        "serie_test.append(serie_etude['Volume_(BTC)'].iloc[temps_separation:])\n",
        "\n",
        "\n",
        "print(\"Taille de l'entrainement : %d\" %len(serie_entrainement[0]))\n",
        "print(\"Taille de la validation : %d\" %len(serie_test[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZUMMMro5oe9"
      },
      "source": [
        "On normalise les données :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu_YxoSI5oe9"
      },
      "source": [
        "# Calcul de la moyenne et de l'écart type de la série\n",
        "mean = tf.math.reduce_mean(np.asarray(serie_entrainement))\n",
        "std = tf.math.reduce_std(np.asarray((serie_entrainement)))\n",
        "\n",
        "# Normalisation des données\n",
        "serie_entrainement = (serie_entrainement-mean)/std\n",
        "serie_test = (serie_test-mean)/std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_4OZJ-p5oe9"
      },
      "source": [
        "# Affiche la série\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "\n",
        "for serie in serie_entrainement:\n",
        "  ax.plot(serie, label=\"Entrainement\")\n",
        "\n",
        "#ax.plot(serie_test[0],label=\"Validation\")\n",
        "\n",
        "ax.set_title(\"Evolution du prix du BTC\")\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-bANnT35oe-"
      },
      "source": [
        "**2. Création des datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wqy52B5xSXDz"
      },
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "\n",
        "def prepare_dataset_XY(series, taille_fenetre, horizon, batch_size):\n",
        "  serie_concat = tf.expand_dims(series[0],1)\n",
        "\n",
        "  for i in range(1,len(series)):\n",
        "    serie_ = tf.expand_dims(series[i],1)\n",
        "    serie_concat = tf.concat([serie_concat,serie_],1)\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie_concat)\n",
        "  dataset = dataset.window(taille_fenetre+horizon, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre + horizon))\n",
        "  dataset = dataset.map(lambda x: (x[0:taille_fenetre],tf.expand_dims(x[-taille_fenetre:][:,0],1)))\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Jh1RZYo5oe_"
      },
      "source": [
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "taille_fenetre = 128\n",
        "horizon = 1\n",
        "batch_size = 1000\n",
        "\n",
        "# Création du dataset\n",
        "dataset = prepare_dataset_XY(serie_entrainement,taille_fenetre,horizon,batch_size)\n",
        "dataset_val = prepare_dataset_XY(serie_test,taille_fenetre,horizon,batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr2pbMox5oe_"
      },
      "source": [
        "print(len(list(dataset.as_numpy_iterator())))\n",
        "for element in dataset.take(1):\n",
        "  print(element[0].shape)\n",
        "  print(element[1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQejjkaeeHxe"
      },
      "source": [
        "for element in dataset.take(1):\n",
        "  print(element)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHCcYn6i5oe_"
      },
      "source": [
        "On extrait maintenant les deux tenseurs (X,Y) pour l'entrainement :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3ZhLIK15ofA"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset))              # #56x((1000,16,5),(1000,16,5)) => x = 56x(1000,16,5) ; y = 56x(1000,16,5)\n",
        "\n",
        "# Recombine les données\n",
        "x = np.asarray(x,dtype=np.float32)      # 56x(1000,16,5) => (56,1000,16,5)\n",
        "y = np.asarray(y,dtype=np.float32)      # 56x(1000,16,5) => (56,1000,16,5)\n",
        "\n",
        "x_train = np.asarray(tf.reshape(x,shape=(x.shape[0]*x.shape[1],taille_fenetre,x.shape[3])))     # (56,1000,16,5) => (56*1000,16,5)\n",
        "y_train = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],taille_fenetre,y.shape[3])))     # (56,1000,16,5) => (56*1000,16,5)\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llnKyLvl5ofA"
      },
      "source": [
        "Puis la même chose pour les données de validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hadrKVrZ5ofB"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset_val))              # #56x((1000,16,5),(1000,16,1)) => x = 56x(1000,16,5) ; y = 56x(1000,16,1)\n",
        "\n",
        "# Recombine les données\n",
        "x = np.asarray(x,dtype=np.float32)      # 56x(1000,16,5) => (56,1000,16,5)\n",
        "y = np.asarray(y,dtype=np.float32)      # 56x(1000,16,1) => (56,1000,16,1)\n",
        "\n",
        "x_val = np.asarray(tf.reshape(x,shape=(x.shape[0]*x.shape[1],taille_fenetre,x.shape[3])))     # (56,1000,16,5) => (56*1000,16,5)\n",
        "y_val = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],taille_fenetre,y.shape[3])))     # (56,1000,16,1) => (56*1000,16,1)\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHfiXLFZhrPs"
      },
      "source": [
        "# Création du modèle type Wavenet Multivarié"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd9AzWFtjnjs"
      },
      "source": [
        "**1. Création du réseau**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDVH4xsDzM-V"
      },
      "source": [
        "from keras.layers import Conv1D\n",
        "from keras.layers import Conv1D\n",
        "from keras.utils.conv_utils import conv_output_length\n",
        "from keras import layers\n",
        "from keras.regularizers import l2\n",
        "import keras.backend as K\n",
        "from keras.engine import Input\n",
        "from keras.engine import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCtJXABdzKB9"
      },
      "source": [
        "class CausalDilatedConv1D(Conv1D):\n",
        "    def __init__(self, nb_filter, filter_length, init='glorot_uniform', activation=None, weights=None,\n",
        "                 border_mode='valid', subsample_length=1, atrous_rate=1, W_regularizer=None, b_regularizer=None,\n",
        "                 activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True, causal=False, **kwargs):\n",
        "        super(CausalDilatedConv1D, self).__init__(nb_filter, filter_length, weights=weights, activation=activation, \n",
        "                padding=border_mode, strides=subsample_length, dilation_rate=atrous_rate, kernel_regularizer=W_regularizer, \n",
        "                bias_regularizer=b_regularizer, activity_regularizer=activity_regularizer, kernel_constraint=W_constraint, \n",
        "                bias_constraint=b_constraint, use_bias=bias, **kwargs)\n",
        "        self.causal = causal\n",
        "        self.nb_filter = nb_filter\n",
        "        self.atrous_rate = atrous_rate\n",
        "        self.filter_length = filter_length\n",
        "        self.subsample_length = subsample_length\n",
        "        self.border_mode = border_mode\n",
        "        if self.causal and border_mode != 'valid':\n",
        "            raise ValueError(\"Causal mode dictates border_mode=valid.\")\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        input_length = input_shape[1]\n",
        "        if self.causal:\n",
        "            input_length += self.atrous_rate * (self.filter_length - 1)\n",
        "        length = conv_output_length(input_length, self.filter_length, self.border_mode, self.strides[0], dilation=self.atrous_rate)\n",
        "        return (input_shape[0], length, self.nb_filter)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        if self.causal:\n",
        "            x = K.temporal_padding(x, padding=(self.atrous_rate * (self.filter_length - 1), 0))\n",
        "        # return super(CausalAtrousConvolution1D, self).call(x, mask)\n",
        "        return super(CausalDilatedConv1D, self).call(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s19zmYCgzTvV"
      },
      "source": [
        "def _compute_receptive_field(dilation_depth, stacks):\n",
        "  return stacks * (2 ** dilation_depth * 2) - (stacks - 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIX-o-lMzVK0"
      },
      "source": [
        "def build_model_residual_block(x, i, s,nb_filters, dim_filters, use_bias,res_l2):\n",
        "        original_x = x\n",
        "        # TODO: initalization, regularization?\n",
        "        relu_out = CausalDilatedConv1D(nb_filters, dim_filters, atrous_rate=2 ** i, border_mode='valid', causal=True, bias=True, name='dilated_conv_%d_relu_s%d' % (2 ** i, s), activation='relu', W_regularizer=l2(res_l2))(x)\n",
        "        res_x = layers.Conv1D(1, 1, padding='same', use_bias=use_bias, kernel_regularizer=l2(res_l2))(relu_out)\n",
        "        skip_x = layers.Conv1D(1, 1, padding='same', use_bias=use_bias, kernel_regularizer=l2(res_l2))(relu_out)\n",
        "        res_x = layers.Add()([original_x, res_x])\n",
        "        return res_x, skip_x\n",
        "\n",
        "def build_model_couche_condition(x, output_bins, nb_filters, dim_filters, use_bias,res_l2):\n",
        "        original_x = x\n",
        "        skip_conditions = []\n",
        "\n",
        "        relu_out = CausalDilatedConv1D(nb_filters, dim_filters, atrous_rate=1, border_mode='valid', causal=True, bias=True, name='dilated_conv_%d_condition_%d_s%d' % (1,1, 0), activation='relu',\n",
        "                                       W_regularizer=l2(res_l2))(tf.expand_dims(x[:,:,0],2))\n",
        "\n",
        "        skip_ = layers.Conv1D(1, 1, padding='same', use_bias=use_bias, kernel_regularizer=l2(res_l2))(relu_out)\n",
        "        skip_conditions.append(skip_)\n",
        "\n",
        "        for i in range(1,output_bins-1):\n",
        "          relu_out = CausalDilatedConv1D(nb_filters, dim_filters, atrous_rate=1, border_mode='valid', causal=True, bias=True, name='dilated_conv_%d_condition_%d_s%d' % (1,i+1,0), activation='relu',\n",
        "                                                    W_regularizer=l2(res_l2))(tf.expand_dims(x[:,:,i],2))\n",
        "\n",
        "          skip_ = layers.Conv1D(1, 1, padding='same', use_bias=use_bias, kernel_regularizer=l2(res_l2))(relu_out)\n",
        "          skip_conditions.append(skip_)\n",
        "\n",
        "        if output_bins > 1:\n",
        "          out = layers.Add()(skip_conditions)\n",
        "        else:\n",
        "          out = skip_\n",
        "        return out\n",
        "\n",
        "\n",
        "def build_model(fragment_length, nb_filters, dim_filters, output_bins, dilation_depth, stacks, use_skip_connections, use_bias, res_l2, final_l2):\n",
        "        input_shape = Input(shape=(fragment_length, output_bins), name='input_part')\n",
        "        out = input_shape\n",
        "        skip_connections = []\n",
        "\n",
        "        for s in range(stacks):\n",
        "            # Couche conditionnée\n",
        "            out = build_model_couche_condition(out, output_bins, nb_filters, dim_filters, use_bias, res_l2)\n",
        "\n",
        "            # Couches intermédiaires\n",
        "            for i in range(1, dilation_depth + 1):\n",
        "                out, skip_out = build_model_residual_block(out, i, s, nb_filters, dim_filters, use_bias, res_l2)\n",
        "                skip_connections.append(skip_out)\n",
        "\n",
        "        if use_skip_connections:\n",
        "            out = layers.Add()(skip_connections)\n",
        "\n",
        "        # Couche de sortie\n",
        "        out = layers.Activation('linear', name=\"output_linear\")(out)\n",
        "        out = layers.Conv1D(1, 1, padding='same', kernel_regularizer=l2(final_l2))(out)\n",
        "        model = Model(input_shape, out)\n",
        "        return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH9rsD5UzZgM"
      },
      "source": [
        "**2. Construction du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln6eVTFezYlq"
      },
      "source": [
        "def compute_receptive_field_(dilation_depth, nb_stacks):\n",
        "    receptive_field = nb_stacks * (2 ** dilation_depth * 2) - (nb_stacks - 1)\n",
        "    return receptive_field\n",
        "\n",
        "nb_filters = 10\n",
        "dim_filters = 20\n",
        "nb_output_bins = 5\n",
        "dilation_depth = 6\n",
        "nb_stacks = 1\n",
        "use_skip_connections = False\n",
        "use_bias = False\n",
        "res_l2 = 0\n",
        "final_l2 = 0\n",
        "\n",
        "fragment_length = compute_receptive_field_(dilation_depth, nb_stacks)\n",
        "fragment_length\n",
        "\n",
        "model = build_model(fragment_length, nb_filters, dim_filters, nb_output_bins, dilation_depth, nb_stacks, use_skip_connections, use_bias, res_l2, final_l2)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_azfJaeUo2nU"
      },
      "source": [
        "**2. Optimisation de l'apprentissage**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf3lwaQBnjxL"
      },
      "source": [
        "Pour accélérer le traitement des données, nous n'allons pas utiliser l'intégralité des données pendant la mise à jour du gradient, comme cela a été fait jusqu'à présent (en utilisant le dataset).  \n",
        "Cette fois-ci, nous allons forcer les mises à jour du gradient à se produire de manière moins fréquente en attribuant la valeur du batch_size à prendre en compte lors de la regression du modèle.  \n",
        "Pour cela, on utilise l'argument \"batch_size\" dans la méthode fit. En précisant un batch_size=1000, cela signifie que :\n",
        " - Sur notre total de 56000 échantillons, 56 seront utilisés pour les calculs du gradient\n",
        " - Il y aura également 56 itérations à chaque période.\n",
        "  \n",
        "    \n",
        "    \n",
        "Si nous avions pris le dataset comme entrée, nous aurions eu :\n",
        "- Un total de 56000 échantillons également\n",
        "- Chaque période aurait également pris 56 itérations pour se compléter\n",
        "- Mais 1000 échantillons auraient été utilisés pour le calcul du gradient, au lieu de 56 avec la méthode utilisée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6Z35rNWj5SA"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "def  My_MSE(y_true,y_pred):\n",
        "  return(tf.keras.metrics.mse(y_true,y_pred)*std.numpy()+mean.numpy())\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam()\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"mse\", optimizer=optimiseur, metrics=[\"mse\",My_MSE])\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint],batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2aP9J3TkNGG"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 1])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZxCgpuYkQ2Q"
      },
      "source": [
        "# Chargement des poids sauvegardés\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1N8pBiZz2Snz"
      },
      "source": [
        "y_train[:,:,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmdbo23qkTKE"
      },
      "source": [
        "max_periodes = 10000\n",
        "\n",
        "# Classe permettant d'arrêter l'entrainement si la variation\n",
        "# devient plus petite qu'une valeur à choisir sur un nombre\n",
        "# de périodes à choisir\n",
        "class StopTrain(keras.callbacks.Callback):\n",
        "    def __init__(self, delta=0.01,periodes=100, term=\"loss\", logs={}):\n",
        "      self.n_periodes = 0\n",
        "      self.periodes = periodes\n",
        "      self.loss_1 = 100\n",
        "      self.delta = delta\n",
        "      self.term = term\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "      diff_loss = abs(self.loss_1 - logs[self.term])\n",
        "      self.loss_1 = logs[self.term]\n",
        "      if (diff_loss < self.delta):\n",
        "        self.n_periodes = self.n_periodes + 1\n",
        "      else:\n",
        "        self.n_periodes = 0\n",
        "      if (self.n_periodes == self.periodes):\n",
        "        print(\"Arrêt de l'entrainement...\")\n",
        "        self.model.stop_training = True\n",
        "\n",
        "def  My_MSE(y_true,y_pred):\n",
        "  return(tf.keras.metrics.mse(y_true,y_pred)*std.numpy()+mean.numpy())\n",
        "  \n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.001,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"mse\", optimizer=optimiseur, metrics=[\"mse\",My_MSE])\n",
        "\n",
        "# Entraine le modèle, avec une réduction des calculs du gradient\n",
        "historique = model.fit(x=x_train,y=y_train,validation_data=(x_val,y_val), epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=1e-7,periodes = 10, term=\"My_MSE\")],batch_size=batch_size)\n",
        "\n",
        "# Entraine le modèle sans réduction de calculs\n",
        "#historique = model.fit(dataset,validation_data=dataset_val, epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=1e-8,periodes = 10, term=\"val_My_MSE\")])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfbomV0LS9LD"
      },
      "source": [
        "model.load_weights(\"poids_train.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MDY8O1-l6kN"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEuSDQ6vZnBm"
      },
      "source": [
        "# Evaluation du modèle\n",
        "\n",
        "model.evaluate(dataset)\n",
        "model.evaluate(dataset_val)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbzro22hgt4b"
      },
      "source": [
        "**3. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMmVn1e5zEAm"
      },
      "source": [
        "# Création des instants d'entrainement et de validation\n",
        "y_train_timing = serie_etude.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_train)]\n",
        "y_val_timing = serie_etude.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_val)]\n",
        "\n",
        "# Calcul des prédictions\n",
        "pred_ent = model.predict(x_train, verbose=1)\n",
        "pred_val = model.predict(x_val, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsT_RRS6iFhA"
      },
      "source": [
        "**4.Affichage sur une période de 1 heure**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoGhoCJWHOuj"
      },
      "source": [
        "Création d'une série contenant les valeurs originales et les prédictions, synchronisées dans le temps :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2xmissP6rXM"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbe originale\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index,y=serie_entrainement[0],line=dict(color='blue', width=1),name=\"Prix BTC\"))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index[taille_fenetre+horizon-1:],y=pred_ent[:,taille_fenetre-1,0],line=dict(color='green', width=1),name=\"Prix BTC\"))\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
