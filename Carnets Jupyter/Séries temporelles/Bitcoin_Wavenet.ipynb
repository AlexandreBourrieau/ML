{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bitcoin_Wavenet.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/Bitcoin_Wavenet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axF1UlzDDE0i"
      },
      "source": [
        "!pip install --upgrade tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sM0IvAJFDM0x"
      },
      "source": [
        "!pip install keras\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVuCzMYKDp3z"
      },
      "source": [
        "# Chargement des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3nLyKtnliCW"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eo2qVS1lrzuX"
      },
      "source": [
        "On télécharge un script depuis Github permettant de télécharger un fichier stocké sur GoogleDrive, puis on utilise ce script écrit en Python pour télécharger le fichier `bitcoin.zip`. Enfin, on décompresse les données pour obtenir le fichier `bitstampUSD_1-min_data_2012-01-01_to_2021-03-31.csv` :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sOdetggHqKE"
      },
      "source": [
        "# Récupération des données au format .csv\n",
        "\n",
        "!git clone https://github.com/chentinghao/download_google_drive.git\n",
        "!python download_google_drive/download_gdrive.py \"1FZsEdpBm-AQ2L9n_pMnm6336-O_IVo7z\" \"/content/bitcoin.zip\"\n",
        "!unzip bitcoin.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPJ6nnrRsUBm"
      },
      "source": [
        "Charge la série sous Pandas et affiche les informations du fichier :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEB_JjihEYTP"
      },
      "source": [
        "# Création de la série sous Pandas\n",
        "serie = pd.read_csv(\"bitstampUSD_1-min_data_2012-01-01_to_2021-03-31.csv\")\n",
        "serie"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF62FumBty9H"
      },
      "source": [
        "# Pré-traitement des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2oeGZgr1UUP"
      },
      "source": [
        "**1. Recherche des erreurs dans les données**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2d3HD1YwmIS"
      },
      "source": [
        "On commence par vérifier qu'il ne manque pas de dates. Pour cela, on vérifie qu'il y a bien 60 secondes entre deux Timestamp. Si on trouve un décalage non cohérent, on enregistre les informations dans une liste."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCXSGY9Swltw"
      },
      "source": [
        "# Fonction permettant de vérifier si chaque intervalle est bien de 60s\n",
        "def recherche_erreur(fenetre):\n",
        "  if fenetre.values[1] - fenetre.values[0] != 60:\n",
        "    Timestamp_Errors.append(fenetre.values)\n",
        "  return 0\n",
        "\n",
        "# Définit une liste pour sauvegarder le résultat des recherches\n",
        "Timestamp_Errors = []\n",
        "\n",
        "# Applique la fonction sur une fenêtre glissante des données\n",
        "serie.Timestamp.rolling(2).apply(recherche_erreur)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l1fTgb_1O8b"
      },
      "source": [
        "On affiche les erreurs trouvées :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZERkJS40sHP"
      },
      "source": [
        "# Affiche les informations sur les erreurs trouvées\n",
        "\n",
        "for erreur in Timestamp_Errors:\n",
        "  print (pd.to_datetime(Timestamp_Errors[0],unit=\"s\"))\n",
        "  print((Timestamp_Errors[0][1] - Timestamp_Errors[0][0])/60 - 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHOSRv8_1aYa"
      },
      "source": [
        "On observe qu'il manque des données entre le 5 janvier 2015 à 9:12:00 et le 9 janvier 2015 à 21:05:00, soit 6472 données."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tjbmho3h2nB7"
      },
      "source": [
        "Recherchons maintenant le nombre de données manquantes :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUzjd-qN2s-T"
      },
      "source": [
        "# Affichage du nombre total de données manquantes\n",
        "\n",
        "data_manquantes = sum(np.isnan(serie['Open']))\n",
        "print (\"Nombre de données manquantes : %s\" %data_manquantes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D41XKoGE3O60"
      },
      "source": [
        "On a donc en tout : 6472 + 1243608 = 1250080 données manquantes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GefH2TaM3a0S"
      },
      "source": [
        "**2. Identification des erreurs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XivWgphwsdIm"
      },
      "source": [
        "On convertit maintenant les `Timestamp` (mesure de temps exprimé en seconde écoulé depuis le 01/01/1970 - 00:00:00 UTC) en format plus standard :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdLSr2IWSOnX"
      },
      "source": [
        "# Conversion des timestamp en date\n",
        "serie.Timestamp = pd.to_datetime(serie['Timestamp'], unit=\"s\")\n",
        "serie"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgHcZyd13gCa"
      },
      "source": [
        "On demande maintenant à échantillonner les données sur 60 secondes :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVNlzo9QWNmP"
      },
      "source": [
        "# Echantillonnage de la série sur 1min\n",
        "serie_minute = serie.set_index('Timestamp').resample('60s').asfreq()\n",
        "\n",
        "# Récupère le nombre de données sans valeurs numériques\n",
        "data_manquantes = sum(np.isnan(serie_minute['Open']))\n",
        "\n",
        "# Affiche le nombre de données manquantes et la série sur 1min \n",
        "print (\"Nombre de données manquantes : %s\" %data_manquantes)\n",
        "serie_minute"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqFOnDlU3tey"
      },
      "source": [
        "On obtient en tout 4863849 données après échantillonnage, soit (4863849-4857377) =  6472 données supplémentaires. Ceci est cohérent avec ce qu'on avait trouvé avant. Il manque 1250080 données. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tkbi7uuBnUD3"
      },
      "source": [
        "**3. Correction des données**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IQuSqAknduG"
      },
      "source": [
        "Pour corriger les données, on va tout simplement utiliser la fonction [fillna](https://pandas.pydata.org/docs/reference/api/pandas.Series.fillna.html) de Pandas avec la fonctionnalité de type `backfill` :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2Oav6gin5aP"
      },
      "source": [
        "# Applique la fonction de remplissage automatique des données non numérique avec l'option backfill\n",
        "serie_minute = serie_minute.interpolate(method=\"slinear\")\n",
        "serie_minute = serie_minute.fillna(method=\"backfill\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfOvQ-BKn80v"
      },
      "source": [
        "# Récupère le nombre de données non numériques et affiche les informations\n",
        "\n",
        "data_manquantes = sum(np.isnan(serie_minute['Open']))\n",
        "print (\"Nombre de données manquantes : %s\" %data_manquantes)\n",
        "serie_minute"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v05rWWccJI26"
      },
      "source": [
        "**4. Affichage des données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1QKMBThNQni"
      },
      "source": [
        "# Affiche la série\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(serie_minute.index, serie_minute.Open)\n",
        "plt.title(\"Evolution du prix du BTC\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU7u1mA1E6jk"
      },
      "source": [
        "# Préparation des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhIs2GS7vu8k"
      },
      "source": [
        "Nous allons réaliser des modélisations sur la série journalière, et pour une période allant du 1er avril 2013 au 31 mars 2021."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeKrafzHv9gd"
      },
      "source": [
        "**1. Création de la série horaire pour la modélisation globale**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUlNO0pswBpr"
      },
      "source": [
        "On va réaliser des prédictions à l'aide d'une série à fréquence journalière. On commence par tenter d'estimer les données manquantes à l'aide d'une interpolation linéaire à l'aide de la fonction [interpolate](https://pandas.pydata.org/docs/reference/api/pandas.Series.interpolate.html#pandas.Series.interpolate) de Pandas, puis on complète avec la méthode `backfill` si nécessaire."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir4BkaUtuiXX"
      },
      "source": [
        "# Echantillonne la série sur 1 heure\n",
        "serie_heure = serie.set_index('Timestamp').resample('1H').asfreq()\n",
        "\n",
        "# Remplissage des données non numériques par interpolation linéraire\n",
        "serie_heure = serie_heure.interpolate(method=\"slinear\")\n",
        "\n",
        "# Remplissage des données non numériques restantes par backfill\n",
        "serie_heure = serie_heure.fillna(method=\"backfill\")\n",
        "\n",
        "# Affiche les informations\n",
        "data_manquantes = sum(np.isnan(serie_heure['Open']))\n",
        "print (\"Nombre de données manquantes : %s\" %data_manquantes)\n",
        "serie_heure"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epIrgQTvca6p"
      },
      "source": [
        "# Affiche la série\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(serie_heure.index, serie_heure.Open)\n",
        "plt.title(\"Evolution du prix du BTC\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hsbej4XvckFD"
      },
      "source": [
        "On construit une nouvelle série avec les dates retenues pour le début et la fin :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMiDRe1ZcqMR"
      },
      "source": [
        "# Définition des dates de début et de fin\n",
        "\n",
        "date_debut = \"2013-04-01 00:00:00\"\n",
        "date_fin = \"2021-03-31 00:00:00\"\n",
        "\n",
        "serie_etude = serie_heure.loc[date_debut:date_fin].copy()\n",
        "serie_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUD1yl1wxkla"
      },
      "source": [
        "serie_etude['x'] = np.linspace(0,serie_etude.index.size-1,serie_etude.index.size)\n",
        "serie_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwOeFLtLSPnv"
      },
      "source": [
        "**2. Détection des anomalies dans la série \"horaire\"**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1joYv2Kd7Js"
      },
      "source": [
        "Les anomalies sont fréquentes dans les séries temporelles, et la performance des prédictions est souvent améliorée lorsque ces anomalies sont traitées.  \n",
        "Pour avoir un apperçu de ces éventuelles anomalies, nous allons utiliser la méthode [\"Isolation Forest\"](https://scikit-learn.org/stable/modules/outlier_detection.html#isolation-forest) disponnible dans Scikit-learn.  \n",
        "\n",
        "Les paramètres utilisés sont les suivants :\n",
        " - **n_estimators** : C'est le nombre de sous-groupes d'échantillons à utiliser. Une valeur de 128 ou 256 est préconnisée dans le document de recherche.\n",
        " - **max_samples** : C'est le nombre d'échantillons maximum à utiliser. Nous utiliserons l'ensemble des échantillons.\n",
        " - **max_features** :  C'est le nombre de motifs aléatoirement choisis sur chaque noeud de l'arbre. Nous choisirons un seul motif.\n",
        " - **contamination** : C'est le pourcentage estimé d'anomalies dans les données. Ce paramètre permet de régler la sensibilité de l'algorithme. On va commencer avec 5% et affiner si nécessaire par la suite."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHag65S4dH7x"
      },
      "source": [
        "# Initialise le modèle\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "clf = IsolationForest(n_estimators=256,max_samples=serie_etude['Open'].size, contamination=0.05,max_features=1, verbose=1)\n",
        "clf.fit(serie_etude['Open'].values.reshape(-1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAPFfAaffb4h"
      },
      "source": [
        "# Réalise les prédictions\n",
        "pred = clf.predict(serie_etude['Open'].values.reshape(-1,1))\n",
        "pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU0TN1UEBqR2"
      },
      "source": [
        "On ajoute maintenant ces informations dans la série journalière et on affiche les informations :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWg0uUb9G5Ws"
      },
      "source": [
        "# Ajoute une colonne \"Anomalie\" dans la série\n",
        "serie_etude['Anomalies']=pred\n",
        "serie_etude['Anomalies'] = serie_etude['Anomalies'].apply(lambda x: 1 if (x==-1) else 0)\n",
        "serie_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "105qNoy1EwWd"
      },
      "source": [
        "# Affiche les informations sur les anomalies\n",
        "print(serie_etude['Anomalies'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaV_MfJyFXkF"
      },
      "source": [
        "**3. Affichage des anomalies sur le graphique**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2idYKYImFh8v"
      },
      "source": [
        "# Affiche la série\n",
        "\n",
        "fig = px.line(x=serie_etude.index,y=serie_etude['Open'],title=\"Evolution du prix du BTC\")\n",
        "fig.add_trace(px.scatter(x=serie_etude.index,y=serie_etude['Anomalies']*serie_etude['Open'],color=serie_etude['Anomalies'].astype(np.bool)).data[0])\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVAWVQioe-kS"
      },
      "source": [
        "Comme les anomalies détectées ne sembles pas cohérentes, nous n'allons pas les traiter..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vDYEK-cxE0C"
      },
      "source": [
        "# Analyse de la série"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGY4fCB3xdUx"
      },
      "source": [
        "serie_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBCbjPSNxWXy"
      },
      "source": [
        "**1. ACF & PACF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CleN4htOxYqZ"
      },
      "source": [
        "# ACF & PACF du bruit blanc\n",
        "\n",
        "serie = serie_etude['Open']\n",
        "\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "f1, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "f1.subplots_adjust(hspace=0.3,wspace=0.2)\n",
        "\n",
        "plot_acf(serie, ax=ax1, lags = range(0,50))\n",
        "ax1.set_title(\"Autocorrélation du bruit blanc\")\n",
        "\n",
        "plot_pacf(serie, ax=ax2, lags = range(0, 50))\n",
        "ax2.set_title(\"Autocorrélation partielle du bruit blanc\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o66MqHZTyHX5"
      },
      "source": [
        "**2. Test de Dickey-Fuller**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm5VXMhkyLSN"
      },
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "serie_test = serie\n",
        "\n",
        "adf, p, usedlag, nobs, cvs, aic = sm.tsa.stattools.adfuller(serie_test)\n",
        "\n",
        "adf_results_string = 'ADF: {}\\np-value: {},\\nN: {}, \\ncritical values: {}'\n",
        "print(adf_results_string.format(adf, p, nobs, cvs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moHyQgGfyTi4"
      },
      "source": [
        "**3. Suppression de la tendance non linéaire et test de sationnarité**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43AcGds6y0pI"
      },
      "source": [
        "from scipy.stats import boxcox\n",
        "\n",
        "serie_log, lam = boxcox(serie)\n",
        "\n",
        "f1, (ax1,ax2) = plt.subplots(2, 1, figsize=(15, 5))\n",
        "ax1.plot(serie_etude.index,serie_log)\n",
        "ax2.plot(serie_etude.index,serie)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fRPXCkO0DUh"
      },
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "serie_test = serie_log\n",
        "\n",
        "adf, p, usedlag, nobs, cvs, aic = sm.tsa.stattools.adfuller(serie_test)\n",
        "\n",
        "adf_results_string = 'ADF: {}\\np-value: {},\\nN: {}, \\ncritical values: {}'\n",
        "print(adf_results_string.format(adf, p, nobs, cvs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI8sp_Rlz6GT"
      },
      "source": [
        "***4. Suppression de la tendance linéaire et test de stationnarité***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iHrAWJH0TdT"
      },
      "source": [
        "f1, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 5))\n",
        "\n",
        "# Calcul des coefficients\n",
        "x = np.linspace(0,len(serie_log),len(serie_log))\n",
        "coefs = np.polyfit(x,serie_log,1)\n",
        "\n",
        "# Calcul de la tendance non linéaire\n",
        "trend = coefs[0]*np.power(x,1) + coefs[1]\n",
        "\n",
        "# Calcul de la série sans tendance\n",
        "serie_log_detrend = serie_log - trend\n",
        "\n",
        "# Affiche les résultats\n",
        "ax1.plot(trend)\n",
        "ax1.plot(serie_log)\n",
        "ax1.set_title(\"Série originale et tendance non linéaire\")\n",
        "\n",
        "ax2.plot(serie_log_detrend)\n",
        "ax2.set_title(\"Série avec tendance non linéaire supprimée\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNXs9Fm--Kcl"
      },
      "source": [
        "# ACF & PACF du bruit blanc\n",
        "\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "f1, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "f1.subplots_adjust(hspace=0.3,wspace=0.2)\n",
        "\n",
        "plot_acf(serie_log_detrend, ax=ax1, lags = range(0,50))\n",
        "ax1.set_title(\"Autocorrélation du bruit blanc\")\n",
        "\n",
        "plot_pacf(serie_log_detrend, ax=ax2, lags = range(0, 50))\n",
        "ax2.set_title(\"Autocorrélation partielle du bruit blanc\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_r01cDgq0xaJ"
      },
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "serie_test = serie_log_detrend\n",
        "\n",
        "adf, p, usedlag, nobs, cvs, aic = sm.tsa.stattools.adfuller(serie_test)\n",
        "\n",
        "adf_results_string = 'ADF: {}\\np-value: {},\\nN: {}, \\ncritical values: {}'\n",
        "print(adf_results_string.format(adf, p, nobs, cvs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ychdf1RxMPDD"
      },
      "source": [
        "**5. Différentiation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qaHkgtQMOqJ"
      },
      "source": [
        "# Différenciation d'odre 1 et saisonnale à l'odre 1 et de période 12\n",
        "\n",
        "from statsmodels.tsa.statespace.tools import diff\n",
        "\n",
        "serie_log_detrend_diff1 = diff(serie_log_detrend,1)       # diff=1 ; diff_saison=1 ; periode = 12\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(serie_log_detrend_diff1)\n",
        "plt.title(\"Signal différencié d'ordre 1 + saisonalité\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFLlzv0JMks5"
      },
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "serie_test = serie_log_detrend_diff1\n",
        "\n",
        "adf, p, usedlag, nobs, cvs, aic = sm.tsa.stattools.adfuller(serie_test)\n",
        "\n",
        "adf_results_string = 'ADF: {}\\np-value: {},\\nN: {}, \\ncritical values: {}'\n",
        "print(adf_results_string.format(adf, p, nobs, cvs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0Oqd_7XMqaZ"
      },
      "source": [
        "# ACF & PACF du bruit blanc\n",
        "\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "f1, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "f1.subplots_adjust(hspace=0.3,wspace=0.2)\n",
        "\n",
        "plot_acf(serie_log_detrend_diff1, ax=ax1, lags = range(0,50))\n",
        "ax1.set_title(\"Autocorrélation du bruit blanc\")\n",
        "\n",
        "plot_pacf(serie_log_detrend_diff1, ax=ax2, lags = range(0, 50))\n",
        "ax2.set_title(\"Autocorrélation partielle du bruit blanc\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5oIY2Yl5Tlt"
      },
      "source": [
        "**5. Enregistrement des données dans le dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjFSWhdeM4KM"
      },
      "source": [
        "serie_log_detrend_diff1 = np.insert(serie_log_detrend_diff1,0,0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ele3kFOp5TTW"
      },
      "source": [
        "serie_etude['diff'] = serie_log_detrend_diff1\n",
        "serie_etude['diff'][0] = \"Nan\"\n",
        "serie_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3q8CAozN-Gt"
      },
      "source": [
        "# Prépartion des datasets diff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWrUXYyFN-Gu"
      },
      "source": [
        "serie_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp724KcgN-Gw"
      },
      "source": [
        "**1. Séparation des données en données pour l'entrainement et la validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIWQZUvVN-Gw"
      },
      "source": [
        "On réserve 20% des données pour l'entrainement et le reste pour la validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZaGwL4YN-Gx"
      },
      "source": [
        "# Sépare les données en entrainement et tests\n",
        "pourcentage = 0.8\n",
        "temps_separation = int(len(serie_etude) * pourcentage)\n",
        "date_separation = serie_etude.index[temps_separation]\n",
        "\n",
        "serie_entrainement = serie_etude['diff'].iloc[1:temps_separation]\n",
        "serie_test = serie_etude['diff'].iloc[temps_separation:]\n",
        "\n",
        "print(\"Taille de l'entrainement : %d\" %len(serie_entrainement))\n",
        "print(\"Taille de la validation : %d\" %len(serie_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-YXKb5TN-Gy"
      },
      "source": [
        "On normalise les données :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcszCbQQN-Gy"
      },
      "source": [
        "# Calcul de la moyenne et de l'écart type de la série\n",
        "mean = tf.math.reduce_mean(np.asarray(serie_entrainement))\n",
        "std = tf.math.reduce_std(np.asarray((serie_entrainement)))\n",
        "\n",
        "# Normalisation des données\n",
        "serie_entrainement = (serie_entrainement-mean)/std\n",
        "serie_test = (serie_test-mean)/std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixzH25StN-Gz"
      },
      "source": [
        "# Affiche la série\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "ax.plot(serie_entrainement, label=\"Entrainement\")\n",
        "ax.plot(serie_test,label=\"Validation\")\n",
        "\n",
        "ax.set_title(\"Evolution du prix du BTC\")\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsIJ_4OON-Gz"
      },
      "source": [
        "**2. Création des datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlwKgWqiN-G0"
      },
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "\n",
        "def prepare_dataset_XY(serie, taille_fenetre, horizon, batch_size):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie)\n",
        "  dataset = dataset.window(taille_fenetre+horizon, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre + horizon))\n",
        "  dataset = dataset.map(lambda x: (tf.expand_dims(x[0:taille_fenetre],axis=1),x[-1:]))\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9qP2YSBN-G1"
      },
      "source": [
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "taille_fenetre = 500\n",
        "horizon = 1\n",
        "batch_size = 100\n",
        "\n",
        "# Création du dataset\n",
        "dataset = prepare_dataset_XY(serie_entrainement,taille_fenetre,horizon,batch_size)\n",
        "dataset_val = prepare_dataset_XY(serie_test,taille_fenetre,horizon,batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFRs-cLHN-HA"
      },
      "source": [
        "print(len(list(dataset.as_numpy_iterator())))\n",
        "for element in dataset.take(1):\n",
        "  print(element[0].shape)\n",
        "  print(element[1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ti9mbzgN-HA"
      },
      "source": [
        "On extrait maintenant les deux tenseurs (X,Y) pour l'entrainement :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIwGFoF7N-HB"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "#56x((1000,4,1),(1000,1)) => (56*1000,4,1) ; (56*1000,1)\n",
        "\n",
        "x,y = tuple(zip(*dataset))\n",
        "\n",
        "# Recombine les données\n",
        "# (56,1000,4,1) => (56*128,4,1)\n",
        "# (56,1000,1) => (56*128,1)\n",
        "x_train = np.asarray(tf.reshape(np.asarray(x,dtype=np.float32),shape=(np.asarray(x).shape[0]*np.asarray(x).shape[1],taille_fenetre,1)))\n",
        "y_train = np.asarray(tf.reshape(np.asarray(y,dtype=np.float32),shape=(np.asarray(y).shape[0]*np.asarray(y).shape[1])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4-eznwiN-HC"
      },
      "source": [
        "Puis la même chose pour les données de validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e25gJ8ymN-HC"
      },
      "source": [
        "# Extrait les X,Y du dataset_val\n",
        "\n",
        "x,y = tuple(zip(*dataset_val))\n",
        "\n",
        "# Recombine les données\n",
        "\n",
        "x_val = np.asarray(tf.reshape(np.asarray(x,dtype=np.float32),shape=(np.asarray(x).shape[0]*np.asarray(x).shape[1],taille_fenetre,1)))\n",
        "y_val = np.asarray(tf.reshape(np.asarray(y,dtype=np.float32),shape=(np.asarray(y).shape[0]*np.asarray(y).shape[1])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG5Fyx5O5oe5"
      },
      "source": [
        "# Prépartion des datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tShkj2wRIp6a"
      },
      "source": [
        "serie_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hVq4bTVIhYN"
      },
      "source": [
        "serie_etude['Open'] = serie_etude['Open']\n",
        "serie_etude['Open'] = serie_etude.interpolate(method=\"slinear\")\n",
        "serie_etude['Open'] = serie_etude.fillna(method=\"backfill\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7cGUeWb5oe7"
      },
      "source": [
        "**1. Séparation des données en données pour l'entrainement et la validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob-EAw_j5oe8"
      },
      "source": [
        "On réserve 20% des données pour l'entrainement et le reste pour la validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5AWeK_Z5oe8"
      },
      "source": [
        "# Sépare les données en entrainement et tests\n",
        "pourcentage = 0.8\n",
        "temps_separation = int(len(serie_etude) * pourcentage)\n",
        "date_separation = serie_etude.index[temps_separation]\n",
        "\n",
        "serie_entrainement = serie_etude['Open'].iloc[:temps_separation]\n",
        "serie_test = serie_etude['Open'].iloc[temps_separation:]\n",
        "\n",
        "print(\"Taille de l'entrainement : %d\" %len(serie_entrainement))\n",
        "print(\"Taille de la validation : %d\" %len(serie_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZUMMMro5oe9"
      },
      "source": [
        "On normalise les données :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu_YxoSI5oe9"
      },
      "source": [
        "# Calcul de la moyenne et de l'écart type de la série\n",
        "mean = tf.math.reduce_mean(np.asarray(serie_entrainement))\n",
        "std = tf.math.reduce_std(np.asarray((serie_entrainement)))\n",
        "\n",
        "# Normalisation des données\n",
        "serie_entrainement = (serie_entrainement-mean)/std\n",
        "serie_test = (serie_test-mean)/std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_4OZJ-p5oe9"
      },
      "source": [
        "# Affiche la série\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "ax.plot(serie_entrainement, label=\"Entrainement\")\n",
        "ax.plot(serie_test,label=\"Validation\")\n",
        "\n",
        "ax.set_title(\"Evolution du prix du BTC\")\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-bANnT35oe-"
      },
      "source": [
        "**2. Création des datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_EweLDJ5oe-"
      },
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "\n",
        "def prepare_dataset_XY(serie, taille_fenetre, horizon, batch_size):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie)\n",
        "  dataset = dataset.window(taille_fenetre+horizon, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre + horizon))\n",
        "  dataset = dataset.map(lambda x: (tf.expand_dims(x[0:taille_fenetre],axis=1),x[-1:]))\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Jh1RZYo5oe_"
      },
      "source": [
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "taille_fenetre = 50\n",
        "horizon = 1\n",
        "batch_size = 1000\n",
        "\n",
        "# Création du dataset\n",
        "dataset = prepare_dataset_XY(serie_entrainement,taille_fenetre,horizon,batch_size)\n",
        "dataset_val = prepare_dataset_XY(serie_test,taille_fenetre,horizon,batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr2pbMox5oe_"
      },
      "source": [
        "print(len(list(dataset.as_numpy_iterator())))\n",
        "for element in dataset.take(1):\n",
        "  print(element[0].shape)\n",
        "  print(element[1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHCcYn6i5oe_"
      },
      "source": [
        "On extrait maintenant les deux tenseurs (X,Y) pour l'entrainement :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3ZhLIK15ofA"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "#56x((1000,4,1),(1000,1)) => (56*1000,4,1) ; (56*1000,1)\n",
        "\n",
        "x,y = tuple(zip(*dataset))\n",
        "\n",
        "# Recombine les données\n",
        "# (56,1000,4,1) => (56*128,4,1)\n",
        "# (56,1000,1) => (56*128,1)\n",
        "x_train = np.asarray(tf.reshape(np.asarray(x,dtype=np.float32),shape=(np.asarray(x).shape[0]*np.asarray(x).shape[1],taille_fenetre,1)))\n",
        "y_train = np.asarray(tf.reshape(np.asarray(y,dtype=np.float32),shape=(np.asarray(y).shape[0]*np.asarray(y).shape[1])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llnKyLvl5ofA"
      },
      "source": [
        "Puis la même chose pour les données de validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hadrKVrZ5ofB"
      },
      "source": [
        "# Extrait les X,Y du dataset_val\n",
        "\n",
        "x,y = tuple(zip(*dataset_val))\n",
        "\n",
        "# Recombine les données\n",
        "\n",
        "x_val = np.asarray(tf.reshape(np.asarray(x,dtype=np.float32),shape=(np.asarray(x).shape[0]*np.asarray(x).shape[1],taille_fenetre,1)))\n",
        "y_val = np.asarray(tf.reshape(np.asarray(y,dtype=np.float32),shape=(np.asarray(y).shape[0]*np.asarray(y).shape[1])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "083QISTMM3AM"
      },
      "source": [
        "# Optimisation des hyperparamètres"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VzGM7ODMf8e"
      },
      "source": [
        "**1. Création de la série horaire pour l'optimisation des hyperparamètres**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKW0cGrbMl7u"
      },
      "source": [
        "# Définition des dates de début et de fin\n",
        "\n",
        "date_debut = \"2020-01-01 00:00:00\"\n",
        "date_fin = \"2021-03-31 00:00:00\"\n",
        "\n",
        "serie_opti = serie_etude['Open'].loc[date_debut:date_fin].copy()\n",
        "serie_opti"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJy04evcNBST"
      },
      "source": [
        "# Affiche la série\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(serie_opti)\n",
        "plt.title(\"Evolution du prix du BTC\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K5rZtb0Nc8-"
      },
      "source": [
        "**1. Préparation des données**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP9rTvLRhrsq"
      },
      "source": [
        "On normalise les données :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYoSqpbuhFHQ"
      },
      "source": [
        "# Calcul de la moyenne et de l'écart type de la série\n",
        "mean = tf.math.reduce_mean(np.asarray(serie_opti))\n",
        "std = tf.math.reduce_std(np.asarray((serie_opti)))\n",
        "\n",
        "# Normalisation des données\n",
        "serie_opti = (serie_opti-mean)/std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AB4yOw4Ohwvw"
      },
      "source": [
        "**2. Création du dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCBTKbgkhzL-"
      },
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "\n",
        "def prepare_dataset_XY(serie, taille_fenetre, horizon, batch_size):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie)\n",
        "  dataset = dataset.window(taille_fenetre+horizon, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre + horizon))\n",
        "  dataset = dataset.map(lambda x: (tf.expand_dims(x[0:taille_fenetre],axis=1),x[-1:]))\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "  return dataset\n",
        "\n",
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "taille_fenetre = 4\n",
        "horizon = 1\n",
        "batch_size = 32\n",
        "\n",
        "# Création du dataset\n",
        "dataset = prepare_dataset_XY(serie_entrainement,taille_fenetre,horizon,batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyzkXx5eh_To"
      },
      "source": [
        "print(len(list(dataset.as_numpy_iterator())))\n",
        "for element in dataset.take(1):\n",
        "  print(element[0].shape)\n",
        "  print(element[1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt06fAi0iz0m"
      },
      "source": [
        "On extriat maintenant les données X et les labels Y du dataset :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qt19cKylizLA"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "# 272x((32,4,1),(32,1)) => (272*32,4,1) ; (272*32,1)\n",
        "\n",
        "x,y = tuple(zip(*dataset))\n",
        "\n",
        "# Recombine les données\n",
        "# (272,32,4,1) => (272*32,4,1)\n",
        "# (272,32,1) => (272*32,1)\n",
        "x_train = np.asarray(tf.reshape(np.asarray(x,dtype=np.float32),shape=(np.asarray(x).shape[0]*np.asarray(x).shape[1],taille_fenetre,1)))\n",
        "y_train = np.asarray(tf.reshape(np.asarray(y,dtype=np.float32),shape=(np.asarray(y).shape[0]*np.asarray(y).shape[1],1)))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUpvJmTQiNk-"
      },
      "source": [
        "**3. Définition du modèle**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuAyb8pciUle"
      },
      "source": [
        "Dans le modèle, les paramètres dim_LSTM, l1_reg, l2_reg seront optimisés :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCFNM5i2iP0Q"
      },
      "source": [
        "def ModelLSTM(dim_LSTM = 10, l1_reg=0, l2_reg=0):\n",
        "\n",
        "  entrees = tf.keras.layers.Input(shape=(taille_fenetre,1))\n",
        "\n",
        "  # Encodeur\n",
        "  s_encodeur = tf.keras.layers.LSTM(dim_LSTM, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg,l2=l2_reg))(entrees)\n",
        "  \n",
        "  # Générateur\n",
        "  sortie = tf.keras.layers.Dense(1,kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg,l2=l2_reg))(s_encodeur)\n",
        "\n",
        "  # Construction du modèle\n",
        "  model = tf.keras.Model(entrees,sortie)\n",
        "  model.compile(loss='mse', optimizer='adam')\n",
        "  return(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYTuqrcYii9w"
      },
      "source": [
        "**4. Cross-validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVCNBdgcihuv"
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import KFold, TimeSeriesSplit, GridSearchCV\n",
        "\n",
        "# Définitions des paramètres\n",
        "dim_LSTM = [5,10,15,20,30,40]\n",
        "l1_reg = [0,0.001,0.01,0.1]\n",
        "l2_reg = [0,0.001,0.01,0.1]\n",
        "batch_size = [32]\n",
        "\n",
        "param_grid = {'dim_LSTM': dim_LSTM, 'l1_reg': l1_reg, 'l2_reg': l2_reg, 'batch_size': batch_size}\n",
        "param_grid = {'dim_LSTM': dim_LSTM, 'batch_size': batch_size}\n",
        "\n",
        "max_periodes = 5\n",
        "\n",
        "# Surveillance de l'entrainement\n",
        "es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=50, min_delta=1e-7, restore_best_weights=True)\n",
        "\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits = 5)\n",
        "model = KerasRegressor(build_fn=ModelLSTM, epochs=max_periodes, verbose=2)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=tscv, n_jobs=-1, verbose=3)\n",
        "\n",
        "grid_result = grid.fit(x_train, y_train,callbacks=[es])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWTWBh8DjJpP"
      },
      "source": [
        "# Affiche les résultats\n",
        "print(\"Meilleur résultat : %f avec %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params_ = grid_result.cv_results_['params']\n",
        "for mean, stdev, param_ in zip(means, stds, params_):\n",
        "  print(\"%f (%f) with %r\" % (mean, stdev, param_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHfiXLFZhrPs"
      },
      "source": [
        "# Création du modèle type CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd9AzWFtjnjs"
      },
      "source": [
        "**1. Création du réseau**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9OCzL7UjAhL"
      },
      "source": [
        "Par défaut, la dimension des vecteurs cachés est de 10 et aucune régularisation n'est utilisée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBcTrsq4Blpk"
      },
      "source": [
        "from keras.layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ym0qZ2PC8mMm"
      },
      "source": [
        "keras.\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.Input(shape=(taille_fenetre,1)))\n",
        "model.add(tf.keras.layers.Convolution1D.\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkZFImzm5LTq"
      },
      "source": [
        "nb_filtres = 10\n",
        "taille_filtre = 5\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.Input(shape=(taille_fenetre,1)))\n",
        "model.add(tf.keras.layers.Conv1D(filters=nb_filtres, kernel_size=taille_filtre, activation='relu'))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(1, activation='linear'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_azfJaeUo2nU"
      },
      "source": [
        "**2. Optimisation de l'apprentissage**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf3lwaQBnjxL"
      },
      "source": [
        "Pour accélérer le traitement des données, nous n'allons pas utiliser l'intégralité des données pendant la mise à jour du gradient, comme cela a été fait jusqu'à présent (en utilisant le dataset).  \n",
        "Cette fois-ci, nous allons forcer les mises à jour du gradient à se produire de manière moins fréquente en attribuant la valeur du batch_size à prendre en compte lors de la regression du modèle.  \n",
        "Pour cela, on utilise l'argument \"batch_size\" dans la méthode fit. En précisant un batch_size=1000, cela signifie que :\n",
        " - Sur notre total de 56000 échantillons, 56 seront utilisés pour les calculs du gradient\n",
        " - Il y aura également 56 itérations à chaque période.\n",
        "  \n",
        "    \n",
        "    \n",
        "Si nous avions pris le dataset comme entrée, nous aurions eu :\n",
        "- Un total de 56000 échantillons également\n",
        "- Chaque période aurait également pris 56 itérations pour se compléter\n",
        "- Mais 1000 échantillons auraient été utilisés pour le calcul du gradient, au lieu de 56 avec la méthode utilisée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6Z35rNWj5SA"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD()\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"mse\", optimizer=optimiseur, metrics=\"mse\")\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(x=x_train,y=y_train,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint],batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2aP9J3TkNGG"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 1])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZxCgpuYkQ2Q"
      },
      "source": [
        "# Chargement des poids sauvegardés\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmdbo23qkTKE"
      },
      "source": [
        "max_periodes = 10000\n",
        "\n",
        "# Classe permettant d'arrêter l'entrainement si la variation\n",
        "# devient plus petite qu'une valeur à choisir sur un nombre\n",
        "# de périodes à choisir\n",
        "class StopTrain(keras.callbacks.Callback):\n",
        "    def __init__(self, delta=0.01,periodes=100, term=\"loss\", logs={}):\n",
        "      self.n_periodes = 0\n",
        "      self.periodes = periodes\n",
        "      self.loss_1 = 100\n",
        "      self.delta = delta\n",
        "      self.term = term\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "      diff_loss = abs(self.loss_1 - logs[self.term])\n",
        "      self.loss_1 = logs[self.term]\n",
        "      if (diff_loss < self.delta):\n",
        "        self.n_periodes = self.n_periodes + 1\n",
        "      else:\n",
        "        self.n_periodes = 0\n",
        "      if (self.n_periodes == self.periodes):\n",
        "        print(\"Arrêt de l'entrainement...\")\n",
        "        self.model.stop_training = True\n",
        "\n",
        "def  My_MSE(y_true,y_pred):\n",
        "  return(tf.keras.metrics.mse(y_true,y_pred)*std.numpy()+mean.numpy())\n",
        "  \n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.001,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0.01)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(learning_rate=lr_schedule,momentum=0.9)\n",
        "\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"mse\", optimizer=optimiseur, metrics=[\"mse\",My_MSE])\n",
        "\n",
        "# Entraine le modèle, avec une réduction des calculs du gradient\n",
        "historique = model.fit(x=x_train,y=y_train,validation_data=(x_val,y_val), epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=1e-7,periodes = 10, term=\"My_MSE\")],batch_size=batch_size)\n",
        "\n",
        "# Entraine le modèle sans réduction de calculs\n",
        "#historique = model.fit(dataset,validation_data=dataset_val, epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=1e-8,periodes = 10, term=\"val_My_MSE\")])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfbomV0LS9LD"
      },
      "source": [
        "model.load_weights(\"poids_train.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MDY8O1-l6kN"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEuSDQ6vZnBm"
      },
      "source": [
        "# Evaluation du modèle\n",
        "\n",
        "model.evaluate(dataset)\n",
        "model.evaluate(dataset_val)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbzro22hgt4b"
      },
      "source": [
        "**3. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMmVn1e5zEAm"
      },
      "source": [
        "# Création des instants d'entrainement et de validation\n",
        "y_train_timing = serie_entrainement.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_train)]\n",
        "y_val_timing = serie_test.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_val)]\n",
        "\n",
        "# Calcul des prédictions\n",
        "pred_ent = model.predict(x_train, verbose=1)\n",
        "pred_val = model.predict(x_val, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsT_RRS6iFhA"
      },
      "source": [
        "**4.Affichage sur une période de 1 heure**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoGhoCJWHOuj"
      },
      "source": [
        "Création d'une série contenant les valeurs originales et les prédictions, synchronisées dans le temps :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBWDeZVBfynP"
      },
      "source": [
        "serie_btc_ent_ori = serie_entrainement*std+mean\n",
        "serie_btc_val_ori = serie_test*std+mean\n",
        "\n",
        "serie_btc_ent_pred = pd.Series(data=(pred_ent[:,0]*std+mean),index=y_train_timing)\n",
        "serie_btc_val_pred = pd.Series(data=(pred_val[:,0]*std+mean),index=y_val_timing)\n",
        "\n",
        "serie_btc_ori = pd.concat([serie_btc_ent_ori,serie_btc_val_ori])\n",
        "serie_btc_pred = pd.concat([serie_btc_ent_pred,serie_btc_val_pred])\n",
        "\n",
        "serie_btc_ori = serie_btc_ori.fillna(method=\"backfill\")\n",
        "serie_btc_pred = serie_btc_pred.fillna(method=\"backfill\")\n",
        "\n",
        "serie_btc_ent_ori = serie_btc_ent_ori.fillna(method=\"backfill\")\n",
        "serie_btc_val_ori = serie_btc_val_ori.fillna(method=\"backfill\")\n",
        "serie_btc_ent_pred = serie_btc_ent_pred.fillna(method=\"backfill\")\n",
        "serie_btc_val_pred = serie_btc_val_pred.fillna(method=\"backfill\")\n",
        "\n",
        "frame = {'BTC_ALL' : serie_btc_ori, 'BTC_ENT': serie_btc_ent_ori, 'BTC_VAL' : serie_btc_val_ori, 'BTC_PRED' : serie_btc_pred, 'BTC_PRED_ENT' : serie_btc_ent_pred, 'BTC_PRED_VAL':serie_btc_val_pred}\n",
        "df_resultats = pd.DataFrame(frame)\n",
        "df_resultats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pFnvAVy-3lM"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbe originale\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_ALL'],line=dict(color='blue', width=1),name=\"Prix BTC\"))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_ENT'],line=dict(color='green', width=1),name=\"Entrainement\"))\n",
        "\n",
        "# Courbe de validation\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_VAL'],line=dict(color='red', width=1),name=\"Validation\"))\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
