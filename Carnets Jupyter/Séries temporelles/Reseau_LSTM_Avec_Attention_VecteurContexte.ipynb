{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reseau_GRU_Avec_Attention_VecteurContexte.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/Reseau_LSTM_Avec_Attention_VecteurContexte.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubCeIvtF6R4W"
      },
      "source": [
        "Dans ce carnet nous allons mettre en place un modèle à réseau de neurones récurrent de type GRU associé à une **couche d'attention** comprenant un **vecteur contexte** pour réaliser des prédictions sur notre série temporelle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRhtHsNn5fc3"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFeah3y_6kif"
      },
      "source": [
        "# Création de la série temporelle et du dataset pour l'entrainement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJfSLtub6sdc"
      },
      "source": [
        "# Fonction permettant d'afficher une série temporelle\n",
        "def affiche_serie(temps, serie, format=\"-\", debut=0, fin=None, label=None):\n",
        "    plt.plot(temps[debut:fin], serie[debut:fin], format, label=label)\n",
        "    plt.xlabel(\"Temps\")\n",
        "    plt.ylabel(\"Valeur\")\n",
        "    if label:\n",
        "        plt.legend(fontsize=14)\n",
        "    plt.grid(True)\n",
        "\n",
        "# Fonction permettant de créer une tendance\n",
        "def tendance(temps, pente=0):\n",
        "    return pente * temps\n",
        "\n",
        "# Fonction permettant de créer un motif\n",
        "def motif_periodique(instants):\n",
        "    return (np.where(instants < 0.4,                            # Si les instants sont < 0.4\n",
        "                    np.cos(instants * 2 * np.pi),               # Alors on retourne la fonction cos(2*pi*t)\n",
        "                    1 / np.exp(3 * instants)))                  # Sinon, on retourne la fonction exp(-3t)\n",
        "\n",
        "# Fonction permettant de créer une saisonnalité avec un motif\n",
        "def saisonnalite(temps, periode, amplitude=1, phase=0):\n",
        "    \"\"\"Répétition du motif sur la même période\"\"\"\n",
        "    instants = ((temps + phase) % periode) / periode            # Mapping du temps =[0 1 2 ... 1460] => instants = [0.0 ... 1.0]\n",
        "    return amplitude * motif_periodique(instants)\n",
        "\n",
        "# Fonction permettant de générer du bruit gaussien N(0,1)\n",
        "def bruit_blanc(temps, niveau_bruit=1, graine=None):\n",
        "    rnd = np.random.RandomState(graine)\n",
        "    return rnd.randn(len(temps)) * niveau_bruit\n",
        "\n",
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "# au format X(X1,X2,...Xn) / Y(Y1,Y2,...,Yn)\n",
        "# X sont les données d'entrées du réseau\n",
        "# Y sont les labels\n",
        "\n",
        "def prepare_dataset_XY(serie, taille_fenetre, batch_size, buffer_melange):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie)\n",
        "  dataset = dataset.window(taille_fenetre+1, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre + 1))\n",
        "  dataset = dataset.shuffle(buffer_melange).map(lambda x: (x[:-1], x[-1:]))\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "# Création de la série temporelle\n",
        "temps = np.arange(4 * 365)                # temps = [0 1 2 .... 4*365] = [0 1 2 .... 1460]\n",
        "amplitude = 40                            # Amplitude de la la saisonnalité\n",
        "niveau_bruit = 5                          # Niveau du bruit\n",
        "offset = 10                               # Offset de la série\n",
        "\n",
        "serie = offset + tendance(temps, 0.1) + saisonnalite(temps, periode=365, amplitude=amplitude) + bruit_blanc(temps,niveau_bruit,graine=40)\n",
        "\n",
        "temps_separation = 1000\n",
        "\n",
        "# Extraction des temps et des données d'entrainement\n",
        "temps_entrainement = temps[:temps_separation]\n",
        "x_entrainement = serie[:temps_separation]\n",
        "\n",
        "# Exctraction des temps et des données de valiadation\n",
        "temps_validation = temps[temps_separation:]\n",
        "x_validation = serie[temps_separation:]\n",
        "\n",
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "taille_fenetre = 20\n",
        "batch_size = 32\n",
        "buffer_melange = 1000\n",
        "\n",
        "# Création du dataset X,Y\n",
        "dataset = prepare_dataset_XY(x_entrainement,taille_fenetre,batch_size,buffer_melange)\n",
        "\n",
        "# Création du dataset X,Y de validation\n",
        "dataset_Val = prepare_dataset_XY(x_validation,taille_fenetre,batch_size,buffer_melange)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6WVzU_X3JxG"
      },
      "source": [
        "# Calcul de la moyenne et de l'écart type de la série\n",
        "mean = tf.math.reduce_mean(serie)\n",
        "std = tf.math.reduce_std(serie)\n",
        "\n",
        "# Normalise les données\n",
        "Serie_Normalisee = (serie-mean)/std\n",
        "min = tf.math.reduce_min(serie)\n",
        "max = tf.math.reduce_max(serie)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYtsc0Yk3LhT"
      },
      "source": [
        "# Création des données pour l'entrainement et le test\n",
        "x_entrainement_norm = Serie_Normalisee[:temps_separation]\n",
        "x_validation_norm = Serie_Normalisee[temps_separation:]\n",
        "\n",
        "# Création du dataset X,Y\n",
        "dataset_norm = prepare_dataset_XY(x_entrainement_norm,taille_fenetre,batch_size,buffer_melange)\n",
        "\n",
        "# Création du dataset X,Y de validation\n",
        "dataset_Val_norm = prepare_dataset_XY(x_validation_norm,taille_fenetre,batch_size,buffer_melange)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Yt-EgZ3sgPY"
      },
      "source": [
        "# Création du modèle GRU avec couche d'attention possédant un vecteur de contexte"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyrcfKcgCsZ7"
      },
      "source": [
        "**1. Création du réseau et adaptation des formats d'entrée et de sortie**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeJyix8HK7Kt"
      },
      "source": [
        "Sous forme de shéma, notre réseau est donc le suivant :\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OZkfsmnBNHY"
      },
      "source": [
        "<img src=\"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/images/Attention_VecteurContexte1.png?raw=true\" width=\"1200\"> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kgTrJOQ5DUo"
      },
      "source": [
        "# Remise à zéro de tous les états générés par Keras\n",
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLNIAGDlBizT"
      },
      "source": [
        "On créé une classe dérivée de la classe [Layer](https://keras.io/api/layers/base_layer/#layer-class) de Keras. Les méthodes utilisées sont les suivantes :  \n",
        " - [build](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#build) : Permet de créer les variables utilisées par la couche (commes les poids et les offsets)\n",
        " - [call](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#call) : Permet d'implanter la logique de la couche"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3COaR59t5WzJ"
      },
      "source": [
        "<img src=\"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/images/Attention_VecteurContexte2.png?raw=true\" width=\"1200\"> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyoh5UpQXCqm"
      },
      "source": [
        "Parmi les nouvelles fonctions de Tensorflow et de Keras utilisées, on trouve :\n",
        "- [transpose](https://www.tensorflow.org/api_docs/python/tf/transpose) : Permet de transposer un tenseur et éventuellement de reconstituer l'ordre des axes avec l'argument `perm`\n",
        "- [add_weight](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#add_weight) : Méthode de la classe Layers de Keras, qui permet d'ajouter un paramètre (poids et offset ou autre) qui sera une variable mémoire pour la couche construite. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hhk0kmPSgqva"
      },
      "source": [
        "# Classe d'attention simple\n",
        "# Applique les poids d'attention sur les vecteurs de la couche récurrente\n",
        "\n",
        "# Importe le Backend de Keras\n",
        "from keras import backend as K\n",
        "\n",
        "# Définit une nouvelle classe Couche_Attention\n",
        "# Héritée de la classe Layer de Keras\n",
        "\n",
        "class Couche_Attention(tf.keras.layers.Layer):\n",
        "  # Fonction d'initialisation de la classe d'attention\n",
        "  def __init__(self,dim_att):\n",
        "    self.dim_att = dim_att          # Dimension du vecteur d'attention\n",
        "    super().__init__()              # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.W = self.add_weight(shape=(self.dim_att,input_shape[2]),initializer=\"normal\",name=\"W\")\n",
        "    self.b = self.add_weight(shape=(self.dim_att,1),initializer=\"zeros\",name=\"b\")\n",
        "    self.u = self.add_weight(shape=(self.dim_att,1),initializer=\"normal\",name=\"u\")\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Définit la logique de la couche d'attention\n",
        "  # Arguments :   x : Tenseur d'entrée de dimension (None, nbr_v,dim)\n",
        "  def call(self,x):\n",
        "    # Calcul de la matrice XH contenant les\n",
        "    # représentations cachées des vecteurs\n",
        "    # issus de la couche GRU\n",
        "    xt = tf.transpose(x,perm=[0,2,1])         # (None,20,40) => (None, 40,20)\n",
        "    Xh = tf.matmul(self.W,xt)                 # (#Att,40)x(None,40,20) = (None,#Att,20)\n",
        "    Xh = K.tanh(Xh + self.b)                  # Xh = (None,#Att,20)\n",
        "\n",
        "    # Calcul des poids d'attention normalisés\n",
        "    Xh = tf.transpose(Xh,perm=[0,2,1])        # Xh = (None,#Att,20) => (None,20,#Att)\n",
        "    a = tf.matmul(Xh,self.u)                  # (None,20,#Att)x(#Att,1) = (None,20,1)\n",
        "    a = tf.keras.activations.softmax(a,axis=1)\n",
        "\n",
        "    # Calcul du vecteur d'attention\n",
        "    xa = tf.multiply(x,a)                     # (None,20,40)_x_(None,20,1) = (None,20,40)\n",
        "    sortie = K.sum(xa,axis=1)                 # sortie = (None,40)\n",
        "    return sortie"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoT5DZoIkPvZ",
        "outputId": "461ec96c-6766-4420-e7ba-6dc3c50733d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dim_LSTM = 40\n",
        "dim_att = 100\n",
        "\n",
        "# Fonction de la couche lambda d'entrée\n",
        "def Traitement_Entrees(x):\n",
        "  return tf.expand_dims(x,axis=-1)\n",
        "\n",
        "# Définition de l'entrée du modèle\n",
        "entrees = tf.keras.layers.Input(shape=(taille_fenetre))\n",
        "\n",
        "# Encodeur\n",
        "e_adapt = tf.keras.layers.Lambda(Traitement_Entrees)(entrees)\n",
        "s_encodeur = tf.keras.layers.LSTM(dim_LSTM,return_sequences=True,recurrent_regularizer=tf.keras.regularizers.l2(1e-5))(e_adapt)\n",
        "s_attention = Couche_Attention(dim_att)(s_encodeur)\n",
        "\n",
        "# Décodeur\n",
        "s_decodeur = tf.keras.layers.Dense(40,activation=\"tanh\")(s_attention)\n",
        "s_decodeur = tf.keras.layers.Concatenate()([s_decodeur,s_attention])\n",
        "\n",
        "# Générateur\n",
        "sortie = tf.keras.layers.Dense(1)(s_decodeur)\n",
        "\n",
        "# Construction du modèle\n",
        "model = tf.keras.Model(entrees,sortie)\n",
        "\n",
        "model.save_weights(\"model_initial.hdf5\")\n",
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 20, 1)        0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 20, 40)       6720        lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "couche__attention (Couche_Atten (None, 40)           4200        lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 40)           1640        couche__attention[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 80)           0           dense[0][0]                      \n",
            "                                                                 couche__attention[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1)            81          concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 12,641\n",
            "Trainable params: 12,641\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUM0-SSXGLIQ"
      },
      "source": [
        "**2. Optimisation du taux d'apprentissage**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1Y-0nmL6mPe"
      },
      "source": [
        "# Charge les meilleurs poids\n",
        "model.load_weights(\"model_initial.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jejCBhXVuNQ4",
        "outputId": "8a8e6fec-97d3-4eb5-e577-572fe50227e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(lr=1e-8)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur, metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset_norm,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "30/30 [==============================] - 3s 12ms/step - loss: 0.4135 - mae: 0.7910\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.39582, saving model to poids.hdf5\n",
            "Epoch 2/100\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.3815 - mae: 0.7531\n",
            "\n",
            "Epoch 00002: loss improved from 0.39582 to 0.39268, saving model to poids.hdf5\n",
            "Epoch 3/100\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.4075 - mae: 0.7880\n",
            "\n",
            "Epoch 00003: loss did not improve from 0.39268\n",
            "Epoch 4/100\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.3981 - mae: 0.7732\n",
            "\n",
            "Epoch 00004: loss did not improve from 0.39268\n",
            "Epoch 5/100\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.3894 - mae: 0.7649\n",
            "\n",
            "Epoch 00005: loss did not improve from 0.39268\n",
            "Epoch 6/100\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.3909 - mae: 0.7698\n",
            "\n",
            "Epoch 00006: loss did not improve from 0.39268\n",
            "Epoch 7/100\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.3967 - mae: 0.7825\n",
            "\n",
            "Epoch 00007: loss did not improve from 0.39268\n",
            "Epoch 8/100\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.4034 - mae: 0.7770\n",
            "\n",
            "Epoch 00008: loss did not improve from 0.39268\n",
            "Epoch 9/100\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.3810 - mae: 0.7545\n",
            "\n",
            "Epoch 00009: loss did not improve from 0.39268\n",
            "Epoch 10/100\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.3900 - mae: 0.7649\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.39268\n",
            "Epoch 11/100\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.3958 - mae: 0.7719\n",
            "\n",
            "Epoch 00011: loss improved from 0.39268 to 0.39143, saving model to poids.hdf5\n",
            "Epoch 12/100\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.4003 - mae: 0.7775\n",
            "\n",
            "Epoch 00012: loss did not improve from 0.39143\n",
            "Epoch 13/100\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.4095 - mae: 0.7927\n",
            "\n",
            "Epoch 00013: loss improved from 0.39143 to 0.38824, saving model to poids.hdf5\n",
            "Epoch 14/100\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.3838 - mae: 0.7599\n",
            "\n",
            "Epoch 00014: loss did not improve from 0.38824\n",
            "Epoch 15/100\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.3851 - mae: 0.7595\n",
            "\n",
            "Epoch 00015: loss did not improve from 0.38824\n",
            "Epoch 16/100\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.4019 - mae: 0.7769\n",
            "\n",
            "Epoch 00016: loss did not improve from 0.38824\n",
            "Epoch 17/100\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.3909 - mae: 0.7664\n",
            "\n",
            "Epoch 00017: loss did not improve from 0.38824\n",
            "Epoch 18/100\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.3822 - mae: 0.7548\n",
            "\n",
            "Epoch 00018: loss did not improve from 0.38824\n",
            "Epoch 19/100\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.3920 - mae: 0.7695\n",
            "\n",
            "Epoch 00019: loss did not improve from 0.38824\n",
            "Epoch 20/100\n",
            "30/30 [==============================] - 1s 13ms/step - loss: 0.4108 - mae: 0.7975\n",
            "\n",
            "Epoch 00020: loss did not improve from 0.38824\n",
            "Epoch 21/100\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.4001 - mae: 0.7802\n",
            "\n",
            "Epoch 00021: loss did not improve from 0.38824\n",
            "Epoch 22/100\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.4288 - mae: 0.8140\n",
            "\n",
            "Epoch 00022: loss did not improve from 0.38824\n",
            "Epoch 23/100\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.3981 - mae: 0.7797\n",
            "\n",
            "Epoch 00023: loss did not improve from 0.38824\n",
            "Epoch 24/100\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.3925 - mae: 0.7700\n",
            "\n",
            "Epoch 00024: loss did not improve from 0.38824\n",
            "Epoch 25/100\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.3897 - mae: 0.7647\n",
            "\n",
            "Epoch 00025: loss did not improve from 0.38824\n",
            "Epoch 26/100\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.4150 - mae: 0.7951\n",
            "\n",
            "Epoch 00026: loss did not improve from 0.38824\n",
            "Epoch 27/100\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.3965 - mae: 0.7725\n",
            "\n",
            "Epoch 00027: loss did not improve from 0.38824\n",
            "Epoch 28/100\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.3920 - mae: 0.7662\n",
            "\n",
            "Epoch 00028: loss did not improve from 0.38824\n",
            "Epoch 29/100\n",
            "19/30 [==================>...........] - ETA: 0s - loss: 0.3957 - mae: 0.7686"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1_WMNlzu2B4"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 0.4])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XdXh_b0GP_F"
      },
      "source": [
        "**3. Entrainement du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80pEtJ10wIfY"
      },
      "source": [
        "# Charge les meilleurs poids\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16ujUiELwR33"
      },
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "class TimingCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, logs={}):\n",
        "        self.n_steps = 0\n",
        "        self.t_step = 0\n",
        "        self.n_batch = 0\n",
        "        self.total_batch = 0\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.starttime = timer()\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.t_step = self.t_step  + timer()-self.starttime\n",
        "        self.n_steps = self.n_steps + 1\n",
        "        if (self.total_batch == 0):\n",
        "          self.total_batch=self.n_batch - 1\n",
        "    def on_train_batch_begin(self,batch,logs=None):\n",
        "      self.n_batch= self.n_batch + 1\n",
        "    def GetInfos(self):\n",
        "      return([self.t_step/(self.n_steps*self.total_batch), self.t_step, self.total_batch])\n",
        "\n",
        "cb = TimingCallback()\n",
        "\n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.1,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0.1)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(learning_rate=lr_schedule,momentum=0.9)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_entrainement.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur,metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle\n",
        "historique = model.fit(dataset_norm,validation_data=dataset_Val_norm, epochs=500,verbose=1, callbacks=[CheckPoint,cb])\n",
        "\n",
        "# Affiche quelques informations sur les timings\n",
        "infos = cb.GetInfos()\n",
        "print(\"Step time : %.3f\" %infos[0])\n",
        "print(\"Total time : %.3f\" %infos[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COP9u4yitvmw"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o2zh6N9t1b7"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement[400:500])),erreur_entrainement[400:500], label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement[400:500])),erreur_validation[400:500], label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6Gq2CkeGR_1"
      },
      "source": [
        "**4. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRxXtHRXGXfF"
      },
      "source": [
        "taille_fenetre = 20\n",
        "\n",
        "# Création d'une liste vide pour recevoir les prédictions\n",
        "predictions = []\n",
        "\n",
        "# Calcul des prédiction pour chaque groupe de 20 valeurs consécutives de la série\n",
        "# dans l'intervalle de validation\n",
        "for t in temps[temps_separation:-taille_fenetre]:\n",
        "    X = np.reshape(Serie_Normalisee[t:t+taille_fenetre],(1,taille_fenetre))\n",
        "    predictions.append(model.predict(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd2nfDcgG8EO"
      },
      "source": [
        "# Affiche la série et les prédictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps,serie,label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title('Prédictions avec le modèle GRU + Attention avec vecteur contexte')\n",
        "plt.show()\n",
        "\n",
        "# Zoom sur l'intervalle de validation\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps[temps_separation:],serie[temps_separation:],label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title(\"Prédictions avec le modèle GRU + Attention avec vecteur contexte (zoom sur l'intervalle de validation)\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDS7BJvZG_e1"
      },
      "source": [
        "# Calcule de l'erreur quadratique moyenne et de l'erreur absolue moyenne \n",
        "\n",
        "mae = tf.keras.metrics.mean_absolute_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "mse = tf.keras.metrics.mean_squared_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "\n",
        "print(mae)\n",
        "print(mse)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}