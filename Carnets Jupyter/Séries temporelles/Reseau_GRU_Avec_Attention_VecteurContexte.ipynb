{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reseau_GRU_Avec_Attention_VecteurContexte.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM3ryAdVKqr6UiaNLfC9Zko",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/Reseau_GRU_Avec_Attention_VecteurContexte.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubCeIvtF6R4W"
      },
      "source": [
        "Dans ce carnet nous allons mettre en place un modèle à réseau de neurones récurrent de type GRU associé à une **couche d'attention** comprenant un **vecteur contexte** pour réaliser des prédictions sur notre série temporelle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRhtHsNn5fc3"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFeah3y_6kif"
      },
      "source": [
        "# Création de la série temporelle et du dataset pour l'entrainement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJfSLtub6sdc"
      },
      "source": [
        "# Fonction permettant d'afficher une série temporelle\r\n",
        "def affiche_serie(temps, serie, format=\"-\", debut=0, fin=None, label=None):\r\n",
        "    plt.plot(temps[debut:fin], serie[debut:fin], format, label=label)\r\n",
        "    plt.xlabel(\"Temps\")\r\n",
        "    plt.ylabel(\"Valeur\")\r\n",
        "    if label:\r\n",
        "        plt.legend(fontsize=14)\r\n",
        "    plt.grid(True)\r\n",
        "\r\n",
        "# Fonction permettant de créer une tendance\r\n",
        "def tendance(temps, pente=0):\r\n",
        "    return pente * temps\r\n",
        "\r\n",
        "# Fonction permettant de créer un motif\r\n",
        "def motif_periodique(instants):\r\n",
        "    return (np.where(instants < 0.4,                            # Si les instants sont < 0.4\r\n",
        "                    np.cos(instants * 2 * np.pi),               # Alors on retourne la fonction cos(2*pi*t)\r\n",
        "                    1 / np.exp(3 * instants)))                  # Sinon, on retourne la fonction exp(-3t)\r\n",
        "\r\n",
        "# Fonction permettant de créer une saisonnalité avec un motif\r\n",
        "def saisonnalite(temps, periode, amplitude=1, phase=0):\r\n",
        "    \"\"\"Répétition du motif sur la même période\"\"\"\r\n",
        "    instants = ((temps + phase) % periode) / periode            # Mapping du temps =[0 1 2 ... 1460] => instants = [0.0 ... 1.0]\r\n",
        "    return amplitude * motif_periodique(instants)\r\n",
        "\r\n",
        "# Fonction permettant de générer du bruit gaussien N(0,1)\r\n",
        "def bruit_blanc(temps, niveau_bruit=1, graine=None):\r\n",
        "    rnd = np.random.RandomState(graine)\r\n",
        "    return rnd.randn(len(temps)) * niveau_bruit\r\n",
        "\r\n",
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\r\n",
        "# au format X(X1,X2,...Xn) / Y(Y1,Y2,...,Yn)\r\n",
        "# X sont les données d'entrées du réseau\r\n",
        "# Y sont les labels\r\n",
        "\r\n",
        "def prepare_dataset_XY(serie, taille_fenetre, batch_size, buffer_melange):\r\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie)\r\n",
        "  dataset = dataset.window(taille_fenetre+1, shift=1, drop_remainder=True)\r\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre + 1))\r\n",
        "  dataset = dataset.shuffle(buffer_melange).map(lambda x: (x[:-1], x[-1:]))\r\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\r\n",
        "  return dataset\r\n",
        "\r\n",
        "\r\n",
        "# Création de la série temporelle\r\n",
        "temps = np.arange(4 * 365)                # temps = [0 1 2 .... 4*365] = [0 1 2 .... 1460]\r\n",
        "amplitude = 40                            # Amplitude de la la saisonnalité\r\n",
        "niveau_bruit = 5                          # Niveau du bruit\r\n",
        "offset = 10                               # Offset de la série\r\n",
        "\r\n",
        "serie = offset + tendance(temps, 0.1) + saisonnalite(temps, periode=365, amplitude=amplitude) + bruit_blanc(temps,niveau_bruit)\r\n",
        "\r\n",
        "temps_separation = 1000\r\n",
        "\r\n",
        "# Extraction des temps et des données d'entrainement\r\n",
        "temps_entrainement = temps[:temps_separation]\r\n",
        "x_entrainement = serie[:temps_separation]\r\n",
        "\r\n",
        "# Exctraction des temps et des données de valiadation\r\n",
        "temps_validation = temps[temps_separation:]\r\n",
        "x_validation = serie[temps_separation:]\r\n",
        "\r\n",
        "# Définition des caractéristiques du dataset que l'on souhaite créer\r\n",
        "taille_fenetre = 20\r\n",
        "batch_size = 32\r\n",
        "buffer_melange = 1000\r\n",
        "\r\n",
        "# Création du dataset X,Y\r\n",
        "dataset = prepare_dataset_XY(serie,taille_fenetre,batch_size,buffer_melange)\r\n",
        "\r\n",
        "# Création du dataset X,Y de validation\r\n",
        "dataset_Val = prepare_dataset_XY(x_validation,taille_fenetre,batch_size,buffer_melange)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Yt-EgZ3sgPY"
      },
      "source": [
        "# Création du modèle GRU avec couche d'attention possédant un vecteur de contexte"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyrcfKcgCsZ7"
      },
      "source": [
        "**1. Création du réseau et adaptation des formats d'entrée et de sortie**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeJyix8HK7Kt"
      },
      "source": [
        "Sous forme de shéma, notre réseau est donc le suivant :\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OZkfsmnBNHY"
      },
      "source": [
        "<img src=\"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/images/Attention_VecteurContexte1.png?raw=true\" width=\"1200\"> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kgTrJOQ5DUo"
      },
      "source": [
        "# Remise à zéro de tous les états générés par Keras\r\n",
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLNIAGDlBizT"
      },
      "source": [
        "On créé une classe dérivée de la classe [Layer](https://keras.io/api/layers/base_layer/#layer-class) de Keras. Les méthodes utilisées sont les suivantes :  \r\n",
        " - [build](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#build) : Permet de créer les variables utilisées par la couche (commes les poids et les offsets)\r\n",
        " - [call](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#call) : Permet d'implanter la logique de la couche"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3COaR59t5WzJ"
      },
      "source": [
        "<img src=\"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/images/Attention_VecteurContexte2.png?raw=true\" width=\"1200\"> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyoh5UpQXCqm"
      },
      "source": [
        "Parmi les nouvelles fonctions de Tensorflow et de Keras utilisées, on trouve :\r\n",
        "- [transpose](https://www.tensorflow.org/api_docs/python/tf/transpose) : Permet de transposer un tenseur et éventuellement de reconstituer l'ordre des axes avec l'argument `perm`\r\n",
        "- [add_weight](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#add_weight) : Méthode de la classe Layers de Keras, qui permet d'ajouter un paramètre (poids et offset ou autre) qui sera une variable mémoire pour la couche construite. \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hhk0kmPSgqva"
      },
      "source": [
        "# Classe d'attention simple\r\n",
        "# Applique les poids d'attention sur les vecteurs de la couche récurrente\r\n",
        "\r\n",
        "# Importe le Backend de Keras\r\n",
        "from keras import backend as K\r\n",
        "\r\n",
        "# Définit une nouvelle classe Couche_Attention\r\n",
        "# Héritée de la classe Layer de Keras\r\n",
        "\r\n",
        "class Couche_Attention(tf.keras.layers.Layer):\r\n",
        "  # Fonction d'initialisation de la classe d'attention\r\n",
        "  def __init__(self,dim_att):\r\n",
        "    self.dim_att = dim_att          # Dimension du vecteur d'attention\r\n",
        "    super().__init__()              # Appel du __init__() de la classe Layer\r\n",
        "  \r\n",
        "  def build(self,input_shape):\r\n",
        "    self.W = self.add_weight(shape=(self.dim_att,input_shape[2]),initializer=\"normal\",name=\"W\")\r\n",
        "    self.b = self.add_weight(shape=(self.dim_att,1),initializer=\"zeros\",name=\"b\")\r\n",
        "    self.u = self.add_weight(shape=(self.dim_att,1),initializer=\"normal\",name=\"u\")\r\n",
        "    super().build(input_shape)        # Appel de la méthode build()\r\n",
        "\r\n",
        "  # Définit la logique de la couche d'attention\r\n",
        "  # Arguments :   x : Tenseur d'entrée de dimension (None, nbr_v,dim)\r\n",
        "  def call(self,x):\r\n",
        "    # Calcul de la matrice XH contenant les\r\n",
        "    # représentations cachées des vecteurs\r\n",
        "    # issus de la couche GRU\r\n",
        "    x = tf.transpose(x,perm=[0,2,1])          # x = (None, dim,20)\r\n",
        "    Xh = K.dot(self.W,x)                      # Xh = (dim_att,None,20)\r\n",
        "    Xh = tf.transpose(Xh,perm=[1,0,2])        # Xh = (None, dim_att,20)\r\n",
        "    Xh = Xh + tf.expand_dims(self.b,axis=0)   # Xh = (None, dim_att,20) + (None, dim_att,1)\r\n",
        "    Xh = K.tanh(Xh)                           # Xh = (None, dim_att,20)\r\n",
        "\r\n",
        "    # Calcul des poids d'attention normalisés\r\n",
        "    Xh = tf.transpose(Xh,perm=[0,2,1])        # Xh = (None,20,dim_att)\r\n",
        "    a = K.dot(Xh,self.u)                      # a = (None,20,1)\r\n",
        "    a = tf.keras.activations.softmax(a,axis=1)\r\n",
        "\r\n",
        "    # Calcul du vecteur d'attention\r\n",
        "    xa = tf.multiply(Xh,a)                    # xa = (None,20,dim)\r\n",
        "    sortie = K.sum(xa,axis=1)                 # sortie = (None,40)\r\n",
        "    return sortie"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTqdYAsF_ici",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd25a4b8-92ab-4752-e713-d2acf86d2d7a"
      },
      "source": [
        "dim_GRU = 40\r\n",
        "\r\n",
        "# Fonction de la couche lambda d'entrée\r\n",
        "def Traitement_Entrees(x):\r\n",
        "  return tf.expand_dims(x,axis=-1)\r\n",
        "\r\n",
        "# Fonction dela couche lambda de sortie\r\n",
        "def Traitement_Sorties(x):\r\n",
        "  return(x*100.0)\r\n",
        "\r\n",
        "model = tf.keras.models.Sequential()\r\n",
        "model.add(tf.keras.Input(shape=(taille_fenetre,)))\r\n",
        "model.add(tf.keras.layers.Lambda(Traitement_Entrees))\r\n",
        "model.add(tf.keras.layers.GRU(dim_GRU,return_sequences=True))\r\n",
        "model.add(Couche_Attention(dim_GRU))\r\n",
        "model.add(tf.keras.layers.Dense(1))\r\n",
        "model.add(tf.keras.layers.Lambda(Traitement_Sorties))\r\n",
        "\r\n",
        "model.save_weights('model_initial.h5')\r\n",
        "\r\n",
        "model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda (Lambda)              (None, 20, 1)             0         \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, 20, 40)            5160      \n",
            "_________________________________________________________________\n",
            "couche__attention (Couche_At (None, 40)                1680      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 41        \n",
            "_________________________________________________________________\n",
            "lambda_1 (Lambda)            (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 6,881\n",
            "Trainable params: 6,881\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUM0-SSXGLIQ"
      },
      "source": [
        "**2. Optimisation du taux d'apprentissage**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jejCBhXVuNQ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b00c7b83-c56a-4e51-ab3d-8a23d6958c6a"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\r\n",
        "def RegulationTauxApprentissage(periode, taux):\r\n",
        "  return 1e-8*10**(periode/10)\r\n",
        "\r\n",
        "# Définition de l'optimiseur à utiliser\r\n",
        "optimiseur=tf.keras.optimizers.Adam()\r\n",
        "\r\n",
        "# Utilisation de la méthode ModelCheckPoint\r\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\r\n",
        "\r\n",
        "# Compile le modèle\r\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur, metrics=\"mae\")\r\n",
        "\r\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\r\n",
        "historique = model.fit(dataset,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "45/45 [==============================] - 2s 9ms/step - loss: 85.2918 - mae: 85.7917\n",
            "\n",
            "Epoch 00001: loss improved from inf to 91.99272, saving model to poids.hdf5\n",
            "Epoch 2/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 85.4956 - mae: 85.9953\n",
            "\n",
            "Epoch 00002: loss improved from 91.99272 to 91.98437, saving model to poids.hdf5\n",
            "Epoch 3/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 85.1618 - mae: 85.6617\n",
            "\n",
            "Epoch 00003: loss improved from 91.98437 to 91.97383, saving model to poids.hdf5\n",
            "Epoch 4/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 87.2952 - mae: 87.7946\n",
            "\n",
            "Epoch 00004: loss improved from 91.97383 to 91.96048, saving model to poids.hdf5\n",
            "Epoch 5/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 84.6971 - mae: 85.1971\n",
            "\n",
            "Epoch 00005: loss improved from 91.96048 to 91.94359, saving model to poids.hdf5\n",
            "Epoch 6/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 83.7884 - mae: 84.2875\n",
            "\n",
            "Epoch 00006: loss improved from 91.94359 to 91.92264, saving model to poids.hdf5\n",
            "Epoch 7/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 85.2588 - mae: 85.7587\n",
            "\n",
            "Epoch 00007: loss improved from 91.92264 to 91.89610, saving model to poids.hdf5\n",
            "Epoch 8/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 86.2449 - mae: 86.7447\n",
            "\n",
            "Epoch 00008: loss improved from 91.89610 to 91.86254, saving model to poids.hdf5\n",
            "Epoch 9/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 84.7086 - mae: 85.2072\n",
            "\n",
            "Epoch 00009: loss improved from 91.86254 to 91.82029, saving model to poids.hdf5\n",
            "Epoch 10/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 84.2664 - mae: 84.7663\n",
            "\n",
            "Epoch 00010: loss improved from 91.82029 to 91.76701, saving model to poids.hdf5\n",
            "Epoch 11/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 85.0381 - mae: 85.5370\n",
            "\n",
            "Epoch 00011: loss improved from 91.76701 to 91.70004, saving model to poids.hdf5\n",
            "Epoch 12/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 85.5517 - mae: 86.0516\n",
            "\n",
            "Epoch 00012: loss improved from 91.70004 to 91.61585, saving model to poids.hdf5\n",
            "Epoch 13/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 85.3652 - mae: 85.8650\n",
            "\n",
            "Epoch 00013: loss improved from 91.61585 to 91.50977, saving model to poids.hdf5\n",
            "Epoch 14/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 84.1545 - mae: 84.6543\n",
            "\n",
            "Epoch 00014: loss improved from 91.50977 to 91.37644, saving model to poids.hdf5\n",
            "Epoch 15/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 85.9042 - mae: 86.4037\n",
            "\n",
            "Epoch 00015: loss improved from 91.37644 to 91.20855, saving model to poids.hdf5\n",
            "Epoch 16/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 83.6826 - mae: 84.1821\n",
            "\n",
            "Epoch 00016: loss improved from 91.20855 to 90.99779, saving model to poids.hdf5\n",
            "Epoch 17/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 84.8534 - mae: 85.3530\n",
            "\n",
            "Epoch 00017: loss improved from 90.99779 to 90.73200, saving model to poids.hdf5\n",
            "Epoch 18/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 85.4248 - mae: 85.9248\n",
            "\n",
            "Epoch 00018: loss improved from 90.73200 to 90.39884, saving model to poids.hdf5\n",
            "Epoch 19/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 81.9329 - mae: 82.4327\n",
            "\n",
            "Epoch 00019: loss improved from 90.39884 to 89.97964, saving model to poids.hdf5\n",
            "Epoch 20/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 82.6292 - mae: 83.1288\n",
            "\n",
            "Epoch 00020: loss improved from 89.97964 to 89.45217, saving model to poids.hdf5\n",
            "Epoch 21/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 82.2479 - mae: 82.7468\n",
            "\n",
            "Epoch 00021: loss improved from 89.45217 to 88.79034, saving model to poids.hdf5\n",
            "Epoch 22/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 82.0392 - mae: 82.5385\n",
            "\n",
            "Epoch 00022: loss improved from 88.79034 to 87.96139, saving model to poids.hdf5\n",
            "Epoch 23/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 81.0463 - mae: 81.5460\n",
            "\n",
            "Epoch 00023: loss improved from 87.96139 to 86.92273, saving model to poids.hdf5\n",
            "Epoch 24/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 79.9703 - mae: 80.4695\n",
            "\n",
            "Epoch 00024: loss improved from 86.92273 to 85.62590, saving model to poids.hdf5\n",
            "Epoch 25/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 77.2760 - mae: 77.7759\n",
            "\n",
            "Epoch 00025: loss improved from 85.62590 to 84.00254, saving model to poids.hdf5\n",
            "Epoch 26/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 75.7593 - mae: 76.2593\n",
            "\n",
            "Epoch 00026: loss improved from 84.00254 to 81.97233, saving model to poids.hdf5\n",
            "Epoch 27/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 73.9964 - mae: 74.4961\n",
            "\n",
            "Epoch 00027: loss improved from 81.97233 to 79.44484, saving model to poids.hdf5\n",
            "Epoch 28/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 69.9800 - mae: 70.4795\n",
            "\n",
            "Epoch 00028: loss improved from 79.44484 to 76.34565, saving model to poids.hdf5\n",
            "Epoch 29/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 66.6162 - mae: 67.1154\n",
            "\n",
            "Epoch 00029: loss improved from 76.34565 to 72.58852, saving model to poids.hdf5\n",
            "Epoch 30/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 64.8127 - mae: 65.3083\n",
            "\n",
            "Epoch 00030: loss improved from 72.58852 to 68.20993, saving model to poids.hdf5\n",
            "Epoch 31/100\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 56.9651 - mae: 57.4629\n",
            "\n",
            "Epoch 00031: loss improved from 68.20993 to 63.80959, saving model to poids.hdf5\n",
            "Epoch 32/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 54.0541 - mae: 54.5500\n",
            "\n",
            "Epoch 00032: loss improved from 63.80959 to 59.53421, saving model to poids.hdf5\n",
            "Epoch 33/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 50.4275 - mae: 50.9265\n",
            "\n",
            "Epoch 00033: loss improved from 59.53421 to 55.42286, saving model to poids.hdf5\n",
            "Epoch 34/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 45.7399 - mae: 46.2396\n",
            "\n",
            "Epoch 00034: loss improved from 55.42286 to 51.02010, saving model to poids.hdf5\n",
            "Epoch 35/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 42.2917 - mae: 42.7917\n",
            "\n",
            "Epoch 00035: loss improved from 51.02010 to 45.89888, saving model to poids.hdf5\n",
            "Epoch 36/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 37.0221 - mae: 37.5195\n",
            "\n",
            "Epoch 00036: loss improved from 45.89888 to 39.99009, saving model to poids.hdf5\n",
            "Epoch 37/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 31.6042 - mae: 32.1002\n",
            "\n",
            "Epoch 00037: loss improved from 39.99009 to 34.54066, saving model to poids.hdf5\n",
            "Epoch 38/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 28.5593 - mae: 29.0573\n",
            "\n",
            "Epoch 00038: loss improved from 34.54066 to 30.25077, saving model to poids.hdf5\n",
            "Epoch 39/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 23.0393 - mae: 23.5363\n",
            "\n",
            "Epoch 00039: loss improved from 30.25077 to 24.86856, saving model to poids.hdf5\n",
            "Epoch 40/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 17.5095 - mae: 18.0059\n",
            "\n",
            "Epoch 00040: loss improved from 24.86856 to 19.13984, saving model to poids.hdf5\n",
            "Epoch 41/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 13.2421 - mae: 13.7329\n",
            "\n",
            "Epoch 00041: loss improved from 19.13984 to 14.06044, saving model to poids.hdf5\n",
            "Epoch 42/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 9.8728 - mae: 10.3608\n",
            "\n",
            "Epoch 00042: loss improved from 14.06044 to 10.37168, saving model to poids.hdf5\n",
            "Epoch 43/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 8.3338 - mae: 8.8218\n",
            "\n",
            "Epoch 00043: loss improved from 10.37168 to 8.18328, saving model to poids.hdf5\n",
            "Epoch 44/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 7.0765 - mae: 7.5516\n",
            "\n",
            "Epoch 00044: loss improved from 8.18328 to 7.13108, saving model to poids.hdf5\n",
            "Epoch 45/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 6.9225 - mae: 7.4012\n",
            "\n",
            "Epoch 00045: loss improved from 7.13108 to 6.74766, saving model to poids.hdf5\n",
            "Epoch 46/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 6.6232 - mae: 7.1038\n",
            "\n",
            "Epoch 00046: loss improved from 6.74766 to 6.70203, saving model to poids.hdf5\n",
            "Epoch 47/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 6.7154 - mae: 7.1974\n",
            "\n",
            "Epoch 00047: loss improved from 6.70203 to 6.63667, saving model to poids.hdf5\n",
            "Epoch 48/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 6.6243 - mae: 7.1043\n",
            "\n",
            "Epoch 00048: loss did not improve from 6.63667\n",
            "Epoch 49/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 6.6255 - mae: 7.1017\n",
            "\n",
            "Epoch 00049: loss improved from 6.63667 to 6.55749, saving model to poids.hdf5\n",
            "Epoch 50/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 6.4685 - mae: 6.9520\n",
            "\n",
            "Epoch 00050: loss improved from 6.55749 to 6.31587, saving model to poids.hdf5\n",
            "Epoch 51/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 6.7580 - mae: 7.2367\n",
            "\n",
            "Epoch 00051: loss did not improve from 6.31587\n",
            "Epoch 52/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 6.6785 - mae: 7.1646\n",
            "\n",
            "Epoch 00052: loss did not improve from 6.31587\n",
            "Epoch 53/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 6.8550 - mae: 7.3387\n",
            "\n",
            "Epoch 00053: loss did not improve from 6.31587\n",
            "Epoch 54/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 7.0575 - mae: 7.5425\n",
            "\n",
            "Epoch 00054: loss did not improve from 6.31587\n",
            "Epoch 55/100\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 7.9304 - mae: 8.4171\n",
            "\n",
            "Epoch 00055: loss did not improve from 6.31587\n",
            "Epoch 56/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 7.3621 - mae: 7.8441\n",
            "\n",
            "Epoch 00056: loss did not improve from 6.31587\n",
            "Epoch 57/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 11.7791 - mae: 12.2691\n",
            "\n",
            "Epoch 00057: loss did not improve from 6.31587\n",
            "Epoch 58/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 8.6410 - mae: 9.1294\n",
            "\n",
            "Epoch 00058: loss did not improve from 6.31587\n",
            "Epoch 59/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 7.7806 - mae: 8.2657\n",
            "\n",
            "Epoch 00059: loss did not improve from 6.31587\n",
            "Epoch 60/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 10.5220 - mae: 11.0136\n",
            "\n",
            "Epoch 00060: loss did not improve from 6.31587\n",
            "Epoch 61/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 9.5521 - mae: 10.0404\n",
            "\n",
            "Epoch 00061: loss did not improve from 6.31587\n",
            "Epoch 62/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 8.1110 - mae: 8.5990\n",
            "\n",
            "Epoch 00062: loss did not improve from 6.31587\n",
            "Epoch 63/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 9.2184 - mae: 9.7092\n",
            "\n",
            "Epoch 00063: loss did not improve from 6.31587\n",
            "Epoch 64/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 13.6024 - mae: 14.0946\n",
            "\n",
            "Epoch 00064: loss did not improve from 6.31587\n",
            "Epoch 65/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 11.2999 - mae: 11.7898\n",
            "\n",
            "Epoch 00065: loss did not improve from 6.31587\n",
            "Epoch 66/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 10.5133 - mae: 11.0037\n",
            "\n",
            "Epoch 00066: loss did not improve from 6.31587\n",
            "Epoch 67/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 17.5500 - mae: 18.0468\n",
            "\n",
            "Epoch 00067: loss did not improve from 6.31587\n",
            "Epoch 68/100\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 13.3448 - mae: 13.8339\n",
            "\n",
            "Epoch 00068: loss did not improve from 6.31587\n",
            "Epoch 69/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 16.2407 - mae: 16.7347\n",
            "\n",
            "Epoch 00069: loss did not improve from 6.31587\n",
            "Epoch 70/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 14.8592 - mae: 15.3518\n",
            "\n",
            "Epoch 00070: loss did not improve from 6.31587\n",
            "Epoch 71/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 20.7323 - mae: 21.2282\n",
            "\n",
            "Epoch 00071: loss did not improve from 6.31587\n",
            "Epoch 72/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 22.3000 - mae: 22.7971\n",
            "\n",
            "Epoch 00072: loss did not improve from 6.31587\n",
            "Epoch 73/100\n",
            "45/45 [==============================] - 1s 9ms/step - loss: 47.4611 - mae: 47.9583\n",
            "\n",
            "Epoch 00073: loss did not improve from 6.31587\n",
            "Epoch 74/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 78.3584 - mae: 78.8570\n",
            "\n",
            "Epoch 00074: loss did not improve from 6.31587\n",
            "Epoch 75/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 324.7840 - mae: 325.2840\n",
            "\n",
            "Epoch 00075: loss did not improve from 6.31587\n",
            "Epoch 76/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 167.5329 - mae: 168.0326\n",
            "\n",
            "Epoch 00076: loss did not improve from 6.31587\n",
            "Epoch 77/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 308.5719 - mae: 309.0716\n",
            "\n",
            "Epoch 00077: loss did not improve from 6.31587\n",
            "Epoch 78/100\n",
            "45/45 [==============================] - 1s 9ms/step - loss: 584.6825 - mae: 585.1825\n",
            "\n",
            "Epoch 00078: loss did not improve from 6.31587\n",
            "Epoch 79/100\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 520.0794 - mae: 520.5794\n",
            "\n",
            "Epoch 00079: loss did not improve from 6.31587\n",
            "Epoch 80/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 476.1846 - mae: 476.6846\n",
            "\n",
            "Epoch 00080: loss did not improve from 6.31587\n",
            "Epoch 81/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 732.9598 - mae: 733.4597\n",
            "\n",
            "Epoch 00081: loss did not improve from 6.31587\n",
            "Epoch 82/100\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 450.7467 - mae: 451.2467\n",
            "\n",
            "Epoch 00082: loss did not improve from 6.31587\n",
            "Epoch 83/100\n",
            "45/45 [==============================] - 1s 9ms/step - loss: 443.2090 - mae: 443.7089\n",
            "\n",
            "Epoch 00083: loss did not improve from 6.31587\n",
            "Epoch 84/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 892.6758 - mae: 893.1758\n",
            "\n",
            "Epoch 00084: loss did not improve from 6.31587\n",
            "Epoch 85/100\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 1672.7794 - mae: 1673.2794\n",
            "\n",
            "Epoch 00085: loss did not improve from 6.31587\n",
            "Epoch 86/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 1863.7326 - mae: 1864.2324\n",
            "\n",
            "Epoch 00086: loss did not improve from 6.31587\n",
            "Epoch 87/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 3050.9702 - mae: 3051.4702\n",
            "\n",
            "Epoch 00087: loss did not improve from 6.31587\n",
            "Epoch 88/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 2287.7483 - mae: 2288.2483\n",
            "\n",
            "Epoch 00088: loss did not improve from 6.31587\n",
            "Epoch 89/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 3477.0055 - mae: 3477.5055\n",
            "\n",
            "Epoch 00089: loss did not improve from 6.31587\n",
            "Epoch 90/100\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 4755.7567 - mae: 4756.2567\n",
            "\n",
            "Epoch 00090: loss did not improve from 6.31587\n",
            "Epoch 91/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 6121.8878 - mae: 6122.3878\n",
            "\n",
            "Epoch 00091: loss did not improve from 6.31587\n",
            "Epoch 92/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 4396.3571 - mae: 4396.8571\n",
            "\n",
            "Epoch 00092: loss did not improve from 6.31587\n",
            "Epoch 93/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 9252.6279 - mae: 9253.1279\n",
            "\n",
            "Epoch 00093: loss did not improve from 6.31587\n",
            "Epoch 94/100\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 5393.3591 - mae: 5393.8591\n",
            "\n",
            "Epoch 00094: loss did not improve from 6.31587\n",
            "Epoch 95/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 9487.6470 - mae: 9488.1470\n",
            "\n",
            "Epoch 00095: loss did not improve from 6.31587\n",
            "Epoch 96/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 8898.8757 - mae: 8899.3757\n",
            "\n",
            "Epoch 00096: loss did not improve from 6.31587\n",
            "Epoch 97/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 18493.9243 - mae: 18494.4243\n",
            "\n",
            "Epoch 00097: loss did not improve from 6.31587\n",
            "Epoch 98/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 23291.0123 - mae: 23291.5123\n",
            "\n",
            "Epoch 00098: loss did not improve from 6.31587\n",
            "Epoch 99/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 32076.2789 - mae: 32076.7789\n",
            "\n",
            "Epoch 00099: loss did not improve from 6.31587\n",
            "Epoch 100/100\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 83457.9352 - mae: 83458.4352\n",
            "\n",
            "Epoch 00100: loss did not improve from 6.31587\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1_WMNlzu2B4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "dceff4a0-3027-4bde-962c-6730d80307e8"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \r\n",
        "taux = 1e-8*(10**(np.arange(100)/10))\r\n",
        "\r\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\r\n",
        "plt.figure(figsize=(10, 6))\r\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\r\n",
        "plt.axis([ taux[0], taux[99], 0, 100])\r\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, \"Evolution de l'erreur en fonction du taux d'apprentissage\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAF5CAYAAABdt2RhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeXxbV5n/8c8jeV+S2IkdO7Gzp9nTLUtL93RLF9oCLbRlaNkpP7YZlin8gKEDdFgGhh/QYaBQhtJ9oZRSuu9r0ixN2qRZmqTZN2exYzu2ZEnn94euUyW1403WleTv+/XSy5KudPVc3cfSo3POPdecc4iIiIhI3wX8DkBEREQkW6iwEhEREUkSFVYiIiIiSaLCSkRERCRJVFiJiIiIJIkKKxEREZEkUWElacPMnJlN6OVzTzOzNcmOqZPX2mhm5/TieWea2db+iCnTmNkpZva2mTWZ2WUpfN3fmtl3U/A6WbGvs2U7jmRmHzWzJ/yOQ7KTCivpMa+waPG+FNsvN6U4hsOKMOfci865SamMoa+893GM33H45PvATc65Eufcg/3xAmb2cTN7KfE+59x1zrkf9MfrJUtHcaeLTMxZMxvjfV7ktN/nnLvDOXeen3FJ9srp+iEiHXq/c+4pv4MYiMwsxzkX6eq+PqzfAHPOxZKxvk6MBlb24/olSyQzt0VSQS1WkjRmlm9m9WY2PeG+Cq91q9K7/RkzW2dm+8zsITMb0cm6njOzTyfcPvQr3sxe8O5e7rWWfeTILgszm+Kto97MVprZJQnL/mRm/21m/zCzRjNbaGbjj7JdHzOzTWa218y+fcSygJl908zWe8vvNbPyHr517e/dz8xss5nt8rqsCr1lZ5rZVjO73sx2Av9rZjeY2f1mdruZHQA+bmaDzewWM9thZtvM7IdmFvTWcYOZ3Z7weof9ivfeqxvN7GXgIDCugxhHmNlfzKzOzN4xsy8nLLvB2/Y/e+/pSjOb1cm2rvfW/3dv/+V7637Iy4t1ZvaZ7q7bzGrN7AEvrr1mdpOZTQF+C5zsvUa999g/mdkPE57baT567891Fu+yrPdyxjrZpkJv3fvN7C1g9hHLD2thPTKOhPs7i/siM3vdzA6Y2RYzuyHhOe/prrOE7moze8TMfp6w7G4z+2NvtuOIxx4tpvb8+qyZbfdy8usJy9vz9x5vny41s2OPiP96M3sDaDazHDM7ycxe8fbFcjM7M+Hxz5nZD8zsZW99T5jZMG9x++dFvfeenmyHf56Ymf3CzHZ72/KmeZ9hZnahmb3lrXNb+zaYWZmZPezl3H7vek1CPGPN7AXveU95uZP4/9fptkgWcM7pokuPLsBG4JxOlv0RuDHh9heAx7zr84A9wAlAPvBr4IWExzpggnf9OeDTCcs+DrzU0WO922cCW73rucA64P8Ced7rNgKTvOV/AvYCc4i32t4B3N3J9kwFmoDTvZj/C4i0bz/wFWABUOMt/x1wVyfrOhRjB8t+ATwElAOlwN+BHyU8LwL8xHuNQuAGoA24jPgPpELgr97rFwOVwGvA57x13ADcnvB6Y7z3MCfh/d4MTPPek9wj4gsAS4B/897TccAG4PyE9bcCFwJB4EfAgu7mEPEvv98ABcBxQB0wr6t1e7eXe+9fsff8UzvKmYR9/8Me5OPDwBBglBfT/E6258fAi97+qwVWJO5r3puvh+LoYF0dxX0mMMPbDzOBXcBlneVV4vsLVAG7ve39qLffSnuzHT2IaYy3zXd5+2WG9/61x3QD8fy9nPj/69eBd/Dyzot/mRdDITCS+P/shd7rnevdrkjI3/XAMd7jnwN+3FGuH/keA+cTz+0hgAFTgGpv2Q7gNO96GXCCd30o8CGgiPj/633AgwnrfxX4GfH/lVOBA3j/f11tiy6Zf/E9AF0y7+J96DUB9QmXz3jLzgHWJzz2ZeAa7/otwE8TlpV4H65jvNvJKqxOA3YCgYTldwE3eNf/BPwhYdmFwOpOtvXfSCi6iH9JhHn3C2IVcHbC8mpvm3I6WNehGI+434BmYHzCfScD7yQ8LwwUJCy/gcOLgOFACChMuO8q4NmEx3dVWH3/KPt8LrD5iPu+BfxvwvqfSlg2FWjpIofa38NaIErClz3x4ulPXa3be5/qOnm/D8uZhH3fXlh1Jx9PTVh+L/DNTrZnAwlFF/BZklhYdfCY/wf8orO84r2F64eALcQLyVOPst6jbkcPYmrPr8kJy38K3JKwTxckLAtweBGzEfhkwvLrgduOeL3HgWsT8vc7Ccv+D+/+oGuPpbPCah6wFjiJhM8Mb9lm4HPAoC62/Thgv3d9FPEfQkUJy2/n3cLqqNuiS+Zf1BUovXWZc25IwuX33v3PAkVmNtfig1yPI96SAjAC2NS+AudcE/FfaiOTHNsIYIs7fIzQpiNeZ2fC9YPEv1Q7XVf7DedcM/GY240G/uo16dcTL7SixAud7qog/st3ScJ6HvPub1fnnGs94nlbEq6PJv7Lf0fCOn5HvOWqu7YcZdloYET7ur31/18O384j39MCSxgwfBQjgH3OucaE+7raX+3rrgU2ud6NwelOPvYqTxLXmwze/9OzXtdTA3AdMKyr5yX4O/HWvTXOuaMNjO/2dnQzpiPXNaKjZd7/6tbOlhPPvyuOyL9Tif+QadfdfXUY59wzwE3AfwO7zexmMxvkLf4Q8R9em8zseTM72dv2IjP7ncWHCBwg3uI6xOJd7+35fLAP2yIZTIWVJJVzLkr8l/1V3uXhhC/M7cQ/VAAws2LiTerbOlhVM/Fio11VD8LYDtSaWWJ+j+rkdbqyg/iXNxD/QCUec7stwAVHFJkFzrmevNYeoAWYlrCOwc65xC8G18HzEu/bQrzFaljCOgY556Z5y7vzfnb0Gonrf+eI7Sx1zl3Y5dZ1bTtQbmalCfd1d39tAUZ1UsAdbXvaX7e7+diVw/KEePyJDtL9fO4o7juJdxXXOucGEx+H1T7e67B96325Vxzx/BuJF/3VZnbVUV67q+3obkztjlzX9o6Wef+rNUcsPzK/bzsi/4qdcz8+SnwdrafjBzj3K+fcicRbQ48BvuHdv8g5dynxHygPEv9sA/gaMAmY65wbRHyoAMS3fwfxfE7c34nvQ1+2RTKACivpD3cCHyE+nuPOhPvvAj5hZseZWT7wH8BC59zGDtaxDPig98twAvCpI5bvooMB1p6FxL/I/tXMcr2Boe8H7u7FttwPXGxmp5pZHvFpAhL/b34L3Ghmo+HQYP1Le/IC3q/13wO/sHcH+Y80s/N7sI4dwBPAz81skMUH1Y83szO8hywDTjezUWY2mHg3Xk+8BjR6A4oLzSxoZtPNrNPBzT2IfQvwCvAjMysws5nE9/ftR3/mobh2AD82s2Lv+ad4y3YBNd5+60hP8rEr9wLf8gY11wBfOmL5MuBq732bD5zxnjW8q6O4S4m3grSa2Rzg6oRla4m34F1kZrnAd4iPGQPAzE4HPgFcA1wL/NrMOmsl7mo7Eh0tpnbf9f6Hp3kx3JOw7EQz+6BXFP8z8R8GCzp5rduB95vZ+d57WGDxQfs1nTw+UR0Qo5PPCzOb7bW+5RIvUluBmJnlWXy+q8HOuTbi46TaW8FLif8Yqrf4wSrfa1+fc24TsBi4wVvHycQ/f5KxLZIBVFhJb7Uf0dV+ae/uwzm3kPgH1Ajg0YT7nwK+C/yF+JfheODKTtb/C+LjinYBtxIfYJ7oBuBWryn9w4kLnHNh4h9kFxBvDfoN8XFeq3u6kc65lcQH4N/pxbyfeJdFu18S/9X+hJk1Ev9imNvT1yE+7mIdsMDrWniK+C/inriG+GDZt7w478frXnDOPUn8S+0N4gN1H+7Jir2WyIuJd+2+Q/x9/QMwuIcxduYq4mNhthPvOv6e68Z0Hl5c7wcmEB8Ps5V4UQ/wDPEpHXaa2Z4OntuTfOzKvxPv6nqHeIF72xHLv+LFWU/8B8fR5u7qKO7/A3zfy7F/492WE5xzDd7yPxBvbWvGy1GvS+vPwBedc9uccy8SH1v2v2YdHuHY1XYk6jSmBM8Tz+ungZ855xIn5fwb8X21H/gY8EGvgHkPr/i+lHj3cx3xVp9v0I3vMK9L7kbgZe/z4qQjHjKI+A+b/cS3fS/wn96yjwEbvf/J64jvO4iPJysk/n+wgHjXfaKPEh//txf4IfH/vVBft0UygznXZSupiIhIt1l8fGX7UX7vGf9m8akZJjjn/im1kfnDzO4hfoDM97p8sGQ8VcgiIiJJ5HUvjve65OcTb6HqlzMMSPrpsrAysz9afOK0FQn3lZvZkxafOO9JMyvz7jcz+5XFJ9x7w8xO6M/gRURE0lAV8SkgmoBfAZ93zr3ua0SSMl12BXoDH5uAPzvn2mej/SnxQYs/NrNvAmXOuevN7ELigx0vJD7O5JfOud6MNxERERHJON0Z+PcCsO+Iuy8lPqAY7+9lCff/2cUtID6vh+bmEBERkQGht2OshnuHd0N8Urb2SQJHcvhEaFtJ/uSPIiIiImmpO7MiH5VzzplZjw8tNLPPEj9dAsXFxSdOnjy5r6GIiIi8RyTmWLXjACMGFzC0JL/rJ4h0YcmSJXucc0dOxAv0vrDaZWbVzrkdXlffbu/+bRw+w2wNncxi7Jy7GbgZYNasWW7x4sW9DEVERKRzOxtaOelHT3PjB2Zw9dyjTSYv0j1m1unpnnrbFfgQ8Rl88f7+LeH+a7yjA08CGhK6DEVERFIuHIlPmJ6XoxmGpP912WJlZncRP3v6MDPbSnzq/h8D95rZp4jPVNs+8/UjxI8IXEf8lCKf6IeYRUREui0cjQIqrCQ1uiysnHOdnbDz7A4e64if/kNERCQthNpbrIIqrKT/KctERCSrtXcF5qvFSlJAWSYiIllNY6wklZRlIiKS1dqi8RmBVFhJKijLREQkq7UPXs/VGCtJAWWZiIhktbAGr0sKKctERCSrhTTGSlJIWSYiIllNRwVKKinLREQkq4WjarGS1FGWiYhIVtMYK0klZZmIiGS1NrVYSQopy0REJKtpglBJJWWZiIhktfbCKidgPkciA4EKKxERyWqhaIy8nABmKqyk/6mwEhGRrBaOxMjXwHVJEWWaiIhktXAkpvFVkjLKNBERyWoqrCSVlGkiIpLVwlEVVpI6yjQREclqbdGYJgeVlFGmiYhIVgtHYuSqsJIUUaaJiEhWC2mMlaRQjt8BAKzb3cSlN70EZhgQMDCz+F8MMzCDgBkBs4TreLffvR4IvPu4YKD9LwnX439zAkZOMEBOIH5fTsAIBo3cQICcYHxZrveY3KCRnxMgN/juJS8nQP6hS5D83Pj1gtwg+ZovRUQkbWjwuqRSWhRWOQFjSFEeDnDOARBzDucS/sYgSoxozOGAmIs/NuYc0di712Pec2IxR9S5+PMOXY//jUbjfyMxF18Wc0nfpsLcIIV5QQpzgxTkBijOz6E4L4fi/CDF+TkU5eVQWpDDoIIcBhXmMqggN367MJeyolzKivIYXJhLjpqvRUT6JByNUZKfFl93MgCkRaaNGVbMrZ+c49vru4Qiqy0aIxJ1tMXifyNRRzgaoy3hEorECHuX0KFLlFBbjNZIlNZwlJY27xKO0dIW4WA4SnMowvb6NprDEZpDURpb2wh5p1rozGCv0CovzqOiNJ/K0gIqS/OpHBS/XjW4gJFlhQwqyE3RuyUiklnCkRh5RfqRKqmRFoWV38yM3KCRG4SC3GBKXzsUidLYGqGxNcKBljYaWtqob2ljf3OYfc1h6g+G2Xewjb1NITbUNbNgwz4aWtres57SghxGDimkpqyQmrIixgwtYlxFCeMrS6geVEBA58gSkQFKXYGSSiqsfJafEyS/JMiwkvxuP6e1LUpdY4jdjSF2NLSwbX8L2+rjf7fub2HBhn00hSKHHl+QG2DssBImVpYwdcQgpo8YzLQRgygrzuuPTRIRSSttmsdKUkiFVQYqyA1SW15EbXkRUPae5c456hpDrK9rZsOeJtbvjv9dvHEfDy3ffuhxI4cUMnXEII6rHcLcseXMrBmiDx8RyTrhiOaxktRRYZWFzIzKQQVUDirg5PFDD1u2vznMyu0HWLm9gZXbD7BiewNPvrULgPycACeMKmP22HJOGlvOiWPKyM9JbdeoiEiyhaMxcvWjUVJEhdUAU1acx6kTh3HqxGGH7tvXHGbRxn0s3LCP1zbu5aZn3uZXDkryczjjmArOmVrJWZMqGVKkrkMRyTwhtVhJCqmwEsqL8zh/WhXnT6sC4EBrG69t2MfTq3fx1Krd/OPNHQQDxqzRZcyfXsWlx42kXOOzRCRDhCMx8tViJSmiwkreY1BBLudMHc45U4dzY8zxxrYGnnxrJ0++tYt///tb/Mcjqzh36nCumFXL6RMrCOqIQxFJU845nYRZUkqFlRxVIGAcVzuE42qH8I3zJ7N65wHuW7yVv76+jUfe3EnVoAI+dOJIrp47mpFDCv0OV0TkMJFYfJJpdQVKqijTpEcmVw3iuxdPZcG3zua3/3QCU0cM4n+eW88ZP32Wr9+3nA11TX6HKCJySFs0PgmzWqwkVdRiJb2SlxNg/vRq5k+vZlt9C79/YQN3vbaZvyzdyoUzqvnCmROYOmKQ32GKyAAXjqiwktRSpkmfjRxSyA2XTOPlb87jujPG8/yaOi781Yt8+tZFrNutFiwR8U97YZWrrkBJEWWaJM2wknyunz+Zl6+fx9fOPYaF7+zjgl++wI8fXU1zwkzwIiKpElKLlaSYMk2SbnBRLl86eyLPfv1MLjtuJL99fj3n/NfzPPLmDpxzfocnIgNI2BtjpekWJFWUadJvhpXk859XHMtfPn8yZUV5/J87lnLNH1/TAHcRSZlDY6zUFSgpokyTfnfi6HIe+uIp/Psl01i2pZ4Lf/Ui9y3e4ndYIjIAaPC6pJoyTVIiJxjg2veN4emvnsHxtWV84/43+Nq9yzkY1tgrEek/YU23ICmmTJOUqhxUwO2fnstXzp7IA69v5dKbXmbtrka/wxKRLNWmrkBJMWWapFwwYPzLucdw+6fmsv9gmEtuekldgyLSL0JqsZIUU6aJb06ZMIxHvnzaoa7BGx5aSSymowZFJHk0j5WkmjJNfNXeNfiJU8bwp1c28q9/eYOI9wtTRKSv2gsrTbcgqaJT2ojvggHj3y6eypDCPH7x1FqaWiP88qrjyM8J+h2aiGQ4HRUoqaZMk7RgZnzlnIn828VTeWzlTj5962IdMSgifaajAiXVlGmSVj556lh+evlMXl63h4/d8hoNLW1+hyQiGUwThEqqKdMk7Xx4Vi3/ffUJvLG1nqtuXqDiSkR6rU0tVpJiyjRJSxfMqOb318zi7d2NfPHOpYc+HEVEekInYZZUU6ZJ2jpzUiU3fmAGL769h+89tFIncBaRHjs03UJAX3eSGjoqUNLah2fVsnFPM795bj3jhhXz6dPG+R2SiGSQcDRGbtAIBMzvUGSAUGElae/r501i495mbnxkFaOHFnPu1OF+hyQiGSIciWnguqSUsk3SXiBg/PyK45g5cjBfvut1Vmxr8DskEckQ4UhM46skpZRtkhEK84L8/tpZlBfn8albF7GzodXvkEQkA6iwklRTtknGqCwt4JaPz6I5FOW625fo1Dci0qVwVIWVpJayTTLK5KpB/OiDM1i2pZ7fPr/e73BEJM2FoxpjJamlbJOM8/5jR3DxzGp++fTbrNyu8VYi0rl4V6DOOyqp06fCysz+xcxWmtkKM7vLzArMbKyZLTSzdWZ2j5nlJStYkXY/uHQ6Q4ry+Oo9ywlFon6HIyJpKn5UoKZakNTpdWFlZiOBLwOznHPTgSBwJfAT4BfOuQnAfuBTyQhUJFFZcR4//dBM1uxq5BdPvu13OCKSpjR4XVKtr9mWAxSaWQ5QBOwA5gH3e8tvBS7r42uIdOisyZVcObuWm19Yz5JN+/wOR0TSkAavS6r1Otucc9uAnwGbiRdUDcASoN45F/EethUY2dcgRTrznYunMmJIIV+9dzkHw5GunyAiA4omCJVU60tXYBlwKTAWGAEUA/N78PzPmtliM1tcV1fX2zBkgCvJz+FnVxzL5n0H+dEjq/0OR0TSjLoCJdX6km3nAO845+qcc23AA8ApwBCvaxCgBtjW0ZOdczc752Y552ZVVFT0IQwZ6E4aN5RPnjKW2xZs4pV1e/wOR0TSSFtURwVKavWlsNoMnGRmRWZmwNnAW8CzwOXeY64F/ta3EEW69o3zJ1FTVsj3H36LaMz5HY6IpImQugIlxfoyxmoh8UHqS4E3vXXdDFwPfNXM1gFDgVuSEKfIURXkBrl+/mRW72zkL0u2+h2OiKSJ+OB1TbcgqdOnMt459z3n3GTn3HTn3MeccyHn3Abn3Bzn3ATn3BXOuVCyghU5motnVnNc7RB+9sQamkMayC4iGrwuqadsk6xhZnz34insbgzx+xc3+B2OiKQBDV6XVFO2SVY5cXQ5F86o4nfPb2DXgVa/wxERn2keK0k1ZZtknevnTyYSi/FfT6z1OxQR8VE05ojGHHlBHRUoqaPCSrLO6KHFXHPyGO5dsoVVOw74HY6I+KQtGgNQi5WklLJNstKX5k1gUEEu//HIKpzT9AsiA1EoosJKUk/ZJllpSFEeXz57Ii++vYfn12pmf5GBKKzCSnygbJOs9bGTRjN6aBH/8cgqYpo0VGTACbd3BQY1j5WkjgoryVp5OQG+dt4k1u5q4slVu/wOR0RSTC1W4gdlm2S1C6dXUVteyG+fX6+xViIDzKHCSkcFSgqpsJKslhMM8JnTxvH65noWb9rvdzgikkJqsRI/KNsk611xYi1lRbn87vn1fociIikUjkYBFVaSWso2yXqFeUGuOXkMT63azbrdjX6HIyIpEo7Eu/91rkBJJWWbDAjXnDyagtwAN7+gcwiKDBRhTRAqPlC2yYAwtCSfD8+q5a+vb9M5BEUGiHcHr+urTlJH2SYDxqdPHUc05vjjy+/4HYqIpIAGr4sflG0yYIwaWsSFM6q5c8FmGlvb/A5HRPqZBq+LH5RtMqB87vTxNIYi3PXaZr9DEZF+phYr8YOyTQaUGTWDed/4odzy0juHPnRFJDtpjJX4QdkmA87nzhjPrgMh/rZsm9+hiEg/Cke96RbUYiUppGyTAef0icOYXFXKra9u9DsUEelH7S1W+SqsJIWUbTLgmBlXzRnFim0HWLGtwe9wRKSftBdWueoKlBRStsmAdNlxI8nPCWgQu0gWC0ejBANGMGB+hyIDiAorGZAGF+Vy0Yxq/rZsOwfDEb/DEZF+EI7ENHBdUk4ZJwPWlXNG0RSK8PAbO/wORUT6QTgS08B1STllnAxYs8eUMb6imLvVHSiSlcJRFVaSeso4GbDMjCtnj2Lp5nrW7mr0OxwRSbKQugLFB8o4GdA+eMJIcoOmQewiWagt6jTVgqScMk4GtKEl+Zw3rYq/vr6N1rao3+GISBKFI1F1BUrKKeNkwLtq9ijqD7bx+MqdfociIkkUjsQ0h5WknDJOBrz3jR9KbXmhugNFsowGr4sflHEy4AUC8UHsCzbs4509zX6HIyJJonmsxA/KOBHgihNrCAaMuxep1UokW2geK/GDMk4EqBxUwNmTK/nLkq2Hzi8mIpktpMJKfKCME/FcNWcUe5rCPL1ql9+hiEgStGmMlfhAGSfiOf2YCqoHF3D3oi1+hyIiSRCOxsjXGCtJMWWciCcYMK6YVcsLb9exdf9Bv8MRkT7SGCvxgzJOJMGHZ9UAcO/irT5HIiJ9pXmsxA/KOJEENWVFnDaxgvsWbyEac36HIyJ9oBYr8YMyTuQIV82uZUdDKy+srfM7FBHpA00QKn5Qxokc4ewpwxlanKc5rUQyWCzmaIs6TRAqKaeMEzlCXk6Ay0+s4elVu9nd2Op3OCLSC22x+Hx0arGSVFPGiXTgw7NricQcf1myze9QRKQX2if6zVdhJSmmjBPpwPiKEuaMLeeeRZtxToPYRTJNe2GlFitJNWWcSCeunF3Lxr0HWbBhn9+hiEgPhaPxwkrTLUiqKeNEOnHhjGpKC3I0iF0kAx1qsVJhJSmmjBPpREFukA8cP5JHV+yk/mDY73BEpAfUFSh+UcaJHMWVs0cRjsT46+saxC6SSUIqrMQnyjiRo5g6YhAzawZz92tbNIhdJIO0j7FSYSWppowT6cJVc0axZlcjSzfv9zsUEemmtvbpFjTGSlJMGSfShUuOHUFJfg53LNQgdpFMoRYr8YsyTqQLxfk5XHb8CB5+Y4cGsYtkCA1eF78o40S64eo5owlHYvxlqQaxi2SC9sJK81hJqinjRLph6ohBHD9qCHcs3KRB7CIZQF2B4hdlnEg3XT1nFBvqmln4jmZiF0l3IU0QKj5Rxol008UzRzCoIIc7NYhdJO3pJMziF2WcSDcV5gX54Ak1PLpiB3ubQn6HIyJH0aauQPGJMk6kBz46dxRtUcf9S7b6HYqIHIWOChS/9CnjzGyImd1vZqvNbJWZnWxm5Wb2pJm97f0tS1awIn6bOLyUOWPKufO1zcRiGsQukq50EmbxS18z7pfAY865ycCxwCrgm8DTzrmJwNPebZGs8dGTRrFp70FeWb/X71BEpBPhaAwzCAbM71BkgOl1YWVmg4HTgVsAnHNh51w9cClwq/ewW4HL+hqkSDqZP72KsqJc7li4ye9QRKQT4UiMvGAAMxVWklp9abEaC9QB/2tmr5vZH8ysGBjunNvhPWYnMLyjJ5vZZ81ssZktrqur60MYIqmVnxPkilm1PPnWLnYfaPU7HBHpQCgS0/gq8UVfsi4HOAH4H+fc8UAzR3T7ufhMih0ORHHO3eycm+Wcm1VRUdGHMERS76o5o4jEHPcs2uJ3KCLSgXA0pqkWxBd9ybqtwFbn3ELv9v3EC61dZlYN4P3d3bcQRdLP2GHFnDphGHe+tpmId1i3iKSPNq8rUCTVep11zrmdwBYzm+TddTbwFvAQcK1337XA3/oUoUiauubk0exoaOWpVbv8DkVEjhCOqitQ/JHTx+d/CbjDzPKADcAniBdr95rZp4BNwIf7+BoiaensKcMZOaSQP7+6ifnTq/0OR0QShDXGSnzSp8LKObcMmNXBorP7sl6RTBAMGPoFi28AACAASURBVB89aRQ/fWwNb+9qZOLwUr9DEhGPCivxi7JOpA8+MquWvJwAf35VUy+IpJNwNEauxliJD5R1In0wtCSfi2dW88DSrTS2tvkdjoh4Qhq8Lj5R1on00bUnj6E5HOWBpdv8DkVEPOoKFL8o60T66NjaIRxbM5g/v7qR+NRtIuK3cETzWIk/lHUiSXDNyWNYX9es8weKpIk2TbcgPlHWiSTBRTOrKS/O49ZXNvodiojgzWOlMVbiA2WdSBIU5Ab5yOxanlq1i231LX6HIzLgaYyV+EVZJ5IkH507CoA7FmjqBRG/hSOabkH8oawTSZKasiLOnjKcuxdtobUt6nc4IgOaWqzEL8o6kSS65uTR7GsO8/jKnX6HIjKghTR4XXyirBNJolPGD6O2vJC7X9vidygiA5ZzLj7dgroCxQfKOpEkCgSMj8yq5dUNe9m4p9nvcEQGpEgsPp+cWqzED8o6kSS7/MRaAgb3LFarlYgfwpEYoMJK/KGsE0myqsEFzJtcyf1LttIWjfkdjsiAc6iwUleg+EBZJ9IPPjJ7FHWNIZ5ZvdvvUEQGnHC0vcUq6HMkMhCpsBLpB2dNqqCyNJ97Fqk7UCTV2luscoPmcyQyEKmwEukHOcEAV8yq4bk1u9nRoJnYRVIppDFW4iNlnUg/+cisUcQc3Ld4q9+hiAwo7S1W+SqsxAfKOpF+MmpoEadMGMo9i7YQ8w7/FpH+9+4YK33FSeop60T60ZWzR7GtvoWX1u3xOxSRAaP9aNy8oAavS+qpsBLpR+dNG05ZUS53L9rsdygiA4bmsRI/KetE+lF+TpAPnlDDk2/tYk9TyO9wRAYEFVbiJ2WdSD+7cnYtbVHHA0s1iF0kFUKabkF8pMJKpJ9NHF7KiaPLuHfxVpzTIHaR/tY+eF1HBYoflHUiKXDZcSNYt7uJtbua/A5FJOu9e0obDV6X1FNhJZIC86dXEzB4+I3tfocikvU0xkr8pKwTSYGK0nxOGjeUf7yxQ92BIv2sTfNYiY+UdSIpcvHMEWzY08yqHY1+hyKS1dRiJX5S1omkyPnThhMMmLoDRfrZoZnXg/qKk9RT1omkyNCSfN43fij/eFPdgSL9SdMtiJ9UWImk0MUzq9m09yArtx/wOxSRrBWOxMgLBjBTYSWpp8JKJIXOm1pFTsD4u7oDRfpNOBLT+CrxjTJPJIXKivM4ZcIwHR0o0o/C0agKK/GNMk8kxS6eWc3W/S28sbXB71BEslJbxGnguvhGmSeSYudNrSI3qKMDRfpLOKquQPGPMk8kxQYX5XL6xAp1B4r0E42xEj8p80R8cNHMarY3tPL6lnq/QxHJOiHvqEARPyjzRHxwztTh5AUDPLx8h9+hiGSdcDRGrlqsxCfKPBEfDCrI5YxJFTzy5g5iMXUHiiRTOBIlXy1W4hNlnohPLp5Zzc4DrSzdvN/vUESyisZYiZ+UeSI+OXtKvDvwsRU7/Q5FJKvoqEDxkzJPxCcl+TnMHVfOM2t2+x2KSFbRPFbiJ2WeiI/mTa5kQ10zm/Y2+x2KSNYIRTTzuvhHmSfio3mTKwF4ZrVarUSSZf/BNoYU5fodhgxQKqxEfDR6aDHjKopVWIkkSSgSpaGljYqSfL9DkQFKhZWIz+ZNqmThhn00hyJ+hyKS8fY2hQGoKFVhJf5QYSXis3mTKwlHY7yyfq/foYhkvLrGEKDCSvyjwkrEZ7PGlFOSn6PuQJEkUGElflNhJeKzvJwAp04YxnNrduukzCJ9VNcUL6yGaYyV+ESFlUgamDe5kh0Nraza0eh3KCIZrb3FamhJns+RyEClwkokDZw5uQKAZzVZqEif1DWGGFKUS35O0O9QZIBSYSWSBipLC5gxcrDGWYn00Z6mkKZaEF+psBJJE2dNruT1zfvZ3xz2OxSRjFXXGNLAdfGVCiuRNDFvciUxB8+vrfM7FJGMVdekwkr8pcJKJE3MHDmYocV56g4U6YO6xpCOCBRfqbASSROBgHHGpAqeX1tHNKZpF0R6qjkU4WA4qhYr8VWfCyszC5rZ62b2sHd7rJktNLN1ZnaPmemYV5Fumje5koaWNl7fvN/vUEQyzqHJQdViJT5KRovVV4BVCbd/AvzCOTcB2A98KgmvITIgnDaxgmDA1B0o0gt7mjTruvivT4WVmdUAFwF/8G4bMA+433vIrcBlfXkNkYFkcGEus0aXqbAS6QWdzkbSQV9brP4f8K9AzLs9FKh3zkW821uBkX18DZEBZd7kSlbvbGRbfYvfoYhklDq1WEka6HVhZWYXA7udc0t6+fzPmtliM1tcV6fDy0XanTt1OABPrNzpcyQimaWuMUTAoKxIQ3vFP31psToFuMTMNgJ3E+8C/CUwxMxyvMfUANs6erJz7mbn3Czn3KyKioo+hCGSXcZVlDCxsoTHVViJ9EhdY4ihJfkEA+Z3KDKA9bqwcs59yzlX45wbA1wJPOOc+yjwLHC597Brgb/1OUqRAeb8aVW89s4+9npdGyLStbpGnc5G/Ncf81hdD3zVzNYRH3N1Sz+8hkhWmz+9ipiDp1dpELtId+3RrOuSBpJSWDnnnnPOXexd3+Ccm+Ocm+Ccu8I5p5/cIj00bcQgRg4pVHegSA/oPIGSDjTzukgaMjPOmzacF9ftoSkU6foJIgOcc07nCZS0oMJKJE2dP62KcCTG82t01KxIVxpa2miLOp0nUHynwkokTc0eU055cZ66A0W6QZODSrpQYSWSpoIB49wpw3lm9W5Ckajf4YikNZ0nUNKFCiuRNHb+9OE0hSK8sn6v36GIpDXNui7pQoWVSBp73/hhFOcFNQu7SBfUFSjpQoWVSBoryA1y5uRKnnxrF9GY8zsckbRV1xQiLyfAoIKcrh8s0o9UWImkufOnVbGnKczSzfv9DkUkbbXPum6m09mIv1RYiaS5syZVkBcM8PgKdQeKdKauMcQwdQNKGlBhJZLmSgtyOWXCUB5buRPn1B0o0hGdJ1DShQorkQxw/rQqtu5v4a0dB/wORSQt7WkKa+C6pAUVViIZ4JypwwkYPL5yl9+hiKSdaMyxr1mns5H0oMJKJAMMK8ln9phyHlq2Td2BIkfY2xwi5jTVgqQHFVYiGeIjs2vZuPcgr2qyUJHDvDvrep7PkYiosBLJGBfOqGZwYS53LNzsdygiaUWTg0o6UWElkiEKcoN86IQaHl+589AXiYgktlgV+ByJiAorkYxy9dxRRGKO+5Zs8TsUkbSxpykMwLBSdQWK/1RYiWSQCZUlzB1bzt2vbSGmU9yIAPEWq5L8HIrydDob8Z8KK5EMc/XcUWzed5CX1u3xOxSRtFDXpKkWJH2osBLJMPOnV1FenMedGsQuAkBdYyvDdESgpAkVViIZJj8nyOUn1vDkql3sOtDqdzgivqtrVIuVpA8VViIZ6Ko5o4jGHPcu0iB2EZ0nUNKJCiuRDDR2WDHvGz+UuxdtIapB7DKAhSJRDrRG1GIlaUOFlUiGunruKLbVt/DC2jq/QxHxTftUCyqsJF2osBLJUOdNrWJYSZ5mYpcBrX1y0GHqCpQ0ocJKJEPl5QS4YlYtz6zexY6GFr/DEfGFTmcj6UaFlUgGu2r2KGIOHnx9u9+hiPhChZWkGxVWIhls1NAiZowczOMrd/odiogv2gurocUqrCQ9qLASyXDzp1exbEu9ugNlQNrTFKKsKJe8HH2dSXpQJopkuPnTqwB4fIVarWTg0eSgkm5UWIlkuPEVJUysLOExdQfKAFTXFNIRgZJWVFiJZIH506t47Z197G0K+R2KSEqpxUrSjQorkSxw/rQqYg6eWrXL71BEUsY5p9PZSNpRYSWSBaaNGERNWSGPaZyVDCDN4SgtbVG1WElaUWElkgXMjPnTqnhp3R4OtLb5HY5ISuzRHFaShlRYiWSJC2ZU0RZ1PLt6t9+hiKREXZMKK0k/KqxEssTxtWVUlOarO1AGDJ0nUNKRCiuRLBEIGOdPG85za+poCUf9Dkek3+l0NpKOVFiJZJH506ppaYvywtt1foci0u827T1Ifk6AsqI8v0MROUSFlUgWmTuunMGFuZqFXQaEJZv2cVztEIIB8zsUkUNUWIlkkdxggHOmDOfJVbsIR2J+hyPSbw6GI6zYfoBZY8r8DkXkMCqsRLLMBdOraGyN8OqGvX6HItJvlm2uJxpzzBpT7ncoIodRYSWSZU6dOIyivKCODpSstnjTfszghFFqsZL0osJKJMsU5AY5a3IlT761k2jM+R2OSL9YtHEfk4aXMrgw1+9QRA6jwkokC503dTh7msIs21LvdygiSReNOV7fXK/xVZKWVFiJZKEzj6kkGDCdlFmy0uqdB2gKRZit8VWShlRYiWShwUW5zBlTztMqrCQLLd64H4ATR6vFStKPCiuRLHX2lErW7mpiy76DfociklSLNu6jenABI4cU+h2KyHuosBLJUudMGQ6g7kDJKs45Fm/cz6wx5ZhpYlBJPyqsRLLUmGHFjK8oVmElWWVbfQs7D7QyS92AkqZUWIlksXOmDmfhhn0caG3zOxSRpGgfX6UjAiVdqbASyWLnTBlOJOZ4Ya1OyizZYfGmfZTk5zC5apDfoYh0SIWVSBY7YVQZZUW5PL1qt9+hiCTF4o37OX6UTrws6UuFlUgWCwaMsyZV8szq3USiOimzZLaGljbW7GrU/FWS1lRYiWS5c6YOp6GljSWb9vsdikifLN28H+c0vkrSmworkSx32sRh5AaNp1erO1Ay2+KN+wgGjONqh/gdikinel1YmVmtmT1rZm+Z2Uoz+4p3f7mZPWlmb3t/9dNCxEelBbmcNG6opl2QjLdo436mjxhEUV6O36GIdKovLVYR4GvOuanAScAXzGwq8E3gaefcROBp77aI+OicKcPZUNfMhromv0MR6ZVwJMbyLfXM0vgqSXO9Lqycczucc0u9643AKmAkcClwq/ewW4HL+hqkiPTN2VMqAXR0oGSsFdsbCEVimhhU0l5SxliZ2RjgeGAhMNw5t8NbtBMYnozXEJHeqykrYnJVqboDJWMtaT/xsgauS5rrc2FlZiXAX4B/ds4dSFzmnHOA6+R5nzWzxWa2uK5OkxeK9Lezp1SyeNN+6g+G/Q5FpMcWbdzHmKFFVJYW+B2KyFH1qbAys1ziRdUdzrkHvLt3mVm1t7wa6LDvwTl3s3NulnNuVkVFRV/CEJFuOGfKcKIxx3Nr9ENGMotzjiWb9nPiaI2vkvTXl6MCDbgFWOWc+6+ERQ8B13rXrwX+1vvwRCRZjq0ZwrCSfJ54a6ffoYj0yHNr69jbHGbuWBVWkv760mJ1CvAxYJ6ZLfMuFwI/Bs41s7eBc7zbIuKzQMCYP304z6zeTXMo4nc4It3SFIrwnb+uYHxFMZccN8LvcES61OvJQJxzLwGdnazp7N6uV0T6z8UzR3D7gs08vXo3lxyrLylJfz95dDXbG1q4/7r3UZAb9DsckS5p5nWRAWT2mHIqS/N5ePl2v0MR6dLCDXu5bcEmPv6+MZyoaRYkQ6iwEhlAggHjwhnVPLe2jsbWNr/DkQEgHIlxx8JN7Gho6dHzWtuifPOBN6ktL+Qb50/qp+hEkk+FlcgA8/5jqwlHYjz5lua0kv61Zd9Brvjdq3z7ryv457uXEZ+Bp3t+8eRa3tnTzI8/OFOnsJGMosJKZIA5vraMEYMLePiNHV0/WKSXnli5k4t+9SIbdjdxxYk1LHxnHw8u29at5y7fUs/vX9zAlbNrOWXCsH6OVCS59DNAZIAJBIyLZlbzp1c20nCwjcFFuX6HJFkkHInxk8dWc8tL7zBj5GBuuvp4asuKeHt3Ezf+YxXzJg9ncGHnOReOxLj+L29QUZrPty6cksLIRZJDLVYiA9D7jx1BW9Tx+ErNaSXJs62+hQ//7lVueekdrj15NPd//mRGDy0mEDB+eNl09jWH+fkTa466jl89/Tardzbyw8tmHLUAE0lXKqxEBqAZIwczqryIv7+howMleb527zLW7W7iNx89gX+/dDr5Oe9OjzB95GCuOXkMty3YxJtbG97zXOccv376bW56dh0fOqGGc6fqNLOSmVRYiQxAZsbFM6t5Zf1e9jaF/A5HssD6uiYWbNjHF86awIUzqjt8zFfPO4ZhJfl858E3icbeHcjunONHj67m50+u5YMnjOQnH5qRqrBFkk6FlcgAdfHMEURjjsfUHShJcPdrm8kJGJefWNPpYwYV5PKdi6awfGsDd722GYBYzPHtB1dw8wsb+NhJo/nZ5ceSE9RXk2QuZa/IADWlupRxFcU8vFxHB0rfhCJR7l+ylXOnDqeiNP+oj73k2BG8b/xQfvrYanYdaOWr9y7jzoWb+fyZ4/n+pdMIBDo7oYdIZlBhJTJAxbsDR7Dgnb3sPtDqdziSwR5fuYv9B9u4as6oLh9rZnz/0um0tEU57xcv8OCy7Xzj/ElcP38yZiqqJPOpsBIZwN4/sxrn4JE31Wo10N27eAtfvXdZr55718LN1JQVcmo355yaUFnCdWeMp6GljX+/ZBpfOGtCr15XJB2psBIZwCYOL2XS8FJNFjrANYci/OiRVTywdBvrdjf16Lkb6pp4dcNerpozqkfdeF899xhe+eY8rn3fmB5GK5LeVFiJDHAXz6xm8ab9bK/v2bncJHvcuXAz+w/Gzx352IqeFdn3LNpCMGBccZRB6x0xM0YMKezRc0QygQorkQHu0uNGEjC4fcEmv0MRH7S2Rbn5xQ2cOmEYJ44u45E3u3+UaCgS5b4lWzlnSiWVgwr6MUqRzKHCSmSAGzW0iAtmVHPbq5s40NrmdziSYvcs2kJdY4gvzpvABdOreGvHATbuae7Wc598axf7msPdGrQuMlCosBIRPn/GeBpDEe5cuNnvUCSFwpEYv31+PbPHlDF3bDkXeBN7Prqie61Wd722mZFDCjltYkV/himSUVRYiQjTRw7mtInDuOWld2hti/odjqTIA0u3sqOhlS/Om4iZMXJIIcfWDuHRboyz2rinmZfX7eXK2bUENfeUyCEqrEQEiLda1TWGeGDpNr9DkRSIRGP85rn1HFszmNMnvjtNwgXTq3hjawNb9h086vPvbh+0Pqu2v0MVySgqrEQEgJPHD+XYmsH87oX1h53HTZJn14HWtHlvH1q+nc37Dh5qrWp3wfQqAB47SndgOBLj/iVbmDe5kqrBGrQukkiFlYgA8cPfP3/meDbtPditriDpmbrGEKf/9Fl+8+w6v0MhGnP897PrmFxVyjlTKg9bNnpoMdNGDOKRo+TAfUu2sKcpzFVz1FolciQVViJyyHlTqxhXUcz/PLce59KjZSVbPPzGdkKRGHe+ttn3VqvHVuxkfV0zXzqitardhTOqeX1zPTsa3ju32bb6Fn70yGpOGlfOmcdUvme5yECnwkpEDgkEjOtOH8/K7Qd48e09foeTVR5ctp38nAA7Glp5YW2db3HEYo5fP/M24yuKme91+x2ps+5A5xzfeuBNYs7xn5cfqxMmi3RAhZWIHObS40dQNaiA/3luvd+hZI139jSzfEs9Xz57IkOL87jrNf+mtfj7G9tZvbORL5w1odOj+cZVlDC5qpRHj5gs9L7FW3lhbR3fvGAyteVFqQhXJOOosBKRw+TnBPn0aWN5dcNeXt+83+9wssJDy7ZjBh88YSSXn1jD06t3s/tAa8rjaGhp4wcPr+LYmsFcetzIoz72gunVLNq071Cc2+tb+MHDbzF3bDn/NHd0KsIVyUgqrETkPa6cM4rBhbn897Nqteor5xx/W7aNuWPLqR5cyIdn1xKNOe5fujXlsfzs8TXsaw5x4wdmdDn31IUzqnAOHl+581AXYCSmLkCRrqiwEpH3KMnP4TOnjeWpVbu4d9EWv8PJaG9ua2DDnmYu81qIxleUMGdsOfcs2kIshYPYl2+p5/aFm7jm5DFMHzm4y8dPHF7KhMoSHnlzJ/ct2crzXhfgqKHqAhQ5GhVWItKh684Yz6kThvGdv63gja31foeTsf62bDt5wQAXTK8+dN9Vc2rZtPcgCzbs7dG6Vmxr4No/vsa3HniTJ1bupDkU6dbzojHHtx98k4qSfL523jHdfr0Lplex8J29/ODvbzFnbDkfO0ldgCJdUWElIh3KCQb41VXHU1GSz3W3LWFvU8jvkDJONOb4+/LtnDmpgsFFuYfuv2B6NYMKcri7m62Bzjnufm0zH/yfV3hzWwN/X76dz962hOO//yQf/cMC/vDihqOeOPm2VzeyYtsB/u39UyktyO30cUe6YHo1MQdtsRj/eflMdQGKdIMKKxHpVHlxHr/9pxPZ0xzmS3e9TiQa8zukjPLq+r3sbgxx2fGHDxQvyA3ygeNH8tiKnexvDh91HS3hKF+/7w2++cCbzB1bzpP/cjpLv3sud35mLh8/ZQx1jSF++I9VnPXz5/jGfcvZ2XD4oPhdB1r52RNrOW3iMC6aUd3Jq3RsSnUplx43ghsvm8HoocU9eq7IQKXCSkSOakbNYP7jAzN4Zf1e/vPxNX6Hk1EeXLaN0vwc5k1+70SaV84ZRTga44HXOz8344a6Jj7wm5d54PWtfOXsifzpE3MYWpJPXk6A940fxv+9cApP/MsZvHT9WXz61LH8bdl2zvzZs/z8iTU0ed2EP3j4LcLRGD+4dHqHk4EejZnxyyuP50Mn1vRsw0UGsBy/AxCR9Hf5iTUs31LP717YwMyaIVw0s2ctH9nKOceuA6EOz5fX2hblsRU7mT+9ioLc4HuWT6kexLG1Q7hn0WY+ecqYw4qetmiMv76+je///S1yg8afPjGHM46p6DSOmrIivn3RVK45eQw/fXwNv35mHXe9tplLjxvJw2/s4F/OOYYxw9TiJJIKarESkW757sVTOWHUEL5x/3Le3Nrgdzi+a22L8s/3LOOkHz3NN+5bTmNr22HLn1m9m6ZQ5NDRgB25cnYta3c1sXRz/OCAg+EI//vyO5z5n8/xr/e/wTHDS/jHl087alGVqLa8iF9fdTwPfuEUxg0r4ZaX3mHcsGKuO3Nc7zdURHrE0uF8YLNmzXKLFy/2OwwR6cKuA61cctNL7G0K86lTx/LlsydSnD/wGr53NLTwuduW8MbWBs6dOpynV+2ienAhP7viWE4ePxSAz/55Mcu21PPqt87udM6oplCEOTc+xVmTKplQWcKfX93I/oNtzB5TxudOH8+8yZW9HjDunOPldXupLS/U+CiRJDOzJc65WR0tG3ifiCLSa8MHFfDoV07nJ4+u5ncvbOCh5dv53vuncv60qh6P38lUSzfv53O3LeFgKMLvr5nFuVOHs3Tzfr5273Ku+v0CPnnKWK47YxzPranjYyePPupEnCX5OVxy7IhDRweeM2U4nz9zHCeOLu9znGbGqROH9Xk9ItIzarESkV5Zsmkf3/7rClbvbOTMSRX8+yXTsr5l5L7FW/j2X1dQNbiAP1w7i2OGlx5adjAc4cePrubPr26itCCHxtYID33xFGbWDDnqOrfVt3DHgk184PiRTExYn4ikr6O1WKmwEpFei0Rj3PrqJv7riTW0RmJMrR7ECaOGcMLoMk4YVUZNWWHatmSFIzF2NLTQ2BqhORShORyhKRSlORShJRwlFIkRjsQIR6OEIzG217fyjzd3cMqEodx01QmUFed1uN4X1tbxr/e/wZCiXB79ymlpu/0i0nsqrESkX+1saOX2BZtYsmk/y7fWczAcBWBYST7jKorJzwmQFwyQl+NdggEKcoMU5AYozA1SkBekICdIbtBoCkVpCrXR1BqhMRShqTVCbk6A4aUFVA7KZ/igfO96ATVlhR0ecdfOOceOhlbW7GxkfV0Tm/YeZOPeZjbubWbb/ha6c0aZYMDICwbIzw1w+Qk1fPOCyeQEj37cT2tblLZorEeTcYpI5lBhJSIpE4nGWLOrkaWb63l903621rcQjsRoi7a3AMX/hiIxWsJRWtqi71lHMGCU5OdQWpBDSX4O4WiM3QdCh+ZmamcGVYMKGD20iDFDixk9tJji/CBrdzWyZmcjq3c20tj67nNKC3IYOyz+uLFDi6gpL2JIYS7F+TkU5+dQkh+kOD+HotycQ0VgVycrFpGBR4WViKQt5xyhSMxr5XGU5OdQkBvosAutKRRh94FWdh0IsfNAC1v2tbBxbzOb9h5k095m9jTFZzEvzc9hUlUpk6pKmVw9iMlVpYyvKKGsKFddcyLSZzoqUETSlpl53YKdd+m1K8nPoaSihHEVJR0ub2xtoykUoWpQgQooEfGFCisRyRqlBbka1yQivtLM6yIiIiJJosJKREREJElUWImIiIgkiQorERERkSRRYSUiIiKSJCqsRERERJJEhZWIiIhIkqiwEhEREUkSFVYiIiIiSaLCSkRERCRJVFiJiIiIJIkKKxEREZEkUWElIiIikiQqrERERESSpF8KKzObb2ZrzGydmX2zP15DREREJN0kvbAysyDw38AFwFTgKjObmuzXEREREUk3/dFiNQdY55zb4JwLA3cDl/bD64iIiIiklf4orEYCWxJub/XuExEREclqOX69sJl9FvisdzNkZiv8ikWSYhiwx+8gpE+0DzOb9l/m0z7MHKM7W9AfhdU2oDbhdo1332GcczcDNwOY2WLn3Kx+iEVSRPsw82kfZjbtv8ynfZgd+qMrcBEw0czGmlkecCXwUD+8joiIiEhaSXqLlXMuYmZfBB4HgsAfnXMrk/06IiIiIummX8ZYOeceAR7pwVNu7o84JKW0DzOf9mFm0/7LfNqHWcCcc37HICIiIpIVdEobERERkSRRYSUiIiKSJCqsRERERJIk7QsrMxtlZg+a2R91QufMY2YBM7vRzH5tZtf6HY/0jpkVm9liM7vY71ik58zsMjP7vZndY2bn+R2PdI/3f3ert+8+6nc80j39Wlh5xdDuI2dVN7P5ZrbGzNZ1o1iaAdzvnPskcHy/BSvvkaT9dynxSWLbiJ/eSFIo6DruyAAAAldJREFUSfsQ4Hrg3v6JUo4mGfvQOfegc+4zwHXAR/ozXjm6Hu7PDxL//vsMcEnKg5Ve6dejAs3sdKAJ+LNzbrp3XxBYC5xL/It2EXAV8TmvfnTEKj4JRIH7AQfc5pz7334LWA6TpP33SWC/c+53Zna/c+7yVMUvSduHxwJDgQJgj3Pu4dREL5Ccfeic2+097+fAHc65pSkKX47Qw/15KfCoc26Zmd3pnLvap7ClB/r1XIHOuRfMbMwRd88B1jnnNgCY2d3Apc65HwHv6WYws68D3/PWdT+gwipFkrT/tgJh72a0/6KVjiRpH54JFANTgRYze8Q5F+vPuOVdSdqHBvyY+Je0iiof9WR/Ei+yaoBlZMDQHYnz4yTMI4EtCbe3AnOP8vjHgBvM7GpgYz/GJd3T0/33APBrMzsNeKE/A5Nu69E+dM59G8DMPk68xUpFlf96+n/4JeAcYLCZTXDO/bY/g5Me62x//gq4ycwuAv7uR2DSc34UVj3inFsBqPsoQznnDgKf8jsO6Tvn3J/8jkF6xzn3K+Jf0pJBnHPNwCf8jkN6xo+mxW1AbcLtGu8+yQzaf5lP+zDzaR9mF+3PLOJHYbUImGhmY80sD7gSeMiHOKR3tP8yn/Zh5tM+zC7an1mkv6dbuAt4FZhkZlvN7FPOuQjwReBxYBVwr3NuZX/GIb2j/Zf5tA8zn/ZhdtH+zH46CbOIiIhIkujwTREREZEkUWElIiLy/9utYwEAAACAQf7Wk9hZFMFErAAAJmIFADARKwCAiVgBAEzECgBgIlYAABOxAgCYBLCxrV2fFE3jAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XdXh_b0GP_F"
      },
      "source": [
        "**3. Entrainement du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80pEtJ10wIfY"
      },
      "source": [
        "# Charge les meilleurs poids\r\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16ujUiELwR33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "outputId": "bbdefceb-f2f4-41a9-a53c-3fb7a8042da3"
      },
      "source": [
        "# Définition de l'optimiseur à utiliser\r\n",
        "optimiseur=tf.keras.optimizers.Adam(lr=1e-3)\r\n",
        "\r\n",
        "\r\n",
        "# Utilisation de la méthode ModelCheckPoint\r\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\r\n",
        "\r\n",
        "# Compile le modèle\r\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur,metrics=\"mae\")\r\n",
        "\r\n",
        "# Entraine le modèle\r\n",
        "historique = model.fit(dataset,validation_data=dataset_Val, epochs=500,verbose=1, callbacks=[CheckPoint])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "45/45 [==============================] - 3s 20ms/step - loss: 27818.9479 - mae: 27819.4479 - val_loss: 28231.7539 - val_mae: 28232.2539\n",
            "\n",
            "Epoch 00001: loss improved from inf to 27946.71680, saving model to poids.hdf5\n",
            "Epoch 2/500\n",
            "45/45 [==============================] - 1s 11ms/step - loss: 27734.9244 - mae: 27735.4244 - val_loss: 28047.3633 - val_mae: 28047.8633\n",
            "\n",
            "Epoch 00002: loss improved from 27946.71680 to 27767.94531, saving model to poids.hdf5\n",
            "Epoch 3/500\n",
            "45/45 [==============================] - 1s 11ms/step - loss: 27531.8904 - mae: 27532.3904 - val_loss: 27862.7012 - val_mae: 27863.2012\n",
            "\n",
            "Epoch 00003: loss improved from 27767.94531 to 27589.46484, saving model to poids.hdf5\n",
            "Epoch 4/500\n",
            "45/45 [==============================] - 1s 11ms/step - loss: 27607.3465 - mae: 27607.8465 - val_loss: 27678.5098 - val_mae: 27679.0098\n",
            "\n",
            "Epoch 00004: loss improved from 27589.46484 to 27410.95312, saving model to poids.hdf5\n",
            "Epoch 5/500\n",
            "45/45 [==============================] - 1s 11ms/step - loss: 27314.1639 - mae: 27314.6639 - val_loss: 27494.2070 - val_mae: 27494.7070\n",
            "\n",
            "Epoch 00005: loss improved from 27410.95312 to 27232.53320, saving model to poids.hdf5\n",
            "Epoch 6/500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-25d61524ae2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Entraine le modèle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mhistorique\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_Val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCheckPoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COP9u4yitvmw"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\r\n",
        "erreur_validation = historique.history[\"val_loss\"]\r\n",
        "\r\n",
        "# Affiche l'erreur en fonction de la période\r\n",
        "plt.figure(figsize=(10, 6))\r\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\r\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\r\n",
        "plt.legend()\r\n",
        "\r\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o2zh6N9t1b7"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\r\n",
        "erreur_validation = historique.history[\"val_loss\"]\r\n",
        "\r\n",
        "# Affiche l'erreur en fonction de la période\r\n",
        "plt.figure(figsize=(10, 6))\r\n",
        "plt.plot(np.arange(0,len(erreur_entrainement[400:500])),erreur_entrainement[400:500], label=\"Erreurs sur les entrainements\")\r\n",
        "plt.plot(np.arange(0,len(erreur_entrainement[400:500])),erreur_validation[400:500], label =\"Erreurs sur les validations\")\r\n",
        "plt.legend()\r\n",
        "\r\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6Gq2CkeGR_1"
      },
      "source": [
        "**4. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcGnqie_GVNE"
      },
      "source": [
        "# Charge les meilleurs poids\r\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRxXtHRXGXfF"
      },
      "source": [
        "taille_fenetre = 20\r\n",
        "\r\n",
        "# Création d'une liste vide pour recevoir les prédictions\r\n",
        "predictions = []\r\n",
        "\r\n",
        "# Calcul des prédiction pour chaque groupe de 20 valeurs consécutives de la série\r\n",
        "# dans l'intervalle de validation\r\n",
        "for t in temps[temps_separation:-taille_fenetre]:\r\n",
        "    X = np.reshape(serie[t:t+taille_fenetre],(1,taille_fenetre))\r\n",
        "    predictions.append(model.predict(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd2nfDcgG8EO"
      },
      "source": [
        "# Affiche la série et les prédictions\r\n",
        "plt.figure(figsize=(10, 6))\r\n",
        "affiche_serie(temps,serie,label=\"Série temporelle\")\r\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions)[:,0,0],label=\"Prédictions\")\r\n",
        "plt.title('Prédictions avec le modèle de régression linéaire')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# Zoom sur l'intervalle de validation\r\n",
        "plt.figure(figsize=(10, 6))\r\n",
        "affiche_serie(temps[temps_separation:],serie[temps_separation:],label=\"Série temporelle\")\r\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions)[:,0,0],label=\"Prédictions\")\r\n",
        "plt.title(\"Prédictions avec le modèle de régression linéaire (zoom sur l'intervalle de validation)\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDS7BJvZG_e1"
      },
      "source": [
        "# Calcule de l'erreur quadratique moyenne et de l'erreur absolue moyenne \r\n",
        "\r\n",
        "mae = tf.keras.metrics.mean_absolute_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions)[:,0,0]).numpy()\r\n",
        "mse = tf.keras.metrics.mean_squared_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions)[:,0,0]).numpy()\r\n",
        "\r\n",
        "print(mae)\r\n",
        "print(mse)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}