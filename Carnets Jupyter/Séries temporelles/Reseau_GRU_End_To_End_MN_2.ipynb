{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reseau_GRU_End_to_End_MN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPBWx9YUmyzGWp1Ls++9hkm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/Reseau_GRU_End_To_End_MN_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubCeIvtF6R4W"
      },
      "source": [
        "Dans ce carnet nous allons mettre en place un modèle à réseau de neurones à mémoire de type End-To-End Memory Network.  \n",
        "Le papier de recherche associé est disponible [ici](https://arxiv.org/pdf/1503.08895)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRhtHsNn5fc3"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFeah3y_6kif"
      },
      "source": [
        "# Création de la série temporelle et du dataset pour l'entrainement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJfSLtub6sdc"
      },
      "source": [
        "# Fonction permettant d'afficher une série temporelle\n",
        "def affiche_serie(temps, serie, format=\"-\", debut=0, fin=None, label=None):\n",
        "    plt.plot(temps[debut:fin], serie[debut:fin], format, label=label)\n",
        "    plt.xlabel(\"Temps\")\n",
        "    plt.ylabel(\"Valeur\")\n",
        "    if label:\n",
        "        plt.legend(fontsize=14)\n",
        "    plt.grid(True)\n",
        "\n",
        "# Fonction permettant de créer une tendance\n",
        "def tendance(temps, pente=0):\n",
        "    return pente * temps\n",
        "\n",
        "# Fonction permettant de créer un motif\n",
        "def motif_periodique(instants):\n",
        "    return (np.where(instants < 0.4,                            # Si les instants sont < 0.4\n",
        "                    np.cos(instants * 2 * np.pi),               # Alors on retourne la fonction cos(2*pi*t)\n",
        "                    1 / np.exp(3 * instants)))                  # Sinon, on retourne la fonction exp(-3t)\n",
        "\n",
        "# Fonction permettant de créer une saisonnalité avec un motif\n",
        "def saisonnalite(temps, periode, amplitude=1, phase=0):\n",
        "    \"\"\"Répétition du motif sur la même période\"\"\"\n",
        "    instants = ((temps + phase) % periode) / periode            # Mapping du temps =[0 1 2 ... 1460] => instants = [0.0 ... 1.0]\n",
        "    return amplitude * motif_periodique(instants)\n",
        "\n",
        "# Fonction permettant de générer du bruit gaussien N(0,1)\n",
        "def bruit_blanc(temps, niveau_bruit=1, graine=None):\n",
        "    rnd = np.random.RandomState(graine)\n",
        "    return rnd.randn(len(temps)) * niveau_bruit\n",
        "\n",
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "# au format X(X1,X2,...Xn) / Y(Y1,Y2,...,Yn)\n",
        "# X sont les données d'entrées du réseau\n",
        "# Y sont les labels\n",
        "\n",
        "def data_map2(x,y):\n",
        "  x1 = []\n",
        "  x2 = []\n",
        "  ys = []\n",
        "  for i in range(0,Nbr_Sequences+1):\n",
        "    x1.append(x[:-1])\n",
        "    x2.append(x[i][:-1])\n",
        "    ys.append(y[i][:])\n",
        "  return [(x1,x2),ys]\n",
        "\n",
        "def data_map(x,y):\n",
        "  x1 = []\n",
        "  x2 = []\n",
        "  ys = []\n",
        "  for i in range(0,Nbr_Sequences+1):\n",
        "    x1.append(x)\n",
        "    x2.append(x[i][:])\n",
        "    ys.append(y[i][:])\n",
        "  return [(x1,x2),ys]\n",
        "\n",
        "\n",
        "def prepare_dataset_XY(serie, taille_fenetre, batch_size, buffer_melange,nbr_sequences):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie)\n",
        "  dataset = dataset.window(taille_fenetre+1, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre + 1))\n",
        "  dataset = dataset.window(nbr_sequences+1, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(nbr_sequences+1,drop_remainder=True))\n",
        "#  dataset = dataset.map(lambda x: (tf.slice(x,[0,0],[nbr_sequences+1,taille_fenetre]), tf.slice(x,[0,taille_fenetre],[nbr_sequences+1,1])))\n",
        "#  dataset = dataset.map(lambda x: x)\n",
        "  dataset = dataset.map(lambda x: [(tf.slice(x,[0,0],[nbr_sequences,taille_fenetre]),                           # (30;20)       [((30,20),(20)),(1)]\n",
        "                                   tf.squeeze(tf.slice(x,[nbr_sequences,0],[1,taille_fenetre]),axis=0)),        # (20)\n",
        "                                   tf.squeeze(tf.slice(x,[nbr_sequences,taille_fenetre],[1,1]),axis=0)])        # (1)\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True)\n",
        "#  dataset = dataset.map(data_map)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "def prepare_dataset_XY2(serie, taille_fenetre, batch_size, buffer_melange):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie)\n",
        "  dataset = dataset.window(taille_fenetre+1, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre + 1))\n",
        "  dataset = dataset.map(lambda x: (x[:-1], x[-1:]))\n",
        "  dataset = dataset.window(Nbr_Sequences+1, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(Nbr_Sequences),drop_remainder=True)\n",
        "\n",
        "#  dataset = dataset.batch(Nbr_Sequences,drop_remainder=True).prefetch(1)\n",
        "#  dataset = dataset.map(data_map)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "# Création de la série temporelle\n",
        "temps = np.arange(4 * 365)                # temps = [0 1 2 .... 4*365] = [0 1 2 .... 1460]\n",
        "amplitude = 40                            # Amplitude de la la saisonnalité\n",
        "niveau_bruit = 5                          # Niveau du bruit\n",
        "offset = 10                               # Offset de la série\n",
        "\n",
        "serie = offset + tendance(temps, 0.1) + saisonnalite(temps, periode=365, amplitude=amplitude) + bruit_blanc(temps,niveau_bruit,graine=40)\n",
        "\n",
        "temps_separation = 1000\n",
        "\n",
        "# Extraction des temps et des données d'entrainement\n",
        "temps_entrainement = temps[:temps_separation]\n",
        "x_entrainement = serie[:temps_separation]\n",
        "\n",
        "# Exctraction des temps et des données de valiadation\n",
        "temps_validation = temps[temps_separation:]\n",
        "x_validation = serie[temps_separation:]\n",
        "\n",
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "taille_fenetre = 20\n",
        "batch_size = 32\n",
        "buffer_melange = 1000\n",
        "Nbr_Sequences = 32\n",
        "\n",
        "# Création du dataset X,Y\n",
        "dataset = prepare_dataset_XY(x_entrainement,taille_fenetre,batch_size,buffer_melange,Nbr_Sequences)\n",
        "\n",
        "# Création du dataset X,Y de validation\n",
        "dataset_Val = prepare_dataset_XY(x_validation,taille_fenetre,batch_size,buffer_melange,Nbr_Sequences)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyndQsxw0wue"
      },
      "source": [
        "# Calcul de la moyenne et de l'écart type de la série\n",
        "mean = tf.math.reduce_mean(serie)\n",
        "std = tf.math.reduce_std(serie)\n",
        "\n",
        "# Normalise les données\n",
        "Serie_Normalisee = (serie-mean)/std\n",
        "min = tf.math.reduce_min(serie)\n",
        "max = tf.math.reduce_max(serie)\n",
        "\n",
        "# Création des données pour l'entrainement et le test\n",
        "x_entrainement_norm = Serie_Normalisee[:temps_separation]\n",
        "x_validation_norm = Serie_Normalisee[temps_separation:]\n",
        "\n",
        "# Création du dataset X,Y\n",
        "dataset_norm = prepare_dataset_XY(x_entrainement_norm,taille_fenetre,batch_size,buffer_melange,Nbr_Sequences)\n",
        "\n",
        "# Création du dataset X,Y de validation\n",
        "dataset_Val_norm = prepare_dataset_XY(x_validation_norm,taille_fenetre,batch_size,buffer_melange,Nbr_Sequences)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRxcpFY-wpxJ",
        "outputId": "9d9b8a95-6b9f-4d80-805f-c144ad1abd1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(len(list(dataset_norm.as_numpy_iterator())))\n",
        "for element in dataset_norm.take(1):\n",
        "  print(element)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "29\n",
            "((<tf.Tensor: shape=(32, 30, 20), dtype=float64, numpy=\n",
            "array([[[-0.99343325, -0.93635495, -0.99820278, ..., -0.86781814,\n",
            "         -0.9492108 , -1.07462285],\n",
            "        [-0.93635495, -0.99820278, -0.81250866, ..., -0.9492108 ,\n",
            "         -1.07462285, -0.89854575],\n",
            "        [-0.99820278, -0.81250866, -1.12759919, ..., -1.07462285,\n",
            "         -0.89854575, -0.92275185],\n",
            "        ...,\n",
            "        [-0.99485756, -1.12158772, -0.81284639, ..., -1.01016079,\n",
            "         -1.05143842, -1.08965153],\n",
            "        [-1.12158772, -0.81284639, -0.971528  , ..., -1.05143842,\n",
            "         -1.08965153, -1.02651943],\n",
            "        [-0.81284639, -0.971528  , -1.01684347, ..., -1.08965153,\n",
            "         -1.02651943, -1.12913165]],\n",
            "\n",
            "       [[-0.93635495, -0.99820278, -0.81250866, ..., -0.9492108 ,\n",
            "         -1.07462285, -0.89854575],\n",
            "        [-0.99820278, -0.81250866, -1.12759919, ..., -1.07462285,\n",
            "         -0.89854575, -0.92275185],\n",
            "        [-0.81250866, -1.12759919, -0.969365  , ..., -0.89854575,\n",
            "         -0.92275185, -0.93496518],\n",
            "        ...,\n",
            "        [-1.12158772, -0.81284639, -0.971528  , ..., -1.05143842,\n",
            "         -1.08965153, -1.02651943],\n",
            "        [-0.81284639, -0.971528  , -1.01684347, ..., -1.08965153,\n",
            "         -1.02651943, -1.12913165],\n",
            "        [-0.971528  , -1.01684347, -1.08266368, ..., -1.02651943,\n",
            "         -1.12913165, -1.03877078]],\n",
            "\n",
            "       [[-0.99820278, -0.81250866, -1.12759919, ..., -1.07462285,\n",
            "         -0.89854575, -0.92275185],\n",
            "        [-0.81250866, -1.12759919, -0.969365  , ..., -0.89854575,\n",
            "         -0.92275185, -0.93496518],\n",
            "        [-1.12759919, -0.969365  , -0.65369641, ..., -0.92275185,\n",
            "         -0.93496518, -0.79702382],\n",
            "        ...,\n",
            "        [-0.81284639, -0.971528  , -1.01684347, ..., -1.08965153,\n",
            "         -1.02651943, -1.12913165],\n",
            "        [-0.971528  , -1.01684347, -1.08266368, ..., -1.02651943,\n",
            "         -1.12913165, -1.03877078],\n",
            "        [-1.01684347, -1.08266368, -1.04162539, ..., -1.12913165,\n",
            "         -1.03877078, -1.26340409]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[-0.81284639, -0.971528  , -1.01684347, ..., -1.08965153,\n",
            "         -1.02651943, -1.12913165],\n",
            "        [-0.971528  , -1.01684347, -1.08266368, ..., -1.02651943,\n",
            "         -1.12913165, -1.03877078],\n",
            "        [-1.01684347, -1.08266368, -1.04162539, ..., -1.12913165,\n",
            "         -1.03877078, -1.26340409],\n",
            "        ...,\n",
            "        [-1.027961  , -1.12707359, -0.95768219, ..., -1.50432025,\n",
            "         -1.61184831, -1.39407056],\n",
            "        [-1.12707359, -0.95768219, -1.28125554, ..., -1.61184831,\n",
            "         -1.39407056, -1.33690724],\n",
            "        [-0.95768219, -1.28125554, -1.17036444, ..., -1.39407056,\n",
            "         -1.33690724, -1.44769959]],\n",
            "\n",
            "       [[-0.971528  , -1.01684347, -1.08266368, ..., -1.02651943,\n",
            "         -1.12913165, -1.03877078],\n",
            "        [-1.01684347, -1.08266368, -1.04162539, ..., -1.12913165,\n",
            "         -1.03877078, -1.26340409],\n",
            "        [-1.08266368, -1.04162539, -1.15101694, ..., -1.03877078,\n",
            "         -1.26340409, -1.17533422],\n",
            "        ...,\n",
            "        [-1.12707359, -0.95768219, -1.28125554, ..., -1.61184831,\n",
            "         -1.39407056, -1.33690724],\n",
            "        [-0.95768219, -1.28125554, -1.17036444, ..., -1.39407056,\n",
            "         -1.33690724, -1.44769959],\n",
            "        [-1.28125554, -1.17036444, -1.25323052, ..., -1.33690724,\n",
            "         -1.44769959, -1.3571063 ]],\n",
            "\n",
            "       [[-1.01684347, -1.08266368, -1.04162539, ..., -1.12913165,\n",
            "         -1.03877078, -1.26340409],\n",
            "        [-1.08266368, -1.04162539, -1.15101694, ..., -1.03877078,\n",
            "         -1.26340409, -1.17533422],\n",
            "        [-1.04162539, -1.15101694, -0.98109805, ..., -1.26340409,\n",
            "         -1.17533422, -1.14394824],\n",
            "        ...,\n",
            "        [-0.95768219, -1.28125554, -1.17036444, ..., -1.39407056,\n",
            "         -1.33690724, -1.44769959],\n",
            "        [-1.28125554, -1.17036444, -1.25323052, ..., -1.33690724,\n",
            "         -1.44769959, -1.3571063 ],\n",
            "        [-1.17036444, -1.25323052, -1.10758111, ..., -1.44769959,\n",
            "         -1.3571063 , -1.48821138]]])>, <tf.Tensor: shape=(32, 20), dtype=float64, numpy=\n",
            "array([[-0.971528  , -1.01684347, -1.08266368, -1.04162539, -1.15101694,\n",
            "        -0.98109805, -1.17422182, -1.08180822, -1.01729042, -1.09090534,\n",
            "        -0.9207246 , -0.93777485, -1.04786244, -1.11448069, -1.01016079,\n",
            "        -1.05143842, -1.08965153, -1.02651943, -1.12913165, -1.03877078],\n",
            "       [-1.01684347, -1.08266368, -1.04162539, -1.15101694, -0.98109805,\n",
            "        -1.17422182, -1.08180822, -1.01729042, -1.09090534, -0.9207246 ,\n",
            "        -0.93777485, -1.04786244, -1.11448069, -1.01016079, -1.05143842,\n",
            "        -1.08965153, -1.02651943, -1.12913165, -1.03877078, -1.26340409],\n",
            "       [-1.08266368, -1.04162539, -1.15101694, -0.98109805, -1.17422182,\n",
            "        -1.08180822, -1.01729042, -1.09090534, -0.9207246 , -0.93777485,\n",
            "        -1.04786244, -1.11448069, -1.01016079, -1.05143842, -1.08965153,\n",
            "        -1.02651943, -1.12913165, -1.03877078, -1.26340409, -1.17533422],\n",
            "       [-1.04162539, -1.15101694, -0.98109805, -1.17422182, -1.08180822,\n",
            "        -1.01729042, -1.09090534, -0.9207246 , -0.93777485, -1.04786244,\n",
            "        -1.11448069, -1.01016079, -1.05143842, -1.08965153, -1.02651943,\n",
            "        -1.12913165, -1.03877078, -1.26340409, -1.17533422, -1.14394824],\n",
            "       [-1.15101694, -0.98109805, -1.17422182, -1.08180822, -1.01729042,\n",
            "        -1.09090534, -0.9207246 , -0.93777485, -1.04786244, -1.11448069,\n",
            "        -1.01016079, -1.05143842, -1.08965153, -1.02651943, -1.12913165,\n",
            "        -1.03877078, -1.26340409, -1.17533422, -1.14394824, -1.28386208],\n",
            "       [-0.98109805, -1.17422182, -1.08180822, -1.01729042, -1.09090534,\n",
            "        -0.9207246 , -0.93777485, -1.04786244, -1.11448069, -1.01016079,\n",
            "        -1.05143842, -1.08965153, -1.02651943, -1.12913165, -1.03877078,\n",
            "        -1.26340409, -1.17533422, -1.14394824, -1.28386208, -1.2304214 ],\n",
            "       [-1.17422182, -1.08180822, -1.01729042, -1.09090534, -0.9207246 ,\n",
            "        -0.93777485, -1.04786244, -1.11448069, -1.01016079, -1.05143842,\n",
            "        -1.08965153, -1.02651943, -1.12913165, -1.03877078, -1.26340409,\n",
            "        -1.17533422, -1.14394824, -1.28386208, -1.2304214 , -1.41002804],\n",
            "       [-1.08180822, -1.01729042, -1.09090534, -0.9207246 , -0.93777485,\n",
            "        -1.04786244, -1.11448069, -1.01016079, -1.05143842, -1.08965153,\n",
            "        -1.02651943, -1.12913165, -1.03877078, -1.26340409, -1.17533422,\n",
            "        -1.14394824, -1.28386208, -1.2304214 , -1.41002804, -1.027961  ],\n",
            "       [-1.01729042, -1.09090534, -0.9207246 , -0.93777485, -1.04786244,\n",
            "        -1.11448069, -1.01016079, -1.05143842, -1.08965153, -1.02651943,\n",
            "        -1.12913165, -1.03877078, -1.26340409, -1.17533422, -1.14394824,\n",
            "        -1.28386208, -1.2304214 , -1.41002804, -1.027961  , -1.12707359],\n",
            "       [-1.09090534, -0.9207246 , -0.93777485, -1.04786244, -1.11448069,\n",
            "        -1.01016079, -1.05143842, -1.08965153, -1.02651943, -1.12913165,\n",
            "        -1.03877078, -1.26340409, -1.17533422, -1.14394824, -1.28386208,\n",
            "        -1.2304214 , -1.41002804, -1.027961  , -1.12707359, -0.95768219],\n",
            "       [-0.9207246 , -0.93777485, -1.04786244, -1.11448069, -1.01016079,\n",
            "        -1.05143842, -1.08965153, -1.02651943, -1.12913165, -1.03877078,\n",
            "        -1.26340409, -1.17533422, -1.14394824, -1.28386208, -1.2304214 ,\n",
            "        -1.41002804, -1.027961  , -1.12707359, -0.95768219, -1.28125554],\n",
            "       [-0.93777485, -1.04786244, -1.11448069, -1.01016079, -1.05143842,\n",
            "        -1.08965153, -1.02651943, -1.12913165, -1.03877078, -1.26340409,\n",
            "        -1.17533422, -1.14394824, -1.28386208, -1.2304214 , -1.41002804,\n",
            "        -1.027961  , -1.12707359, -0.95768219, -1.28125554, -1.17036444],\n",
            "       [-1.04786244, -1.11448069, -1.01016079, -1.05143842, -1.08965153,\n",
            "        -1.02651943, -1.12913165, -1.03877078, -1.26340409, -1.17533422,\n",
            "        -1.14394824, -1.28386208, -1.2304214 , -1.41002804, -1.027961  ,\n",
            "        -1.12707359, -0.95768219, -1.28125554, -1.17036444, -1.25323052],\n",
            "       [-1.11448069, -1.01016079, -1.05143842, -1.08965153, -1.02651943,\n",
            "        -1.12913165, -1.03877078, -1.26340409, -1.17533422, -1.14394824,\n",
            "        -1.28386208, -1.2304214 , -1.41002804, -1.027961  , -1.12707359,\n",
            "        -0.95768219, -1.28125554, -1.17036444, -1.25323052, -1.10758111],\n",
            "       [-1.01016079, -1.05143842, -1.08965153, -1.02651943, -1.12913165,\n",
            "        -1.03877078, -1.26340409, -1.17533422, -1.14394824, -1.28386208,\n",
            "        -1.2304214 , -1.41002804, -1.027961  , -1.12707359, -0.95768219,\n",
            "        -1.28125554, -1.17036444, -1.25323052, -1.10758111, -1.28671617],\n",
            "       [-1.05143842, -1.08965153, -1.02651943, -1.12913165, -1.03877078,\n",
            "        -1.26340409, -1.17533422, -1.14394824, -1.28386208, -1.2304214 ,\n",
            "        -1.41002804, -1.027961  , -1.12707359, -0.95768219, -1.28125554,\n",
            "        -1.17036444, -1.25323052, -1.10758111, -1.28671617, -1.08564011],\n",
            "       [-1.08965153, -1.02651943, -1.12913165, -1.03877078, -1.26340409,\n",
            "        -1.17533422, -1.14394824, -1.28386208, -1.2304214 , -1.41002804,\n",
            "        -1.027961  , -1.12707359, -0.95768219, -1.28125554, -1.17036444,\n",
            "        -1.25323052, -1.10758111, -1.28671617, -1.08564011, -1.40944376],\n",
            "       [-1.02651943, -1.12913165, -1.03877078, -1.26340409, -1.17533422,\n",
            "        -1.14394824, -1.28386208, -1.2304214 , -1.41002804, -1.027961  ,\n",
            "        -1.12707359, -0.95768219, -1.28125554, -1.17036444, -1.25323052,\n",
            "        -1.10758111, -1.28671617, -1.08564011, -1.40944376, -1.22866364],\n",
            "       [-1.12913165, -1.03877078, -1.26340409, -1.17533422, -1.14394824,\n",
            "        -1.28386208, -1.2304214 , -1.41002804, -1.027961  , -1.12707359,\n",
            "        -0.95768219, -1.28125554, -1.17036444, -1.25323052, -1.10758111,\n",
            "        -1.28671617, -1.08564011, -1.40944376, -1.22866364, -1.33369539],\n",
            "       [-1.03877078, -1.26340409, -1.17533422, -1.14394824, -1.28386208,\n",
            "        -1.2304214 , -1.41002804, -1.027961  , -1.12707359, -0.95768219,\n",
            "        -1.28125554, -1.17036444, -1.25323052, -1.10758111, -1.28671617,\n",
            "        -1.08564011, -1.40944376, -1.22866364, -1.33369539, -1.45298826],\n",
            "       [-1.26340409, -1.17533422, -1.14394824, -1.28386208, -1.2304214 ,\n",
            "        -1.41002804, -1.027961  , -1.12707359, -0.95768219, -1.28125554,\n",
            "        -1.17036444, -1.25323052, -1.10758111, -1.28671617, -1.08564011,\n",
            "        -1.40944376, -1.22866364, -1.33369539, -1.45298826, -1.3407002 ],\n",
            "       [-1.17533422, -1.14394824, -1.28386208, -1.2304214 , -1.41002804,\n",
            "        -1.027961  , -1.12707359, -0.95768219, -1.28125554, -1.17036444,\n",
            "        -1.25323052, -1.10758111, -1.28671617, -1.08564011, -1.40944376,\n",
            "        -1.22866364, -1.33369539, -1.45298826, -1.3407002 , -1.55761282],\n",
            "       [-1.14394824, -1.28386208, -1.2304214 , -1.41002804, -1.027961  ,\n",
            "        -1.12707359, -0.95768219, -1.28125554, -1.17036444, -1.25323052,\n",
            "        -1.10758111, -1.28671617, -1.08564011, -1.40944376, -1.22866364,\n",
            "        -1.33369539, -1.45298826, -1.3407002 , -1.55761282, -1.33989769],\n",
            "       [-1.28386208, -1.2304214 , -1.41002804, -1.027961  , -1.12707359,\n",
            "        -0.95768219, -1.28125554, -1.17036444, -1.25323052, -1.10758111,\n",
            "        -1.28671617, -1.08564011, -1.40944376, -1.22866364, -1.33369539,\n",
            "        -1.45298826, -1.3407002 , -1.55761282, -1.33989769, -1.31496315],\n",
            "       [-1.2304214 , -1.41002804, -1.027961  , -1.12707359, -0.95768219,\n",
            "        -1.28125554, -1.17036444, -1.25323052, -1.10758111, -1.28671617,\n",
            "        -1.08564011, -1.40944376, -1.22866364, -1.33369539, -1.45298826,\n",
            "        -1.3407002 , -1.55761282, -1.33989769, -1.31496315, -1.50432025],\n",
            "       [-1.41002804, -1.027961  , -1.12707359, -0.95768219, -1.28125554,\n",
            "        -1.17036444, -1.25323052, -1.10758111, -1.28671617, -1.08564011,\n",
            "        -1.40944376, -1.22866364, -1.33369539, -1.45298826, -1.3407002 ,\n",
            "        -1.55761282, -1.33989769, -1.31496315, -1.50432025, -1.61184831],\n",
            "       [-1.027961  , -1.12707359, -0.95768219, -1.28125554, -1.17036444,\n",
            "        -1.25323052, -1.10758111, -1.28671617, -1.08564011, -1.40944376,\n",
            "        -1.22866364, -1.33369539, -1.45298826, -1.3407002 , -1.55761282,\n",
            "        -1.33989769, -1.31496315, -1.50432025, -1.61184831, -1.39407056],\n",
            "       [-1.12707359, -0.95768219, -1.28125554, -1.17036444, -1.25323052,\n",
            "        -1.10758111, -1.28671617, -1.08564011, -1.40944376, -1.22866364,\n",
            "        -1.33369539, -1.45298826, -1.3407002 , -1.55761282, -1.33989769,\n",
            "        -1.31496315, -1.50432025, -1.61184831, -1.39407056, -1.33690724],\n",
            "       [-0.95768219, -1.28125554, -1.17036444, -1.25323052, -1.10758111,\n",
            "        -1.28671617, -1.08564011, -1.40944376, -1.22866364, -1.33369539,\n",
            "        -1.45298826, -1.3407002 , -1.55761282, -1.33989769, -1.31496315,\n",
            "        -1.50432025, -1.61184831, -1.39407056, -1.33690724, -1.44769959],\n",
            "       [-1.28125554, -1.17036444, -1.25323052, -1.10758111, -1.28671617,\n",
            "        -1.08564011, -1.40944376, -1.22866364, -1.33369539, -1.45298826,\n",
            "        -1.3407002 , -1.55761282, -1.33989769, -1.31496315, -1.50432025,\n",
            "        -1.61184831, -1.39407056, -1.33690724, -1.44769959, -1.3571063 ],\n",
            "       [-1.17036444, -1.25323052, -1.10758111, -1.28671617, -1.08564011,\n",
            "        -1.40944376, -1.22866364, -1.33369539, -1.45298826, -1.3407002 ,\n",
            "        -1.55761282, -1.33989769, -1.31496315, -1.50432025, -1.61184831,\n",
            "        -1.39407056, -1.33690724, -1.44769959, -1.3571063 , -1.48821138],\n",
            "       [-1.25323052, -1.10758111, -1.28671617, -1.08564011, -1.40944376,\n",
            "        -1.22866364, -1.33369539, -1.45298826, -1.3407002 , -1.55761282,\n",
            "        -1.33989769, -1.31496315, -1.50432025, -1.61184831, -1.39407056,\n",
            "        -1.33690724, -1.44769959, -1.3571063 , -1.48821138, -1.48923242]])>), <tf.Tensor: shape=(32, 1), dtype=float64, numpy=\n",
            "array([[-1.26340409],\n",
            "       [-1.17533422],\n",
            "       [-1.14394824],\n",
            "       [-1.28386208],\n",
            "       [-1.2304214 ],\n",
            "       [-1.41002804],\n",
            "       [-1.027961  ],\n",
            "       [-1.12707359],\n",
            "       [-0.95768219],\n",
            "       [-1.28125554],\n",
            "       [-1.17036444],\n",
            "       [-1.25323052],\n",
            "       [-1.10758111],\n",
            "       [-1.28671617],\n",
            "       [-1.08564011],\n",
            "       [-1.40944376],\n",
            "       [-1.22866364],\n",
            "       [-1.33369539],\n",
            "       [-1.45298826],\n",
            "       [-1.3407002 ],\n",
            "       [-1.55761282],\n",
            "       [-1.33989769],\n",
            "       [-1.31496315],\n",
            "       [-1.50432025],\n",
            "       [-1.61184831],\n",
            "       [-1.39407056],\n",
            "       [-1.33690724],\n",
            "       [-1.44769959],\n",
            "       [-1.3571063 ],\n",
            "       [-1.48821138],\n",
            "       [-1.48923242],\n",
            "       [-1.39251269]])>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Yt-EgZ3sgPY"
      },
      "source": [
        "# Création du modèle End-To-End Memory Network (à base de couches GRU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyrcfKcgCsZ7"
      },
      "source": [
        "**1. Création du réseau et adaptation des formats d'entrée et de sortie**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeJyix8HK7Kt"
      },
      "source": [
        "Sous forme de shéma, notre réseau est donc le suivant :\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OZkfsmnBNHY"
      },
      "source": [
        "<img src=\"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/images/End-To-End.png?raw=true\" width=\"1200\"> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hhk0kmPSgqva"
      },
      "source": [
        "# Définition du de la couche du modèle\n",
        "# End-to-End Memory Network\n",
        "# Epaquetage des données avec le dernier état caché d'une couche GRU\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "class Couche_End_to_End_MN(tf.keras.layers.Layer):\n",
        "  # Fonction d'initialisation de la classe d'attention\n",
        "  # dim_GRU : Dimension des vecteurs GRU\n",
        "  # x : Séquences à mémoriser (batch_size, Nbr_Sequence, taille_fenetre)\n",
        "  # Fonction de la couche lambda d'entrée\n",
        "  def __init__(self,dim_GRU):\n",
        "    self.dim_GRU = dim_GRU\n",
        "    super().__init__()          # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    # Définition des couches GRU pour traiter les séquences d'entrée\n",
        "    self.couche_GRU_A = tf.keras.layers.GRU(self.dim_GRU)\n",
        "    self.couche_GRU_B = tf.keras.layers.GRU(self.dim_GRU)\n",
        "    self.couche_GRU_C = tf.keras.layers.GRU(self.dim_GRU)\n",
        "\n",
        "    # Poids d'attention\n",
        "    self.p = self.add_weight(shape=(input_shape[1],1),initializer=\"zeros\",name=\"p\")\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Définit la logique de la couche d'attention\n",
        "  # Arguments :     x : (batch_size, Nbr_Sequence, taille_fenetre)\n",
        "  #                 y : (batch_size, taille_fenetre)\n",
        "  # Exemple :   batch_size = 32\n",
        "  #             Nbr_Sequence =30\n",
        "  #             taille_fenetre = 20\n",
        "  #             dim_GRU = 40 \n",
        "  def call(self,x,y):\n",
        "    # Création des vecteurs mi dans le tenseur M\n",
        "    M = tf.expand_dims(x,axis=-1)                                   # (32,30,20) => (32,30,20,1)\n",
        "    M = tf.keras.layers.TimeDistributed(self.couche_GRU_A)(M)       # (32,30,20,1) => (32,30,40) : TimeStep = 30 : (32,20,1) envoyé\n",
        "    M = K.tanh(M)\n",
        "\n",
        "    # Création du vecteur d'état u\n",
        "    u = tf.expand_dims(y,axis=-1)                                   # (32,20) => (32,20,1)\n",
        "    u = self.couche_GRU_B(u)                                        # (32,20,1) => (32,40)\n",
        "    u = tf.expand_dims(u,axis=-1)                                   # (32,40) => (32,40,1)\n",
        "    u = K.tanh(u)                                                   # (32,40,1)\n",
        "\n",
        "    # Calcul des poids d'attention\n",
        "#    p = tf.matmul(M,u)                                              # (32,30,40)x(32,40,1)=(32,30,1)\n",
        "    p = tf.keras.layers.Dot(axes=(2,1))([M,u])                      # (32,30,1)\n",
        "    p = tf.keras.activations.softmax(p,axis=1)                      # (32,30,1)\n",
        "\n",
        "    # Création des vecteurs ci dans le tenseur C\n",
        "    C = tf.expand_dims(x,axis=-1)                                   # (32,30,20) => (32,30,20,1)\n",
        "    C = tf.keras.layers.TimeDistributed(self.couche_GRU_C)(C)       # (32,30,20,1) => (32,30,40) : TimeStep = 30 : (32,20,1) envoyé\n",
        "    C = K.tanh(C)\n",
        "\n",
        "    # Calcul du vecteur réponse issu de la mémoire\n",
        "    o = tf.multiply(C,p)                                            # (32,30,40)_x_(32,30,1) = (32,30,40)\n",
        "    o = K.sum(o, axis=1)                                            # (32,40)\n",
        "    o = K.tanh(o)                                                   # (32,40)\n",
        "    \n",
        "    # Retourne le vecteur d'attention\n",
        "    return (o+tf.squeeze(u,axis=2))\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-UkZgGgzifK",
        "outputId": "dfcabefc-8fc9-436d-decc-4facc12897ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dim_GRU = 40\n",
        "\n",
        "# Fonction de la couche lambda d'entrée\n",
        "def Traitement_Entrees(x):\n",
        "  return tf.expand_dims(x,axis=-1)\n",
        "\n",
        "# Définition des entrées du modèle\n",
        "entrees_sequences = tf.keras.layers.Input(shape=(Nbr_Sequences,taille_fenetre),batch_size=batch_size)\n",
        "entrees_entrainement = tf.keras.layers.Input(shape=(taille_fenetre),batch_size=batch_size)\n",
        "\n",
        "# Encodeur\n",
        "s_encodeur = Couche_End_to_End_MN(dim_GRU=dim_GRU)(entrees_sequences,entrees_entrainement)\n",
        "\n",
        "# Décodeur\n",
        "s_decodeur = tf.keras.layers.Dense(dim_GRU,activation=\"tanh\")(s_encodeur)\n",
        "s_decodeur = tf.keras.layers.Concatenate()([s_decodeur,s_encodeur])\n",
        "\n",
        "# Générateur\n",
        "sortie = tf.keras.layers.Dense(1)(s_decodeur)\n",
        "\n",
        "# Construction du modèle\n",
        "model = tf.keras.Model([entrees_sequences,entrees_entrainement],sortie)\n",
        "\n",
        "model.save_weights(\"model_initial.hdf5\")\n",
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(32, 32, 20)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(32, 20)]           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "couche__end_to__end_mn (Couche_ (32, 40)             15512       input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (32, 40)             1640        couche__end_to__end_mn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (32, 80)             0           dense[0][0]                      \n",
            "                                                                 couche__end_to__end_mn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (32, 1)              81          concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 17,233\n",
            "Trainable params: 17,233\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUM0-SSXGLIQ"
      },
      "source": [
        "**2. Optimisation du taux d'apprentissage**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jejCBhXVuNQ4",
        "outputId": "e1e1d10f-a361-4762-8416-cda2f19f2f32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(lr=1e-8)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_opti.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur, metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset_norm,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['couche__end_to__end_mn_1/p:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['couche__end_to__end_mn_1/p:0'] when minimizing the loss.\n",
            "29/29 [==============================] - 7s 98ms/step - loss: 0.6951 - mae: 1.1296\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.38871, saving model to poids_opti.hdf5\n",
            "Epoch 2/100\n",
            "29/29 [==============================] - 3s 97ms/step - loss: 0.6951 - mae: 1.1296\n",
            "\n",
            "Epoch 00002: loss improved from 0.38871 to 0.38870, saving model to poids_opti.hdf5\n",
            "Epoch 3/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.6951 - mae: 1.1296\n",
            "\n",
            "Epoch 00003: loss improved from 0.38870 to 0.38870, saving model to poids_opti.hdf5\n",
            "Epoch 4/100\n",
            "29/29 [==============================] - 3s 97ms/step - loss: 0.6950 - mae: 1.1296\n",
            "\n",
            "Epoch 00004: loss improved from 0.38870 to 0.38870, saving model to poids_opti.hdf5\n",
            "Epoch 5/100\n",
            "29/29 [==============================] - 3s 97ms/step - loss: 0.6950 - mae: 1.1296\n",
            "\n",
            "Epoch 00005: loss improved from 0.38870 to 0.38870, saving model to poids_opti.hdf5\n",
            "Epoch 6/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.6950 - mae: 1.1296\n",
            "\n",
            "Epoch 00006: loss improved from 0.38870 to 0.38870, saving model to poids_opti.hdf5\n",
            "Epoch 7/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.6950 - mae: 1.1296\n",
            "\n",
            "Epoch 00007: loss improved from 0.38870 to 0.38870, saving model to poids_opti.hdf5\n",
            "Epoch 8/100\n",
            "29/29 [==============================] - 3s 96ms/step - loss: 0.6950 - mae: 1.1296\n",
            "\n",
            "Epoch 00008: loss improved from 0.38870 to 0.38869, saving model to poids_opti.hdf5\n",
            "Epoch 9/100\n",
            "29/29 [==============================] - 3s 96ms/step - loss: 0.6950 - mae: 1.1295\n",
            "\n",
            "Epoch 00009: loss improved from 0.38869 to 0.38869, saving model to poids_opti.hdf5\n",
            "Epoch 10/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.6950 - mae: 1.1295\n",
            "\n",
            "Epoch 00010: loss improved from 0.38869 to 0.38869, saving model to poids_opti.hdf5\n",
            "Epoch 11/100\n",
            "29/29 [==============================] - 3s 97ms/step - loss: 0.6950 - mae: 1.1295\n",
            "\n",
            "Epoch 00011: loss improved from 0.38869 to 0.38868, saving model to poids_opti.hdf5\n",
            "Epoch 12/100\n",
            "29/29 [==============================] - 3s 97ms/step - loss: 0.6950 - mae: 1.1295\n",
            "\n",
            "Epoch 00012: loss improved from 0.38868 to 0.38867, saving model to poids_opti.hdf5\n",
            "Epoch 13/100\n",
            "29/29 [==============================] - 3s 97ms/step - loss: 0.6950 - mae: 1.1295\n",
            "\n",
            "Epoch 00013: loss improved from 0.38867 to 0.38866, saving model to poids_opti.hdf5\n",
            "Epoch 14/100\n",
            "29/29 [==============================] - 3s 97ms/step - loss: 0.6949 - mae: 1.1295\n",
            "\n",
            "Epoch 00014: loss improved from 0.38866 to 0.38864, saving model to poids_opti.hdf5\n",
            "Epoch 15/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.6949 - mae: 1.1294\n",
            "\n",
            "Epoch 00015: loss improved from 0.38864 to 0.38862, saving model to poids_opti.hdf5\n",
            "Epoch 16/100\n",
            "29/29 [==============================] - 3s 97ms/step - loss: 0.6949 - mae: 1.1294\n",
            "\n",
            "Epoch 00016: loss improved from 0.38862 to 0.38860, saving model to poids_opti.hdf5\n",
            "Epoch 17/100\n",
            "29/29 [==============================] - 3s 97ms/step - loss: 0.6948 - mae: 1.1293\n",
            "\n",
            "Epoch 00017: loss improved from 0.38860 to 0.38857, saving model to poids_opti.hdf5\n",
            "Epoch 18/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.6948 - mae: 1.1293\n",
            "\n",
            "Epoch 00018: loss improved from 0.38857 to 0.38853, saving model to poids_opti.hdf5\n",
            "Epoch 19/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.6947 - mae: 1.1292\n",
            "\n",
            "Epoch 00019: loss improved from 0.38853 to 0.38849, saving model to poids_opti.hdf5\n",
            "Epoch 20/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.6946 - mae: 1.1290\n",
            "\n",
            "Epoch 00020: loss improved from 0.38849 to 0.38842, saving model to poids_opti.hdf5\n",
            "Epoch 21/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.6945 - mae: 1.1289\n",
            "\n",
            "Epoch 00021: loss improved from 0.38842 to 0.38835, saving model to poids_opti.hdf5\n",
            "Epoch 22/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.6943 - mae: 1.1287\n",
            "\n",
            "Epoch 00022: loss improved from 0.38835 to 0.38825, saving model to poids_opti.hdf5\n",
            "Epoch 23/100\n",
            "29/29 [==============================] - 3s 97ms/step - loss: 0.6941 - mae: 1.1285\n",
            "\n",
            "Epoch 00023: loss improved from 0.38825 to 0.38813, saving model to poids_opti.hdf5\n",
            "Epoch 24/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.6938 - mae: 1.1282\n",
            "\n",
            "Epoch 00024: loss improved from 0.38813 to 0.38798, saving model to poids_opti.hdf5\n",
            "Epoch 25/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.6935 - mae: 1.1279\n",
            "\n",
            "Epoch 00025: loss improved from 0.38798 to 0.38778, saving model to poids_opti.hdf5\n",
            "Epoch 26/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.6931 - mae: 1.1274\n",
            "\n",
            "Epoch 00026: loss improved from 0.38778 to 0.38754, saving model to poids_opti.hdf5\n",
            "Epoch 27/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.6926 - mae: 1.1268\n",
            "\n",
            "Epoch 00027: loss improved from 0.38754 to 0.38724, saving model to poids_opti.hdf5\n",
            "Epoch 28/100\n",
            "29/29 [==============================] - 3s 100ms/step - loss: 0.6919 - mae: 1.1261\n",
            "\n",
            "Epoch 00028: loss improved from 0.38724 to 0.38685, saving model to poids_opti.hdf5\n",
            "Epoch 29/100\n",
            "29/29 [==============================] - 3s 97ms/step - loss: 0.6911 - mae: 1.1252\n",
            "\n",
            "Epoch 00029: loss improved from 0.38685 to 0.38637, saving model to poids_opti.hdf5\n",
            "Epoch 30/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.6901 - mae: 1.1241\n",
            "\n",
            "Epoch 00030: loss improved from 0.38637 to 0.38576, saving model to poids_opti.hdf5\n",
            "Epoch 31/100\n",
            "29/29 [==============================] - 3s 97ms/step - loss: 0.6888 - mae: 1.1227\n",
            "\n",
            "Epoch 00031: loss improved from 0.38576 to 0.38499, saving model to poids_opti.hdf5\n",
            "Epoch 32/100\n",
            "29/29 [==============================] - 3s 99ms/step - loss: 0.6872 - mae: 1.1209\n",
            "\n",
            "Epoch 00032: loss improved from 0.38499 to 0.38403, saving model to poids_opti.hdf5\n",
            "Epoch 33/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.6851 - mae: 1.1186\n",
            "\n",
            "Epoch 00033: loss improved from 0.38403 to 0.38282, saving model to poids_opti.hdf5\n",
            "Epoch 34/100\n",
            "29/29 [==============================] - 3s 96ms/step - loss: 0.6826 - mae: 1.1158\n",
            "\n",
            "Epoch 00034: loss improved from 0.38282 to 0.38129, saving model to poids_opti.hdf5\n",
            "Epoch 35/100\n",
            "29/29 [==============================] - 3s 96ms/step - loss: 0.6793 - mae: 1.1122\n",
            "\n",
            "Epoch 00035: loss improved from 0.38129 to 0.37938, saving model to poids_opti.hdf5\n",
            "Epoch 36/100\n",
            "29/29 [==============================] - 3s 101ms/step - loss: 0.6753 - mae: 1.1078\n",
            "\n",
            "Epoch 00036: loss improved from 0.37938 to 0.37698, saving model to poids_opti.hdf5\n",
            "Epoch 37/100\n",
            "29/29 [==============================] - 3s 97ms/step - loss: 0.6702 - mae: 1.1022\n",
            "\n",
            "Epoch 00037: loss improved from 0.37698 to 0.37397, saving model to poids_opti.hdf5\n",
            "Epoch 38/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.6638 - mae: 1.0951\n",
            "\n",
            "Epoch 00038: loss improved from 0.37397 to 0.37021, saving model to poids_opti.hdf5\n",
            "Epoch 39/100\n",
            "29/29 [==============================] - 3s 99ms/step - loss: 0.6558 - mae: 1.0863\n",
            "\n",
            "Epoch 00039: loss improved from 0.37021 to 0.36549, saving model to poids_opti.hdf5\n",
            "Epoch 40/100\n",
            "29/29 [==============================] - 3s 99ms/step - loss: 0.6458 - mae: 1.0752\n",
            "\n",
            "Epoch 00040: loss improved from 0.36549 to 0.35961, saving model to poids_opti.hdf5\n",
            "Epoch 41/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.6333 - mae: 1.0613\n",
            "\n",
            "Epoch 00041: loss improved from 0.35961 to 0.35229, saving model to poids_opti.hdf5\n",
            "Epoch 42/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.6177 - mae: 1.0440\n",
            "\n",
            "Epoch 00042: loss improved from 0.35229 to 0.34321, saving model to poids_opti.hdf5\n",
            "Epoch 43/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.5984 - mae: 1.0225\n",
            "\n",
            "Epoch 00043: loss improved from 0.34321 to 0.33200, saving model to poids_opti.hdf5\n",
            "Epoch 44/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.5746 - mae: 0.9959\n",
            "\n",
            "Epoch 00044: loss improved from 0.33200 to 0.31827, saving model to poids_opti.hdf5\n",
            "Epoch 45/100\n",
            "29/29 [==============================] - 3s 97ms/step - loss: 0.5454 - mae: 0.9632\n",
            "\n",
            "Epoch 00045: loss improved from 0.31827 to 0.30163, saving model to poids_opti.hdf5\n",
            "Epoch 46/100\n",
            "29/29 [==============================] - 3s 99ms/step - loss: 0.5102 - mae: 0.9236\n",
            "\n",
            "Epoch 00046: loss improved from 0.30163 to 0.28174, saving model to poids_opti.hdf5\n",
            "Epoch 47/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.4684 - mae: 0.8761\n",
            "\n",
            "Epoch 00047: loss improved from 0.28174 to 0.25848, saving model to poids_opti.hdf5\n",
            "Epoch 48/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.4200 - mae: 0.8205\n",
            "\n",
            "Epoch 00048: loss improved from 0.25848 to 0.23209, saving model to poids_opti.hdf5\n",
            "Epoch 49/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.3660 - mae: 0.7566\n",
            "\n",
            "Epoch 00049: loss improved from 0.23209 to 0.20328, saving model to poids_opti.hdf5\n",
            "Epoch 50/100\n",
            "29/29 [==============================] - 3s 99ms/step - loss: 0.3084 - mae: 0.6849\n",
            "\n",
            "Epoch 00050: loss improved from 0.20328 to 0.17326, saving model to poids_opti.hdf5\n",
            "Epoch 51/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.2499 - mae: 0.6070\n",
            "\n",
            "Epoch 00051: loss improved from 0.17326 to 0.14358, saving model to poids_opti.hdf5\n",
            "Epoch 52/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.1942 - mae: 0.5248\n",
            "\n",
            "Epoch 00052: loss improved from 0.14358 to 0.11590, saving model to poids_opti.hdf5\n",
            "Epoch 53/100\n",
            "29/29 [==============================] - 3s 99ms/step - loss: 0.1451 - mae: 0.4415\n",
            "\n",
            "Epoch 00053: loss improved from 0.11590 to 0.09165, saving model to poids_opti.hdf5\n",
            "Epoch 54/100\n",
            "29/29 [==============================] - 3s 100ms/step - loss: 0.1058 - mae: 0.3638\n",
            "\n",
            "Epoch 00054: loss improved from 0.09165 to 0.07173, saving model to poids_opti.hdf5\n",
            "Epoch 55/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.0776 - mae: 0.3021\n",
            "\n",
            "Epoch 00055: loss improved from 0.07173 to 0.05630, saving model to poids_opti.hdf5\n",
            "Epoch 56/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.0592 - mae: 0.2591\n",
            "\n",
            "Epoch 00056: loss improved from 0.05630 to 0.04479, saving model to poids_opti.hdf5\n",
            "Epoch 57/100\n",
            "29/29 [==============================] - 3s 99ms/step - loss: 0.0474 - mae: 0.2327\n",
            "\n",
            "Epoch 00057: loss improved from 0.04479 to 0.03650, saving model to poids_opti.hdf5\n",
            "Epoch 58/100\n",
            "29/29 [==============================] - 3s 97ms/step - loss: 0.0397 - mae: 0.2161\n",
            "\n",
            "Epoch 00058: loss improved from 0.03650 to 0.03099, saving model to poids_opti.hdf5\n",
            "Epoch 59/100\n",
            "29/29 [==============================] - 3s 97ms/step - loss: 0.0346 - mae: 0.2050\n",
            "\n",
            "Epoch 00059: loss improved from 0.03099 to 0.02771, saving model to poids_opti.hdf5\n",
            "Epoch 60/100\n",
            "29/29 [==============================] - 3s 99ms/step - loss: 0.0313 - mae: 0.1972\n",
            "\n",
            "Epoch 00060: loss improved from 0.02771 to 0.02588, saving model to poids_opti.hdf5\n",
            "Epoch 61/100\n",
            "29/29 [==============================] - 3s 97ms/step - loss: 0.0292 - mae: 0.1918\n",
            "\n",
            "Epoch 00061: loss improved from 0.02588 to 0.02479, saving model to poids_opti.hdf5\n",
            "Epoch 62/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.0280 - mae: 0.1879\n",
            "\n",
            "Epoch 00062: loss improved from 0.02479 to 0.02399, saving model to poids_opti.hdf5\n",
            "Epoch 63/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.0272 - mae: 0.1847\n",
            "\n",
            "Epoch 00063: loss improved from 0.02399 to 0.02327, saving model to poids_opti.hdf5\n",
            "Epoch 64/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.0266 - mae: 0.1816\n",
            "\n",
            "Epoch 00064: loss improved from 0.02327 to 0.02256, saving model to poids_opti.hdf5\n",
            "Epoch 65/100\n",
            "29/29 [==============================] - 3s 97ms/step - loss: 0.0261 - mae: 0.1777\n",
            "\n",
            "Epoch 00065: loss improved from 0.02256 to 0.02181, saving model to poids_opti.hdf5\n",
            "Epoch 66/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.0254 - mae: 0.1726\n",
            "\n",
            "Epoch 00066: loss improved from 0.02181 to 0.02099, saving model to poids_opti.hdf5\n",
            "Epoch 67/100\n",
            "29/29 [==============================] - 3s 99ms/step - loss: 0.0247 - mae: 0.1679\n",
            "\n",
            "Epoch 00067: loss improved from 0.02099 to 0.02010, saving model to poids_opti.hdf5\n",
            "Epoch 68/100\n",
            "29/29 [==============================] - 3s 99ms/step - loss: 0.0240 - mae: 0.1648\n",
            "\n",
            "Epoch 00068: loss improved from 0.02010 to 0.01925, saving model to poids_opti.hdf5\n",
            "Epoch 69/100\n",
            "29/29 [==============================] - 3s 97ms/step - loss: 0.0241 - mae: 0.1655\n",
            "\n",
            "Epoch 00069: loss improved from 0.01925 to 0.01882, saving model to poids_opti.hdf5\n",
            "Epoch 70/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.0293 - mae: 0.1815\n",
            "\n",
            "Epoch 00070: loss did not improve from 0.01882\n",
            "Epoch 71/100\n",
            "29/29 [==============================] - 3s 99ms/step - loss: 0.0625 - mae: 0.2533\n",
            "\n",
            "Epoch 00071: loss did not improve from 0.01882\n",
            "Epoch 72/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.1033 - mae: 0.3034\n",
            "\n",
            "Epoch 00072: loss did not improve from 0.01882\n",
            "Epoch 73/100\n",
            "29/29 [==============================] - 3s 99ms/step - loss: 0.1045 - mae: 0.3145\n",
            "\n",
            "Epoch 00073: loss did not improve from 0.01882\n",
            "Epoch 74/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.1312 - mae: 0.3649\n",
            "\n",
            "Epoch 00074: loss did not improve from 0.01882\n",
            "Epoch 75/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.1826 - mae: 0.4481\n",
            "\n",
            "Epoch 00075: loss did not improve from 0.01882\n",
            "Epoch 76/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.2053 - mae: 0.4940\n",
            "\n",
            "Epoch 00076: loss did not improve from 0.01882\n",
            "Epoch 77/100\n",
            "29/29 [==============================] - 3s 99ms/step - loss: 0.1665 - mae: 0.4396\n",
            "\n",
            "Epoch 00077: loss did not improve from 0.01882\n",
            "Epoch 78/100\n",
            "29/29 [==============================] - 3s 100ms/step - loss: 0.1441 - mae: 0.3826\n",
            "\n",
            "Epoch 00078: loss did not improve from 0.01882\n",
            "Epoch 79/100\n",
            "29/29 [==============================] - 3s 99ms/step - loss: 9.7757 - mae: 10.2724\n",
            "\n",
            "Epoch 00079: loss did not improve from 0.01882\n",
            "Epoch 80/100\n",
            "29/29 [==============================] - 3s 99ms/step - loss: 19.1130 - mae: 19.6130\n",
            "\n",
            "Epoch 00080: loss did not improve from 0.01882\n",
            "Epoch 81/100\n",
            "29/29 [==============================] - 3s 99ms/step - loss: 27.5259 - mae: 28.0259\n",
            "\n",
            "Epoch 00081: loss did not improve from 0.01882\n",
            "Epoch 82/100\n",
            "29/29 [==============================] - 3s 100ms/step - loss: 38.3484 - mae: 38.8484\n",
            "\n",
            "Epoch 00082: loss did not improve from 0.01882\n",
            "Epoch 83/100\n",
            "29/29 [==============================] - 3s 100ms/step - loss: 43.9541 - mae: 44.4541\n",
            "\n",
            "Epoch 00083: loss did not improve from 0.01882\n",
            "Epoch 84/100\n",
            "29/29 [==============================] - 3s 99ms/step - loss: 53.4668 - mae: 53.9668\n",
            "\n",
            "Epoch 00084: loss did not improve from 0.01882\n",
            "Epoch 85/100\n",
            "29/29 [==============================] - 3s 102ms/step - loss: 69.7443 - mae: 70.2443\n",
            "\n",
            "Epoch 00085: loss did not improve from 0.01882\n",
            "Epoch 86/100\n",
            "29/29 [==============================] - 3s 99ms/step - loss: 87.9700 - mae: 88.4700\n",
            "\n",
            "Epoch 00086: loss did not improve from 0.01882\n",
            "Epoch 87/100\n",
            "29/29 [==============================] - 3s 100ms/step - loss: 111.6933 - mae: 112.1933\n",
            "\n",
            "Epoch 00087: loss did not improve from 0.01882\n",
            "Epoch 88/100\n",
            "29/29 [==============================] - 3s 99ms/step - loss: 137.5243 - mae: 138.0243\n",
            "\n",
            "Epoch 00088: loss did not improve from 0.01882\n",
            "Epoch 89/100\n",
            "29/29 [==============================] - 3s 101ms/step - loss: 170.3755 - mae: 170.8755\n",
            "\n",
            "Epoch 00089: loss did not improve from 0.01882\n",
            "Epoch 90/100\n",
            "29/29 [==============================] - 3s 100ms/step - loss: 202.6794 - mae: 203.1794\n",
            "\n",
            "Epoch 00090: loss did not improve from 0.01882\n",
            "Epoch 91/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 258.9130 - mae: 259.4130\n",
            "\n",
            "Epoch 00091: loss did not improve from 0.01882\n",
            "Epoch 92/100\n",
            "29/29 [==============================] - 3s 99ms/step - loss: 324.9571 - mae: 325.4571\n",
            "\n",
            "Epoch 00092: loss did not improve from 0.01882\n",
            "Epoch 93/100\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 401.7693 - mae: 402.2693\n",
            "\n",
            "Epoch 00093: loss did not improve from 0.01882\n",
            "Epoch 94/100\n",
            "29/29 [==============================] - 3s 99ms/step - loss: 487.4272 - mae: 487.9272\n",
            "\n",
            "Epoch 00094: loss did not improve from 0.01882\n",
            "Epoch 95/100\n",
            "29/29 [==============================] - 3s 101ms/step - loss: 632.7225 - mae: 633.2225\n",
            "\n",
            "Epoch 00095: loss did not improve from 0.01882\n",
            "Epoch 96/100\n",
            "29/29 [==============================] - 3s 104ms/step - loss: 811.1557 - mae: 811.6557\n",
            "\n",
            "Epoch 00096: loss did not improve from 0.01882\n",
            "Epoch 97/100\n",
            "29/29 [==============================] - 3s 103ms/step - loss: 1057.7553 - mae: 1058.2553\n",
            "\n",
            "Epoch 00097: loss did not improve from 0.01882\n",
            "Epoch 98/100\n",
            "29/29 [==============================] - 3s 101ms/step - loss: 1301.0622 - mae: 1301.5622\n",
            "\n",
            "Epoch 00098: loss did not improve from 0.01882\n",
            "Epoch 99/100\n",
            "29/29 [==============================] - 3s 101ms/step - loss: 1700.8533 - mae: 1701.3533\n",
            "\n",
            "Epoch 00099: loss did not improve from 0.01882\n",
            "Epoch 100/100\n",
            "29/29 [==============================] - 3s 100ms/step - loss: 2115.1506 - mae: 2115.6506\n",
            "\n",
            "Epoch 00100: loss did not improve from 0.01882\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1_WMNlzu2B4",
        "outputId": "0b02cd73-9c47-4066-9c2f-428ed5896670",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 1])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, \"Evolution de l'erreur en fonction du taux d'apprentissage\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XdXh_b0GP_F"
      },
      "source": [
        "**3. Entrainement du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80pEtJ10wIfY"
      },
      "source": [
        "# Charge les meilleurs poids\n",
        "model.load_weights(\"poids_opti.hdf5\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16ujUiELwR33",
        "outputId": "47d83905-615c-4a85-e258-d58c3078fad2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "class TimingCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, logs={}):\n",
        "        self.n_steps = 0\n",
        "        self.t_step = 0\n",
        "        self.n_batch = 0\n",
        "        self.total_batch = 0\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.starttime = timer()\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.t_step = self.t_step  + timer()-self.starttime\n",
        "        self.n_steps = self.n_steps + 1\n",
        "        if (self.total_batch == 0):\n",
        "          self.total_batch=self.n_batch - 1\n",
        "    def on_train_batch_begin(self,batch,logs=None):\n",
        "      self.n_batch= self.n_batch + 1\n",
        "    def GetInfos(self):\n",
        "      return([self.t_step/(self.n_steps*self.total_batch), self.t_step, self.total_batch])\n",
        "\n",
        "cb = TimingCallback()\n",
        "\n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.02,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0.1)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(learning_rate=lr_schedule,momentum=0.9)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur,metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle\n",
        "historique = model.fit(dataset_norm,validation_data=dataset_Val_norm, epochs=100,verbose=1, callbacks=[CheckPoint,cb])\n",
        "\n",
        "# Affiche quelques informations sur les timings\n",
        "infos = cb.GetInfos()\n",
        "print(\"Step time : %.3f\" %infos[0])\n",
        "print(\"Total time : %.3f\" %infos[1])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['couche__end_to__end_mn/p:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['couche__end_to__end_mn/p:0'] when minimizing the loss.\n",
            "29/29 [==============================] - 9s 173ms/step - loss: 0.4657 - mae: 0.8666 - val_loss: 0.2063 - val_mae: 0.5833\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.27868, saving model to poids_train.hdf5\n",
            "Epoch 2/100\n",
            "29/29 [==============================] - 4s 129ms/step - loss: 0.2688 - mae: 0.6016 - val_loss: 0.0888 - val_mae: 0.3812\n",
            "\n",
            "Epoch 00002: loss improved from 0.27868 to 0.12971, saving model to poids_train.hdf5\n",
            "Epoch 3/100\n",
            "29/29 [==============================] - 4s 129ms/step - loss: 0.0262 - mae: 0.1842 - val_loss: 0.0784 - val_mae: 0.3468\n",
            "\n",
            "Epoch 00003: loss improved from 0.12971 to 0.02848, saving model to poids_train.hdf5\n",
            "Epoch 4/100\n",
            "29/29 [==============================] - 4s 129ms/step - loss: 0.1000 - mae: 0.3635 - val_loss: 0.0134 - val_mae: 0.1215\n",
            "\n",
            "Epoch 00004: loss did not improve from 0.02848\n",
            "Epoch 5/100\n",
            "29/29 [==============================] - 4s 129ms/step - loss: 0.0175 - mae: 0.1428 - val_loss: 0.0883 - val_mae: 0.3752\n",
            "\n",
            "Epoch 00005: loss improved from 0.02848 to 0.02125, saving model to poids_train.hdf5\n",
            "Epoch 6/100\n",
            "29/29 [==============================] - 4s 132ms/step - loss: 0.0645 - mae: 0.2800 - val_loss: 0.0204 - val_mae: 0.1604\n",
            "\n",
            "Epoch 00006: loss did not improve from 0.02125\n",
            "Epoch 7/100\n",
            "29/29 [==============================] - 4s 132ms/step - loss: 0.0136 - mae: 0.1248 - val_loss: 0.0259 - val_mae: 0.1834\n",
            "\n",
            "Epoch 00007: loss improved from 0.02125 to 0.01352, saving model to poids_train.hdf5\n",
            "Epoch 8/100\n",
            "29/29 [==============================] - 4s 130ms/step - loss: 0.0206 - mae: 0.1571 - val_loss: 0.0129 - val_mae: 0.1192\n",
            "\n",
            "Epoch 00008: loss did not improve from 0.01352\n",
            "Epoch 9/100\n",
            "29/29 [==============================] - 4s 131ms/step - loss: 0.0135 - mae: 0.1258 - val_loss: 0.0144 - val_mae: 0.1279\n",
            "\n",
            "Epoch 00009: loss improved from 0.01352 to 0.01308, saving model to poids_train.hdf5\n",
            "Epoch 10/100\n",
            "29/29 [==============================] - 4s 130ms/step - loss: 0.0142 - mae: 0.1285 - val_loss: 0.0144 - val_mae: 0.1280\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.01308\n",
            "Epoch 11/100\n",
            "29/29 [==============================] - 4s 130ms/step - loss: 0.0141 - mae: 0.1279 - val_loss: 0.0139 - val_mae: 0.1249\n",
            "\n",
            "Epoch 00011: loss did not improve from 0.01308\n",
            "Epoch 12/100\n",
            "29/29 [==============================] - 4s 129ms/step - loss: 0.0136 - mae: 0.1258 - val_loss: 0.0140 - val_mae: 0.1260\n",
            "\n",
            "Epoch 00012: loss improved from 0.01308 to 0.01297, saving model to poids_train.hdf5\n",
            "Epoch 13/100\n",
            "29/29 [==============================] - 4s 129ms/step - loss: 0.0136 - mae: 0.1258 - val_loss: 0.0140 - val_mae: 0.1254\n",
            "\n",
            "Epoch 00013: loss improved from 0.01297 to 0.01294, saving model to poids_train.hdf5\n",
            "Epoch 14/100\n",
            "10/29 [=========>....................] - ETA: 2s - loss: 0.0144 - mae: 0.1351"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-3abaf4fae974>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Entraine le modèle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mhistorique\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_Val_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCheckPoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Affiche quelques informations sur les timings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COP9u4yitvmw"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6Gq2CkeGR_1"
      },
      "source": [
        "**4. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEm-s_2lhET4"
      },
      "source": [
        "dataPredict = prepare_dataset_XY(x_validation_norm,taille_fenetre,batch_size,buffer_melange)\n",
        "\n",
        "predictions = model.predict(dataPredict)\n",
        "predictions = tf.reshape(predictions,shape=(predictions.shape[0],1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd2nfDcgG8EO"
      },
      "source": [
        "reste = len(temps[temps_separation:])-predictions.shape[0]\n",
        "\n",
        "# Affiche la série et les prédictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps,serie,label=\"Série temporelle\")\n",
        "\n",
        "if reste != 0:\n",
        "  affiche_serie(temps[temps_separation+taille_fenetre:-reste+taille_fenetre],np.asarray(predictions*std.numpy()+mean.numpy()),label=\"Prédictions\")\n",
        "else :\n",
        "  affiche_serie(temps[temps_separation+taille_fenetre:taille_fenetre],np.asarray(predictions*std.numpy()+mean.numpy()),label=\"Prédictions\")\n",
        "\n",
        "\n",
        "plt.title('Prédictions avec le modèle End-to-End Memory Network GRU')\n",
        "plt.show()\n",
        "\n",
        "# Zoom sur l'intervalle de validation\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps[temps_separation:],serie[temps_separation:],label=\"Série temporelle\")\n",
        "if reste != 0:\n",
        "  affiche_serie(temps[temps_separation+taille_fenetre:-reste+taille_fenetre],np.asarray(predictions*std.numpy()+mean.numpy()),label=\"Prédictions\")\n",
        "else :\n",
        "  affiche_serie(temps[temps_separation+taille_fenetre:taille_fenetre],np.asarray(predictions*std.numpy()+mean.numpy()),label=\"Prédictions\")\n",
        "plt.title(\"Prédictions avec le modèle End-to-End Memory Network GRU (zoom sur l'intervalle de validation)\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDS7BJvZG_e1"
      },
      "source": [
        "# Calcule de l'erreur quadratique moyenne et de l'erreur absolue moyenne \n",
        "\n",
        "mae = tf.keras.metrics.mean_absolute_error(serie[temps_separation+taille_fenetre:-reste+taille_fenetre],np.asarray(predictions[:,0]*std.numpy()+mean.numpy())).numpy()\n",
        "mse = tf.keras.metrics.mean_squared_error(serie[temps_separation+taille_fenetre:-reste+taille_fenetre],np.asarray(predictions[:,0]*std.numpy()+mean.numpy())).numpy()\n",
        "\n",
        "print(mae)\n",
        "print(mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjGkiby67fAo"
      },
      "source": [
        "# Création du modèle End-To-End Memory Network (à base de couches LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0jie_T27fA7"
      },
      "source": [
        "**1. Création du réseau et adaptation des formats d'entrée et de sortie**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulxCs7Vp7fA7"
      },
      "source": [
        "Sous forme de shéma, notre réseau est donc le suivant :\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NOJRToi7fA7"
      },
      "source": [
        "<img src=\"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/images/End-To-End.png?raw=true\" width=\"1200\"> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evUlgc8B7fA8"
      },
      "source": [
        "# Définition du de la couche du modèle\n",
        "# End-to-End Memory Network\n",
        "# Epaquetage des données avec le dernier état caché d'une couche GRU\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "class Couche_End_to_End_MN(tf.keras.layers.Layer):\n",
        "  # Fonction d'initialisation de la classe d'attention\n",
        "  # dim_GRU : Dimension des vecteurs GRU\n",
        "  # x : Séquences à mémoriser (batch_size, Nbr_Sequence, taille_fenetre)\n",
        "  # Fonction de la couche lambda d'entrée\n",
        "  def __init__(self,dim_LSTM):\n",
        "    self.dim_LSTM = dim_LSTM\n",
        "    super().__init__()          # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    # Définition des couches GRU pour traiter les séquences d'entrée\n",
        "    self.couche_LSTM_A = tf.keras.layers.LSTM(self.dim_LSTM)\n",
        "    self.couche_LSTM_B = tf.keras.layers.LSTM(self.dim_LSTM)\n",
        "    self.couche_LSTM_C = tf.keras.layers.LSTM(self.dim_LSTM)\n",
        "\n",
        "    # Poids d'attention\n",
        "    self.p = self.add_weight(shape=(input_shape[1],1),initializer=\"zeros\",name=\"p\")\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Définit la logique de la couche d'attention\n",
        "  # Arguments :     x : (batch_size, Nbr_Sequence, taille_fenetre)\n",
        "  #                 y : (batch_size, taille_fenetre)\n",
        "  # Exemple :   batch_size = 32\n",
        "  #             Nbr_Sequence =30\n",
        "  #             taille_fenetre = 20\n",
        "  #             dim_LSTM = 40 \n",
        "  def call(self,x,y):\n",
        "    # Création des vecteurs mi dans le tenseur M\n",
        "    M = tf.expand_dims(x,axis=-1)                                   # (32,30,20) => (32,30,20,1)\n",
        "    M = tf.keras.layers.TimeDistributed(self.couche_LSTM_A)(M)      # (32,30,20,1) => (32,30,40) : TimeStep = 30 : (32,20,1) envoyé\n",
        "    M = K.tanh(M)\n",
        "\n",
        "    # Création du vecteur d'état u\n",
        "    u = tf.expand_dims(y,axis=-1)                                   # (32,20) => (32,20,1)\n",
        "    u = self.couche_LSTM_B(u)                                       # (32,20,1) => (32,40)\n",
        "    u = tf.expand_dims(u,axis=-1)                                   # (32,40) => (32,40,1)\n",
        "    u = K.tanh(u)                                                   # (32,40,1)\n",
        "\n",
        "    # Calcul des poids d'attention\n",
        "    p = tf.keras.layers.Dot(axes=(2,1))([M,u])                      # (32,30,1)\n",
        "    p = tf.keras.activations.softmax(p,axis=1)                      # (32,30,1)\n",
        "\n",
        "    # Création des vecteurs ci dans le tenseur C\n",
        "    C = tf.expand_dims(x,axis=-1)                                   # (32,30,20) => (32,30,20,1)\n",
        "    C = tf.keras.layers.TimeDistributed(self.couche_LSTM_C)(C)      # (32,30,20,1) => (32,30,40) : TimeStep = 30 : (32,20,1) envoyé\n",
        "    C = K.tanh(C)\n",
        "\n",
        "    # Calcul du vecteur réponse issu de la mémoire\n",
        "    o = tf.multiply(C,p)                                            # (32,30,40)_x_(32,30,1) = (32,30,40)\n",
        "    o = K.sum(o, axis=1)                                            # (32,40)\n",
        "    o = K.tanh(o)                                                   # (32,40)\n",
        "    \n",
        "    # Retourne le vecteur d'attention\n",
        "    return (o+tf.squeeze(u,axis=2))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mewcz_H97fA9"
      },
      "source": [
        "Nbr_Sequences = batch_size\n",
        "dim_LSTM = 40\n",
        "\n",
        "# Fonction de la couche lambda d'entrée\n",
        "def Traitement_Entrees(x):\n",
        "  return tf.expand_dims(x,axis=-1)\n",
        "\n",
        "# Définition des entrées du modèle\n",
        "entrees_sequences = tf.keras.layers.Input(shape=(Nbr_Sequences,taille_fenetre),batch_size=batch_size)\n",
        "entrees_entrainement = tf.keras.layers.Input(shape=(taille_fenetre),batch_size=batch_size)\n",
        "\n",
        "# Encodeur\n",
        "s_encodeur = Couche_End_to_End_MN(dim_LSTM=dim_LSTM)(entrees_sequences,entrees_entrainement)\n",
        "\n",
        "# Décodeur\n",
        "s_decodeur = tf.keras.layers.Dense(dim_LSTM,activation=\"tanh\")(s_encodeur)\n",
        "s_decodeur = tf.keras.layers.Concatenate()([s_decodeur,s_encodeur])\n",
        "\n",
        "# Générateur\n",
        "sortie = tf.keras.layers.Dense(1)(s_decodeur)\n",
        "\n",
        "# Construction du modèle\n",
        "model = tf.keras.Model([entrees_sequences,entrees_entrainement],sortie)\n",
        "\n",
        "model.save_weights(\"model_initial.hdf5\")\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqLsHZq37fA-"
      },
      "source": [
        "**2. Optimisation du taux d'apprentissage**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4suZDe37fA-"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(lr=1e-8)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_opti.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur, metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset_norm,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9TKwG7a7fA_"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 1])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGPJ9Ehp7fA_"
      },
      "source": [
        "**3. Entrainement du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6bhCMrv7fA_"
      },
      "source": [
        "# Charge les meilleurs poids\n",
        "model.load_weights(\"poids_opti.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuBZ-UVp7fBA"
      },
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "class TimingCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, logs={}):\n",
        "        self.n_steps = 0\n",
        "        self.t_step = 0\n",
        "        self.n_batch = 0\n",
        "        self.total_batch = 0\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.starttime = timer()\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.t_step = self.t_step  + timer()-self.starttime\n",
        "        self.n_steps = self.n_steps + 1\n",
        "        if (self.total_batch == 0):\n",
        "          self.total_batch=self.n_batch - 1\n",
        "    def on_train_batch_begin(self,batch,logs=None):\n",
        "      self.n_batch= self.n_batch + 1\n",
        "    def GetInfos(self):\n",
        "      return([self.t_step/(self.n_steps*self.total_batch), self.t_step, self.total_batch])\n",
        "\n",
        "cb = TimingCallback()\n",
        "\n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.008,\n",
        "    decay_steps=5,\n",
        "    decay_rate=0.01)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(learning_rate=lr_schedule,momentum=0.9)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur,metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle\n",
        "historique = model.fit(dataset_norm,validation_data=dataset_Val_norm, epochs=100,verbose=1, callbacks=[CheckPoint,cb])\n",
        "\n",
        "# Affiche quelques informations sur les timings\n",
        "infos = cb.GetInfos()\n",
        "print(\"Step time : %.3f\" %infos[0])\n",
        "print(\"Total time : %.3f\" %infos[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yd5MGwwM7fBA"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igB1VhNf7fBB"
      },
      "source": [
        "**4. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57jfHcwK7fBB"
      },
      "source": [
        "dataPredict = prepare_dataset_XY(x_validation_norm,taille_fenetre,batch_size,buffer_melange)\n",
        "\n",
        "predictions = model.predict(dataPredict)\n",
        "predictions = tf.reshape(predictions,shape=(predictions.shape[0],1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jhrj2E557fBC"
      },
      "source": [
        "reste = len(temps[temps_separation:])-predictions.shape[0]\n",
        "\n",
        "# Affiche la série et les prédictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps,serie,label=\"Série temporelle\")\n",
        "\n",
        "if reste != 0:\n",
        "  affiche_serie(temps[temps_separation+taille_fenetre:-reste+taille_fenetre],np.asarray(predictions*std.numpy()+mean.numpy()),label=\"Prédictions\")\n",
        "else :\n",
        "  affiche_serie(temps[temps_separation+taille_fenetre:taille_fenetre],np.asarray(predictions*std.numpy()+mean.numpy()),label=\"Prédictions\")\n",
        "\n",
        "\n",
        "plt.title('Prédictions avec le modèle End-to-End Memory Network GRU')\n",
        "plt.show()\n",
        "\n",
        "# Zoom sur l'intervalle de validation\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps[temps_separation:],serie[temps_separation:],label=\"Série temporelle\")\n",
        "if reste != 0:\n",
        "  affiche_serie(temps[temps_separation+taille_fenetre:-reste+taille_fenetre],np.asarray(predictions*std.numpy()+mean.numpy()),label=\"Prédictions\")\n",
        "else :\n",
        "  affiche_serie(temps[temps_separation+taille_fenetre:taille_fenetre],np.asarray(predictions*std.numpy()+mean.numpy()),label=\"Prédictions\")\n",
        "plt.title(\"Prédictions avec le modèle End-to-End Memory Network GRU (zoom sur l'intervalle de validation)\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHDQjnkq7fBC"
      },
      "source": [
        "# Calcule de l'erreur quadratique moyenne et de l'erreur absolue moyenne \n",
        "\n",
        "mae = tf.keras.metrics.mean_absolute_error(serie[temps_separation+taille_fenetre:-reste+taille_fenetre],np.asarray(predictions[:,0]*std.numpy()+mean.numpy())).numpy()\n",
        "mse = tf.keras.metrics.mean_squared_error(serie[temps_separation+taille_fenetre:-reste+taille_fenetre],np.asarray(predictions[:,0]*std.numpy()+mean.numpy())).numpy()\n",
        "\n",
        "print(mae)\n",
        "print(mse)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}