{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bitcoin_Wavenet_Multi.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/Bitcoin_Wavenet_Class_Multi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVuCzMYKDp3z"
      },
      "source": [
        "# Chargement des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3nLyKtnliCW"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import re\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyxxcCcl1Kb3"
      },
      "source": [
        "!rm *.zip\n",
        "!rm *.csv\n",
        "!wget -q --no-check-certificate --content-disposition \"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/Bitcoin/Bitcoin_complet.zip?raw=true\"\n",
        "!unzip \"Bitcoin_complet.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPJ6nnrRsUBm"
      },
      "source": [
        "Charge la série sous Pandas et affiche les informations du fichier :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV3Mq38VhmWO"
      },
      "source": [
        "# Création des dataframe X_Avec_Prix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEB_JjihEYTP"
      },
      "source": [
        "# Création de la série sous Pandas\n",
        "df_data = pd.read_csv(\"Bitcoin_complet.csv\")\n",
        "df_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp4klS6yfTM-"
      },
      "source": [
        "X_Avec_Prix = df_data\n",
        "X_Avec_Prix = X_Avec_Prix.drop(columns=['Dates'])\n",
        "Xdrop = SimpleImputer(missing_values=np.nan,strategy='most_frequent').fit_transform(X_Avec_Prix)\n",
        "Xdrop = pd.DataFrame(Xdrop)\n",
        "Xdrop.columns = X_Avec_Prix.columns\n",
        "X_Avec_Prix = Xdrop\n",
        "X_Avec_Prix = X_Avec_Prix\n",
        "X_Avec_Prix = X_Avec_Prix[ ['Price'] + [ col for col in X_Avec_Prix.columns if col != 'Price' ] ]\n",
        "X_Avec_Prix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG5Fyx5O5oe5"
      },
      "source": [
        "# Normalisation des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7cGUeWb5oe7"
      },
      "source": [
        "**1. Séparation des données en données pour l'entrainement et la validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob-EAw_j5oe8"
      },
      "source": [
        "On réserve 20% des données pour l'entrainement et le reste pour la validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZO8MRwCBb1n"
      },
      "source": [
        "# Sépare les données en entrainement et tests\n",
        "pourcentage = 0.8\n",
        "temps_separation = int(len(X_Avec_Prix) * pourcentage)\n",
        "date_separation = X_Avec_Prix.index[temps_separation]\n",
        "\n",
        "serie_entrainement_X = []\n",
        "serie_test_X = []\n",
        "\n",
        "for column in X_Avec_Prix:\n",
        "  serie_entrainement_X.append(np.array(X_Avec_Prix[column][:temps_separation],dtype=np.float32))\n",
        "  serie_test_X.append(np.array(X_Avec_Prix[column][temps_separation:],dtype=np.float32))\n",
        "\n",
        "print(\"Taille de l'entrainement : %d\" %len(serie_entrainement_X[0]))\n",
        "print(\"Taille de la validation : %d\" %len(serie_test_X[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFPdBcIqFeg3"
      },
      "source": [
        "serie_entrainement_X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCWEF76fHW1F"
      },
      "source": [
        "**1. Normalisation standard**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZUMMMro5oe9"
      },
      "source": [
        "On normalise les données :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu_YxoSI5oe9"
      },
      "source": [
        "mean_X = []\n",
        "std_X = []\n",
        "\n",
        "serie_entrainement_X_norm = []\n",
        "serie_test_X_norm = []\n",
        "\n",
        "# Calcul de la moyenne et de l'écart type de la série X\n",
        "for serie in serie_entrainement_X:\n",
        "  mean_X.append(tf.math.reduce_mean(np.asarray(serie)))\n",
        "  std_X.append(tf.math.reduce_std(np.asarray((serie))))\n",
        "\n",
        "# Normalisation des données X\n",
        "for i in range(0,len(mean_X)):\n",
        "  serie_entrainement_X_norm.append((serie_entrainement_X[i]-mean_X[i])/std_X[i])\n",
        "  serie_test_X_norm.append((serie_test_X[i]-mean_X[i])/std_X[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuVF7JbQE3DG"
      },
      "source": [
        "# Affiche la série\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "\n",
        "for i in range(0,10):\n",
        "  ax.plot(X_Avec_Prix.index[:temps_separation].values,serie_entrainement_X_norm[i], label=\"X_Ent\")\n",
        "  ax.plot(X_Avec_Prix.index[temps_separation:].values,serie_test_X_norm[i], label=\"X_Val\")\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_lNpyPUHkDG"
      },
      "source": [
        "**2. Normalisaiton robuste**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSZN7bXxHvaK"
      },
      "source": [
        "estimators=[]\n",
        "\n",
        "estimators.append(['robust',RobustScaler()])\n",
        "estimators.append(['mixmax',MinMaxScaler()])\n",
        "\n",
        "scale=Pipeline(estimators,verbose=True)\n",
        "\n",
        "scale.fit_transform(tf.reshape(serie_entrainement_X,shape=(len(serie_entrainement_X[0]),len(serie_entrainement_X))))\n",
        "serie_entrainement_X_norm=scale.transform(tf.reshape(serie_entrainement_X,shape=(len(serie_entrainement_X[0]),len(serie_entrainement_X))))\n",
        "serie_test_X_norm=scale.transform(tf.reshape(serie_test_X,shape=(len(serie_test_X[0]),len(serie_test_X))))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUhMpDeoIa2M"
      },
      "source": [
        "# Affiche la série X\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "\n",
        "for i in range(0,10):\n",
        "  ax.plot(X_Avec_Prix.index[:temps_separation].values,serie_entrainement_X_norm[:,i], label=\"X_Ent\")\n",
        "  ax.plot(X_Avec_Prix.index[temps_separation:].values,serie_test_X_norm[:,i], label=\"X_Val\")\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md3xxa5dNCFd"
      },
      "source": [
        "# Création des datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wqy52B5xSXDz"
      },
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "\n",
        "def prepare_dataset_XY(series, taille_fenetre, horizon, batch_size):\n",
        "  serie_concat = tf.expand_dims(series[0],1)\n",
        "\n",
        "  for i in range(1,len(series)):\n",
        "    serie_ = tf.expand_dims(series[i],1)\n",
        "    serie_concat = tf.concat([serie_concat,serie_],1)\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie_concat)\n",
        "  dataset = dataset.window(taille_fenetre+horizon, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre + horizon))\n",
        "  dataset = dataset.map(lambda x: (x[0:taille_fenetre][:,:],tf.expand_dims(x[-taille_fenetre:][:,0],1)))\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Jh1RZYo5oe_"
      },
      "source": [
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "taille_fenetre = 64\n",
        "horizon = 1\n",
        "batch_size = 1\n",
        "\n",
        "# Création du dataset\n",
        "dataset = prepare_dataset_XY(serie_entrainement_X_norm,taille_fenetre,horizon,batch_size)\n",
        "dataset_val = prepare_dataset_XY(serie_test_X_norm,taille_fenetre,horizon,batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr2pbMox5oe_"
      },
      "source": [
        "print(len(list(dataset.as_numpy_iterator())))\n",
        "for element in dataset.take(1):\n",
        "  print(element[0].shape)\n",
        "  print(element[1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQejjkaeeHxe"
      },
      "source": [
        "for element in dataset.take(1):\n",
        "  print(element)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHCcYn6i5oe_"
      },
      "source": [
        "On extrait maintenant les deux tenseurs (X,Y) pour l'entrainement :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3ZhLIK15ofA"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset))              # #60x((32,5,779),(32,5,1)) => x = 60x(32,5,779) ; y = 60x(32,5,1)\n",
        "\n",
        "# Recombine les données\n",
        "x = np.asarray(x,dtype=np.float32)      # 60x(32,5,779) => (60,32,5,779)\n",
        "y = np.asarray(y,dtype=np.float32)      # 60x(32,5,1) => (60,32,5,1)\n",
        "\n",
        "x_train = np.asarray(tf.reshape(x,shape=(x.shape[0]*x.shape[1],taille_fenetre,x.shape[3])))     # (60,32,5,779) => (60*32,5,779)\n",
        "y_train = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],taille_fenetre,y.shape[3])))     # (60,32,5,1) => (60*32,5,1)\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llnKyLvl5ofA"
      },
      "source": [
        "Puis la même chose pour les données de validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hadrKVrZ5ofB"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset_val))\n",
        "\n",
        "# Recombine les données\n",
        "x = np.asarray(x,dtype=np.float32)\n",
        "y = np.asarray(y,dtype=np.float32)\n",
        "\n",
        "x_val = np.asarray(tf.reshape(x,shape=(x.shape[0]*x.shape[1],taille_fenetre,x.shape[3])))\n",
        "y_val = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],taille_fenetre,y.shape[3])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHfiXLFZhrPs"
      },
      "source": [
        "# Création du modèle type Wavenet Multivarié"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd9AzWFtjnjs"
      },
      "source": [
        "**1. Création du réseau**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDVH4xsDzM-V"
      },
      "source": [
        "from keras.layers import Conv1D\n",
        "from keras.layers import Conv1D\n",
        "from keras.utils.conv_utils import conv_output_length\n",
        "from keras import layers\n",
        "from keras.regularizers import l2\n",
        "import keras.backend as K\n",
        "from keras.engine import Input\n",
        "from keras.engine import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCtJXABdzKB9"
      },
      "source": [
        "class CausalDilatedConv1D(Conv1D):\n",
        "    def __init__(self, nb_filter, filter_length, init='glorot_uniform', activation=None, weights=None,\n",
        "                 border_mode='valid', subsample_length=1, atrous_rate=1, W_regularizer=None, b_regularizer=None,\n",
        "                 activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True, causal=False, **kwargs):\n",
        "        super(CausalDilatedConv1D, self).__init__(nb_filter, filter_length, weights=weights, activation=activation, \n",
        "                padding=border_mode, strides=subsample_length, dilation_rate=atrous_rate, kernel_regularizer=W_regularizer, \n",
        "                bias_regularizer=b_regularizer, activity_regularizer=activity_regularizer, kernel_constraint=W_constraint, \n",
        "                bias_constraint=b_constraint, use_bias=bias, **kwargs)\n",
        "        self.causal = causal\n",
        "        self.nb_filter = nb_filter\n",
        "        self.atrous_rate = atrous_rate\n",
        "        self.filter_length = filter_length\n",
        "        self.subsample_length = subsample_length\n",
        "        self.border_mode = border_mode\n",
        "        if self.causal and border_mode != 'valid':\n",
        "            raise ValueError(\"Causal mode dictates border_mode=valid.\")\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        input_length = input_shape[1]\n",
        "        if self.causal:\n",
        "            input_length += self.atrous_rate * (self.filter_length - 1)\n",
        "        length = conv_output_length(input_length, self.filter_length, self.border_mode, self.strides[0], dilation=self.atrous_rate)\n",
        "        return (input_shape[0], length, self.nb_filter)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        if self.causal:\n",
        "            x = K.temporal_padding(x, padding=(self.atrous_rate * (self.filter_length - 1), 0))\n",
        "        # return super(CausalAtrousConvolution1D, self).call(x, mask)\n",
        "        return super(CausalDilatedConv1D, self).call(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s19zmYCgzTvV"
      },
      "source": [
        "def _compute_receptive_field(dilation_depth, stacks):\n",
        "  return stacks * (2 ** dilation_depth * 2) - (stacks - 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIX-o-lMzVK0"
      },
      "source": [
        "def build_model_residual_block(x, i, s,nb_filters, dim_filters, use_bias,res_l2):\n",
        "        original_x = x\n",
        "        # TODO: initalization, regularization?\n",
        "        relu_out = CausalDilatedConv1D(nb_filters, dim_filters, atrous_rate=2 ** i, border_mode='valid', causal=True, bias=True, name='dilated_conv_%d_relu_s%d' % (2 ** i, s), activation='relu', W_regularizer=l2(res_l2))(x)\n",
        "        res_x = layers.Conv1D(1, 1, padding='same', use_bias=use_bias, kernel_regularizer=l2(res_l2))(relu_out)\n",
        "        skip_x = layers.Conv1D(1, 1, padding='same', use_bias=use_bias, kernel_regularizer=l2(res_l2))(relu_out)\n",
        "        res_x = layers.Add()([original_x, res_x])\n",
        "        return res_x, skip_x\n",
        "\n",
        "def build_model_couche_condition(x, output_bins, nb_filters, dim_filters, use_bias,res_l2):\n",
        "        original_x = x\n",
        "        skip_conditions = []\n",
        "\n",
        "        relu_out = CausalDilatedConv1D(nb_filters, dim_filters, atrous_rate=1, border_mode='valid', causal=True, bias=True, name='dilated_conv_%d_condition_%d_s%d' % (1,1, 0), activation='relu',\n",
        "                                       W_regularizer=l2(res_l2))(tf.expand_dims(x[:,:,0],2))\n",
        "\n",
        "        skip_ = layers.Conv1D(1, 1, padding='same', use_bias=use_bias, kernel_regularizer=l2(res_l2))(relu_out)\n",
        "        skip_conditions.append(skip_)\n",
        "\n",
        "        for i in range(1,output_bins-1):\n",
        "          relu_out = CausalDilatedConv1D(nb_filters, dim_filters, atrous_rate=1, border_mode='valid', causal=True, bias=True, name='dilated_conv_%d_condition_%d_s%d' % (1,i+1,0), activation='relu',\n",
        "                                                    W_regularizer=l2(res_l2))(tf.expand_dims(x[:,:,i],2))\n",
        "\n",
        "          skip_ = layers.Conv1D(1, 1, padding='same', use_bias=use_bias, kernel_regularizer=l2(res_l2))(relu_out)\n",
        "          skip_conditions.append(skip_)\n",
        "\n",
        "        if output_bins > 1:\n",
        "          out = layers.Add()(skip_conditions)\n",
        "        else:\n",
        "          out = skip_\n",
        "        return out\n",
        "\n",
        "\n",
        "def build_model(fragment_length, nb_filters, dim_filters, output_bins, dilation_depth, stacks, use_skip_connections, use_bias, res_l2, final_l2):\n",
        "        input_shape = Input(shape=(fragment_length, output_bins), name='input_part')\n",
        "        out = input_shape\n",
        "        skip_connections = []\n",
        "\n",
        "        for s in range(stacks):\n",
        "            # Couche conditionnée\n",
        "            out = build_model_couche_condition(out, output_bins, nb_filters, dim_filters, use_bias, res_l2)\n",
        "\n",
        "            # Couches intermédiaires\n",
        "            for i in range(1, dilation_depth + 1):\n",
        "                out, skip_out = build_model_residual_block(out, i, s, nb_filters, dim_filters, use_bias, res_l2)\n",
        "                skip_connections.append(skip_out)\n",
        "\n",
        "        if use_skip_connections:\n",
        "            out = layers.Add()(skip_connections)\n",
        "\n",
        "        # Couche de sortie\n",
        "        out = layers.Activation('linear', name=\"output_linear\")(out)\n",
        "        out = layers.Conv1D(1, 1, padding='same', kernel_regularizer=l2(final_l2))(out)\n",
        "        model = Model(input_shape, out)\n",
        "        return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH9rsD5UzZgM"
      },
      "source": [
        "**2. Construction du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln6eVTFezYlq"
      },
      "source": [
        "def compute_receptive_field_(dilation_depth, nb_stacks):\n",
        "    receptive_field = nb_stacks * (2 ** dilation_depth * 2) - (nb_stacks - 1)\n",
        "    return receptive_field\n",
        "\n",
        "nb_filters = 2\n",
        "dim_filters = 2\n",
        "nb_output_bins = 779\n",
        "dilation_depth = 5\n",
        "nb_stacks = 1\n",
        "use_skip_connections = False\n",
        "use_bias = False\n",
        "res_l2 = 0.001\n",
        "final_l2 = 0.001\n",
        "\n",
        "fragment_length = compute_receptive_field_(dilation_depth, nb_stacks)\n",
        "fragment_length\n",
        "\n",
        "model = build_model(fragment_length, nb_filters, dim_filters, nb_output_bins, dilation_depth, nb_stacks, use_skip_connections, use_bias, res_l2, final_l2)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhUrCu1tsfqx"
      },
      "source": [
        "# Entrainement du modèle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_azfJaeUo2nU"
      },
      "source": [
        "**1. Optimisation de l'apprentissage**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf3lwaQBnjxL"
      },
      "source": [
        "Pour accélérer le traitement des données, nous n'allons pas utiliser l'intégralité des données pendant la mise à jour du gradient, comme cela a été fait jusqu'à présent (en utilisant le dataset).  \n",
        "Cette fois-ci, nous allons forcer les mises à jour du gradient à se produire de manière moins fréquente en attribuant la valeur du batch_size à prendre en compte lors de la regression du modèle.  \n",
        "Pour cela, on utilise l'argument \"batch_size\" dans la méthode fit. En précisant un batch_size=1000, cela signifie que :\n",
        " - Sur notre total de 56000 échantillons, 56 seront utilisés pour les calculs du gradient\n",
        " - Il y aura également 56 itérations à chaque période.\n",
        "  \n",
        "    \n",
        "    \n",
        "Si nous avions pris le dataset comme entrée, nous aurions eu :\n",
        "- Un total de 56000 échantillons également\n",
        "- Chaque période aurait également pris 56 itérations pour se compléter\n",
        "- Mais 1000 échantillons auraient été utilisés pour le calcul du gradient, au lieu de 56 avec la méthode utilisée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6Z35rNWj5SA"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "def  My_MSE(y_true,y_pred):\n",
        "  return(tf.keras.metrics.mse(y_true,y_pred)*std_X[0].numpy()+mean_X[0].numpy())\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam()\n",
        "#optimiseur=tf.keras.optimizers.SGD()\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"mse\", optimizer=optimiseur, metrics=[\"mse\",My_MSE])\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(x=x_train,y=y_train,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint],batch_size=batch_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2aP9J3TkNGG"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[30], taux[99], 0, 0.4])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZxCgpuYkQ2Q"
      },
      "source": [
        "# Chargement des poids sauvegardés\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2QKWcmMv4yg"
      },
      "source": [
        "**2. Entrainement du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmdbo23qkTKE"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "max_periodes = 5000\n",
        "\n",
        "# Classe permettant d'arrêter l'entrainement si la variation\n",
        "# devient plus petite qu'une valeur à choisir sur un nombre\n",
        "# de périodes à choisir\n",
        "class StopTrain(keras.callbacks.Callback):\n",
        "    def __init__(self, delta=0.01,periodes=100, term=\"loss\", logs={}):\n",
        "      self.n_periodes = 0\n",
        "      self.periodes = periodes\n",
        "      self.loss_1 = 100\n",
        "      self.delta = delta\n",
        "      self.term = term\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "      diff_loss = abs(self.loss_1 - logs[self.term])\n",
        "      self.loss_1 = logs[self.term]\n",
        "      if (diff_loss < self.delta):\n",
        "        self.n_periodes = self.n_periodes + 1\n",
        "      else:\n",
        "        self.n_periodes = 0\n",
        "      if (self.n_periodes == self.periodes):\n",
        "        print(\"Arrêt de l'entrainement...\")\n",
        "        self.model.stop_training = True\n",
        "\n",
        "def  My_MSE(y_true,y_pred):\n",
        "  return(tf.keras.metrics.mse(y_true,y_pred)*std_X[0].numpy()+mean_X[0].numpy())\n",
        "  \n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.002,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0.01)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"mse\", optimizer=optimiseur, metrics=[\"mse\",My_MSE])\n",
        "\n",
        "# Entraine le modèle, avec une réduction des calculs du gradient\n",
        "historique = model.fit(x=x_train,y=y_train,validation_data=(x_val,y_val), epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=1e-2,periodes = 10, term=\"My_MSE\")],batch_size=32)\n",
        "\n",
        "# Entraine le modèle sans réduction de calculs\n",
        "#historique = model.fit(dataset,validation_data=dataset_val, epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=1e-2,periodes = 10, term=\"My_MSE\")])\n",
        "\n",
        "files.download('poids_train.hdf5')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAy58R-tuwoJ"
      },
      "source": [
        "# Télécharge les résultats d'entrainement du modèle\n",
        "! wget --no-check-certificate --content-disposition \"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/Bitcoin/poids_train_BitWave_One_All.hdf5?raw=true\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfbomV0LS9LD"
      },
      "source": [
        "model.load_weights(\"poids_train.hdf5\")\n",
        "#model.load_weights(\"poids_train_BitWave_One_All.hdf5\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MDY8O1-l6kN"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEuSDQ6vZnBm"
      },
      "source": [
        "# Evaluation du modèle\n",
        "\n",
        "model.evaluate(dataset)\n",
        "model.evaluate(dataset_val)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbzro22hgt4b"
      },
      "source": [
        "**3. Prédictions \"single step\"**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMmVn1e5zEAm"
      },
      "source": [
        "# Création des instants d'entrainement et de validation\n",
        "y_train_timing = X_Avec_Prix.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_train)]\n",
        "y_val_timing = X_Avec_Prix.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_val)]\n",
        "\n",
        "# Calcul des prédictions\n",
        "pred_ent = model.predict(x_train, verbose=1)\n",
        "pred_val = model.predict(x_val, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1FyFb649FjN"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbes originales\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index,y=serie_entrainement_X_norm[0],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index[temps_separation:],y=serie_test_X_norm[0],line=dict(color='blue', width=1)))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index[taille_fenetre+horizon-1:],y=pred_ent[:,taille_fenetre-1,0],line=dict(color='green', width=1)))\n",
        "\n",
        "# Courbes des prédictions de validations\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index[temps_separation+taille_fenetre+horizon-1:],y=pred_val[:,taille_fenetre-1,0],line=dict(color='red', width=1)))\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWa9g3DnwFif"
      },
      "source": [
        "**4. Prédictions multi-step**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67PwI1pz97Q7"
      },
      "source": [
        "predictions = []\n",
        "                                                        # x_val : (448,16,779)\n",
        "data_to_predict = x_val[0,:,:]                          # [[VAL#1_0,VAL#2_0, ... , VAL#779_0],[VAL#1_1,VAL#2_1, ... , VAL#779_1], ... , [VAL#1_15,VAL#2_15, ..., VAL#779_15]] : (16,779)\n",
        "data_to_predict = tf.expand_dims(data_to_predict,0)     # (1,16,779)\n",
        "\n",
        "prediction = model.predict(data_to_predict)             # [[[^VAL#1_1][^VAL#1_2]...[^VAL#1_16]]] : (1,16,1)\n",
        "predictions.append(prediction[0,taille_fenetre-1,0])    # [^VAL#1_16] : (1,)\n",
        "\n",
        "data_to_predict = x_val[1,0:taille_fenetre-1,:]         # [[VAL#1_1,VAL#2_1, ... , VAL#779_1],[VAL#1_2,VAL#2_2, ... , VAL#779_2], ... , [VAL#1_15,VAL#2_15, ..., VAL#779_15]] : (15,779)\n",
        "\n",
        "data_to_predict = np.insert(data_to_predict,taille_fenetre-1,     # [[VAL#1_1,VAL#2_1, ... , VAL#779_1],[VAL#1_2,VAL#2_2, ... , VAL#779_2], ... , [^VAL#1_16,VAL#2_16, ..., VAL#779_16]] : (16,779)\n",
        "                            np.concatenate((tf.expand_dims(np.array(predictions[0]),0),x_val[1,taille_fenetre-2,1:779]),axis=0),axis=0)\n",
        "\n",
        "for i in range(1,300):\n",
        "  data_to_predict = tf.expand_dims(data_to_predict,0)   # (16,779) => (1,16,779)\n",
        "  prediction = model.predict(data_to_predict)           # [[[^VAL#1_2][^VAL#1_3]...[^VAL#1_17]]] : (1,16,1)\n",
        "\n",
        "  predictions.append(prediction[0,taille_fenetre-1,0])  # [^VAL#1_17] : (1,)\n",
        "  data_to_predict = x_val[i+1,0:taille_fenetre-1,:]     # [[VAL#1_2,VAL#2_2, ... , VAL#779_2],[VAL#1_3,VAL#2_3, ... , VAL#779_3], ... , [VAL#1_17,VAL#2_17, ..., VAL#779_17]] : (15,779)\n",
        "\n",
        "  data_to_predict = np.insert(data_to_predict,taille_fenetre-1,     # [[VAL#1_1,VAL#2_1, ... , VAL#779_1],[VAL#1_2,VAL#2_2, ... , VAL#779_2], ... , [^VAL#1_16,VAL#2_16, ..., VAL#779_16]] : (16,779)\n",
        "                            np.concatenate((tf.expand_dims(np.array(predictions[i]),0),x_val[1,taille_fenetre-2,1:779]),axis=0),axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2xSz7w_wWVJ"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "n_max = len(predictions)\n",
        "\n",
        "# Courbes originales\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index,y=serie_entrainement_X_norm[0],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index[temps_separation:],y=serie_test_X_norm[0],line=dict(color='blue', width=1)))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index[taille_fenetre+horizon-1:],y=pred_ent[:,taille_fenetre-1,0],line=dict(color='green', width=1)))\n",
        "\n",
        "# Courbes des prédictions de validations\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index[temps_separation+taille_fenetre+horizon-1:temps_separation+taille_fenetre+horizon-1+n_max],y=predictions[0:n_max],line=dict(color='red', width=1)))\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix2qXtbTIyf2"
      },
      "source": [
        "# Analyse des indicateurs obtenus par RF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYl8AbWsNvpX"
      },
      "source": [
        "!rm *.zip\n",
        "!wget --no-check-certificate --content-disposition \"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/Bitcoin/Bitcoin_X_one.zip?raw=true\"\n",
        "!wget --no-check-certificate --content-disposition \"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/Bitcoin/Bitcoin_Y_one.zip?raw=true\"\n",
        "!unzip \"Bitcoin_X_one.zip\"\n",
        "!unzip \"Bitcoin_Y_one.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xK_Uv9XOJsN"
      },
      "source": [
        "df_X_one = pd.read_csv(\"Bitcoin_X_one.csv\").iloc[:,1:]\n",
        "df_Y_one = pd.read_csv(\"Bitcoin_Y_one.csv\")['Price']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuzTUfEfQ8RU"
      },
      "source": [
        "**1. Cross-Validation sur les variables retenues**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bEv2NxbJJ4Z"
      },
      "source": [
        "Comparons les résultats obtenus par régression avec les variables retenues  :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lKZd_JfJLZ1"
      },
      "source": [
        "var_thres = [593,671,386,82,367,485,10,101,63,196,177,612,599,618,308,656,124,314,476,637,675,67,631,175,333,493,143,162,48,365,371,61,29,118,194,139,669,99,352,95,346,156,200,190,181,80,158,384,105,650,137,42,327,86,640,206,683,390,157,329,487,348,163,480,388,347,23,396,474,621,486,189,372,103,176,199,366,203,391,104,182,202,398,385,107,111,392,670,198,379,85,100,481,676,674,491,389,195,208,113,652,106,201,108,129,146,184,355,62,479,70,659,68,127,393,336,165,678,374,89,51,32,317,370,47,186,81,680,395,188,205,397,110,378,633,44,180,94,87,93,25,120,558,96,64,169,682,359,187,681,602,13,207,515,319,677,445,777,302,310,662,623,604,570,167,376,74,433,150,183,148,340,33,131,338,191,17,66,138,470,625,642,98,112,673,168,357,383,360,575,569,321,34,387,606,170,350,380,36,465,550,356,484,455,19,52,456,587,614,473,84,567,477,471,581,102,577,91,160,358,459,644,298,540,586,351,739,53,533,55,353,313,568,583,76,193,15,464,451,97,564,65,663,542,457,133,38,539,349,79,192,54,469,665,178,483,536,166,381,490,664,83,544,444,56,655,415,161,472,45,559,654,588,440,467,109,369,563,50,72,142,454,362,149,595,448,197,323,171,334,566,627,628,332,492,574,179,773,557,151,585,185,377,560,580,571,672,518,78,330,75,608,579,463,489,534,427,43,555,421,152,219,425,49,58,270,572,661,213,295,164,402,638,561,382,690,26,668,222,141,276,289,514,541,565,342,394,418,204,646,46,636,361,60,368,92,666,232,144]\n",
        "var_interp = [593,671,386,82,367,485,10,101,63,196,177,612,599,618,308,656,124,314,476,637,675,67,631,175,333,493,143,162,48,365,371,61,29,118,194,139,669,99,352,95,346,156,200,190,181,80,158,384,105,650,137,42,327,86,640,206,683,390,157,329,487,348,163,480,388,347,23,396,474,621,486,189,372,103,176,199,366,203,391,104,182,202,398,385,107,111,392,670,198,379,85,100,481,676,674,491,389,195,208,113,652,106,201,108,129,146,184,355,62,479,70,659,68,127,393,336,165,678,374,89,51,32,317,370,47,186,81,680,395,188,205,397,110,378,633,44,180,94,87,93,25,120,558,96,64,169,682,359,187,681,602,13,207,515,319,677,445,777,302,310,662,623,604,570,167,376,74,433,150,183,148,340,33,131,338,191,17,66,138,470,625,642,98,112,673,168,357,383,360,575,569,321,34,387,606,170,350,380,36,465,550,356,484,455,19,52,456,587,614,473]\n",
        "var_pred = [593,671,386,101,675,67,175,493,194,669,99,640,683,390,621,486,372,89,51,32,473]\n",
        "\n",
        "var_thres = np.subtract(var_thres,1).tolist()\n",
        "var_interp = np.subtract(var_interp,1).tolist()\n",
        "var_pred = np.subtract(var_pred,1).tolist()\n",
        "\n",
        "df_all = df_X_one\n",
        "df_thres = df_X_one.iloc[:,var_thres]\n",
        "df_interp = df_X_one.iloc[:,var_interp]\n",
        "df_pred = df_X_one.iloc[:,var_pred]\n",
        "\n",
        "#df_ = [df_all, df_thres, df_interp,df_pred]\n",
        "#df_ = [df_thres, df_interp,df_pred]\n",
        "#df_ = [df_interp,df_pred]\n",
        "df_ = [df_pred]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydye47_nPcpl"
      },
      "source": [
        "df_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkivMknQMde2"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "kf = KFold(n_splits=5)\n",
        "\n",
        "means = []\n",
        "\n",
        "for df in df_:\n",
        "  print(\"# Variables : %d\" %len(df.columns))\n",
        "  n_arbres = 500\n",
        "  m_try = len(df.columns)\n",
        "  clf = RandomForestRegressor(n_estimators=n_arbres, bootstrap=True, oob_score=True, max_features=m_try, n_jobs=-1)\n",
        "\n",
        "  for train_index, test_index in kf.split(df):\n",
        "    test_index = train_index\n",
        "    clf.fit(df.iloc[train_index],np.asarray(df_Y_one.iloc[train_index]))\n",
        "    y_predict = clf.predict(df.iloc[test_index])\n",
        "    mse = mean_squared_error(y_predict,df_Y_one.iloc[test_index])\n",
        "    print(mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hr03cDV9RCdj"
      },
      "source": [
        "**2. Cross validation sur des variables tirées au hasard**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foEs5HhcRD8j"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "kf = KFold(n_splits=5)\n",
        "\n",
        "n = 21\n",
        "n_arbres = 500\n",
        "m_try = n\n",
        "\n",
        "clf = RandomForestRegressor(n_estimators=n_arbres, bootstrap=True, oob_score=True, max_features=m_try, n_jobs=-1)\n",
        "for i in range(1,1000):\n",
        "  df_all = df_X_one\n",
        "  index = np.random.randint(0,778,n)\n",
        "  df = df_all.iloc[:,index]\n",
        "  print(\"%s\" %index)\n",
        "\n",
        "  for train_index, test_index in kf.split(df):\n",
        "    test_index = train_index\n",
        "    clf.fit(df.iloc[train_index], np.asarray(df_Y_one.iloc[train_index]))\n",
        "    y_predict = clf.predict(df.iloc[test_index])\n",
        "    mse = mean_squared_error(y_predict,df_Y_one.iloc[test_index])\n",
        "    print(mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCJnSWwPR-Nb"
      },
      "source": [
        "**3. Corrélation des variables retenues**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Go93TI5SIwL"
      },
      "source": [
        "corr = df_pred.corr()\n",
        "\n",
        "# Generate a mask for the upper triangle\n",
        "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
        "\n",
        "# Set up the matplotlib figure\n",
        "f, ax = plt.subplots(figsize=(15, 8))\n",
        "\n",
        "sns.heatmap(corr,mask=mask, cmap='coolwarm',annot=True, fmt='.2f')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPAeov3Qv7pd"
      },
      "source": [
        "**4. VIF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFxPmXpSwkGy"
      },
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tools.tools import add_constant\n",
        "\n",
        "#function for removing features with high vif\n",
        "def drop_high_vif(X, thresh=100):\n",
        "    cols = X.columns\n",
        "    variables = np.arange(X.shape[1])\n",
        "    dropped=True\n",
        "    while dropped:\n",
        "        dropped=False\n",
        "        c = X[cols[variables]].values\n",
        "        vif = [variance_inflation_factor(c, ix) for ix in np.arange(c.shape[1])]\n",
        "\n",
        "        maxloc = vif.index(max(vif))\n",
        "        if max(vif) > thresh:\n",
        "            print('dropping \\'' + X[cols[variables]].columns[maxloc] + '\\' at index: ' + str(maxloc))\n",
        "            variables = np.delete(variables, maxloc)\n",
        "            dropped=True\n",
        "\n",
        "    print('Remaining variables:')\n",
        "    print(X.columns[variables])\n",
        "    return X[cols[variables]]\n",
        "\n",
        "#function for listing vif values\n",
        "def vif_values(X):\n",
        "    add_constant(X)\n",
        "    df=pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns)\n",
        "    return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIXThg_Zwl8L"
      },
      "source": [
        "X_reduit_VIF = drop_high_vif(df_pred,thresh=10)\n",
        "X_reduit_VIF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ML7TrVHkw13n"
      },
      "source": [
        "corr = X_reduit_VIF.corr()\n",
        "\n",
        "# Generate a mask for the upper triangle\n",
        "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
        "\n",
        "# Set up the matplotlib figure\n",
        "f, ax = plt.subplots(figsize=(15, 8))\n",
        "\n",
        "sns.heatmap(corr,mask=mask, cmap='coolwarm',annot=True, fmt='.2f')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ4NdLOQTJ7b"
      },
      "source": [
        "# Normalisation des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_mw7JMiW54q"
      },
      "source": [
        "X_Avec_Prix = df_data\n",
        "X_Avec_Prix = X_Avec_Prix.drop(columns=['Dates', 'Price'])\n",
        "Xdrop = SimpleImputer(missing_values=np.nan,strategy='most_frequent').fit_transform(X_Avec_Prix)\n",
        "Xdrop = pd.DataFrame(Xdrop)\n",
        "Xdrop.columns = X_Avec_Prix.columns\n",
        "X_Avec_Prix = Xdrop\n",
        "X_Avec_Prix = X_Avec_Prix\n",
        "X_Avec_Prix = X_Avec_Prix.iloc[:,var_pred]\n",
        "X_Avec_Prix.insert(0,'Price',df_data['Price'])\n",
        "X_Avec_Prix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eah20FyiTJ7c"
      },
      "source": [
        "**1. Séparation des données en données pour l'entrainement et la validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnhC-39gTJ7d"
      },
      "source": [
        "On réserve 20% des données pour l'entrainement et le reste pour la validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwXIe8_ETJ7d"
      },
      "source": [
        "# Sépare les données en entrainement et tests\n",
        "pourcentage = 0.8\n",
        "temps_separation = int(len(X_Avec_Prix) * pourcentage)\n",
        "date_separation = X_Avec_Prix.index[temps_separation]\n",
        "\n",
        "serie_entrainement_X = []\n",
        "serie_test_X = []\n",
        "\n",
        "for column in X_Avec_Prix:\n",
        "  serie_entrainement_X.append(np.array(X_Avec_Prix[column][:temps_separation],dtype=np.float32))\n",
        "  serie_test_X.append(np.array(X_Avec_Prix[column][temps_separation:],dtype=np.float32))\n",
        "\n",
        "print(\"Taille de l'entrainement : %d\" %len(serie_entrainement_X[0]))\n",
        "print(\"Taille de la validation : %d\" %len(serie_test_X[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_tTUC3LTJ73"
      },
      "source": [
        "serie_entrainement_X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxZ1c7kwTJ74"
      },
      "source": [
        "**1. Normalisation standard**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axYZ_W2GTJ74"
      },
      "source": [
        "On normalise les données :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6LeyK3MTJ74"
      },
      "source": [
        "mean_X = []\n",
        "std_X = []\n",
        "\n",
        "serie_entrainement_X_norm = []\n",
        "serie_test_X_norm = []\n",
        "\n",
        "# Calcul de la moyenne et de l'écart type de la série X\n",
        "for serie in serie_entrainement_X:\n",
        "  mean_X.append(tf.math.reduce_mean(np.asarray(serie)))\n",
        "  std_X.append(tf.math.reduce_std(np.asarray((serie))))\n",
        "\n",
        "# Normalisation des données X\n",
        "for i in range(0,len(mean_X)):\n",
        "  serie_entrainement_X_norm.append((serie_entrainement_X[i]-mean_X[i])/std_X[i])\n",
        "  serie_test_X_norm.append((serie_test_X[i]-mean_X[i])/std_X[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F49e996wTJ75"
      },
      "source": [
        "# Affiche la série\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "\n",
        "for i in range(0,8):\n",
        "  ax.plot(X_Avec_Prix.index[:temps_separation].values,serie_entrainement_X_norm[i], label=\"X_Ent\")\n",
        "  ax.plot(X_Avec_Prix.index[temps_separation:].values,serie_test_X_norm[i], label=\"X_Val\")\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yq6EgrfATJ75"
      },
      "source": [
        "**2. Normalisaiton robuste**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wx-mRJWUTJ76"
      },
      "source": [
        "estimators=[]\n",
        "\n",
        "estimators.append(['robust',RobustScaler()])\n",
        "estimators.append(['mixmax',MinMaxScaler()])\n",
        "\n",
        "scale=Pipeline(estimators,verbose=True)\n",
        "\n",
        "scale.fit_transform(tf.reshape(serie_entrainement_X,shape=(len(serie_entrainement_X[0]),len(serie_entrainement_X))))\n",
        "serie_entrainement_X_norm=scale.transform(tf.reshape(serie_entrainement_X,shape=(len(serie_entrainement_X[0]),len(serie_entrainement_X))))\n",
        "serie_test_X_norm=scale.transform(tf.reshape(serie_test_X,shape=(len(serie_test_X[0]),len(serie_test_X))))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8g7cFxmTJ76"
      },
      "source": [
        "# Affiche la série X\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "\n",
        "for i in range(0,10):\n",
        "  ax.plot(X_Avec_Prix.index[:temps_separation].values,serie_entrainement_X_norm[:,i], label=\"X_Ent\")\n",
        "  ax.plot(X_Avec_Prix.index[temps_separation:].values,serie_test_X_norm[:,i], label=\"X_Val\")\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wr9CM2ZmYNl0"
      },
      "source": [
        "# Création des datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nK6G5AVZYNmU"
      },
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "\n",
        "def prepare_dataset_XY(series, taille_fenetre, horizon, batch_size):\n",
        "  serie_concat = tf.expand_dims(series[0],1)\n",
        "\n",
        "  for i in range(1,len(series)):\n",
        "    serie_ = tf.expand_dims(series[i],1)\n",
        "    serie_concat = tf.concat([serie_concat,serie_],1)\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie_concat)\n",
        "  dataset = dataset.window(taille_fenetre+horizon, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre + horizon))\n",
        "  dataset = dataset.map(lambda x: (x[0:taille_fenetre],tf.expand_dims(x[-taille_fenetre:][:,0],1)))\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqoAHg2MYNmW"
      },
      "source": [
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "taille_fenetre = 2\n",
        "horizon = 1\n",
        "batch_size = 8\n",
        "\n",
        "# Création du dataset\n",
        "dataset = prepare_dataset_XY(serie_entrainement_X_norm,taille_fenetre,horizon,batch_size)\n",
        "dataset_val = prepare_dataset_XY(serie_test_X_norm,taille_fenetre,horizon,batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USVHzg-GYNmX"
      },
      "source": [
        "print(len(list(dataset.as_numpy_iterator())))\n",
        "for element in dataset.take(1):\n",
        "  print(element[0].shape)\n",
        "  print(element[1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OA6tsfl4YNmY"
      },
      "source": [
        "for element in dataset.take(1):\n",
        "  print(element)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAwyM48zYNmZ"
      },
      "source": [
        "On extrait maintenant les deux tenseurs (X,Y) pour l'entrainement :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMjh2eH5YNmZ"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset))              # #60x((32,5,779),(32,5,1)) => x = 60x(32,5,779) ; y = 60x(32,5,1)\n",
        "\n",
        "# Recombine les données\n",
        "x = np.asarray(x,dtype=np.float32)      # 60x(32,5,779) => (60,32,5,779)\n",
        "y = np.asarray(y,dtype=np.float32)      # 60x(32,5,1) => (60,32,5,1)\n",
        "\n",
        "x_train = np.asarray(tf.reshape(x,shape=(x.shape[0]*x.shape[1],taille_fenetre,x.shape[3])))     # (60,32,5,779) => (60*32,5,779)\n",
        "y_train = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],taille_fenetre,y.shape[3])))     # (60,32,5,1) => (60*32,5,1)\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQlnU0USYNma"
      },
      "source": [
        "Puis la même chose pour les données de validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBroUa5XYNma"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset_val))\n",
        "\n",
        "# Recombine les données\n",
        "x = np.asarray(x,dtype=np.float32)\n",
        "y = np.asarray(y,dtype=np.float32)\n",
        "\n",
        "x_val = np.asarray(tf.reshape(x,shape=(x.shape[0]*x.shape[1],taille_fenetre,x.shape[3])))\n",
        "y_val = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],taille_fenetre,y.shape[3])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVVTuw0xYXt8"
      },
      "source": [
        "# Création du modèle type Wavenet Multivarié"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB6dP-d8YXt9"
      },
      "source": [
        "**1. Création du réseau**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzpdbYrbYXt-"
      },
      "source": [
        "from keras.layers import Conv1D\n",
        "from keras.layers import Conv1D\n",
        "from keras.utils.conv_utils import conv_output_length\n",
        "from keras import layers\n",
        "from keras.regularizers import l2\n",
        "import keras.backend as K\n",
        "from keras.engine import Input\n",
        "from keras.engine import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNMpYRuEYXt_"
      },
      "source": [
        "class CausalDilatedConv1D(Conv1D):\n",
        "    def __init__(self, nb_filter, filter_length, init='glorot_uniform', activation=None, weights=None,\n",
        "                 border_mode='valid', subsample_length=1, atrous_rate=1, W_regularizer=None, b_regularizer=None,\n",
        "                 activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True, causal=False, **kwargs):\n",
        "        super(CausalDilatedConv1D, self).__init__(nb_filter, filter_length, weights=weights, activation=activation, \n",
        "                padding=border_mode, strides=subsample_length, dilation_rate=atrous_rate, kernel_regularizer=W_regularizer, \n",
        "                bias_regularizer=b_regularizer, activity_regularizer=activity_regularizer, kernel_constraint=W_constraint, \n",
        "                bias_constraint=b_constraint, use_bias=bias, **kwargs)\n",
        "        self.causal = causal\n",
        "        self.nb_filter = nb_filter\n",
        "        self.atrous_rate = atrous_rate\n",
        "        self.filter_length = filter_length\n",
        "        self.subsample_length = subsample_length\n",
        "        self.border_mode = border_mode\n",
        "        if self.causal and border_mode != 'valid':\n",
        "            raise ValueError(\"Causal mode dictates border_mode=valid.\")\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        input_length = input_shape[1]\n",
        "        if self.causal:\n",
        "            input_length += self.atrous_rate * (self.filter_length - 1)\n",
        "        length = conv_output_length(input_length, self.filter_length, self.border_mode, self.strides[0], dilation=self.atrous_rate)\n",
        "        return (input_shape[0], length, self.nb_filter)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        if self.causal:\n",
        "            x = K.temporal_padding(x, padding=(self.atrous_rate * (self.filter_length - 1), 0))\n",
        "        # return super(CausalAtrousConvolution1D, self).call(x, mask)\n",
        "        return super(CausalDilatedConv1D, self).call(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93bV8CcwYXuA"
      },
      "source": [
        "def _compute_receptive_field(dilation_depth, stacks):\n",
        "  return stacks * (2 ** dilation_depth * 2) - (stacks - 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAmgCh3_YXuB"
      },
      "source": [
        "def build_model_residual_block(x, i, s,nb_filters, dim_filters, use_bias,res_l2):\n",
        "        original_x = x\n",
        "        # TODO: initalization, regularization?\n",
        "        relu_out = CausalDilatedConv1D(nb_filters, dim_filters, atrous_rate=2 ** i, border_mode='valid', causal=True, bias=True, name='dilated_conv_%d_relu_s%d' % (2 ** i, s), activation='relu', W_regularizer=l2(res_l2))(x)\n",
        "        res_x = layers.Conv1D(1, 1, padding='same', use_bias=use_bias, kernel_regularizer=l2(res_l2))(relu_out)\n",
        "        skip_x = layers.Conv1D(1, 1, padding='same', use_bias=use_bias, kernel_regularizer=l2(res_l2))(relu_out)\n",
        "        res_x = layers.Add()([original_x, res_x])\n",
        "        return res_x, skip_x\n",
        "\n",
        "def build_model_couche_condition(x, output_bins, nb_filters, dim_filters, use_bias,res_l2):\n",
        "        original_x = x\n",
        "        skip_conditions = []\n",
        "\n",
        "        relu_out = CausalDilatedConv1D(nb_filters, dim_filters, atrous_rate=1, border_mode='valid', causal=True, bias=True, name='dilated_conv_%d_condition_%d_s%d' % (1,1, 0), activation='relu',\n",
        "                                       W_regularizer=l2(res_l2))(tf.expand_dims(x[:,:,0],2))\n",
        "\n",
        "        skip_ = layers.Conv1D(1, 1, padding='same', use_bias=use_bias, kernel_regularizer=l2(res_l2))(relu_out)\n",
        "        skip_conditions.append(skip_)\n",
        "\n",
        "        for i in range(1,output_bins-1):\n",
        "          relu_out = CausalDilatedConv1D(nb_filters, dim_filters, atrous_rate=1, border_mode='valid', causal=True, bias=True, name='dilated_conv_%d_condition_%d_s%d' % (1,i+1,0), activation='relu',\n",
        "                                                    W_regularizer=l2(res_l2))(tf.expand_dims(x[:,:,i],2))\n",
        "\n",
        "          skip_ = layers.Conv1D(1, 1, padding='same', use_bias=use_bias, kernel_regularizer=l2(res_l2))(relu_out)\n",
        "          skip_conditions.append(skip_)\n",
        "\n",
        "        if output_bins > 1:\n",
        "          out = layers.Add()(skip_conditions)\n",
        "        else:\n",
        "          out = skip_\n",
        "        return out\n",
        "\n",
        "\n",
        "def build_model(fragment_length, nb_filters, dim_filters, output_bins, dilation_depth, stacks, use_skip_connections, use_bias, res_l2, final_l2):\n",
        "        input_shape = Input(shape=(fragment_length, output_bins), name='input_part')\n",
        "        out = input_shape\n",
        "        skip_connections = []\n",
        "\n",
        "        for s in range(stacks):\n",
        "            # Couche conditionnée\n",
        "            out = build_model_couche_condition(out, output_bins, nb_filters, dim_filters, use_bias, res_l2)\n",
        "\n",
        "            # Couches intermédiaires\n",
        "            for i in range(1, dilation_depth + 1):\n",
        "                out, skip_out = build_model_residual_block(out, i, s, nb_filters, dim_filters, use_bias, res_l2)\n",
        "                skip_connections.append(skip_out)\n",
        "\n",
        "        if use_skip_connections:\n",
        "            out = layers.Add()(skip_connections)\n",
        "\n",
        "        # Couche de sortie\n",
        "        out = layers.Activation('linear', name=\"output_linear\")(out)\n",
        "        out = layers.Conv1D(1, 1, padding='same', kernel_regularizer=l2(final_l2))(out)\n",
        "        model = Model(input_shape, out)\n",
        "        return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-i75DMlNYXuD"
      },
      "source": [
        "**2. Construction du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EX9vTKthYXuE"
      },
      "source": [
        "def compute_receptive_field_(dilation_depth, nb_stacks):\n",
        "    receptive_field = nb_stacks * (2 ** dilation_depth * 2) - (nb_stacks - 1)\n",
        "    return receptive_field\n",
        "\n",
        "nb_filters = 3\n",
        "dim_filters = 2\n",
        "nb_output_bins = 22\n",
        "dilation_depth = 0\n",
        "nb_stacks = 1\n",
        "use_skip_connections = False\n",
        "use_bias = False\n",
        "res_l2 = 0\n",
        "final_l2 = 0\n",
        "\n",
        "fragment_length = compute_receptive_field_(dilation_depth, nb_stacks)\n",
        "fragment_length\n",
        "\n",
        "model = build_model(fragment_length, nb_filters, dim_filters, nb_output_bins, dilation_depth, nb_stacks, use_skip_connections, use_bias, res_l2, final_l2)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufsIFFXvYkiW"
      },
      "source": [
        "# Entrainement du modèle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyEdiKMmYkiX"
      },
      "source": [
        "**1. Optimisation de l'apprentissage**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOFjr4yXYkiX"
      },
      "source": [
        "Pour accélérer le traitement des données, nous n'allons pas utiliser l'intégralité des données pendant la mise à jour du gradient, comme cela a été fait jusqu'à présent (en utilisant le dataset).  \n",
        "Cette fois-ci, nous allons forcer les mises à jour du gradient à se produire de manière moins fréquente en attribuant la valeur du batch_size à prendre en compte lors de la regression du modèle.  \n",
        "Pour cela, on utilise l'argument \"batch_size\" dans la méthode fit. En précisant un batch_size=1000, cela signifie que :\n",
        " - Sur notre total de 56000 échantillons, 56 seront utilisés pour les calculs du gradient\n",
        " - Il y aura également 56 itérations à chaque période.\n",
        "  \n",
        "    \n",
        "    \n",
        "Si nous avions pris le dataset comme entrée, nous aurions eu :\n",
        "- Un total de 56000 échantillons également\n",
        "- Chaque période aurait également pris 56 itérations pour se compléter\n",
        "- Mais 1000 échantillons auraient été utilisés pour le calcul du gradient, au lieu de 56 avec la méthode utilisée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU0a-eQ9YkiY"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "def  My_MSE(y_true,y_pred):\n",
        "  return(tf.keras.metrics.mse(y_true,y_pred)*std_X[0].numpy()+mean_X[0].numpy())\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "#optimiseur=tf.keras.optimizers.Adam()\n",
        "optimiseur=tf.keras.optimizers.SGD()\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"mse\", optimizer=optimiseur, metrics=[\"mse\",My_MSE])\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint],batch_size=batch_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHchQmnoYkiZ"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[30], taux[99], 0, 0.4])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc5-yJEqYkia"
      },
      "source": [
        "# Chargement des poids sauvegardés\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fx8H8AHKYkia"
      },
      "source": [
        "**2. Entrainement du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZWWRL2WYkib"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "max_periodes = 5000\n",
        "\n",
        "# Classe permettant d'arrêter l'entrainement si la variation\n",
        "# devient plus petite qu'une valeur à choisir sur un nombre\n",
        "# de périodes à choisir\n",
        "class StopTrain(keras.callbacks.Callback):\n",
        "    def __init__(self, delta=0.01,periodes=100, term=\"loss\", logs={}):\n",
        "      self.n_periodes = 0\n",
        "      self.periodes = periodes\n",
        "      self.loss_1 = 100\n",
        "      self.delta = delta\n",
        "      self.term = term\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "      diff_loss = abs(self.loss_1 - logs[self.term])\n",
        "      self.loss_1 = logs[self.term]\n",
        "      if (diff_loss < self.delta):\n",
        "        self.n_periodes = self.n_periodes + 1\n",
        "      else:\n",
        "        self.n_periodes = 0\n",
        "      if (self.n_periodes == self.periodes):\n",
        "        print(\"Arrêt de l'entrainement...\")\n",
        "        self.model.stop_training = True\n",
        "\n",
        "def  My_MSE(y_true,y_pred):\n",
        "  return(tf.keras.metrics.mse(y_true,y_pred)*std_X[0].numpy()+mean_X[0].numpy())\n",
        "  \n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.004,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0.001)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "#optimiseur=tf.keras.optimizers.SGD(learning_rate=lr_schedule,momentum=0.9)\n",
        "\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"mse\", optimizer=optimiseur, metrics=[\"mse\",My_MSE])\n",
        "\n",
        "# Entraine le modèle, avec une réduction des calculs du gradient\n",
        "historique = model.fit(x=x_train,y=y_train,validation_data=(x_val,y_val), epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=1e-2,periodes = 10, term=\"My_MSE\")],batch_size=batch_size)\n",
        "\n",
        "# Entraine le modèle sans réduction de calculs\n",
        "#historique = model.fit(dataset,validation_data=dataset_val, epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=1e-3,periodes = 20, term=\"My_MSE\")])\n",
        "\n",
        "files.download('poids_train.hdf5')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EKaVqPCYkid"
      },
      "source": [
        "# Télécharge les résultats d'entrainement du modèle\n",
        "! wget --no-check-certificate --content-disposition \"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/Bitcoin/poids_train_BitWave_One_All.hdf5?raw=true\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMUbH3CaYkid"
      },
      "source": [
        "model.load_weights(\"poids_train.hdf5\")\n",
        "#model.load_weights(\"poids_train_BitWave_One_All.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idMYWg8mYkie"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6763fLQ6Ykif"
      },
      "source": [
        "# Evaluation du modèle\n",
        "\n",
        "model.evaluate(dataset)\n",
        "model.evaluate(dataset_val)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj3qvyK6Ykir"
      },
      "source": [
        "**3. Prédictions \"single step\"**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iOJE_lAYkir"
      },
      "source": [
        "# Création des instants d'entrainement et de validation\n",
        "y_train_timing = X_Avec_Prix.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_train)]\n",
        "y_val_timing = X_Avec_Prix.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_val)]\n",
        "\n",
        "# Calcul des prédictions\n",
        "pred_ent = model.predict(x_train, verbose=1)\n",
        "pred_val = model.predict(x_val, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfUyGJ0RYkir"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbes originales\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index,y=serie_entrainement_X_norm[0],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index[temps_separation:],y=serie_test_X_norm[0],line=dict(color='blue', width=1)))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index[taille_fenetre+horizon-1:],y=pred_ent[:,taille_fenetre-1,0],line=dict(color='green', width=1)))\n",
        "\n",
        "# Courbes des prédictions de validations\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index[temps_separation+taille_fenetre+horizon-1:],y=pred_val[:,taille_fenetre-1,0],line=dict(color='red', width=1)))\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8jvHwA0Ykis"
      },
      "source": [
        "**4. Prédictions multi-step**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYHqaGThYkit"
      },
      "source": [
        "predictions = []\n",
        "                                                        # x_val : (448,16,779)\n",
        "data_to_predict = x_val[0,:,:]                          # [[VAL#1_0,VAL#2_0, ... , VAL#779_0],[VAL#1_1,VAL#2_1, ... , VAL#779_1], ... , [VAL#1_15,VAL#2_15, ..., VAL#779_15]] : (16,779)\n",
        "data_to_predict = tf.expand_dims(data_to_predict,0)     # (1,16,779)\n",
        "\n",
        "prediction = model.predict(data_to_predict)             # [[[^VAL#1_1][^VAL#1_2]...[^VAL#1_16]]] : (1,16,1)\n",
        "predictions.append(prediction[0,taille_fenetre-1,0])    # [^VAL#1_16] : (1,)\n",
        "\n",
        "data_to_predict = x_val[1,0:taille_fenetre-1,:]         # [[VAL#1_1,VAL#2_1, ... , VAL#779_1],[VAL#1_2,VAL#2_2, ... , VAL#779_2], ... , [VAL#1_15,VAL#2_15, ..., VAL#779_15]] : (15,779)\n",
        "\n",
        "data_to_predict = np.insert(data_to_predict,taille_fenetre-1,     # [[VAL#1_1,VAL#2_1, ... , VAL#779_1],[VAL#1_2,VAL#2_2, ... , VAL#779_2], ... , [^VAL#1_16,VAL#2_16, ..., VAL#779_16]] : (16,779)\n",
        "                            np.concatenate((tf.expand_dims(np.array(predictions[0]),0),x_val[1,taille_fenetre-2,1:779]),axis=0),axis=0)\n",
        "\n",
        "for i in range(1,300):\n",
        "  data_to_predict = tf.expand_dims(data_to_predict,0)   # (16,779) => (1,16,779)\n",
        "  prediction = model.predict(data_to_predict)           # [[[^VAL#1_2][^VAL#1_3]...[^VAL#1_17]]] : (1,16,1)\n",
        "\n",
        "  predictions.append(prediction[0,taille_fenetre-1,0])  # [^VAL#1_17] : (1,)\n",
        "  data_to_predict = x_val[i+1,0:taille_fenetre-1,:]     # [[VAL#1_2,VAL#2_2, ... , VAL#779_2],[VAL#1_3,VAL#2_3, ... , VAL#779_3], ... , [VAL#1_17,VAL#2_17, ..., VAL#779_17]] : (15,779)\n",
        "\n",
        "  data_to_predict = np.insert(data_to_predict,taille_fenetre-1,     # [[VAL#1_1,VAL#2_1, ... , VAL#779_1],[VAL#1_2,VAL#2_2, ... , VAL#779_2], ... , [^VAL#1_16,VAL#2_16, ..., VAL#779_16]] : (16,779)\n",
        "                            np.concatenate((tf.expand_dims(np.array(predictions[i]),0),x_val[1,taille_fenetre-2,1:779]),axis=0),axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAcNpvbvYkit"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "n_max = len(predictions)\n",
        "\n",
        "# Courbes originales\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index,y=serie_entrainement_X_norm[0],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index[temps_separation:],y=serie_test_X_norm[0],line=dict(color='blue', width=1)))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index[taille_fenetre+horizon-1:],y=pred_ent[:,taille_fenetre-1,0],line=dict(color='green', width=1)))\n",
        "\n",
        "# Courbes des prédictions de validations\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index[temps_separation+taille_fenetre+horizon-1:temps_separation+taille_fenetre+horizon-1+n_max],y=predictions[0:n_max],line=dict(color='red', width=1)))\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5CeF8IyjQFT"
      },
      "source": [
        "# Création des datasets - Sans prix en X"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZ1MBty1xgoX"
      },
      "source": [
        "X_reduit_VIF.insert(0,'Price',df_data['Price'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoP-8584xjYv"
      },
      "source": [
        "X_reduit_VIF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_a3GiFgyxlbT"
      },
      "source": [
        "X_Avec_Prix = X_reduit_VIF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gy8wD4VLu5Qg"
      },
      "source": [
        "X_Avec_Prix = df_data\n",
        "X_Avec_Prix = X_Avec_Prix.drop(columns=['Dates', 'Price'])\n",
        "Xdrop = SimpleImputer(missing_values=np.nan,strategy='most_frequent').fit_transform(X_Avec_Prix)\n",
        "Xdrop = pd.DataFrame(Xdrop)\n",
        "Xdrop.columns = X_Avec_Prix.columns\n",
        "X_Avec_Prix = Xdrop\n",
        "X_Avec_Prix = X_Avec_Prix\n",
        "X_Avec_Prix = X_Avec_Prix.iloc[:,var_pred]\n",
        "X_Avec_Prix.insert(0,'Price',df_data['Price'])\n",
        "X_Avec_Prix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-44FsypRjQFe"
      },
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "\n",
        "def prepare_dataset_XY(series, taille_fenetre, horizon, batch_size):\n",
        "  serie_concat = tf.expand_dims(series[0],1)\n",
        "\n",
        "  for i in range(1,len(series)):\n",
        "    serie_ = tf.expand_dims(series[i],1)\n",
        "    serie_concat = tf.concat([serie_concat,serie_],1)\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie_concat)\n",
        "  dataset = dataset.window(taille_fenetre+horizon, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre + horizon))\n",
        "  dataset = dataset.map(lambda x: (x[0:taille_fenetre][:,1:],tf.expand_dims(x[-taille_fenetre:][:,0],1)))\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6t272rIjQFg"
      },
      "source": [
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "taille_fenetre = 64\n",
        "horizon = 1\n",
        "batch_size = 1\n",
        "\n",
        "# Création du dataset\n",
        "dataset = prepare_dataset_XY(serie_entrainement_X_norm,taille_fenetre,horizon,batch_size)\n",
        "dataset_val = prepare_dataset_XY(serie_test_X_norm,taille_fenetre,horizon,batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKO1WzQkjQFh"
      },
      "source": [
        "print(len(list(dataset.as_numpy_iterator())))\n",
        "for element in dataset.take(1):\n",
        "  print(element[0].shape)\n",
        "  print(element[1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqHV6Ac5jQFi"
      },
      "source": [
        "for element in dataset.take(1):\n",
        "  print(element)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvR1N4gojQFj"
      },
      "source": [
        "On extrait maintenant les deux tenseurs (X,Y) pour l'entrainement :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwJtYSVtjQFk"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset))              # #60x((32,5,779),(32,5,1)) => x = 60x(32,5,779) ; y = 60x(32,5,1)\n",
        "\n",
        "# Recombine les données\n",
        "x = np.asarray(x,dtype=np.float32)      # 60x(32,5,779) => (60,32,5,779)\n",
        "y = np.asarray(y,dtype=np.float32)      # 60x(32,5,1) => (60,32,5,1)\n",
        "\n",
        "x_train = np.asarray(tf.reshape(x,shape=(x.shape[0]*x.shape[1],taille_fenetre,x.shape[3])))     # (60,32,5,779) => (60*32,5,779)\n",
        "y_train = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],taille_fenetre,y.shape[3])))     # (60,32,5,1) => (60*32,5,1)\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Zny4A0jjQFk"
      },
      "source": [
        "Puis la même chose pour les données de validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhoA0UOxjQFl"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset_val))\n",
        "\n",
        "# Recombine les données\n",
        "x = np.asarray(x,dtype=np.float32)\n",
        "y = np.asarray(y,dtype=np.float32)\n",
        "\n",
        "x_val = np.asarray(tf.reshape(x,shape=(x.shape[0]*x.shape[1],taille_fenetre,x.shape[3])))\n",
        "y_val = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],taille_fenetre,y.shape[3])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6TE-8uonbYq"
      },
      "source": [
        "# Création du modèle type Wavenet Multivarié"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFUUNiNYnbZH"
      },
      "source": [
        "**1. Création du réseau**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfUkzVAwnbZH"
      },
      "source": [
        "from keras.layers import Conv1D\n",
        "from keras.layers import Conv1D\n",
        "from keras.utils.conv_utils import conv_output_length\n",
        "from keras import layers\n",
        "from keras.regularizers import l2\n",
        "import keras.backend as K\n",
        "from keras.engine import Input\n",
        "from keras.engine import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mDTWTKDnbZI"
      },
      "source": [
        "class CausalDilatedConv1D(Conv1D):\n",
        "    def __init__(self, nb_filter, filter_length, init='glorot_uniform', activation=None, weights=None,\n",
        "                 border_mode='valid', subsample_length=1, atrous_rate=1, W_regularizer=None, b_regularizer=None,\n",
        "                 activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True, causal=False, **kwargs):\n",
        "        super(CausalDilatedConv1D, self).__init__(nb_filter, filter_length, weights=weights, activation=activation, \n",
        "                padding=border_mode, strides=subsample_length, dilation_rate=atrous_rate, kernel_regularizer=W_regularizer, \n",
        "                bias_regularizer=b_regularizer, activity_regularizer=activity_regularizer, kernel_constraint=W_constraint, \n",
        "                bias_constraint=b_constraint, use_bias=bias, **kwargs)\n",
        "        self.causal = causal\n",
        "        self.nb_filter = nb_filter\n",
        "        self.atrous_rate = atrous_rate\n",
        "        self.filter_length = filter_length\n",
        "        self.subsample_length = subsample_length\n",
        "        self.border_mode = border_mode\n",
        "        if self.causal and border_mode != 'valid':\n",
        "            raise ValueError(\"Causal mode dictates border_mode=valid.\")\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        input_length = input_shape[1]\n",
        "        if self.causal:\n",
        "            input_length += self.atrous_rate * (self.filter_length - 1)\n",
        "        length = conv_output_length(input_length, self.filter_length, self.border_mode, self.strides[0], dilation=self.atrous_rate)\n",
        "        return (input_shape[0], length, self.nb_filter)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        if self.causal:\n",
        "            x = K.temporal_padding(x, padding=(self.atrous_rate * (self.filter_length - 1), 0))\n",
        "        # return super(CausalAtrousConvolution1D, self).call(x, mask)\n",
        "        return super(CausalDilatedConv1D, self).call(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVxqodltnbZJ"
      },
      "source": [
        "def _compute_receptive_field(dilation_depth, stacks):\n",
        "  return stacks * (2 ** dilation_depth * 2) - (stacks - 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKSxA3XVnbZK"
      },
      "source": [
        "def build_model_residual_block(x, i, s,nb_filters, dim_filters, use_bias,res_l2):\n",
        "        original_x = x\n",
        "        # TODO: initalization, regularization?\n",
        "        relu_out = CausalDilatedConv1D(nb_filters, dim_filters, atrous_rate=2 ** i, border_mode='valid', causal=True, bias=True, name='dilated_conv_%d_relu_s%d' % (2 ** i, s), activation='relu', W_regularizer=l2(res_l2))(x)\n",
        "        res_x = layers.Conv1D(1, 1, padding='same', use_bias=use_bias, kernel_regularizer=l2(res_l2))(relu_out)\n",
        "        skip_x = layers.Conv1D(1, 1, padding='same', use_bias=use_bias, kernel_regularizer=l2(res_l2))(relu_out)\n",
        "        res_x = layers.Add()([original_x, res_x])\n",
        "        return res_x, skip_x\n",
        "\n",
        "def build_model_couche_condition(x, output_bins, nb_filters, dim_filters, use_bias,res_l2):\n",
        "        original_x = x\n",
        "        skip_conditions = []\n",
        "\n",
        "        relu_out = CausalDilatedConv1D(nb_filters, dim_filters, atrous_rate=1, border_mode='valid', causal=True, bias=True, name='dilated_conv_%d_condition_%d_s%d' % (1,1, 0), activation='relu',\n",
        "                                       W_regularizer=l2(res_l2))(tf.expand_dims(x[:,:,0],2))\n",
        "\n",
        "        skip_ = layers.Conv1D(1, 1, padding='same', use_bias=use_bias, kernel_regularizer=l2(res_l2))(relu_out)\n",
        "        skip_conditions.append(skip_)\n",
        "\n",
        "        for i in range(1,output_bins-1):\n",
        "          relu_out = CausalDilatedConv1D(nb_filters, dim_filters, atrous_rate=1, border_mode='valid', causal=True, bias=True, name='dilated_conv_%d_condition_%d_s%d' % (1,i+1,0), activation='relu',\n",
        "                                                    W_regularizer=l2(res_l2))(tf.expand_dims(x[:,:,i],2))\n",
        "\n",
        "          skip_ = layers.Conv1D(1, 1, padding='same', use_bias=use_bias, kernel_regularizer=l2(res_l2))(relu_out)\n",
        "          skip_conditions.append(skip_)\n",
        "\n",
        "        if output_bins > 1:\n",
        "          out = layers.Add()(skip_conditions)\n",
        "        else:\n",
        "          out = skip_\n",
        "        return out\n",
        "\n",
        "\n",
        "def build_model(fragment_length, nb_filters, dim_filters, output_bins, dilation_depth, stacks, use_skip_connections, use_bias, res_l2, final_l2):\n",
        "        input_shape = Input(shape=(fragment_length, output_bins), name='input_part')\n",
        "        out = input_shape\n",
        "        skip_connections = []\n",
        "\n",
        "        for s in range(stacks):\n",
        "            # Couche conditionnée\n",
        "            out = build_model_couche_condition(out, output_bins, nb_filters, dim_filters, use_bias, res_l2)\n",
        "\n",
        "            # Couches intermédiaires\n",
        "            for i in range(1, dilation_depth + 1):\n",
        "                out, skip_out = build_model_residual_block(out, i, s, nb_filters, dim_filters, use_bias, res_l2)\n",
        "                skip_connections.append(skip_out)\n",
        "\n",
        "        if use_skip_connections:\n",
        "            out = layers.Add()(skip_connections)\n",
        "\n",
        "        # Couche de sortie\n",
        "        out = layers.Activation('linear', name=\"output_linear\")(out)\n",
        "        out = layers.Conv1D(1, 1, padding='same', kernel_regularizer=l2(final_l2))(out)\n",
        "        model = Model(input_shape, out)\n",
        "        return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBHv2GWNnbZL"
      },
      "source": [
        "**2. Construction du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNMIkaHYnbZL"
      },
      "source": [
        "def compute_receptive_field_(dilation_depth, nb_stacks):\n",
        "    receptive_field = nb_stacks * (2 ** dilation_depth * 2) - (nb_stacks - 1)\n",
        "    return receptive_field\n",
        "\n",
        "nb_filters = 5\n",
        "dim_filters = 4\n",
        "nb_output_bins = 7\n",
        "dilation_depth = 5\n",
        "nb_stacks = 1\n",
        "use_skip_connections = False\n",
        "use_bias = False\n",
        "res_l2 = 0.001\n",
        "final_l2 = 0.001\n",
        "\n",
        "fragment_length = compute_receptive_field_(dilation_depth, nb_stacks)\n",
        "fragment_length\n",
        "\n",
        "model = build_model(fragment_length, nb_filters, dim_filters, nb_output_bins, dilation_depth, nb_stacks, use_skip_connections, use_bias, res_l2, final_l2)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3uWCqzcnkB2"
      },
      "source": [
        "# Entrainement du modèle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kly6bhi8nkB3"
      },
      "source": [
        "**1. Optimisation de l'apprentissage**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB1gffpQnkB4"
      },
      "source": [
        "Pour accélérer le traitement des données, nous n'allons pas utiliser l'intégralité des données pendant la mise à jour du gradient, comme cela a été fait jusqu'à présent (en utilisant le dataset).  \n",
        "Cette fois-ci, nous allons forcer les mises à jour du gradient à se produire de manière moins fréquente en attribuant la valeur du batch_size à prendre en compte lors de la regression du modèle.  \n",
        "Pour cela, on utilise l'argument \"batch_size\" dans la méthode fit. En précisant un batch_size=1000, cela signifie que :\n",
        " - Sur notre total de 56000 échantillons, 56 seront utilisés pour les calculs du gradient\n",
        " - Il y aura également 56 itérations à chaque période.\n",
        "  \n",
        "    \n",
        "    \n",
        "Si nous avions pris le dataset comme entrée, nous aurions eu :\n",
        "- Un total de 56000 échantillons également\n",
        "- Chaque période aurait également pris 56 itérations pour se compléter\n",
        "- Mais 1000 échantillons auraient été utilisés pour le calcul du gradient, au lieu de 56 avec la méthode utilisée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJaE8myAnkB5"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "def  My_MSE(y_true,y_pred):\n",
        "  return(tf.keras.metrics.mse(y_true,y_pred)*std_X[0].numpy()+mean_X[0].numpy())\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam()\n",
        "#optimiseur=tf.keras.optimizers.SGD()\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"mse\", optimizer=optimiseur, metrics=[\"mse\",My_MSE])\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint],batch_size=batch_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mJxFSJNnkB7"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[30], taux[99], 0, 0.4])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2icNDS5nkB8"
      },
      "source": [
        "# Chargement des poids sauvegardés\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKErsgPwnkB9"
      },
      "source": [
        "**2. Entrainement du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJTgBXrZnkB9"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "max_periodes = 5000\n",
        "\n",
        "# Classe permettant d'arrêter l'entrainement si la variation\n",
        "# devient plus petite qu'une valeur à choisir sur un nombre\n",
        "# de périodes à choisir\n",
        "class StopTrain(keras.callbacks.Callback):\n",
        "    def __init__(self, delta=0.01,periodes=100, term=\"loss\", logs={}):\n",
        "      self.n_periodes = 0\n",
        "      self.periodes = periodes\n",
        "      self.loss_1 = 100\n",
        "      self.delta = delta\n",
        "      self.term = term\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "      diff_loss = abs(self.loss_1 - logs[self.term])\n",
        "      self.loss_1 = logs[self.term]\n",
        "      if (diff_loss < self.delta):\n",
        "        self.n_periodes = self.n_periodes + 1\n",
        "      else:\n",
        "        self.n_periodes = 0\n",
        "      if (self.n_periodes == self.periodes):\n",
        "        print(\"Arrêt de l'entrainement...\")\n",
        "        self.model.stop_training = True\n",
        "\n",
        "def  My_MSE(y_true,y_pred):\n",
        "  return(tf.keras.metrics.mse(y_true,y_pred)*std_X[0].numpy()+mean_X[0].numpy())\n",
        "  \n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.001,\n",
        "    decay_steps=20,\n",
        "    decay_rate=0.001)\n",
        "\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam(learning_rate=lr_schedule,amsgrad=True)\n",
        "#optimiseur=tf.keras.optimizers.SGD(learning_rate=lr_schedule,momentum=0.9)\n",
        "\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"logcosh\", optimizer=optimiseur, metrics=[\"mse\",My_MSE])\n",
        "\n",
        "# Entraine le modèle, avec une réduction des calculs du gradient\n",
        "historique = model.fit(x=x_train,y=y_train,validation_data=(x_val,y_val), epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=1e-2,periodes = 10, term=\"My_MSE\")],batch_size=16)\n",
        "\n",
        "# Entraine le modèle sans réduction de calculs\n",
        "#historique = model.fit(dataset,validation_data=dataset_val, epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=1e-3,periodes = 20, term=\"My_MSE\")])\n",
        "\n",
        "files.download('poids_train.hdf5')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcCcIoABnkB_"
      },
      "source": [
        "# Télécharge les résultats d'entrainement du modèle\n",
        "! wget --no-check-certificate --content-disposition \"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/Bitcoin/poids_train_BitWave_One_All.hdf5?raw=true\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwCBv230nkCA"
      },
      "source": [
        "model.load_weights(\"poids_train.hdf5\")\n",
        "#model.load_weights(\"poids_train_BitWave_One_All.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8zkTV7dnkCA"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMcgYWLMnkCB"
      },
      "source": [
        "# Evaluation du modèle\n",
        "\n",
        "model.evaluate(dataset)\n",
        "model.evaluate(dataset_val)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRxuxZCRnkCC"
      },
      "source": [
        "**3. Prédictions \"single step\"**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOnVsZVznkCD"
      },
      "source": [
        "# Création des instants d'entrainement et de validation\n",
        "y_train_timing = X_Avec_Prix.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_train)]\n",
        "y_val_timing = X_Avec_Prix.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_val)]\n",
        "\n",
        "# Calcul des prédictions\n",
        "pred_ent = model.predict(x_train, verbose=1)\n",
        "pred_val = model.predict(x_val, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBixEA6vnkCD"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbes originales\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index,y=serie_entrainement_X_norm[0],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index[temps_separation:],y=serie_test_X_norm[0],line=dict(color='blue', width=1)))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index[taille_fenetre+horizon-1:],y=pred_ent[:,taille_fenetre-1,0],line=dict(color='green', width=1)))\n",
        "\n",
        "# Courbes des prédictions de validations\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index[temps_separation+taille_fenetre+horizon-1:],y=pred_val[:,taille_fenetre-1,0],line=dict(color='red', width=1)))\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7in3IeG1nkCF"
      },
      "source": [
        "**4. Prédictions multi-step**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buIyA-dNnkCF"
      },
      "source": [
        "predictions = []\n",
        "                                                        # x_val : (448,16,779)\n",
        "data_to_predict = x_val[0,:,:]                          # [[VAL#1_0,VAL#2_0, ... , VAL#779_0],[VAL#1_1,VAL#2_1, ... , VAL#779_1], ... , [VAL#1_15,VAL#2_15, ..., VAL#779_15]] : (16,779)\n",
        "data_to_predict = tf.expand_dims(data_to_predict,0)     # (1,16,779)\n",
        "\n",
        "prediction = model.predict(data_to_predict)             # [[[^VAL#1_1][^VAL#1_2]...[^VAL#1_16]]] : (1,16,1)\n",
        "predictions.append(prediction[0,taille_fenetre-1,0])    # [^VAL#1_16] : (1,)\n",
        "\n",
        "data_to_predict = x_val[1,0:taille_fenetre-1,:]         # [[VAL#1_1,VAL#2_1, ... , VAL#779_1],[VAL#1_2,VAL#2_2, ... , VAL#779_2], ... , [VAL#1_15,VAL#2_15, ..., VAL#779_15]] : (15,779)\n",
        "\n",
        "data_to_predict = np.insert(data_to_predict,taille_fenetre-1,     # [[VAL#1_1,VAL#2_1, ... , VAL#779_1],[VAL#1_2,VAL#2_2, ... , VAL#779_2], ... , [^VAL#1_16,VAL#2_16, ..., VAL#779_16]] : (16,779)\n",
        "                            np.concatenate((tf.expand_dims(np.array(predictions[0]),0),x_val[1,taille_fenetre-2,1:779]),axis=0),axis=0)\n",
        "\n",
        "for i in range(1,300):\n",
        "  data_to_predict = tf.expand_dims(data_to_predict,0)   # (16,779) => (1,16,779)\n",
        "  prediction = model.predict(data_to_predict)           # [[[^VAL#1_2][^VAL#1_3]...[^VAL#1_17]]] : (1,16,1)\n",
        "\n",
        "  predictions.append(prediction[0,taille_fenetre-1,0])  # [^VAL#1_17] : (1,)\n",
        "  data_to_predict = x_val[i+1,0:taille_fenetre-1,:]     # [[VAL#1_2,VAL#2_2, ... , VAL#779_2],[VAL#1_3,VAL#2_3, ... , VAL#779_3], ... , [VAL#1_17,VAL#2_17, ..., VAL#779_17]] : (15,779)\n",
        "\n",
        "  data_to_predict = np.insert(data_to_predict,taille_fenetre-1,     # [[VAL#1_1,VAL#2_1, ... , VAL#779_1],[VAL#1_2,VAL#2_2, ... , VAL#779_2], ... , [^VAL#1_16,VAL#2_16, ..., VAL#779_16]] : (16,779)\n",
        "                            np.concatenate((tf.expand_dims(np.array(predictions[i]),0),x_val[1,taille_fenetre-2,1:779]),axis=0),axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "za6q5srpnkCH"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "n_max = len(predictions)\n",
        "\n",
        "# Courbes originales\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index,y=serie_entrainement_X_norm[0],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index[temps_separation:],y=serie_test_X_norm[0],line=dict(color='blue', width=1)))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index[taille_fenetre+horizon-1:],y=pred_ent[:,taille_fenetre-1,0],line=dict(color='green', width=1)))\n",
        "\n",
        "# Courbes des prédictions de validations\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index[temps_separation+taille_fenetre+horizon-1:temps_separation+taille_fenetre+horizon-1+n_max],y=predictions[0:n_max],line=dict(color='red', width=1)))\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}