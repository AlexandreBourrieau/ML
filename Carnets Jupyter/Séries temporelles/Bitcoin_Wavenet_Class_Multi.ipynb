{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bitcoin_Wavenet_Class_Multi.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/Bitcoin_Wavenet_Class_Multi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVuCzMYKDp3z"
      },
      "source": [
        "# Chargement des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3nLyKtnliCW"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import re\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyxxcCcl1Kb3",
        "outputId": "94203ede-a7c4-4ac8-82b2-0bb1843c351d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!rm *.zip\n",
        "!rm *.csv\n",
        "!wget -q --no-check-certificate --content-disposition \"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/Bitcoin/Bitcoin_complet.zip?raw=true\"\n",
        "!unzip \"Bitcoin_complet.zip\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '*.zip': No such file or directory\n",
            "rm: cannot remove '*.csv': No such file or directory\n",
            "Archive:  Bitcoin_complet.zip\n",
            "  inflating: Bitcoin_complet.csv     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPJ6nnrRsUBm"
      },
      "source": [
        "Charge la série sous Pandas et affiche les informations du fichier :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR4EPWWwbO3O",
        "outputId": "ed041048-93db-4ecd-ba09-ad6d49ac07a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        }
      },
      "source": [
        "# Création de la série sous Pandas\n",
        "df_data = pd.read_csv(\"Bitcoin_complet.csv\")\n",
        "df_data"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dates</th>\n",
              "      <th>#Active_Adresses</th>\n",
              "      <th>Block Size</th>\n",
              "      <th>Block Time</th>\n",
              "      <th>FeeInReward</th>\n",
              "      <th>Price</th>\n",
              "      <th>Transaction Fee</th>\n",
              "      <th>Transaction Value</th>\n",
              "      <th>#From_Adresses</th>\n",
              "      <th>Google GTrends</th>\n",
              "      <th>Hashrate</th>\n",
              "      <th>Capitalization</th>\n",
              "      <th>Median Transaction Fee</th>\n",
              "      <th>Median Transaction Value</th>\n",
              "      <th>Difficulty</th>\n",
              "      <th>#Transactions</th>\n",
              "      <th>Profiltability</th>\n",
              "      <th>Sent</th>\n",
              "      <th>Top100</th>\n",
              "      <th>Tweets</th>\n",
              "      <th>#Active_Adresses_sma3</th>\n",
              "      <th>Block Size_sma3</th>\n",
              "      <th>Block Time_sma3</th>\n",
              "      <th>FeeInReward_sma3</th>\n",
              "      <th>Price_sma3</th>\n",
              "      <th>Transaction Fee_sma3</th>\n",
              "      <th>Transaction Value_sma3</th>\n",
              "      <th>#From_Adresses_sma3</th>\n",
              "      <th>Google GTrends_sma3</th>\n",
              "      <th>Hashrate_sma3</th>\n",
              "      <th>Capitalization_sma3</th>\n",
              "      <th>Median Transaction Fee_sma3</th>\n",
              "      <th>Median Transaction Value_sma3</th>\n",
              "      <th>Difficulty_sma3</th>\n",
              "      <th>#Transactions_sma3</th>\n",
              "      <th>Profiltability_sma3</th>\n",
              "      <th>Sent_sma3</th>\n",
              "      <th>Top100_sma3</th>\n",
              "      <th>Tweets_sma3</th>\n",
              "      <th>#Active_Adresses_sma7</th>\n",
              "      <th>...</th>\n",
              "      <th>Top100_roc14</th>\n",
              "      <th>Tweets_roc14</th>\n",
              "      <th>#Active_Adresses_roc30</th>\n",
              "      <th>Block Size_roc30</th>\n",
              "      <th>Block Time_roc30</th>\n",
              "      <th>FeeInReward_roc30</th>\n",
              "      <th>Price_roc30</th>\n",
              "      <th>Transaction Fee_roc30</th>\n",
              "      <th>Transaction Value_roc30</th>\n",
              "      <th>#From_Adresses_roc30</th>\n",
              "      <th>Google GTrends_roc30</th>\n",
              "      <th>Hashrate_roc30</th>\n",
              "      <th>Capitalization_roc30</th>\n",
              "      <th>Median Transaction Fee_roc30</th>\n",
              "      <th>Median Transaction Value_roc30</th>\n",
              "      <th>Difficulty_roc30</th>\n",
              "      <th>#Transactions_roc30</th>\n",
              "      <th>Profiltability_roc30</th>\n",
              "      <th>Sent_roc30</th>\n",
              "      <th>Top100_roc30</th>\n",
              "      <th>Tweets_roc30</th>\n",
              "      <th>#Active_Adresses_roc90</th>\n",
              "      <th>Block Size_roc90</th>\n",
              "      <th>Block Time_roc90</th>\n",
              "      <th>FeeInReward_roc90</th>\n",
              "      <th>Price_roc90</th>\n",
              "      <th>Transaction Fee_roc90</th>\n",
              "      <th>Transaction Value_roc90</th>\n",
              "      <th>#From_Adresses_roc90</th>\n",
              "      <th>Google GTrends_roc90</th>\n",
              "      <th>Hashrate_roc90</th>\n",
              "      <th>Capitalization_roc90</th>\n",
              "      <th>Median Transaction Fee_roc90</th>\n",
              "      <th>Median Transaction Value_roc90</th>\n",
              "      <th>Difficulty_roc90</th>\n",
              "      <th>#Transactions_roc90</th>\n",
              "      <th>Profiltability_roc90</th>\n",
              "      <th>Sent_roc90</th>\n",
              "      <th>Top100_roc90</th>\n",
              "      <th>Tweets_roc90</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2014-08-30</td>\n",
              "      <td>150886.0</td>\n",
              "      <td>186250.0</td>\n",
              "      <td>7.912</td>\n",
              "      <td>0.201</td>\n",
              "      <td>502.665</td>\n",
              "      <td>0.0752</td>\n",
              "      <td>4962.0</td>\n",
              "      <td>101469.0</td>\n",
              "      <td>7.377</td>\n",
              "      <td>2.040071e+17</td>\n",
              "      <td>6.587252e+09</td>\n",
              "      <td>0.0503</td>\n",
              "      <td>79.849</td>\n",
              "      <td>2.384467e+10</td>\n",
              "      <td>61032.0</td>\n",
              "      <td>11.234</td>\n",
              "      <td>2.326117e+08</td>\n",
              "      <td>19.946</td>\n",
              "      <td>18905.0</td>\n",
              "      <td>155328.666667</td>\n",
              "      <td>207422.000000</td>\n",
              "      <td>8.235667</td>\n",
              "      <td>0.228667</td>\n",
              "      <td>506.365000</td>\n",
              "      <td>0.076600</td>\n",
              "      <td>4901.666667</td>\n",
              "      <td>110979.000000</td>\n",
              "      <td>7.296333</td>\n",
              "      <td>2.055306e+17</td>\n",
              "      <td>6.624317e+09</td>\n",
              "      <td>0.050667</td>\n",
              "      <td>101.313333</td>\n",
              "      <td>2.384467e+10</td>\n",
              "      <td>65977.000000</td>\n",
              "      <td>10.804333</td>\n",
              "      <td>2.817489e+08</td>\n",
              "      <td>19.879667</td>\n",
              "      <td>20324.000000</td>\n",
              "      <td>164702.571429</td>\n",
              "      <td>...</td>\n",
              "      <td>2.114371</td>\n",
              "      <td>-19.508664</td>\n",
              "      <td>-2.992780</td>\n",
              "      <td>-29.809685</td>\n",
              "      <td>-15.388728</td>\n",
              "      <td>-31.164384</td>\n",
              "      <td>-12.569204</td>\n",
              "      <td>-19.139785</td>\n",
              "      <td>42.340792</td>\n",
              "      <td>-13.373572</td>\n",
              "      <td>-9.540159</td>\n",
              "      <td>34.502450</td>\n",
              "      <td>-11.983567</td>\n",
              "      <td>-12.521739</td>\n",
              "      <td>-22.974746</td>\n",
              "      <td>27.263600</td>\n",
              "      <td>-12.319163</td>\n",
              "      <td>-23.244056</td>\n",
              "      <td>23.159820</td>\n",
              "      <td>3.588678</td>\n",
              "      <td>-14.688628</td>\n",
              "      <td>-21.140408</td>\n",
              "      <td>-9.125063</td>\n",
              "      <td>-3.852230</td>\n",
              "      <td>-18.292683</td>\n",
              "      <td>-22.005409</td>\n",
              "      <td>-40.317460</td>\n",
              "      <td>-43.478756</td>\n",
              "      <td>-4.659488</td>\n",
              "      <td>-25.477321</td>\n",
              "      <td>128.784743</td>\n",
              "      <td>-19.517102</td>\n",
              "      <td>-21.894410</td>\n",
              "      <td>-47.650985</td>\n",
              "      <td>128.053828</td>\n",
              "      <td>10.872527</td>\n",
              "      <td>-64.560396</td>\n",
              "      <td>-47.014142</td>\n",
              "      <td>0.737374</td>\n",
              "      <td>47.132073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2014-08-31</td>\n",
              "      <td>177973.0</td>\n",
              "      <td>179521.0</td>\n",
              "      <td>7.660</td>\n",
              "      <td>0.189</td>\n",
              "      <td>488.836</td>\n",
              "      <td>0.0755</td>\n",
              "      <td>4343.0</td>\n",
              "      <td>102679.0</td>\n",
              "      <td>7.740</td>\n",
              "      <td>2.245391e+17</td>\n",
              "      <td>6.389446e+09</td>\n",
              "      <td>0.0489</td>\n",
              "      <td>93.017</td>\n",
              "      <td>2.390186e+10</td>\n",
              "      <td>57567.0</td>\n",
              "      <td>10.252</td>\n",
              "      <td>2.101746e+08</td>\n",
              "      <td>19.938</td>\n",
              "      <td>17226.0</td>\n",
              "      <td>157914.333333</td>\n",
              "      <td>192453.666667</td>\n",
              "      <td>8.014333</td>\n",
              "      <td>0.211667</td>\n",
              "      <td>499.380000</td>\n",
              "      <td>0.075967</td>\n",
              "      <td>4543.333333</td>\n",
              "      <td>104920.666667</td>\n",
              "      <td>7.417333</td>\n",
              "      <td>2.105118e+17</td>\n",
              "      <td>6.532561e+09</td>\n",
              "      <td>0.049967</td>\n",
              "      <td>96.025667</td>\n",
              "      <td>2.386373e+10</td>\n",
              "      <td>62347.666667</td>\n",
              "      <td>10.706667</td>\n",
              "      <td>2.398264e+08</td>\n",
              "      <td>19.911333</td>\n",
              "      <td>18656.333333</td>\n",
              "      <td>162913.571429</td>\n",
              "      <td>...</td>\n",
              "      <td>2.036847</td>\n",
              "      <td>-23.805732</td>\n",
              "      <td>16.226506</td>\n",
              "      <td>-26.185924</td>\n",
              "      <td>-16.484954</td>\n",
              "      <td>-30.514706</td>\n",
              "      <td>-17.296009</td>\n",
              "      <td>-18.991416</td>\n",
              "      <td>8.466533</td>\n",
              "      <td>-12.470590</td>\n",
              "      <td>4.920699</td>\n",
              "      <td>55.753823</td>\n",
              "      <td>-16.770244</td>\n",
              "      <td>-17.258883</td>\n",
              "      <td>-19.567473</td>\n",
              "      <td>27.568839</td>\n",
              "      <td>-15.040290</td>\n",
              "      <td>-36.465047</td>\n",
              "      <td>-7.094303</td>\n",
              "      <td>3.113364</td>\n",
              "      <td>-20.194580</td>\n",
              "      <td>1.440329</td>\n",
              "      <td>-10.034829</td>\n",
              "      <td>2.667203</td>\n",
              "      <td>-22.222222</td>\n",
              "      <td>-23.122313</td>\n",
              "      <td>-34.913793</td>\n",
              "      <td>-53.104416</td>\n",
              "      <td>-12.412352</td>\n",
              "      <td>-13.257873</td>\n",
              "      <td>143.057467</td>\n",
              "      <td>-20.689616</td>\n",
              "      <td>-23.113208</td>\n",
              "      <td>-37.208800</td>\n",
              "      <td>128.600810</td>\n",
              "      <td>-10.389004</td>\n",
              "      <td>-69.204890</td>\n",
              "      <td>-63.327065</td>\n",
              "      <td>0.732582</td>\n",
              "      <td>4.863944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2014-09-01</td>\n",
              "      <td>169807.0</td>\n",
              "      <td>296280.0</td>\n",
              "      <td>10.588</td>\n",
              "      <td>0.321</td>\n",
              "      <td>479.421</td>\n",
              "      <td>0.0745</td>\n",
              "      <td>4833.0</td>\n",
              "      <td>117614.0</td>\n",
              "      <td>9.070</td>\n",
              "      <td>2.217182e+17</td>\n",
              "      <td>6.309097e+09</td>\n",
              "      <td>0.0479</td>\n",
              "      <td>97.231</td>\n",
              "      <td>2.742863e+10</td>\n",
              "      <td>70247.0</td>\n",
              "      <td>7.375</td>\n",
              "      <td>2.772926e+08</td>\n",
              "      <td>19.890</td>\n",
              "      <td>18661.0</td>\n",
              "      <td>166222.000000</td>\n",
              "      <td>220683.666667</td>\n",
              "      <td>8.720000</td>\n",
              "      <td>0.237000</td>\n",
              "      <td>490.307333</td>\n",
              "      <td>0.075067</td>\n",
              "      <td>4712.666667</td>\n",
              "      <td>107254.000000</td>\n",
              "      <td>8.062333</td>\n",
              "      <td>2.167548e+17</td>\n",
              "      <td>6.428598e+09</td>\n",
              "      <td>0.049033</td>\n",
              "      <td>90.032333</td>\n",
              "      <td>2.505839e+10</td>\n",
              "      <td>62948.666667</td>\n",
              "      <td>9.620333</td>\n",
              "      <td>2.400263e+08</td>\n",
              "      <td>19.924667</td>\n",
              "      <td>18264.000000</td>\n",
              "      <td>164547.714286</td>\n",
              "      <td>...</td>\n",
              "      <td>1.707916</td>\n",
              "      <td>-14.661362</td>\n",
              "      <td>32.607847</td>\n",
              "      <td>55.838418</td>\n",
              "      <td>19.855105</td>\n",
              "      <td>54.326923</td>\n",
              "      <td>-18.800007</td>\n",
              "      <td>-18.311404</td>\n",
              "      <td>17.849305</td>\n",
              "      <td>25.836133</td>\n",
              "      <td>38.876129</td>\n",
              "      <td>47.890157</td>\n",
              "      <td>-17.237931</td>\n",
              "      <td>-18.813559</td>\n",
              "      <td>1.814698</td>\n",
              "      <td>46.391890</td>\n",
              "      <td>28.127166</td>\n",
              "      <td>-54.141276</td>\n",
              "      <td>53.263001</td>\n",
              "      <td>2.721686</td>\n",
              "      <td>0.701527</td>\n",
              "      <td>-9.464755</td>\n",
              "      <td>26.216239</td>\n",
              "      <td>30.877627</td>\n",
              "      <td>6.291391</td>\n",
              "      <td>-27.843900</td>\n",
              "      <td>-41.796875</td>\n",
              "      <td>-43.750000</td>\n",
              "      <td>-8.698251</td>\n",
              "      <td>-16.597701</td>\n",
              "      <td>124.097209</td>\n",
              "      <td>-24.474173</td>\n",
              "      <td>-27.861446</td>\n",
              "      <td>-40.218882</td>\n",
              "      <td>162.331342</td>\n",
              "      <td>1.050103</td>\n",
              "      <td>-75.395343</td>\n",
              "      <td>-48.746650</td>\n",
              "      <td>0.368371</td>\n",
              "      <td>4.496584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2014-09-02</td>\n",
              "      <td>163408.0</td>\n",
              "      <td>282380.0</td>\n",
              "      <td>9.796</td>\n",
              "      <td>0.315</td>\n",
              "      <td>477.176</td>\n",
              "      <td>0.0771</td>\n",
              "      <td>3921.0</td>\n",
              "      <td>123859.0</td>\n",
              "      <td>7.982</td>\n",
              "      <td>1.951898e+17</td>\n",
              "      <td>6.273471e+09</td>\n",
              "      <td>0.0477</td>\n",
              "      <td>102.216</td>\n",
              "      <td>2.742863e+10</td>\n",
              "      <td>71762.0</td>\n",
              "      <td>9.013</td>\n",
              "      <td>2.192583e+08</td>\n",
              "      <td>19.901</td>\n",
              "      <td>22901.0</td>\n",
              "      <td>170396.000000</td>\n",
              "      <td>252727.000000</td>\n",
              "      <td>9.348000</td>\n",
              "      <td>0.275000</td>\n",
              "      <td>481.811000</td>\n",
              "      <td>0.075700</td>\n",
              "      <td>4365.666667</td>\n",
              "      <td>114717.333333</td>\n",
              "      <td>8.264000</td>\n",
              "      <td>2.138157e+17</td>\n",
              "      <td>6.324004e+09</td>\n",
              "      <td>0.048167</td>\n",
              "      <td>97.488000</td>\n",
              "      <td>2.625304e+10</td>\n",
              "      <td>66525.333333</td>\n",
              "      <td>8.880000</td>\n",
              "      <td>2.355752e+08</td>\n",
              "      <td>19.909667</td>\n",
              "      <td>19596.000000</td>\n",
              "      <td>163692.857143</td>\n",
              "      <td>...</td>\n",
              "      <td>1.629047</td>\n",
              "      <td>-4.998755</td>\n",
              "      <td>3.933903</td>\n",
              "      <td>42.326477</td>\n",
              "      <td>4.758849</td>\n",
              "      <td>41.891892</td>\n",
              "      <td>-18.196530</td>\n",
              "      <td>-17.363344</td>\n",
              "      <td>-15.166595</td>\n",
              "      <td>42.340489</td>\n",
              "      <td>15.798636</td>\n",
              "      <td>27.465039</td>\n",
              "      <td>-17.154408</td>\n",
              "      <td>-18.181818</td>\n",
              "      <td>36.289817</td>\n",
              "      <td>46.391890</td>\n",
              "      <td>34.217368</td>\n",
              "      <td>-38.678732</td>\n",
              "      <td>3.650241</td>\n",
              "      <td>2.778495</td>\n",
              "      <td>33.246058</td>\n",
              "      <td>-4.765013</td>\n",
              "      <td>38.185164</td>\n",
              "      <td>23.127200</td>\n",
              "      <td>18.867925</td>\n",
              "      <td>-25.618255</td>\n",
              "      <td>-35.210084</td>\n",
              "      <td>-60.813512</td>\n",
              "      <td>4.779670</td>\n",
              "      <td>-13.257987</td>\n",
              "      <td>103.067515</td>\n",
              "      <td>-22.479649</td>\n",
              "      <td>-25.700935</td>\n",
              "      <td>-34.528964</td>\n",
              "      <td>162.331342</td>\n",
              "      <td>10.619210</td>\n",
              "      <td>-70.235461</td>\n",
              "      <td>-64.785807</td>\n",
              "      <td>0.206445</td>\n",
              "      <td>27.157135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2014-09-03</td>\n",
              "      <td>153695.0</td>\n",
              "      <td>238464.0</td>\n",
              "      <td>8.521</td>\n",
              "      <td>0.263</td>\n",
              "      <td>475.398</td>\n",
              "      <td>0.0738</td>\n",
              "      <td>3284.0</td>\n",
              "      <td>117748.0</td>\n",
              "      <td>8.103</td>\n",
              "      <td>2.156571e+17</td>\n",
              "      <td>6.243903e+09</td>\n",
              "      <td>0.0475</td>\n",
              "      <td>106.199</td>\n",
              "      <td>2.742863e+10</td>\n",
              "      <td>71699.0</td>\n",
              "      <td>9.338</td>\n",
              "      <td>1.809135e+08</td>\n",
              "      <td>19.906</td>\n",
              "      <td>22113.0</td>\n",
              "      <td>162303.333333</td>\n",
              "      <td>272374.666667</td>\n",
              "      <td>9.635000</td>\n",
              "      <td>0.299667</td>\n",
              "      <td>477.331667</td>\n",
              "      <td>0.075133</td>\n",
              "      <td>4012.666667</td>\n",
              "      <td>119740.333333</td>\n",
              "      <td>8.385000</td>\n",
              "      <td>2.108550e+17</td>\n",
              "      <td>6.275490e+09</td>\n",
              "      <td>0.047700</td>\n",
              "      <td>101.882000</td>\n",
              "      <td>2.742863e+10</td>\n",
              "      <td>71236.000000</td>\n",
              "      <td>8.575333</td>\n",
              "      <td>2.258215e+08</td>\n",
              "      <td>19.899000</td>\n",
              "      <td>21225.000000</td>\n",
              "      <td>161552.714286</td>\n",
              "      <td>...</td>\n",
              "      <td>1.628631</td>\n",
              "      <td>-17.868816</td>\n",
              "      <td>2.319404</td>\n",
              "      <td>-20.834730</td>\n",
              "      <td>-24.257778</td>\n",
              "      <td>-23.988439</td>\n",
              "      <td>-18.867139</td>\n",
              "      <td>-23.125000</td>\n",
              "      <td>-10.224166</td>\n",
              "      <td>15.689877</td>\n",
              "      <td>15.526091</td>\n",
              "      <td>66.073439</td>\n",
              "      <td>-17.996231</td>\n",
              "      <td>-18.941980</td>\n",
              "      <td>-4.144741</td>\n",
              "      <td>46.391890</td>\n",
              "      <td>5.877228</td>\n",
              "      <td>-35.551108</td>\n",
              "      <td>-3.357904</td>\n",
              "      <td>2.767166</td>\n",
              "      <td>7.297783</td>\n",
              "      <td>-16.453309</td>\n",
              "      <td>-9.286163</td>\n",
              "      <td>-10.059109</td>\n",
              "      <td>-19.076923</td>\n",
              "      <td>-27.004692</td>\n",
              "      <td>-38.500000</td>\n",
              "      <td>-64.277167</td>\n",
              "      <td>-4.311150</td>\n",
              "      <td>-11.943056</td>\n",
              "      <td>158.536761</td>\n",
              "      <td>-24.464584</td>\n",
              "      <td>-27.035330</td>\n",
              "      <td>-21.865389</td>\n",
              "      <td>159.149596</td>\n",
              "      <td>7.345081</td>\n",
              "      <td>-68.628637</td>\n",
              "      <td>-68.864609</td>\n",
              "      <td>0.110642</td>\n",
              "      <td>18.207088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2428</th>\n",
              "      <td>2021-04-23</td>\n",
              "      <td>931624.0</td>\n",
              "      <td>893218.0</td>\n",
              "      <td>10.667</td>\n",
              "      <td>24.229</td>\n",
              "      <td>49891.000</td>\n",
              "      <td>45.5180</td>\n",
              "      <td>368625.0</td>\n",
              "      <td>498055.0</td>\n",
              "      <td>278.568</td>\n",
              "      <td>1.640463e+20</td>\n",
              "      <td>9.324378e+11</td>\n",
              "      <td>20.5970</td>\n",
              "      <td>1123.000</td>\n",
              "      <td>2.358198e+13</td>\n",
              "      <td>295596.0</td>\n",
              "      <td>0.339</td>\n",
              "      <td>5.959777e+10</td>\n",
              "      <td>14.601</td>\n",
              "      <td>132963.0</td>\n",
              "      <td>900267.333333</td>\n",
              "      <td>892279.333333</td>\n",
              "      <td>11.664667</td>\n",
              "      <td>26.060333</td>\n",
              "      <td>53051.666667</td>\n",
              "      <td>55.671667</td>\n",
              "      <td>423817.666667</td>\n",
              "      <td>486720.666667</td>\n",
              "      <td>201.405667</td>\n",
              "      <td>1.463713e+20</td>\n",
              "      <td>9.914630e+11</td>\n",
              "      <td>24.452667</td>\n",
              "      <td>1220.000000</td>\n",
              "      <td>2.358198e+13</td>\n",
              "      <td>265327.666667</td>\n",
              "      <td>0.386667</td>\n",
              "      <td>6.232623e+10</td>\n",
              "      <td>14.552000</td>\n",
              "      <td>112924.000000</td>\n",
              "      <td>877790.142857</td>\n",
              "      <td>...</td>\n",
              "      <td>1.940934</td>\n",
              "      <td>29.545587</td>\n",
              "      <td>-5.932387</td>\n",
              "      <td>1.443715</td>\n",
              "      <td>12.592358</td>\n",
              "      <td>203.241552</td>\n",
              "      <td>-9.524328</td>\n",
              "      <td>205.285044</td>\n",
              "      <td>24.199798</td>\n",
              "      <td>-9.789804</td>\n",
              "      <td>75.441646</td>\n",
              "      <td>-4.647722</td>\n",
              "      <td>-9.397016</td>\n",
              "      <td>173.423603</td>\n",
              "      <td>14.518568</td>\n",
              "      <td>7.849895</td>\n",
              "      <td>-3.064527</td>\n",
              "      <td>2.416918</td>\n",
              "      <td>77.508809</td>\n",
              "      <td>1.649958</td>\n",
              "      <td>2.109572</td>\n",
              "      <td>0.658760</td>\n",
              "      <td>0.972628</td>\n",
              "      <td>15.556278</td>\n",
              "      <td>231.813202</td>\n",
              "      <td>54.241637</td>\n",
              "      <td>464.879623</td>\n",
              "      <td>114.726339</td>\n",
              "      <td>-6.273170</td>\n",
              "      <td>97.214887</td>\n",
              "      <td>15.746085</td>\n",
              "      <td>54.912521</td>\n",
              "      <td>398.354706</td>\n",
              "      <td>103.923392</td>\n",
              "      <td>13.723436</td>\n",
              "      <td>-4.064650</td>\n",
              "      <td>41.250000</td>\n",
              "      <td>419.113979</td>\n",
              "      <td>7.796235</td>\n",
              "      <td>151.814325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2429</th>\n",
              "      <td>2021-04-24</td>\n",
              "      <td>911731.0</td>\n",
              "      <td>880543.0</td>\n",
              "      <td>9.931</td>\n",
              "      <td>15.677</td>\n",
              "      <td>50077.000</td>\n",
              "      <td>29.0250</td>\n",
              "      <td>275845.0</td>\n",
              "      <td>406399.0</td>\n",
              "      <td>278.568</td>\n",
              "      <td>1.625112e+20</td>\n",
              "      <td>9.359612e+11</td>\n",
              "      <td>12.5620</td>\n",
              "      <td>654.858</td>\n",
              "      <td>2.358198e+13</td>\n",
              "      <td>290545.0</td>\n",
              "      <td>0.331</td>\n",
              "      <td>3.100297e+10</td>\n",
              "      <td>14.611</td>\n",
              "      <td>86049.0</td>\n",
              "      <td>941650.333333</td>\n",
              "      <td>887144.333333</td>\n",
              "      <td>10.222667</td>\n",
              "      <td>21.671333</td>\n",
              "      <td>51260.000000</td>\n",
              "      <td>44.420333</td>\n",
              "      <td>357857.666667</td>\n",
              "      <td>466420.333333</td>\n",
              "      <td>243.256333</td>\n",
              "      <td>1.564829e+20</td>\n",
              "      <td>9.580243e+11</td>\n",
              "      <td>19.176000</td>\n",
              "      <td>1010.286000</td>\n",
              "      <td>2.358198e+13</td>\n",
              "      <td>286869.333333</td>\n",
              "      <td>0.373000</td>\n",
              "      <td>5.053616e+10</td>\n",
              "      <td>14.581667</td>\n",
              "      <td>109324.333333</td>\n",
              "      <td>897570.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.754997</td>\n",
              "      <td>-22.372777</td>\n",
              "      <td>-15.857976</td>\n",
              "      <td>-1.385353</td>\n",
              "      <td>6.202545</td>\n",
              "      <td>94.745342</td>\n",
              "      <td>-3.932703</td>\n",
              "      <td>110.493872</td>\n",
              "      <td>-1.680217</td>\n",
              "      <td>-32.402929</td>\n",
              "      <td>61.742796</td>\n",
              "      <td>-1.261883</td>\n",
              "      <td>-3.798708</td>\n",
              "      <td>103.334412</td>\n",
              "      <td>-28.754888</td>\n",
              "      <td>7.849895</td>\n",
              "      <td>-8.750722</td>\n",
              "      <td>-0.301205</td>\n",
              "      <td>-26.856250</td>\n",
              "      <td>1.968037</td>\n",
              "      <td>-37.617625</td>\n",
              "      <td>5.892840</td>\n",
              "      <td>-1.710961</td>\n",
              "      <td>-4.140927</td>\n",
              "      <td>152.163423</td>\n",
              "      <td>55.325682</td>\n",
              "      <td>297.058824</td>\n",
              "      <td>43.366856</td>\n",
              "      <td>-9.949457</td>\n",
              "      <td>114.088750</td>\n",
              "      <td>0.209870</td>\n",
              "      <td>56.000440</td>\n",
              "      <td>289.640199</td>\n",
              "      <td>18.906621</td>\n",
              "      <td>13.246794</td>\n",
              "      <td>14.463503</td>\n",
              "      <td>79.891304</td>\n",
              "      <td>260.697995</td>\n",
              "      <td>7.655467</td>\n",
              "      <td>68.779789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2430</th>\n",
              "      <td>2021-04-25</td>\n",
              "      <td>849115.0</td>\n",
              "      <td>884132.0</td>\n",
              "      <td>10.213</td>\n",
              "      <td>11.663</td>\n",
              "      <td>49657.000</td>\n",
              "      <td>23.9710</td>\n",
              "      <td>337302.0</td>\n",
              "      <td>327497.0</td>\n",
              "      <td>278.568</td>\n",
              "      <td>1.711981e+20</td>\n",
              "      <td>9.281555e+11</td>\n",
              "      <td>9.0170</td>\n",
              "      <td>649.973</td>\n",
              "      <td>2.358198e+13</td>\n",
              "      <td>240900.0</td>\n",
              "      <td>0.289</td>\n",
              "      <td>2.894934e+10</td>\n",
              "      <td>14.694</td>\n",
              "      <td>81258.0</td>\n",
              "      <td>897490.000000</td>\n",
              "      <td>885964.333333</td>\n",
              "      <td>10.270333</td>\n",
              "      <td>17.189667</td>\n",
              "      <td>49875.000000</td>\n",
              "      <td>32.838000</td>\n",
              "      <td>327257.333333</td>\n",
              "      <td>410650.333333</td>\n",
              "      <td>278.568000</td>\n",
              "      <td>1.659185e+20</td>\n",
              "      <td>9.321848e+11</td>\n",
              "      <td>14.058667</td>\n",
              "      <td>809.277000</td>\n",
              "      <td>2.358198e+13</td>\n",
              "      <td>275680.333333</td>\n",
              "      <td>0.319667</td>\n",
              "      <td>3.985003e+10</td>\n",
              "      <td>14.635333</td>\n",
              "      <td>100090.000000</td>\n",
              "      <td>882545.571429</td>\n",
              "      <td>...</td>\n",
              "      <td>2.454330</td>\n",
              "      <td>-14.206981</td>\n",
              "      <td>-4.780621</td>\n",
              "      <td>1.330625</td>\n",
              "      <td>-3.541745</td>\n",
              "      <td>33.627406</td>\n",
              "      <td>-6.673808</td>\n",
              "      <td>62.746962</td>\n",
              "      <td>27.856474</td>\n",
              "      <td>-41.164540</td>\n",
              "      <td>96.025558</td>\n",
              "      <td>12.250949</td>\n",
              "      <td>-6.542228</td>\n",
              "      <td>21.637664</td>\n",
              "      <td>-36.773054</td>\n",
              "      <td>7.849895</td>\n",
              "      <td>-17.914364</td>\n",
              "      <td>-11.076923</td>\n",
              "      <td>-9.258258</td>\n",
              "      <td>2.884750</td>\n",
              "      <td>-40.947079</td>\n",
              "      <td>-6.058152</td>\n",
              "      <td>1.466220</td>\n",
              "      <td>-0.709703</td>\n",
              "      <td>12.969779</td>\n",
              "      <td>48.869769</td>\n",
              "      <td>120.524379</td>\n",
              "      <td>35.094802</td>\n",
              "      <td>-40.605844</td>\n",
              "      <td>109.926299</td>\n",
              "      <td>19.641494</td>\n",
              "      <td>49.519322</td>\n",
              "      <td>67.167223</td>\n",
              "      <td>-22.058485</td>\n",
              "      <td>13.246794</td>\n",
              "      <td>-22.029246</td>\n",
              "      <td>27.312775</td>\n",
              "      <td>84.727332</td>\n",
              "      <td>7.909231</td>\n",
              "      <td>27.431547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2431</th>\n",
              "      <td>2021-04-26</td>\n",
              "      <td>791356.0</td>\n",
              "      <td>893442.0</td>\n",
              "      <td>12.632</td>\n",
              "      <td>19.065</td>\n",
              "      <td>53000.000</td>\n",
              "      <td>36.9570</td>\n",
              "      <td>474291.0</td>\n",
              "      <td>365921.0</td>\n",
              "      <td>278.568</td>\n",
              "      <td>1.582209e+20</td>\n",
              "      <td>9.906756e+11</td>\n",
              "      <td>17.2670</td>\n",
              "      <td>1230.000</td>\n",
              "      <td>2.358198e+13</td>\n",
              "      <td>240573.0</td>\n",
              "      <td>0.295</td>\n",
              "      <td>5.094032e+10</td>\n",
              "      <td>14.685</td>\n",
              "      <td>110552.0</td>\n",
              "      <td>850734.000000</td>\n",
              "      <td>886039.000000</td>\n",
              "      <td>10.925333</td>\n",
              "      <td>15.468333</td>\n",
              "      <td>50911.333333</td>\n",
              "      <td>29.984333</td>\n",
              "      <td>362479.333333</td>\n",
              "      <td>366605.666667</td>\n",
              "      <td>278.568000</td>\n",
              "      <td>1.639767e+20</td>\n",
              "      <td>9.515974e+11</td>\n",
              "      <td>12.948667</td>\n",
              "      <td>844.943667</td>\n",
              "      <td>2.358198e+13</td>\n",
              "      <td>257339.333333</td>\n",
              "      <td>0.305000</td>\n",
              "      <td>3.696421e+10</td>\n",
              "      <td>14.663333</td>\n",
              "      <td>92619.666667</td>\n",
              "      <td>888309.285714</td>\n",
              "      <td>...</td>\n",
              "      <td>2.064220</td>\n",
              "      <td>-16.360005</td>\n",
              "      <td>-20.957591</td>\n",
              "      <td>-0.618906</td>\n",
              "      <td>39.472231</td>\n",
              "      <td>203.244791</td>\n",
              "      <td>-3.942003</td>\n",
              "      <td>189.608965</td>\n",
              "      <td>100.810795</td>\n",
              "      <td>-25.632161</td>\n",
              "      <td>116.573761</td>\n",
              "      <td>-5.407786</td>\n",
              "      <td>-3.809688</td>\n",
              "      <td>219.404366</td>\n",
              "      <td>58.631692</td>\n",
              "      <td>7.849895</td>\n",
              "      <td>-16.496703</td>\n",
              "      <td>-15.714286</td>\n",
              "      <td>150.612643</td>\n",
              "      <td>2.792944</td>\n",
              "      <td>4.067551</td>\n",
              "      <td>-22.505122</td>\n",
              "      <td>1.742663</td>\n",
              "      <td>41.234347</td>\n",
              "      <td>153.591381</td>\n",
              "      <td>65.361455</td>\n",
              "      <td>374.964657</td>\n",
              "      <td>192.654799</td>\n",
              "      <td>-34.890081</td>\n",
              "      <td>117.036096</td>\n",
              "      <td>-1.846424</td>\n",
              "      <td>66.077560</td>\n",
              "      <td>316.373282</td>\n",
              "      <td>90.204617</td>\n",
              "      <td>13.246794</td>\n",
              "      <td>-28.565447</td>\n",
              "      <td>36.574074</td>\n",
              "      <td>217.097924</td>\n",
              "      <td>7.511531</td>\n",
              "      <td>98.022498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2432</th>\n",
              "      <td>2021-04-27</td>\n",
              "      <td>969613.0</td>\n",
              "      <td>883378.0</td>\n",
              "      <td>10.141</td>\n",
              "      <td>15.467</td>\n",
              "      <td>54583.000</td>\n",
              "      <td>29.7320</td>\n",
              "      <td>419084.0</td>\n",
              "      <td>314676.0</td>\n",
              "      <td>278.568</td>\n",
              "      <td>1.527917e+20</td>\n",
              "      <td>1.020304e+12</td>\n",
              "      <td>14.2190</td>\n",
              "      <td>971.659</td>\n",
              "      <td>2.358198e+13</td>\n",
              "      <td>297967.0</td>\n",
              "      <td>0.375</td>\n",
              "      <td>6.684934e+10</td>\n",
              "      <td>14.719</td>\n",
              "      <td>102971.0</td>\n",
              "      <td>870028.000000</td>\n",
              "      <td>886984.000000</td>\n",
              "      <td>10.995333</td>\n",
              "      <td>15.398333</td>\n",
              "      <td>52413.333333</td>\n",
              "      <td>30.220000</td>\n",
              "      <td>410225.666667</td>\n",
              "      <td>336031.333333</td>\n",
              "      <td>278.568000</td>\n",
              "      <td>1.607369e+20</td>\n",
              "      <td>9.797116e+11</td>\n",
              "      <td>13.501000</td>\n",
              "      <td>950.544000</td>\n",
              "      <td>2.358198e+13</td>\n",
              "      <td>259813.333333</td>\n",
              "      <td>0.319667</td>\n",
              "      <td>4.891300e+10</td>\n",
              "      <td>14.699333</td>\n",
              "      <td>98260.333333</td>\n",
              "      <td>888945.285714</td>\n",
              "      <td>...</td>\n",
              "      <td>2.144344</td>\n",
              "      <td>-39.821049</td>\n",
              "      <td>4.784738</td>\n",
              "      <td>0.769996</td>\n",
              "      <td>11.968643</td>\n",
              "      <td>180.300834</td>\n",
              "      <td>-2.242321</td>\n",
              "      <td>128.093594</td>\n",
              "      <td>102.499082</td>\n",
              "      <td>-24.772292</td>\n",
              "      <td>124.582789</td>\n",
              "      <td>-11.757279</td>\n",
              "      <td>-2.108785</td>\n",
              "      <td>170.992948</td>\n",
              "      <td>28.573602</td>\n",
              "      <td>7.849895</td>\n",
              "      <td>19.932299</td>\n",
              "      <td>10.619469</td>\n",
              "      <td>270.958685</td>\n",
              "      <td>3.067012</td>\n",
              "      <td>19.233219</td>\n",
              "      <td>8.328902</td>\n",
              "      <td>-2.129409</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>80.583771</td>\n",
              "      <td>75.097039</td>\n",
              "      <td>271.232364</td>\n",
              "      <td>117.998148</td>\n",
              "      <td>-44.769654</td>\n",
              "      <td>101.383678</td>\n",
              "      <td>3.876802</td>\n",
              "      <td>75.856061</td>\n",
              "      <td>226.123853</td>\n",
              "      <td>32.818141</td>\n",
              "      <td>13.246794</td>\n",
              "      <td>-7.867389</td>\n",
              "      <td>82.038835</td>\n",
              "      <td>224.862804</td>\n",
              "      <td>8.212028</td>\n",
              "      <td>69.773462</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2433 rows × 780 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           Dates  #Active_Adresses  ...  Top100_roc90  Tweets_roc90\n",
              "0     2014-08-30          150886.0  ...      0.737374     47.132073\n",
              "1     2014-08-31          177973.0  ...      0.732582      4.863944\n",
              "2     2014-09-01          169807.0  ...      0.368371      4.496584\n",
              "3     2014-09-02          163408.0  ...      0.206445     27.157135\n",
              "4     2014-09-03          153695.0  ...      0.110642     18.207088\n",
              "...          ...               ...  ...           ...           ...\n",
              "2428  2021-04-23          931624.0  ...      7.796235    151.814325\n",
              "2429  2021-04-24          911731.0  ...      7.655467     68.779789\n",
              "2430  2021-04-25          849115.0  ...      7.909231     27.431547\n",
              "2431  2021-04-26          791356.0  ...      7.511531     98.022498\n",
              "2432  2021-04-27          969613.0  ...      8.212028     69.773462\n",
              "\n",
              "[2433 rows x 780 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix2qXtbTIyf2"
      },
      "source": [
        "# Analyse des indicateurs obtenus par RF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYl8AbWsNvpX",
        "outputId": "b51ec16d-2acf-469a-b64a-c1fdd3c3b0a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!rm *.zip\n",
        "!wget --no-check-certificate --content-disposition \"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/Bitcoin/Bitcoin_X_one.zip?raw=true\"\n",
        "!wget --no-check-certificate --content-disposition \"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/Bitcoin/Bitcoin_Y_one.zip?raw=true\"\n",
        "!unzip \"Bitcoin_X_one.zip\"\n",
        "!unzip \"Bitcoin_Y_one.zip\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-09 08:52:03--  https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/Bitcoin/Bitcoin_X_one.zip?raw=true\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/AlexandreBourrieau/ML/raw/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/Bitcoin/Bitcoin_X_one.zip [following]\n",
            "--2021-05-09 08:52:04--  https://github.com/AlexandreBourrieau/ML/raw/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/Bitcoin/Bitcoin_X_one.zip\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/AlexandreBourrieau/ML/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/Bitcoin/Bitcoin_X_one.zip [following]\n",
            "--2021-05-09 08:52:04--  https://raw.githubusercontent.com/AlexandreBourrieau/ML/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/Bitcoin/Bitcoin_X_one.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16003526 (15M) [application/zip]\n",
            "Saving to: ‘Bitcoin_X_one.zip’\n",
            "\n",
            "Bitcoin_X_one.zip   100%[===================>]  15.26M  64.8MB/s    in 0.2s    \n",
            "\n",
            "2021-05-09 08:52:04 (64.8 MB/s) - ‘Bitcoin_X_one.zip’ saved [16003526/16003526]\n",
            "\n",
            "--2021-05-09 08:52:04--  https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/Bitcoin/Bitcoin_Y_one.zip?raw=true\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/AlexandreBourrieau/ML/raw/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/Bitcoin/Bitcoin_Y_one.zip [following]\n",
            "--2021-05-09 08:52:05--  https://github.com/AlexandreBourrieau/ML/raw/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/Bitcoin/Bitcoin_Y_one.zip\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/AlexandreBourrieau/ML/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/Bitcoin/Bitcoin_Y_one.zip [following]\n",
            "--2021-05-09 08:52:05--  https://raw.githubusercontent.com/AlexandreBourrieau/ML/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/Bitcoin/Bitcoin_Y_one.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13004 (13K) [application/zip]\n",
            "Saving to: ‘Bitcoin_Y_one.zip’\n",
            "\n",
            "Bitcoin_Y_one.zip   100%[===================>]  12.70K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-05-09 08:52:05 (79.3 MB/s) - ‘Bitcoin_Y_one.zip’ saved [13004/13004]\n",
            "\n",
            "Archive:  Bitcoin_X_one.zip\n",
            "  inflating: Bitcoin_X_one.csv       \n",
            "Archive:  Bitcoin_Y_one.zip\n",
            "  inflating: Bitcoin_Y_one.csv       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xK_Uv9XOJsN"
      },
      "source": [
        "df_X_one = pd.read_csv(\"Bitcoin_X_one.csv\").iloc[:,1:]\n",
        "df_Y_one = pd.read_csv(\"Bitcoin_Y_one.csv\")['Price']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuzTUfEfQ8RU"
      },
      "source": [
        "**1. Cross-Validation sur les variables retenues**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bEv2NxbJJ4Z"
      },
      "source": [
        "Comparons les résultats obtenus par régression avec les variables retenues  :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lKZd_JfJLZ1"
      },
      "source": [
        "var_thres = [593,671,386,82,367,485,10,101,63,196,177,612,599,618,308,656,124,314,476,637,675,67,631,175,333,493,143,162,48,365,371,61,29,118,194,139,669,99,352,95,346,156,200,190,181,80,158,384,105,650,137,42,327,86,640,206,683,390,157,329,487,348,163,480,388,347,23,396,474,621,486,189,372,103,176,199,366,203,391,104,182,202,398,385,107,111,392,670,198,379,85,100,481,676,674,491,389,195,208,113,652,106,201,108,129,146,184,355,62,479,70,659,68,127,393,336,165,678,374,89,51,32,317,370,47,186,81,680,395,188,205,397,110,378,633,44,180,94,87,93,25,120,558,96,64,169,682,359,187,681,602,13,207,515,319,677,445,777,302,310,662,623,604,570,167,376,74,433,150,183,148,340,33,131,338,191,17,66,138,470,625,642,98,112,673,168,357,383,360,575,569,321,34,387,606,170,350,380,36,465,550,356,484,455,19,52,456,587,614,473,84,567,477,471,581,102,577,91,160,358,459,644,298,540,586,351,739,53,533,55,353,313,568,583,76,193,15,464,451,97,564,65,663,542,457,133,38,539,349,79,192,54,469,665,178,483,536,166,381,490,664,83,544,444,56,655,415,161,472,45,559,654,588,440,467,109,369,563,50,72,142,454,362,149,595,448,197,323,171,334,566,627,628,332,492,574,179,773,557,151,585,185,377,560,580,571,672,518,78,330,75,608,579,463,489,534,427,43,555,421,152,219,425,49,58,270,572,661,213,295,164,402,638,561,382,690,26,668,222,141,276,289,514,541,565,342,394,418,204,646,46,636,361,60,368,92,666,232,144]\n",
        "var_interp = [593,671,386,82,367,485,10,101,63,196,177,612,599,618,308,656,124,314,476,637,675,67,631,175,333,493,143,162,48,365,371,61,29,118,194,139,669,99,352,95,346,156,200,190,181,80,158,384,105,650,137,42,327,86,640,206,683,390,157,329,487,348,163,480,388,347,23,396,474,621,486,189,372,103,176,199,366,203,391,104,182,202,398,385,107,111,392,670,198,379,85,100,481,676,674,491,389,195,208,113,652,106,201,108,129,146,184,355,62,479,70,659,68,127,393,336,165,678,374,89,51,32,317,370,47,186,81,680,395,188,205,397,110,378,633,44,180,94,87,93,25,120,558,96,64,169,682,359,187,681,602,13,207,515,319,677,445,777,302,310,662,623,604,570,167,376,74,433,150,183,148,340,33,131,338,191,17,66,138,470,625,642,98,112,673,168,357,383,360,575,569,321,34,387,606,170,350,380,36,465,550,356,484,455,19,52,456,587,614,473]\n",
        "var_pred = [593,671,386,101,675,67,175,493,194,669,99,640,683,390,621,486,372,89,51,32,473]\n",
        "\n",
        "var_thres = np.subtract(var_thres,1).tolist()\n",
        "var_interp = np.subtract(var_interp,1).tolist()\n",
        "var_pred = np.subtract(var_pred,1).tolist()\n",
        "\n",
        "df_all = df_X_one\n",
        "df_thres = df_X_one.iloc[:,var_thres]\n",
        "df_interp = df_X_one.iloc[:,var_interp]\n",
        "df_pred = df_X_one.iloc[:,var_pred]\n",
        "\n",
        "#df_ = [df_all, df_thres, df_interp,df_pred]\n",
        "#df_ = [df_thres, df_interp,df_pred]\n",
        "#df_ = [df_interp,df_pred]\n",
        "df_ = [df_pred]\n"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydye47_nPcpl",
        "outputId": "a53129b6-9800-4975-c01a-2c5b16bbc045",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        }
      },
      "source": [
        "df_pred"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Price_trix3</th>\n",
              "      <th>Transaction Value_trix90</th>\n",
              "      <th>Transaction Value_wma90</th>\n",
              "      <th>Transaction Value_sma90</th>\n",
              "      <th>Capitalization_trix90</th>\n",
              "      <th>Capitalization_sma14</th>\n",
              "      <th>Price_ema30</th>\n",
              "      <th>Tweets_std90</th>\n",
              "      <th>Price_ema90</th>\n",
              "      <th>Price_trix90</th>\n",
              "      <th>Price_sma90</th>\n",
              "      <th>Difficulty_trix14</th>\n",
              "      <th>Tweets_trix90</th>\n",
              "      <th>Capitalization_wma90</th>\n",
              "      <th>Difficulty_trix7</th>\n",
              "      <th>Median Transaction Fee_std90</th>\n",
              "      <th>Median Transaction Fee_wma30</th>\n",
              "      <th>Difficulty_sma30</th>\n",
              "      <th>Difficulty_sma7</th>\n",
              "      <th>Difficulty_sma3</th>\n",
              "      <th>Top100_std30</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>502.896486</td>\n",
              "      <td>5627.621788</td>\n",
              "      <td>5405.723810</td>\n",
              "      <td>5631.100000</td>\n",
              "      <td>7.535826e+09</td>\n",
              "      <td>6.527684e+09</td>\n",
              "      <td>532.611494</td>\n",
              "      <td>3157.049204</td>\n",
              "      <td>588.578428</td>\n",
              "      <td>584.843512</td>\n",
              "      <td>588.933267</td>\n",
              "      <td>2.445454e+10</td>\n",
              "      <td>19560.757414</td>\n",
              "      <td>7.347748e+09</td>\n",
              "      <td>2.389215e+10</td>\n",
              "      <td>0.004803</td>\n",
              "      <td>0.051876</td>\n",
              "      <td>2.102089e+10</td>\n",
              "      <td>2.384467e+10</td>\n",
              "      <td>2.384467e+10</td>\n",
              "      <td>0.164101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>490.004663</td>\n",
              "      <td>5543.811872</td>\n",
              "      <td>5377.413919</td>\n",
              "      <td>5576.455556</td>\n",
              "      <td>7.460525e+09</td>\n",
              "      <td>6.519881e+09</td>\n",
              "      <td>529.787269</td>\n",
              "      <td>3148.930980</td>\n",
              "      <td>586.386287</td>\n",
              "      <td>578.533240</td>\n",
              "      <td>587.299644</td>\n",
              "      <td>2.442159e+10</td>\n",
              "      <td>19409.232580</td>\n",
              "      <td>7.321506e+09</td>\n",
              "      <td>2.387996e+10</td>\n",
              "      <td>0.004891</td>\n",
              "      <td>0.051563</td>\n",
              "      <td>2.119307e+10</td>\n",
              "      <td>2.385284e+10</td>\n",
              "      <td>2.386373e+10</td>\n",
              "      <td>0.172641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>479.293991</td>\n",
              "      <td>5495.275745</td>\n",
              "      <td>5361.074237</td>\n",
              "      <td>5534.688889</td>\n",
              "      <td>7.383348e+09</td>\n",
              "      <td>6.545086e+09</td>\n",
              "      <td>526.537832</td>\n",
              "      <td>3144.839341</td>\n",
              "      <td>584.035401</td>\n",
              "      <td>571.892825</td>\n",
              "      <td>585.244078</td>\n",
              "      <td>2.560602e+10</td>\n",
              "      <td>19356.884080</td>\n",
              "      <td>7.293904e+09</td>\n",
              "      <td>2.590544e+10</td>\n",
              "      <td>0.004953</td>\n",
              "      <td>0.051206</td>\n",
              "      <td>2.148281e+10</td>\n",
              "      <td>2.436483e+10</td>\n",
              "      <td>2.505839e+10</td>\n",
              "      <td>0.176796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>475.793424</td>\n",
              "      <td>5390.077526</td>\n",
              "      <td>5325.608547</td>\n",
              "      <td>5467.077778</td>\n",
              "      <td>7.307283e+09</td>\n",
              "      <td>6.559739e+09</td>\n",
              "      <td>523.353198</td>\n",
              "      <td>3157.696653</td>\n",
              "      <td>581.686843</td>\n",
              "      <td>565.401051</td>\n",
              "      <td>583.418000</td>\n",
              "      <td>2.647010e+10</td>\n",
              "      <td>19580.341937</td>\n",
              "      <td>7.266019e+09</td>\n",
              "      <td>2.701003e+10</td>\n",
              "      <td>0.005045</td>\n",
              "      <td>0.050861</td>\n",
              "      <td>2.177255e+10</td>\n",
              "      <td>2.487683e+10</td>\n",
              "      <td>2.625304e+10</td>\n",
              "      <td>0.179679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>474.403351</td>\n",
              "      <td>5248.440352</td>\n",
              "      <td>5277.628816</td>\n",
              "      <td>5401.422222</td>\n",
              "      <td>7.232698e+09</td>\n",
              "      <td>6.548938e+09</td>\n",
              "      <td>520.259314</td>\n",
              "      <td>3165.512264</td>\n",
              "      <td>579.350824</td>\n",
              "      <td>559.083579</td>\n",
              "      <td>581.463844</td>\n",
              "      <td>2.708592e+10</td>\n",
              "      <td>19743.304418</td>\n",
              "      <td>7.237929e+09</td>\n",
              "      <td>2.756006e+10</td>\n",
              "      <td>0.005121</td>\n",
              "      <td>0.050525</td>\n",
              "      <td>2.206229e+10</td>\n",
              "      <td>2.538882e+10</td>\n",
              "      <td>2.742863e+10</td>\n",
              "      <td>0.181297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2427</th>\n",
              "      <td>53921.052011</td>\n",
              "      <td>362657.361479</td>\n",
              "      <td>313052.715018</td>\n",
              "      <td>296590.855556</td>\n",
              "      <td>1.177123e+12</td>\n",
              "      <td>1.103332e+12</td>\n",
              "      <td>57503.588671</td>\n",
              "      <td>33160.954234</td>\n",
              "      <td>50525.859464</td>\n",
              "      <td>62997.795532</td>\n",
              "      <td>50784.388889</td>\n",
              "      <td>2.366906e+13</td>\n",
              "      <td>113127.405877</td>\n",
              "      <td>1.029980e+12</td>\n",
              "      <td>2.360898e+13</td>\n",
              "      <td>4.862331</td>\n",
              "      <td>15.181624</td>\n",
              "      <td>2.285985e+13</td>\n",
              "      <td>2.358198e+13</td>\n",
              "      <td>2.358198e+13</td>\n",
              "      <td>0.106595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2428</th>\n",
              "      <td>50271.303907</td>\n",
              "      <td>364703.552330</td>\n",
              "      <td>314635.883028</td>\n",
              "      <td>298779.222222</td>\n",
              "      <td>1.166655e+12</td>\n",
              "      <td>1.092275e+12</td>\n",
              "      <td>57012.453918</td>\n",
              "      <td>32907.738375</td>\n",
              "      <td>50511.906509</td>\n",
              "      <td>62434.836781</td>\n",
              "      <td>50979.333333</td>\n",
              "      <td>2.366659e+13</td>\n",
              "      <td>114707.987853</td>\n",
              "      <td>1.029652e+12</td>\n",
              "      <td>2.359981e+13</td>\n",
              "      <td>4.942729</td>\n",
              "      <td>15.733026</td>\n",
              "      <td>2.291706e+13</td>\n",
              "      <td>2.358198e+13</td>\n",
              "      <td>2.358198e+13</td>\n",
              "      <td>0.113671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2429</th>\n",
              "      <td>49740.497274</td>\n",
              "      <td>360643.453301</td>\n",
              "      <td>314131.834188</td>\n",
              "      <td>299706.333333</td>\n",
              "      <td>1.156761e+12</td>\n",
              "      <td>1.079254e+12</td>\n",
              "      <td>56565.005278</td>\n",
              "      <td>32494.548974</td>\n",
              "      <td>50502.348124</td>\n",
              "      <td>61902.581955</td>\n",
              "      <td>51177.522222</td>\n",
              "      <td>2.366021e+13</td>\n",
              "      <td>113186.978260</td>\n",
              "      <td>1.029321e+12</td>\n",
              "      <td>2.359144e+13</td>\n",
              "      <td>4.889702</td>\n",
              "      <td>15.737946</td>\n",
              "      <td>2.297428e+13</td>\n",
              "      <td>2.358198e+13</td>\n",
              "      <td>2.358198e+13</td>\n",
              "      <td>0.119850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2430</th>\n",
              "      <td>49469.774482</td>\n",
              "      <td>360690.917599</td>\n",
              "      <td>314958.112576</td>\n",
              "      <td>300679.933333</td>\n",
              "      <td>1.146690e+12</td>\n",
              "      <td>1.065769e+12</td>\n",
              "      <td>56119.327518</td>\n",
              "      <td>32309.000469</td>\n",
              "      <td>50483.769044</td>\n",
              "      <td>61360.866494</td>\n",
              "      <td>51358.644444</td>\n",
              "      <td>2.365141e+13</td>\n",
              "      <td>111416.060134</td>\n",
              "      <td>1.028736e+12</td>\n",
              "      <td>2.358457e+13</td>\n",
              "      <td>4.862619</td>\n",
              "      <td>15.500428</td>\n",
              "      <td>2.303149e+13</td>\n",
              "      <td>2.358198e+13</td>\n",
              "      <td>2.358198e+13</td>\n",
              "      <td>0.129055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2431</th>\n",
              "      <td>52472.951755</td>\n",
              "      <td>369539.942232</td>\n",
              "      <td>318773.740415</td>\n",
              "      <td>304149.111111</td>\n",
              "      <td>1.140990e+12</td>\n",
              "      <td>1.056326e+12</td>\n",
              "      <td>55918.080581</td>\n",
              "      <td>31920.080339</td>\n",
              "      <td>50539.070823</td>\n",
              "      <td>61053.046985</td>\n",
              "      <td>51591.411111</td>\n",
              "      <td>2.364131e+13</td>\n",
              "      <td>111605.050933</td>\n",
              "      <td>1.029450e+12</td>\n",
              "      <td>2.357935e+13</td>\n",
              "      <td>4.867598</td>\n",
              "      <td>15.791718</td>\n",
              "      <td>2.308871e+13</td>\n",
              "      <td>2.358198e+13</td>\n",
              "      <td>2.358198e+13</td>\n",
              "      <td>0.135769</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2432 rows × 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Price_trix3  Transaction Value_trix90  ...  Difficulty_sma3  Top100_std30\n",
              "0       502.896486               5627.621788  ...     2.384467e+10      0.164101\n",
              "1       490.004663               5543.811872  ...     2.386373e+10      0.172641\n",
              "2       479.293991               5495.275745  ...     2.505839e+10      0.176796\n",
              "3       475.793424               5390.077526  ...     2.625304e+10      0.179679\n",
              "4       474.403351               5248.440352  ...     2.742863e+10      0.181297\n",
              "...            ...                       ...  ...              ...           ...\n",
              "2427  53921.052011             362657.361479  ...     2.358198e+13      0.106595\n",
              "2428  50271.303907             364703.552330  ...     2.358198e+13      0.113671\n",
              "2429  49740.497274             360643.453301  ...     2.358198e+13      0.119850\n",
              "2430  49469.774482             360690.917599  ...     2.358198e+13      0.129055\n",
              "2431  52472.951755             369539.942232  ...     2.358198e+13      0.135769\n",
              "\n",
              "[2432 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkivMknQMde2"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "kf = KFold(n_splits=5)\n",
        "\n",
        "means = []\n",
        "\n",
        "for df in df_:\n",
        "  print(\"# Variables : %d\" %len(df.columns))\n",
        "  n_arbres = 500\n",
        "  m_try = len(df.columns)\n",
        "  clf = RandomForestRegressor(n_estimators=n_arbres, bootstrap=True, oob_score=True, max_features=m_try, n_jobs=-1)\n",
        "\n",
        "  for train_index, test_index in kf.split(df):\n",
        "    test_index = train_index\n",
        "    clf.fit(df.iloc[train_index],np.asarray(df_Y_one.iloc[train_index]))\n",
        "    y_predict = clf.predict(df.iloc[test_index])\n",
        "    mse = mean_squared_error(y_predict,df_Y_one.iloc[test_index])\n",
        "    print(mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hr03cDV9RCdj"
      },
      "source": [
        "**2. Cross validation sur des variables tirées au hasard**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foEs5HhcRD8j"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "kf = KFold(n_splits=5)\n",
        "\n",
        "n = 21\n",
        "n_arbres = 500\n",
        "m_try = n\n",
        "\n",
        "clf = RandomForestRegressor(n_estimators=n_arbres, bootstrap=True, oob_score=True, max_features=m_try, n_jobs=-1)\n",
        "for i in range(1,1000):\n",
        "  df_all = df_X_one\n",
        "  index = np.random.randint(0,778,n)\n",
        "  df = df_all.iloc[:,index]\n",
        "  print(\"%s\" %index)\n",
        "\n",
        "  for train_index, test_index in kf.split(df):\n",
        "    test_index = train_index\n",
        "    clf.fit(df.iloc[train_index], np.asarray(df_Y_one.iloc[train_index]))\n",
        "    y_predict = clf.predict(df.iloc[test_index])\n",
        "    mse = mean_squared_error(y_predict,df_Y_one.iloc[test_index])\n",
        "    print(mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCJnSWwPR-Nb"
      },
      "source": [
        "**3. Corrélation des variables retenues**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Go93TI5SIwL"
      },
      "source": [
        "corr = df_pred.corr()\n",
        "\n",
        "# Generate a mask for the upper triangle\n",
        "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
        "\n",
        "# Set up the matplotlib figure\n",
        "f, ax = plt.subplots(figsize=(15, 8))\n",
        "\n",
        "sns.heatmap(corr,mask=mask, cmap='coolwarm',annot=True, fmt='.2f')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPAeov3Qv7pd"
      },
      "source": [
        "**4. VIF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFxPmXpSwkGy",
        "outputId": "caa06cf3-5b1f-4360-8aae-2be304260493",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tools.tools import add_constant\n",
        "\n",
        "#function for removing features with high vif\n",
        "def drop_high_vif(X, thresh=100):\n",
        "    cols = X.columns\n",
        "    variables = np.arange(X.shape[1])\n",
        "    dropped=True\n",
        "    while dropped:\n",
        "        dropped=False\n",
        "        c = X[cols[variables]].values\n",
        "        vif = [variance_inflation_factor(c, ix) for ix in np.arange(c.shape[1])]\n",
        "\n",
        "        maxloc = vif.index(max(vif))\n",
        "        if max(vif) > thresh:\n",
        "            print('dropping \\'' + X[cols[variables]].columns[maxloc] + '\\' at index: ' + str(maxloc))\n",
        "            variables = np.delete(variables, maxloc)\n",
        "            dropped=True\n",
        "\n",
        "    print('Remaining variables:')\n",
        "    print(X.columns[variables])\n",
        "    return X[cols[variables]]\n",
        "\n",
        "#function for listing vif values\n",
        "def vif_values(X):\n",
        "    add_constant(X)\n",
        "    df=pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns)\n",
        "    return df\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning:\n",
            "\n",
            "pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIXThg_Zwl8L",
        "outputId": "1c84e86d-984c-47f6-d4ba-820b74dad3ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "X_reduit_VIF = drop_high_vif(df_interp,thresh=10)\n",
        "X_reduit_VIF"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dropping 'Difficulty_ema7' at index: 105\n",
            "dropping 'Difficulty_ema3' at index: 112\n",
            "dropping 'Capitalization_ema7' at index: 26\n",
            "dropping 'Difficulty_wma3' at index: 119\n",
            "dropping 'Capitalization_ema3' at index: 16\n",
            "dropping 'Capitalization_wma3' at index: 16\n",
            "dropping 'Difficulty_ema14' at index: 111\n",
            "dropping 'Capitalization_ema14' at index: 24\n",
            "dropping 'Difficulty_trix7' at index: 65\n",
            "dropping 'Difficulty_trix14' at index: 50\n",
            "dropping 'Hashrate_ema90' at index: 69\n",
            "dropping 'Difficulty_ema30' at index: 98\n",
            "dropping 'Capitalization_trix7' at index: 13\n",
            "dropping 'Difficulty_trix30' at index: 101\n",
            "dropping 'Capitalization_trix14' at index: 16\n",
            "dropping 'Capitalization_ema30' at index: 38\n",
            "dropping 'Difficulty_wma90' at index: 100\n",
            "dropping 'Difficulty_wma7' at index: 100\n",
            "dropping 'Hashrate_ema30' at index: 118\n",
            "dropping 'Capitalization_trix30' at index: 14\n",
            "dropping 'Difficulty_trix3' at index: 130\n",
            "dropping 'Difficulty_wma14' at index: 94\n",
            "dropping 'Hashrate_wma90' at index: 85\n",
            "dropping 'Hashrate_trix90' at index: 83\n",
            "dropping 'Difficulty_wma30' at index: 97\n",
            "dropping 'Capitalization_wma7' at index: 19\n",
            "dropping 'Capitalization_wma90' at index: 47\n",
            "dropping 'Capitalization_wma14' at index: 30\n",
            "dropping 'Capitalization_trix3' at index: 12\n",
            "dropping 'Price_ema30' at index: 17\n",
            "dropping 'Price_trix90' at index: 26\n",
            "dropping 'Hashrate_wma30' at index: 94\n",
            "dropping 'Price_sma30' at index: 32\n",
            "dropping 'Price_wma30' at index: 19\n",
            "dropping 'Price_sma14' at index: 20\n",
            "dropping 'Price_ema14' at index: 27\n",
            "dropping 'Price_wma90' at index: 30\n",
            "dropping 'Price_sma7' at index: 33\n",
            "dropping 'Price_sma3' at index: 45\n",
            "dropping 'Price_ema7' at index: 32\n",
            "dropping 'Price_ema90' at index: 22\n",
            "dropping 'Price_sma90' at index: 23\n",
            "dropping 'Hashrate_sma90' at index: 52\n",
            "dropping 'Price_ema3' at index: 21\n",
            "dropping 'Sent_ema30' at index: 105\n",
            "dropping 'Capitalization_wma30' at index: 19\n",
            "dropping 'Price_wma3' at index: 12\n",
            "dropping 'Sent_ema14' at index: 138\n",
            "dropping 'Price_wma14' at index: 21\n",
            "dropping 'Price_wma7' at index: 26\n",
            "dropping 'Price_trix7' at index: 11\n",
            "dropping 'Price_trix3' at index: 0\n",
            "dropping 'Price_trix14' at index: 13\n",
            "dropping 'Difficulty_ema90' at index: 42\n",
            "dropping 'Price_std90' at index: 67\n",
            "dropping 'Difficulty_sma3' at index: 72\n",
            "dropping 'Price_trix30' at index: 22\n",
            "dropping 'Hashrate_sma30' at index: 52\n",
            "dropping 'Capitalization_ema90' at index: 18\n",
            "dropping 'Capitalization_trix90' at index: 11\n",
            "dropping 'Difficulty_sma7' at index: 67\n",
            "dropping 'Difficulty_sma30' at index: 66\n",
            "dropping 'Sent_wma90' at index: 31\n",
            "dropping 'Capitalization_sma3' at index: 14\n",
            "dropping 'Difficulty_sma14' at index: 61\n",
            "dropping 'Hashrate_sma14' at index: 113\n",
            "dropping 'Capitalization_sma14' at index: 11\n",
            "dropping 'Difficulty_trix90' at index: 61\n",
            "dropping 'Capitalization_sma30' at index: 18\n",
            "dropping 'Capitalization_sma7' at index: 12\n",
            "dropping 'Difficulty' at index: 83\n",
            "dropping 'Sent_ema90' at index: 17\n",
            "dropping '#Active_Adresses_ema90' at index: 14\n",
            "dropping 'Hashrate_sma7' at index: 57\n",
            "dropping 'Transaction Value_ema7' at index: 12\n",
            "dropping 'Transaction Value_ema14' at index: 13\n",
            "dropping 'Transaction Value_ema30' at index: 9\n",
            "dropping 'Tweets_ema30' at index: 24\n",
            "dropping 'Transaction Value_wma14' at index: 17\n",
            "dropping '#Active_Adresses_wma90' at index: 118\n",
            "dropping 'Transaction Value_wma30' at index: 3\n",
            "dropping 'Transaction Value_trix14' at index: 60\n",
            "dropping 'Transaction Value_wma3' at index: 80\n",
            "dropping 'Transaction Value_ema90' at index: 7\n",
            "dropping 'Transaction Value_wma7' at index: 13\n",
            "dropping 'Transaction Value_ema3' at index: 63\n",
            "dropping 'Transaction Value_trix90' at index: 0\n",
            "dropping 'Transaction Value_trix30' at index: 41\n",
            "dropping '#Transactions_wma14' at index: 113\n",
            "dropping 'Block Size_ema90' at index: 90\n",
            "dropping '#Active_Adresses_sma90' at index: 8\n",
            "dropping 'Tweets_wma30' at index: 32\n",
            "dropping 'Transaction Value_wma90' at index: 0\n",
            "dropping 'Transaction Value_sma3' at index: 57\n",
            "dropping 'Tweets_trix90' at index: 8\n",
            "dropping '#From_Adresses_wma90' at index: 100\n",
            "dropping 'Tweets_wma90' at index: 23\n",
            "dropping 'Transaction Value_sma14' at index: 4\n",
            "dropping 'Transaction Value_sma90' at index: 3\n",
            "dropping 'Google GTrends_wma90' at index: 10\n",
            "dropping 'Sent_sma90' at index: 22\n",
            "dropping 'Tweets_ema14' at index: 96\n",
            "dropping 'Capitalization' at index: 2\n",
            "dropping 'Tweets_sma30' at index: 47\n",
            "dropping '#Transactions_sma7' at index: 101\n",
            "dropping 'Difficulty_sma90' at index: 33\n",
            "dropping 'Transaction Value_sma30' at index: 0\n",
            "dropping 'Tweets_ema90' at index: 28\n",
            "dropping 'Sent_trix90' at index: 52\n",
            "dropping 'Transaction Value_sma7' at index: 43\n",
            "dropping '#Active_Adresses_sma3' at index: 95\n",
            "dropping 'Transaction Fee_sma90' at index: 23\n",
            "dropping 'Google GTrends_sma90' at index: 12\n",
            "dropping 'Tweets_std90' at index: 2\n",
            "dropping 'Hashrate_std90' at index: 90\n",
            "dropping 'Hashrate_std30' at index: 88\n",
            "dropping 'Top100_ema3' at index: 68\n",
            "dropping 'Top100_wma3' at index: 82\n",
            "dropping 'Profiltability_ema14' at index: 59\n",
            "dropping 'Top100_ema7' at index: 62\n",
            "dropping 'Profiltability_ema3' at index: 28\n",
            "dropping 'Transaction Fee_ema30' at index: 11\n",
            "dropping 'Top100_ema14' at index: 43\n",
            "dropping 'Profiltability_ema7' at index: 60\n",
            "dropping 'Median Transaction Fee_ema90' at index: 26\n",
            "dropping 'Top100_trix7' at index: 65\n",
            "dropping 'Top100_ema30' at index: 32\n",
            "dropping 'Transaction Fee_trix90' at index: 18\n",
            "dropping 'Profiltability_trix14' at index: 63\n",
            "dropping 'Transaction Fee_ema14' at index: 3\n",
            "dropping 'Top100_trix3' at index: 71\n",
            "dropping 'Median Transaction Fee_ema30' at index: 12\n",
            "dropping 'Profiltability_ema30' at index: 25\n",
            "dropping 'Top100_wma14' at index: 38\n",
            "dropping 'Median Transaction Fee_wma30' at index: 9\n",
            "dropping 'Top100_wma90' at index: 28\n",
            "dropping 'Profiltability_trix7' at index: 44\n",
            "dropping 'Top100_wma30' at index: 29\n",
            "dropping 'Transaction Fee_wma90' at index: 12\n",
            "dropping 'Median Transaction Value_ema90' at index: 11\n",
            "dropping 'Profiltability_wma3' at index: 35\n",
            "dropping 'Top100_sma3' at index: 62\n",
            "dropping 'Median Transaction Value_trix90' at index: 35\n",
            "dropping 'Transaction Fee_sma14' at index: 20\n",
            "dropping 'Top100_ema90' at index: 32\n",
            "dropping 'Transaction Fee_sma30' at index: 21\n",
            "dropping 'Median Transaction Fee_wma90' at index: 10\n",
            "dropping 'Profiltability_wma7' at index: 43\n",
            "dropping 'Median Transaction Fee_std90' at index: 8\n",
            "dropping 'Profiltability_wma90' at index: 20\n",
            "dropping 'Top100_sma30' at index: 23\n",
            "dropping 'Top100_wma7' at index: 38\n",
            "dropping 'Profiltability_ema90' at index: 20\n",
            "dropping 'Median Transaction Fee_ema14' at index: 4\n",
            "dropping 'Transaction Fee_wma30' at index: 7\n",
            "dropping 'Profiltability_wma14' at index: 42\n",
            "dropping 'Median Transaction Value_wma90' at index: 8\n",
            "dropping 'Top100_sma14' at index: 31\n",
            "dropping 'Transaction Fee_wma14' at index: 5\n",
            "dropping 'Median Transaction Fee_trix90' at index: 9\n",
            "dropping 'Transaction Fee_ema90' at index: 10\n",
            "dropping 'Top100_sma90' at index: 35\n",
            "dropping 'Profiltability_sma3' at index: 40\n",
            "dropping 'Profiltability_wma30' at index: 27\n",
            "dropping 'Top100' at index: 30\n",
            "dropping 'Google GTrends_ema90' at index: 7\n",
            "dropping 'Median Transaction Fee_sma90' at index: 10\n",
            "dropping 'Transaction Fee_std90' at index: 4\n",
            "dropping 'FeeInReward_wma90' at index: 31\n",
            "dropping 'Google GTrends_trix90' at index: 30\n",
            "dropping 'Median Transaction Value_sma90' at index: 5\n",
            "dropping 'Median Transaction Fee_sma14' at index: 8\n",
            "dropping 'Median Transaction Value_ema30' at index: 23\n",
            "dropping 'Capitalization_sma90' at index: 2\n",
            "dropping 'Block Size_sma90' at index: 11\n",
            "dropping 'Top100_rsi90' at index: 16\n",
            "dropping '#From_Adresses_sma14' at index: 11\n",
            "dropping 'Tweets_sma90' at index: 6\n",
            "dropping 'Top100_trix90' at index: 10\n",
            "dropping 'Google GTrends_wma14' at index: 24\n",
            "dropping 'Tweets_wma14' at index: 21\n",
            "dropping '#Transactions_std30' at index: 19\n",
            "dropping 'Tweets_std30' at index: 3\n",
            "dropping 'Sent_std90' at index: 4\n",
            "dropping 'Profiltability_trix3' at index: 12\n",
            "dropping '#Transactions_sma3' at index: 14\n",
            "dropping 'Sent_trix30' at index: 11\n",
            "dropping '#Active_Adresses_std30' at index: 19\n",
            "dropping 'Profiltability_trix90' at index: 4\n",
            "dropping 'Median Transaction Fee_sma30' at index: 5\n",
            "dropping 'Capitalization_std90' at index: 0\n",
            "Remaining variables:\n",
            "Index(['Block Size_std90', 'Median Transaction Value_std90',\n",
            "       'Transaction Value_std90', 'Profiltability_sma90',\n",
            "       '#From_Adresses_skew30', 'Block Time_skew7', 'Google GTrends_std14',\n",
            "       'Top100_roc90', '#Active_Adresses_skew90', 'Profiltability_std7',\n",
            "       'Transaction Fee_ema7', 'FeeInReward_sma90', 'Transaction Fee_skew90',\n",
            "       'Tweets_skew30', 'Tweets_skew14', 'Tweets_std14', 'Top100_skew90',\n",
            "       'Transaction Value_trix7', 'Top100_std30'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Block Size_std90</th>\n",
              "      <th>Median Transaction Value_std90</th>\n",
              "      <th>Transaction Value_std90</th>\n",
              "      <th>Profiltability_sma90</th>\n",
              "      <th>#From_Adresses_skew30</th>\n",
              "      <th>Block Time_skew7</th>\n",
              "      <th>Google GTrends_std14</th>\n",
              "      <th>Top100_roc90</th>\n",
              "      <th>#Active_Adresses_skew90</th>\n",
              "      <th>Profiltability_std7</th>\n",
              "      <th>Transaction Fee_ema7</th>\n",
              "      <th>FeeInReward_sma90</th>\n",
              "      <th>Transaction Fee_skew90</th>\n",
              "      <th>Tweets_skew30</th>\n",
              "      <th>Tweets_skew14</th>\n",
              "      <th>Tweets_std14</th>\n",
              "      <th>Top100_skew90</th>\n",
              "      <th>Transaction Value_trix7</th>\n",
              "      <th>Top100_std30</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>37405.636722</td>\n",
              "      <td>21.850662</td>\n",
              "      <td>2926.509233</td>\n",
              "      <td>18.546367</td>\n",
              "      <td>-0.454348</td>\n",
              "      <td>1.198692</td>\n",
              "      <td>0.775962</td>\n",
              "      <td>0.737374</td>\n",
              "      <td>-0.195150</td>\n",
              "      <td>0.689911</td>\n",
              "      <td>0.081840</td>\n",
              "      <td>0.280778</td>\n",
              "      <td>5.898864</td>\n",
              "      <td>-0.127467</td>\n",
              "      <td>0.505434</td>\n",
              "      <td>2759.778106</td>\n",
              "      <td>0.374866</td>\n",
              "      <td>4762.684205</td>\n",
              "      <td>0.164101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>37674.544792</td>\n",
              "      <td>21.478904</td>\n",
              "      <td>2903.796230</td>\n",
              "      <td>18.290378</td>\n",
              "      <td>-0.374010</td>\n",
              "      <td>2.004091</td>\n",
              "      <td>0.774713</td>\n",
              "      <td>0.732582</td>\n",
              "      <td>-0.196855</td>\n",
              "      <td>0.561202</td>\n",
              "      <td>0.080255</td>\n",
              "      <td>0.280178</td>\n",
              "      <td>5.834639</td>\n",
              "      <td>-0.092574</td>\n",
              "      <td>0.720903</td>\n",
              "      <td>2868.392540</td>\n",
              "      <td>0.379693</td>\n",
              "      <td>4068.383571</td>\n",
              "      <td>0.172641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>38237.068820</td>\n",
              "      <td>20.675639</td>\n",
              "      <td>2886.919577</td>\n",
              "      <td>18.039278</td>\n",
              "      <td>-0.407112</td>\n",
              "      <td>2.040239</td>\n",
              "      <td>0.559380</td>\n",
              "      <td>0.368371</td>\n",
              "      <td>-0.197071</td>\n",
              "      <td>1.450959</td>\n",
              "      <td>0.078816</td>\n",
              "      <td>0.280389</td>\n",
              "      <td>5.841085</td>\n",
              "      <td>-0.092250</td>\n",
              "      <td>0.962280</td>\n",
              "      <td>2875.654533</td>\n",
              "      <td>0.380491</td>\n",
              "      <td>4058.456659</td>\n",
              "      <td>0.176796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>38419.091510</td>\n",
              "      <td>19.992603</td>\n",
              "      <td>2852.070150</td>\n",
              "      <td>17.802967</td>\n",
              "      <td>-0.242339</td>\n",
              "      <td>1.053292</td>\n",
              "      <td>0.521028</td>\n",
              "      <td>0.206445</td>\n",
              "      <td>-0.181406</td>\n",
              "      <td>1.358996</td>\n",
              "      <td>0.078387</td>\n",
              "      <td>0.280944</td>\n",
              "      <td>5.819986</td>\n",
              "      <td>-0.169993</td>\n",
              "      <td>1.023698</td>\n",
              "      <td>2766.822921</td>\n",
              "      <td>0.382186</td>\n",
              "      <td>3616.934480</td>\n",
              "      <td>0.179679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>38312.108937</td>\n",
              "      <td>19.735688</td>\n",
              "      <td>2833.286181</td>\n",
              "      <td>17.575989</td>\n",
              "      <td>-0.270716</td>\n",
              "      <td>1.053292</td>\n",
              "      <td>0.480542</td>\n",
              "      <td>0.110642</td>\n",
              "      <td>-0.144187</td>\n",
              "      <td>1.305018</td>\n",
              "      <td>0.077240</td>\n",
              "      <td>0.280256</td>\n",
              "      <td>5.785434</td>\n",
              "      <td>-0.234782</td>\n",
              "      <td>-0.038450</td>\n",
              "      <td>2072.026158</td>\n",
              "      <td>0.383474</td>\n",
              "      <td>3083.556288</td>\n",
              "      <td>0.181297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2427</th>\n",
              "      <td>13193.797161</td>\n",
              "      <td>212.020150</td>\n",
              "      <td>69190.208981</td>\n",
              "      <td>0.330456</td>\n",
              "      <td>-0.569929</td>\n",
              "      <td>-0.270844</td>\n",
              "      <td>40.637395</td>\n",
              "      <td>7.136012</td>\n",
              "      <td>-2.686847</td>\n",
              "      <td>0.057523</td>\n",
              "      <td>52.345831</td>\n",
              "      <td>12.020033</td>\n",
              "      <td>2.337403</td>\n",
              "      <td>-0.177154</td>\n",
              "      <td>1.174640</td>\n",
              "      <td>27795.339830</td>\n",
              "      <td>-0.258725</td>\n",
              "      <td>432996.594732</td>\n",
              "      <td>0.106595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2428</th>\n",
              "      <td>13232.341183</td>\n",
              "      <td>207.741041</td>\n",
              "      <td>68303.812287</td>\n",
              "      <td>0.331556</td>\n",
              "      <td>-0.500339</td>\n",
              "      <td>0.359701</td>\n",
              "      <td>47.007473</td>\n",
              "      <td>7.796235</td>\n",
              "      <td>-2.689953</td>\n",
              "      <td>0.043473</td>\n",
              "      <td>50.638873</td>\n",
              "      <td>12.208111</td>\n",
              "      <td>2.231877</td>\n",
              "      <td>-0.179657</td>\n",
              "      <td>1.002057</td>\n",
              "      <td>27295.603708</td>\n",
              "      <td>-0.269179</td>\n",
              "      <td>401365.212038</td>\n",
              "      <td>0.113671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2429</th>\n",
              "      <td>13172.381028</td>\n",
              "      <td>205.604200</td>\n",
              "      <td>67404.089500</td>\n",
              "      <td>0.333189</td>\n",
              "      <td>-0.421358</td>\n",
              "      <td>0.889810</td>\n",
              "      <td>51.630438</td>\n",
              "      <td>7.655467</td>\n",
              "      <td>-2.725547</td>\n",
              "      <td>0.050232</td>\n",
              "      <td>45.235405</td>\n",
              "      <td>12.313222</td>\n",
              "      <td>2.237656</td>\n",
              "      <td>-0.060843</td>\n",
              "      <td>0.844288</td>\n",
              "      <td>29019.842310</td>\n",
              "      <td>-0.282014</td>\n",
              "      <td>329043.554910</td>\n",
              "      <td>0.119850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2430</th>\n",
              "      <td>13108.447206</td>\n",
              "      <td>207.981777</td>\n",
              "      <td>67306.108820</td>\n",
              "      <td>0.333878</td>\n",
              "      <td>-0.531352</td>\n",
              "      <td>1.078419</td>\n",
              "      <td>53.039777</td>\n",
              "      <td>7.909231</td>\n",
              "      <td>-2.685443</td>\n",
              "      <td>0.061256</td>\n",
              "      <td>39.919304</td>\n",
              "      <td>12.328100</td>\n",
              "      <td>2.243276</td>\n",
              "      <td>0.068292</td>\n",
              "      <td>0.695535</td>\n",
              "      <td>30226.943499</td>\n",
              "      <td>-0.279338</td>\n",
              "      <td>324696.110385</td>\n",
              "      <td>0.129055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2431</th>\n",
              "      <td>13135.153735</td>\n",
              "      <td>206.599683</td>\n",
              "      <td>68122.784763</td>\n",
              "      <td>0.334756</td>\n",
              "      <td>-0.463820</td>\n",
              "      <td>1.281804</td>\n",
              "      <td>52.451699</td>\n",
              "      <td>7.511531</td>\n",
              "      <td>-2.570948</td>\n",
              "      <td>0.066277</td>\n",
              "      <td>39.178728</td>\n",
              "      <td>12.456400</td>\n",
              "      <td>2.199331</td>\n",
              "      <td>0.055010</td>\n",
              "      <td>0.866580</td>\n",
              "      <td>30222.308843</td>\n",
              "      <td>-0.286269</td>\n",
              "      <td>403171.135784</td>\n",
              "      <td>0.135769</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2432 rows × 19 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Block Size_std90  ...  Top100_std30\n",
              "0         37405.636722  ...      0.164101\n",
              "1         37674.544792  ...      0.172641\n",
              "2         38237.068820  ...      0.176796\n",
              "3         38419.091510  ...      0.179679\n",
              "4         38312.108937  ...      0.181297\n",
              "...                ...  ...           ...\n",
              "2427      13193.797161  ...      0.106595\n",
              "2428      13232.341183  ...      0.113671\n",
              "2429      13172.381028  ...      0.119850\n",
              "2430      13108.447206  ...      0.129055\n",
              "2431      13135.153735  ...      0.135769\n",
              "\n",
              "[2432 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw63tADprF6D",
        "outputId": "d5064201-dff5-42c1-a00b-8a725200c4b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "import pickle\n",
        "\n",
        "f = open('VIF_interp.pckl', 'wb')\n",
        "pickle.dump(X_reduit_VIF, f)\n",
        "f.close()\n",
        "\n",
        "files.download('VIF_interp.pckl')"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_9414ed37-815d-4c97-8d9d-e851d5754bd5\", \"VIF_interp.pckl\", 370983)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLux-KCCrpqh"
      },
      "source": [
        "f = open('VIF_interp.pckl', 'rb')\n",
        "X_reduit_VIF = pickle.load(f)\n",
        "f.close()"
      ],
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ML7TrVHkw13n",
        "outputId": "e5a00a72-c7a1-44bf-a5c0-23dd5d809492",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        }
      },
      "source": [
        "corr = X_reduit_VIF.corr()\n",
        "\n",
        "# Generate a mask for the upper triangle\n",
        "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
        "\n",
        "# Set up the matplotlib figure\n",
        "f, ax = plt.subplots(figsize=(15, 8))\n",
        "\n",
        "sns.heatmap(corr,mask=mask, cmap='coolwarm',annot=True, fmt='.2f')"
      ],
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fb3a9f66850>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 227
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QCF6HQbVLS7",
        "outputId": "24ec8398-fefc-4941-d5ca-fca2388165f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "X_Avec_Prix = X_reduit_VIF\n",
        "X_Avec_Prix.insert(0,'Price',df_data['Price'])\n",
        "X_Avec_Prix = X_Avec_Prix.astype(dtype=np.float32)\n",
        "X_Avec_Prix\n"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-224-cf870c3d4ddd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX_Avec_Prix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_reduit_VIF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_Avec_Prix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Price'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_Avec_Prix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_Avec_Prix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_Avec_Prix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, loc, column, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   3626\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3627\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3628\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_duplicates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_duplicates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3630\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, loc, item, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   1184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_duplicates\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;31m# Should this be a different kind of error??\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"cannot insert {item}, already exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot insert Price, already exists"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKFVK01eC_Tj"
      },
      "source": [
        "# Normalisation mu-law"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlawkkUGKaH-",
        "outputId": "5b810e18-105a-4420-a9d6-b24e891bd50b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        }
      },
      "source": [
        "X_Avec_Prix = df_data\n",
        "X_Avec_Prix = X_Avec_Prix.drop(columns=['Dates', 'Price'])\n",
        "Xdrop = SimpleImputer(missing_values=np.nan,strategy='most_frequent').fit_transform(X_Avec_Prix)\n",
        "Xdrop = pd.DataFrame(Xdrop)\n",
        "Xdrop.columns = X_Avec_Prix.columns\n",
        "X_Avec_Prix = Xdrop\n",
        "X_Avec_Prix = X_Avec_Prix\n",
        "X_Avec_Prix = X_Avec_Prix.iloc[:,var_pred]\n",
        "X_Avec_Prix.insert(0,'Price',df_data['Price'])\n",
        "X_Avec_Prix = X_Avec_Prix.astype(dtype=np.float32)\n",
        "X_Avec_Prix"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Price</th>\n",
              "      <th>Price_trix3</th>\n",
              "      <th>Transaction Value_trix90</th>\n",
              "      <th>Transaction Value_wma90</th>\n",
              "      <th>Transaction Value_sma90</th>\n",
              "      <th>Capitalization_trix90</th>\n",
              "      <th>Capitalization_sma14</th>\n",
              "      <th>Price_ema30</th>\n",
              "      <th>Tweets_std90</th>\n",
              "      <th>Price_ema90</th>\n",
              "      <th>Price_trix90</th>\n",
              "      <th>Price_sma90</th>\n",
              "      <th>Difficulty_trix14</th>\n",
              "      <th>Tweets_trix90</th>\n",
              "      <th>Capitalization_wma90</th>\n",
              "      <th>Difficulty_trix7</th>\n",
              "      <th>Median Transaction Fee_std90</th>\n",
              "      <th>Median Transaction Fee_wma30</th>\n",
              "      <th>Difficulty_sma30</th>\n",
              "      <th>Difficulty_sma7</th>\n",
              "      <th>Difficulty_sma3</th>\n",
              "      <th>Top100_std30</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>502.665009</td>\n",
              "      <td>502.896484</td>\n",
              "      <td>5627.621582</td>\n",
              "      <td>5405.723633</td>\n",
              "      <td>5631.100098</td>\n",
              "      <td>7.535826e+09</td>\n",
              "      <td>6.527684e+09</td>\n",
              "      <td>532.611511</td>\n",
              "      <td>3157.049316</td>\n",
              "      <td>588.578430</td>\n",
              "      <td>584.843506</td>\n",
              "      <td>588.933289</td>\n",
              "      <td>2.445454e+10</td>\n",
              "      <td>19560.757812</td>\n",
              "      <td>7.347748e+09</td>\n",
              "      <td>2.389215e+10</td>\n",
              "      <td>0.004803</td>\n",
              "      <td>0.051876</td>\n",
              "      <td>2.102089e+10</td>\n",
              "      <td>2.384467e+10</td>\n",
              "      <td>2.384467e+10</td>\n",
              "      <td>0.164101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>488.835999</td>\n",
              "      <td>490.004669</td>\n",
              "      <td>5543.812012</td>\n",
              "      <td>5377.414062</td>\n",
              "      <td>5576.455566</td>\n",
              "      <td>7.460525e+09</td>\n",
              "      <td>6.519881e+09</td>\n",
              "      <td>529.787292</td>\n",
              "      <td>3148.930908</td>\n",
              "      <td>586.386292</td>\n",
              "      <td>578.533264</td>\n",
              "      <td>587.299622</td>\n",
              "      <td>2.442159e+10</td>\n",
              "      <td>19409.232422</td>\n",
              "      <td>7.321505e+09</td>\n",
              "      <td>2.387996e+10</td>\n",
              "      <td>0.004891</td>\n",
              "      <td>0.051563</td>\n",
              "      <td>2.119307e+10</td>\n",
              "      <td>2.385284e+10</td>\n",
              "      <td>2.386373e+10</td>\n",
              "      <td>0.172641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>479.420990</td>\n",
              "      <td>479.293976</td>\n",
              "      <td>5495.275879</td>\n",
              "      <td>5361.074219</td>\n",
              "      <td>5534.688965</td>\n",
              "      <td>7.383348e+09</td>\n",
              "      <td>6.545086e+09</td>\n",
              "      <td>526.537842</td>\n",
              "      <td>3144.839355</td>\n",
              "      <td>584.035400</td>\n",
              "      <td>571.892822</td>\n",
              "      <td>585.244080</td>\n",
              "      <td>2.560602e+10</td>\n",
              "      <td>19356.884766</td>\n",
              "      <td>7.293904e+09</td>\n",
              "      <td>2.590544e+10</td>\n",
              "      <td>0.004953</td>\n",
              "      <td>0.051206</td>\n",
              "      <td>2.148281e+10</td>\n",
              "      <td>2.436483e+10</td>\n",
              "      <td>2.505839e+10</td>\n",
              "      <td>0.176796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>477.175995</td>\n",
              "      <td>475.793427</td>\n",
              "      <td>5390.077637</td>\n",
              "      <td>5325.608398</td>\n",
              "      <td>5467.077637</td>\n",
              "      <td>7.307283e+09</td>\n",
              "      <td>6.559739e+09</td>\n",
              "      <td>523.353210</td>\n",
              "      <td>3157.696533</td>\n",
              "      <td>581.686829</td>\n",
              "      <td>565.401062</td>\n",
              "      <td>583.418030</td>\n",
              "      <td>2.647010e+10</td>\n",
              "      <td>19580.341797</td>\n",
              "      <td>7.266019e+09</td>\n",
              "      <td>2.701003e+10</td>\n",
              "      <td>0.005045</td>\n",
              "      <td>0.050861</td>\n",
              "      <td>2.177255e+10</td>\n",
              "      <td>2.487683e+10</td>\n",
              "      <td>2.625304e+10</td>\n",
              "      <td>0.179679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>475.398010</td>\n",
              "      <td>474.403351</td>\n",
              "      <td>5248.440430</td>\n",
              "      <td>5277.628906</td>\n",
              "      <td>5401.422363</td>\n",
              "      <td>7.232698e+09</td>\n",
              "      <td>6.548938e+09</td>\n",
              "      <td>520.259338</td>\n",
              "      <td>3165.512207</td>\n",
              "      <td>579.350830</td>\n",
              "      <td>559.083557</td>\n",
              "      <td>581.463867</td>\n",
              "      <td>2.708592e+10</td>\n",
              "      <td>19743.304688</td>\n",
              "      <td>7.237929e+09</td>\n",
              "      <td>2.756006e+10</td>\n",
              "      <td>0.005121</td>\n",
              "      <td>0.050525</td>\n",
              "      <td>2.206229e+10</td>\n",
              "      <td>2.538882e+10</td>\n",
              "      <td>2.742863e+10</td>\n",
              "      <td>0.181297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2428</th>\n",
              "      <td>49891.000000</td>\n",
              "      <td>50271.304688</td>\n",
              "      <td>364703.562500</td>\n",
              "      <td>314635.875000</td>\n",
              "      <td>298779.218750</td>\n",
              "      <td>1.166655e+12</td>\n",
              "      <td>1.092275e+12</td>\n",
              "      <td>57012.453125</td>\n",
              "      <td>32907.738281</td>\n",
              "      <td>50511.906250</td>\n",
              "      <td>62434.835938</td>\n",
              "      <td>50979.332031</td>\n",
              "      <td>2.366659e+13</td>\n",
              "      <td>114707.984375</td>\n",
              "      <td>1.029652e+12</td>\n",
              "      <td>2.359981e+13</td>\n",
              "      <td>4.942729</td>\n",
              "      <td>15.733026</td>\n",
              "      <td>2.291706e+13</td>\n",
              "      <td>2.358198e+13</td>\n",
              "      <td>2.358198e+13</td>\n",
              "      <td>0.113671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2429</th>\n",
              "      <td>50077.000000</td>\n",
              "      <td>49740.496094</td>\n",
              "      <td>360643.468750</td>\n",
              "      <td>314131.843750</td>\n",
              "      <td>299706.343750</td>\n",
              "      <td>1.156761e+12</td>\n",
              "      <td>1.079254e+12</td>\n",
              "      <td>56565.003906</td>\n",
              "      <td>32494.548828</td>\n",
              "      <td>50502.347656</td>\n",
              "      <td>61902.582031</td>\n",
              "      <td>51177.523438</td>\n",
              "      <td>2.366021e+13</td>\n",
              "      <td>113186.976562</td>\n",
              "      <td>1.029321e+12</td>\n",
              "      <td>2.359144e+13</td>\n",
              "      <td>4.889702</td>\n",
              "      <td>15.737947</td>\n",
              "      <td>2.297428e+13</td>\n",
              "      <td>2.358198e+13</td>\n",
              "      <td>2.358198e+13</td>\n",
              "      <td>0.119850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2430</th>\n",
              "      <td>49657.000000</td>\n",
              "      <td>49469.773438</td>\n",
              "      <td>360690.906250</td>\n",
              "      <td>314958.125000</td>\n",
              "      <td>300679.937500</td>\n",
              "      <td>1.146690e+12</td>\n",
              "      <td>1.065769e+12</td>\n",
              "      <td>56119.328125</td>\n",
              "      <td>32309.000000</td>\n",
              "      <td>50483.769531</td>\n",
              "      <td>61360.867188</td>\n",
              "      <td>51358.644531</td>\n",
              "      <td>2.365141e+13</td>\n",
              "      <td>111416.062500</td>\n",
              "      <td>1.028736e+12</td>\n",
              "      <td>2.358457e+13</td>\n",
              "      <td>4.862618</td>\n",
              "      <td>15.500428</td>\n",
              "      <td>2.303149e+13</td>\n",
              "      <td>2.358198e+13</td>\n",
              "      <td>2.358198e+13</td>\n",
              "      <td>0.129055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2431</th>\n",
              "      <td>53000.000000</td>\n",
              "      <td>52472.953125</td>\n",
              "      <td>369539.937500</td>\n",
              "      <td>318773.750000</td>\n",
              "      <td>304149.125000</td>\n",
              "      <td>1.140990e+12</td>\n",
              "      <td>1.056326e+12</td>\n",
              "      <td>55918.082031</td>\n",
              "      <td>31920.080078</td>\n",
              "      <td>50539.070312</td>\n",
              "      <td>61053.046875</td>\n",
              "      <td>51591.410156</td>\n",
              "      <td>2.364131e+13</td>\n",
              "      <td>111605.054688</td>\n",
              "      <td>1.029450e+12</td>\n",
              "      <td>2.357935e+13</td>\n",
              "      <td>4.867598</td>\n",
              "      <td>15.791718</td>\n",
              "      <td>2.308871e+13</td>\n",
              "      <td>2.358198e+13</td>\n",
              "      <td>2.358198e+13</td>\n",
              "      <td>0.135769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2432</th>\n",
              "      <td>54583.000000</td>\n",
              "      <td>54581.160156</td>\n",
              "      <td>374411.156250</td>\n",
              "      <td>321299.781250</td>\n",
              "      <td>306669.562500</td>\n",
              "      <td>1.137354e+12</td>\n",
              "      <td>1.046273e+12</td>\n",
              "      <td>55831.945312</td>\n",
              "      <td>31587.187500</td>\n",
              "      <td>50627.949219</td>\n",
              "      <td>60855.652344</td>\n",
              "      <td>51851.523438</td>\n",
              "      <td>2.363070e+13</td>\n",
              "      <td>111290.820312</td>\n",
              "      <td>1.030719e+12</td>\n",
              "      <td>2.357570e+13</td>\n",
              "      <td>4.836071</td>\n",
              "      <td>15.860856</td>\n",
              "      <td>2.314592e+13</td>\n",
              "      <td>2.358198e+13</td>\n",
              "      <td>2.358198e+13</td>\n",
              "      <td>0.142835</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2433 rows × 22 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             Price   Price_trix3  ...  Difficulty_sma3  Top100_std30\n",
              "0       502.665009    502.896484  ...     2.384467e+10      0.164101\n",
              "1       488.835999    490.004669  ...     2.386373e+10      0.172641\n",
              "2       479.420990    479.293976  ...     2.505839e+10      0.176796\n",
              "3       477.175995    475.793427  ...     2.625304e+10      0.179679\n",
              "4       475.398010    474.403351  ...     2.742863e+10      0.181297\n",
              "...            ...           ...  ...              ...           ...\n",
              "2428  49891.000000  50271.304688  ...     2.358198e+13      0.113671\n",
              "2429  50077.000000  49740.496094  ...     2.358198e+13      0.119850\n",
              "2430  49657.000000  49469.773438  ...     2.358198e+13      0.129055\n",
              "2431  53000.000000  52472.953125  ...     2.358198e+13      0.135769\n",
              "2432  54583.000000  54581.160156  ...     2.358198e+13      0.142835\n",
              "\n",
              "[2433 rows x 22 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 225
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpEgVy8S9rHb",
        "outputId": "b3b8516c-7453-4c59-d1b9-c455ebced0c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "\n",
        "mu = 255.0\n",
        "\n",
        "x = X_Avec_Prix.values #returns a numpy array\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "x_scaled = min_max_scaler.fit_transform(x)\n",
        "x_scaled = tf.math.log(1+mu*tf.math.abs(x))/tf.math.log(1+mu)\n",
        "X_Avec_Prix_norm = pd.DataFrame(x_scaled,dtype=np.float32)\n",
        "X_Avec_Prix_norm[0:10]"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.120977</td>\n",
              "      <td>2.121060</td>\n",
              "      <td>2.556583</td>\n",
              "      <td>2.549328</td>\n",
              "      <td>2.556695</td>\n",
              "      <td>5.100684</td>\n",
              "      <td>5.074785</td>\n",
              "      <td>2.131413</td>\n",
              "      <td>2.452339</td>\n",
              "      <td>2.149432</td>\n",
              "      <td>2.148284</td>\n",
              "      <td>2.149540</td>\n",
              "      <td>5.312967</td>\n",
              "      <td>2.781254</td>\n",
              "      <td>5.096126</td>\n",
              "      <td>5.308772</td>\n",
              "      <td>0.144217</td>\n",
              "      <td>0.478839</td>\n",
              "      <td>5.285683</td>\n",
              "      <td>5.308413</td>\n",
              "      <td>5.308413</td>\n",
              "      <td>0.677635</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.115946</td>\n",
              "      <td>2.116377</td>\n",
              "      <td>2.553877</td>\n",
              "      <td>2.548382</td>\n",
              "      <td>2.554936</td>\n",
              "      <td>5.098873</td>\n",
              "      <td>5.074569</td>\n",
              "      <td>2.130454</td>\n",
              "      <td>2.451875</td>\n",
              "      <td>2.148759</td>\n",
              "      <td>2.146327</td>\n",
              "      <td>2.149040</td>\n",
              "      <td>5.312724</td>\n",
              "      <td>2.779851</td>\n",
              "      <td>5.095481</td>\n",
              "      <td>5.308679</td>\n",
              "      <td>0.146013</td>\n",
              "      <td>0.477822</td>\n",
              "      <td>5.287153</td>\n",
              "      <td>5.308474</td>\n",
              "      <td>5.308557</td>\n",
              "      <td>0.686576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.112439</td>\n",
              "      <td>2.112391</td>\n",
              "      <td>2.552291</td>\n",
              "      <td>2.547833</td>\n",
              "      <td>2.553580</td>\n",
              "      <td>5.096998</td>\n",
              "      <td>5.075265</td>\n",
              "      <td>2.129345</td>\n",
              "      <td>2.451641</td>\n",
              "      <td>2.148034</td>\n",
              "      <td>2.144245</td>\n",
              "      <td>2.148407</td>\n",
              "      <td>5.321265</td>\n",
              "      <td>2.779364</td>\n",
              "      <td>5.094800</td>\n",
              "      <td>5.323361</td>\n",
              "      <td>0.147286</td>\n",
              "      <td>0.476660</td>\n",
              "      <td>5.289602</td>\n",
              "      <td>5.312304</td>\n",
              "      <td>5.317366</td>\n",
              "      <td>0.690770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.111593</td>\n",
              "      <td>2.111070</td>\n",
              "      <td>2.548805</td>\n",
              "      <td>2.546636</td>\n",
              "      <td>2.551364</td>\n",
              "      <td>5.095130</td>\n",
              "      <td>5.075668</td>\n",
              "      <td>2.128251</td>\n",
              "      <td>2.452376</td>\n",
              "      <td>2.147308</td>\n",
              "      <td>2.142187</td>\n",
              "      <td>2.147844</td>\n",
              "      <td>5.327250</td>\n",
              "      <td>2.781434</td>\n",
              "      <td>5.094109</td>\n",
              "      <td>5.330891</td>\n",
              "      <td>0.149148</td>\n",
              "      <td>0.475526</td>\n",
              "      <td>5.292018</td>\n",
              "      <td>5.316054</td>\n",
              "      <td>5.325765</td>\n",
              "      <td>0.693625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.110920</td>\n",
              "      <td>2.110542</td>\n",
              "      <td>2.544003</td>\n",
              "      <td>2.545004</td>\n",
              "      <td>2.549185</td>\n",
              "      <td>5.093280</td>\n",
              "      <td>5.075371</td>\n",
              "      <td>2.127181</td>\n",
              "      <td>2.452822</td>\n",
              "      <td>2.146582</td>\n",
              "      <td>2.140161</td>\n",
              "      <td>2.147239</td>\n",
              "      <td>5.331397</td>\n",
              "      <td>2.782929</td>\n",
              "      <td>5.093410</td>\n",
              "      <td>5.334527</td>\n",
              "      <td>0.150663</td>\n",
              "      <td>0.474419</td>\n",
              "      <td>5.294402</td>\n",
              "      <td>5.319728</td>\n",
              "      <td>5.333664</td>\n",
              "      <td>0.695207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2.113144</td>\n",
              "      <td>2.112629</td>\n",
              "      <td>2.538742</td>\n",
              "      <td>2.543224</td>\n",
              "      <td>2.546281</td>\n",
              "      <td>5.091650</td>\n",
              "      <td>5.074531</td>\n",
              "      <td>2.126308</td>\n",
              "      <td>2.454430</td>\n",
              "      <td>2.145910</td>\n",
              "      <td>2.138328</td>\n",
              "      <td>2.146642</td>\n",
              "      <td>5.334206</td>\n",
              "      <td>2.786008</td>\n",
              "      <td>5.092774</td>\n",
              "      <td>5.336013</td>\n",
              "      <td>0.151640</td>\n",
              "      <td>0.473512</td>\n",
              "      <td>5.296755</td>\n",
              "      <td>5.323329</td>\n",
              "      <td>5.333664</td>\n",
              "      <td>0.697131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2.114370</td>\n",
              "      <td>2.114296</td>\n",
              "      <td>2.534597</td>\n",
              "      <td>2.541829</td>\n",
              "      <td>2.544715</td>\n",
              "      <td>5.090134</td>\n",
              "      <td>5.073939</td>\n",
              "      <td>2.125561</td>\n",
              "      <td>2.449487</td>\n",
              "      <td>2.145273</td>\n",
              "      <td>2.136630</td>\n",
              "      <td>2.146066</td>\n",
              "      <td>5.336032</td>\n",
              "      <td>2.787416</td>\n",
              "      <td>5.092165</td>\n",
              "      <td>5.336344</td>\n",
              "      <td>0.152396</td>\n",
              "      <td>0.472761</td>\n",
              "      <td>5.299078</td>\n",
              "      <td>5.326859</td>\n",
              "      <td>5.333664</td>\n",
              "      <td>0.698160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2.113213</td>\n",
              "      <td>2.113616</td>\n",
              "      <td>2.527973</td>\n",
              "      <td>2.539632</td>\n",
              "      <td>2.541539</td>\n",
              "      <td>5.088624</td>\n",
              "      <td>5.073533</td>\n",
              "      <td>2.124789</td>\n",
              "      <td>2.442805</td>\n",
              "      <td>2.144626</td>\n",
              "      <td>2.134928</td>\n",
              "      <td>2.145471</td>\n",
              "      <td>5.337140</td>\n",
              "      <td>2.787999</td>\n",
              "      <td>5.091549</td>\n",
              "      <td>5.336097</td>\n",
              "      <td>0.153106</td>\n",
              "      <td>0.471988</td>\n",
              "      <td>5.301371</td>\n",
              "      <td>5.330321</td>\n",
              "      <td>5.333664</td>\n",
              "      <td>0.697826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2.113312</td>\n",
              "      <td>2.113452</td>\n",
              "      <td>2.522747</td>\n",
              "      <td>2.537897</td>\n",
              "      <td>2.541049</td>\n",
              "      <td>5.087210</td>\n",
              "      <td>5.073034</td>\n",
              "      <td>2.124071</td>\n",
              "      <td>2.439843</td>\n",
              "      <td>2.143993</td>\n",
              "      <td>2.133296</td>\n",
              "      <td>2.144891</td>\n",
              "      <td>5.337724</td>\n",
              "      <td>2.786646</td>\n",
              "      <td>5.090955</td>\n",
              "      <td>5.335606</td>\n",
              "      <td>0.153793</td>\n",
              "      <td>0.471309</td>\n",
              "      <td>5.303428</td>\n",
              "      <td>5.333664</td>\n",
              "      <td>5.333664</td>\n",
              "      <td>0.695957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2.108888</td>\n",
              "      <td>2.109518</td>\n",
              "      <td>2.518194</td>\n",
              "      <td>2.536336</td>\n",
              "      <td>2.539895</td>\n",
              "      <td>5.085608</td>\n",
              "      <td>5.072304</td>\n",
              "      <td>2.123129</td>\n",
              "      <td>2.439179</td>\n",
              "      <td>2.143291</td>\n",
              "      <td>2.131471</td>\n",
              "      <td>2.144259</td>\n",
              "      <td>5.337934</td>\n",
              "      <td>2.788236</td>\n",
              "      <td>5.090289</td>\n",
              "      <td>5.335058</td>\n",
              "      <td>0.154726</td>\n",
              "      <td>0.470445</td>\n",
              "      <td>5.305413</td>\n",
              "      <td>5.333664</td>\n",
              "      <td>5.333664</td>\n",
              "      <td>0.693063</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         0         1         2   ...        19        20        21\n",
              "0  2.120977  2.121060  2.556583  ...  5.308413  5.308413  0.677635\n",
              "1  2.115946  2.116377  2.553877  ...  5.308474  5.308557  0.686576\n",
              "2  2.112439  2.112391  2.552291  ...  5.312304  5.317366  0.690770\n",
              "3  2.111593  2.111070  2.548805  ...  5.316054  5.325765  0.693625\n",
              "4  2.110920  2.110542  2.544003  ...  5.319728  5.333664  0.695207\n",
              "5  2.113144  2.112629  2.538742  ...  5.323329  5.333664  0.697131\n",
              "6  2.114370  2.114296  2.534597  ...  5.326859  5.333664  0.698160\n",
              "7  2.113213  2.113616  2.527973  ...  5.330321  5.333664  0.697826\n",
              "8  2.113312  2.113452  2.522747  ...  5.333664  5.333664  0.695957\n",
              "9  2.108888  2.109518  2.518194  ...  5.333664  5.333664  0.693063\n",
              "\n",
              "[10 rows x 22 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 228
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eah20FyiTJ7c"
      },
      "source": [
        "**1. Séparation des données en données pour l'entrainement et la validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnhC-39gTJ7d"
      },
      "source": [
        "On réserve 20% des données pour l'entrainement et le reste pour la validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwXIe8_ETJ7d",
        "outputId": "d9615a8d-0576-4bec-afa5-b0e5a915fa9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Sépare les données en entrainement et tests\n",
        "pourcentage = 0.8\n",
        "temps_separation = int(len(X_Avec_Prix_norm) * pourcentage)\n",
        "date_separation = X_Avec_Prix_norm.index[temps_separation]\n",
        "\n",
        "serie_entrainement_X = []\n",
        "serie_test_X = []\n",
        "\n",
        "for column in X_Avec_Prix_norm:\n",
        "  serie_entrainement_X.append(np.array(X_Avec_Prix_norm[column][:temps_separation],dtype=np.float32))\n",
        "  serie_test_X.append(np.array(X_Avec_Prix_norm[column][temps_separation:],dtype=np.float32))\n",
        "\n",
        "print(\"Taille de l'entrainement : %d\" %len(serie_entrainement_X[0]))\n",
        "print(\"Taille de la validation : %d\" %len(serie_test_X[0]))"
      ],
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Taille de l'entrainement : 1946\n",
            "Taille de la validation : 487\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8g7cFxmTJ76",
        "outputId": "881ec9a4-b458-4eca-d749-4bd229651a21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        }
      },
      "source": [
        "# Affiche la série X\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "\n",
        "for i in range(0,len(X_Avec_Prix_norm.columns)):\n",
        "  ax.plot(X_Avec_Prix_norm.index[:temps_separation].values,serie_entrainement_X[i], label=\"X_Ent\")\n",
        "  ax.plot(X_Avec_Prix_norm.index[temps_separation:].values,serie_test_X[i], label=\"X_Val\")\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/pylabtools.py:125: UserWarning:\n",
            "\n",
            "constrained_layout not applied.  At least one axes collapsed to zero width or height.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wr9CM2ZmYNl0"
      },
      "source": [
        "# Création des datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Za_l_dloebyZ"
      },
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "# Sans les prix\n",
        "\n",
        "def prepare_dataset_XY(series, taille_fenetre, horizon, batch_size):\n",
        "  serie_concat = tf.expand_dims(series[0],1)\n",
        "\n",
        "  for i in range(1,len(series)):\n",
        "    serie_ = tf.expand_dims(series[i],1)\n",
        "    serie_concat = tf.concat([serie_concat,serie_],1)\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie_concat)\n",
        "  dataset = dataset.window(taille_fenetre+horizon, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre + horizon))\n",
        "  dataset = dataset.map(lambda x: (x[0:taille_fenetre][:,1:],tf.expand_dims(x[-1:][:,0],1)))\n",
        "\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "  return dataset"
      ],
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nK6G5AVZYNmU"
      },
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "\n",
        "def prepare_dataset_XY(series, taille_fenetre, horizon, batch_size):\n",
        "  serie_concat = tf.expand_dims(series[0],1)\n",
        "\n",
        "  for i in range(1,len(series)):\n",
        "    serie_ = tf.expand_dims(series[i],1)\n",
        "    serie_concat = tf.concat([serie_concat,serie_],1)\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie_concat)\n",
        "  dataset = dataset.window(taille_fenetre+horizon, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre + horizon))\n",
        "  dataset = dataset.map(lambda x: (x[0:taille_fenetre],tf.expand_dims(x[-1:][:,0],1)))\n",
        "\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "  return dataset"
      ],
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqoAHg2MYNmW"
      },
      "source": [
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "taille_fenetre = 16\n",
        "horizon = 1\n",
        "batch_size = 1\n",
        "\n",
        "# Création du dataset\n",
        "dataset = prepare_dataset_XY(serie_entrainement_X,taille_fenetre,horizon,batch_size)\n",
        "dataset_val = prepare_dataset_XY(serie_test_X,taille_fenetre,horizon,batch_size)"
      ],
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USVHzg-GYNmX",
        "outputId": "0ee69ecf-170b-420b-f311-c1b489647259",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(len(list(dataset.as_numpy_iterator())))\n",
        "for element in dataset.take(1):\n",
        "  print(element[0].shape)\n",
        "  print(element[1].shape)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1943\n",
            "(1, 3, 21)\n",
            "(1, 1, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OA6tsfl4YNmY",
        "outputId": "184446e9-9da5-4946-b1f6-569e013b78c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for element in dataset.take(2):\n",
        "  print(element)\n"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(<tf.Tensor: shape=(1, 3, 21), dtype=float32, numpy=\n",
            "array([[[2.1210604 , 2.556583  , 2.5493283 , 2.5566945 , 5.100684  ,\n",
            "         5.0747848 , 2.131413  , 2.4523394 , 2.1494317 , 2.1482837 ,\n",
            "         2.1495404 , 5.3129673 , 2.7812536 , 5.096126  , 5.3087716 ,\n",
            "         0.14421685, 0.47883856, 5.2856827 , 5.3084126 , 5.3084126 ,\n",
            "         0.677635  ],\n",
            "        [2.116377  , 2.553877  , 2.5483816 , 2.554936  , 5.0988727 ,\n",
            "         5.0745687 , 2.130454  , 2.4518752 , 2.148759  , 2.1463275 ,\n",
            "         2.1490395 , 5.312724  , 2.7798512 , 5.095481  , 5.308679  ,\n",
            "         0.14601283, 0.47782165, 5.2871532 , 5.308474  , 5.308557  ,\n",
            "         0.68657553],\n",
            "        [2.1123915 , 2.5522914 , 2.5478327 , 2.55358   , 5.0969977 ,\n",
            "         5.075265  , 2.1293447 , 2.4516406 , 2.1480343 , 2.1442454 ,\n",
            "         2.1484072 , 5.3212647 , 2.779364  , 5.0947995 , 5.323361  ,\n",
            "         0.14728597, 0.4766597 , 5.2896023 , 5.3123045 , 5.317366  ,\n",
            "         0.6907703 ]]], dtype=float32)>, <tf.Tensor: shape=(1, 3, 1), dtype=float32, numpy=\n",
            "array([[[2.1159463],\n",
            "        [2.1124394],\n",
            "        [2.1115928]]], dtype=float32)>)\n",
            "(<tf.Tensor: shape=(1, 3, 21), dtype=float32, numpy=\n",
            "array([[[2.116377  , 2.553877  , 2.5483816 , 2.554936  , 5.0988727 ,\n",
            "         5.0745687 , 2.130454  , 2.4518752 , 2.148759  , 2.1463275 ,\n",
            "         2.1490395 , 5.312724  , 2.7798512 , 5.095481  , 5.308679  ,\n",
            "         0.14601283, 0.47782165, 5.2871532 , 5.308474  , 5.308557  ,\n",
            "         0.68657553],\n",
            "        [2.1123915 , 2.5522914 , 2.5478327 , 2.55358   , 5.0969977 ,\n",
            "         5.075265  , 2.1293447 , 2.4516406 , 2.1480343 , 2.1442454 ,\n",
            "         2.1484072 , 5.3212647 , 2.779364  , 5.0947995 , 5.323361  ,\n",
            "         0.14728597, 0.4766597 , 5.2896023 , 5.3123045 , 5.317366  ,\n",
            "         0.6907703 ],\n",
            "        [2.1110697 , 2.5488055 , 2.5466356 , 2.5513637 , 5.09513   ,\n",
            "         5.0756683 , 2.1282508 , 2.4523764 , 2.1473079 , 2.1421869 ,\n",
            "         2.1478436 , 5.3272495 , 2.781434  , 5.094109  , 5.330891  ,\n",
            "         0.14914803, 0.4755263 , 5.2920184 , 5.3160543 , 5.3257647 ,\n",
            "         0.6936247 ]]], dtype=float32)>, <tf.Tensor: shape=(1, 3, 1), dtype=float32, numpy=\n",
            "array([[[2.1124394],\n",
            "        [2.1115928],\n",
            "        [2.1109197]]], dtype=float32)>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAwyM48zYNmZ"
      },
      "source": [
        "On extrait maintenant les deux tenseurs (X,Y) pour l'entrainement :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q87LZFryIx2b"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset))              # #60x((32,16,22),(32,1,1)) => x = 60x(32,16,22) ; y = 60x(32,1,1)\n",
        "\n",
        "# Recombine les données\n",
        "x = np.asarray(x,dtype=np.float32)      # 60x(32,16,22) => (60,32,16,22)\n",
        "y = np.asarray(y,dtype=np.float32)      # 60x(32,1,1) => (60,32,1,1)\n",
        "\n",
        "x_train = np.asarray(tf.reshape(x,shape=(x.shape[0]*x.shape[1],taille_fenetre,x.shape[3])))     # (60,32,16,22) => (60*32,16,22)\n",
        "\n",
        "y_train = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],1,y.shape[3])))     # (60,32,1,1) => (60*32,1,1)"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQlnU0USYNma"
      },
      "source": [
        "Puis la même chose pour les données de validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBroUa5XYNma"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset_val))              # #60x((32,16,22),(32,1,1)) => x = 60x(32,16,22) ; y = 60x(32,1,1)\n",
        "\n",
        "# Recombine les données\n",
        "x = np.asarray(x,dtype=np.float32)      # 60x(32,16,22) => (60,32,16,22)\n",
        "y = np.asarray(y,dtype=np.float32)      # 60x(32,1,1) => (60,32,1,1)\n",
        "\n",
        "x_val = np.asarray(tf.reshape(x,shape=(x.shape[0]*x.shape[1],taille_fenetre,x.shape[3])))     # (60,32,16,22) => (60*32,16,22)\n",
        "y_val = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],1,y.shape[3])))     # (60,32,1,1) => (60*32,1,1)"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhAeAUHhjXoa",
        "outputId": "f46871b3-7c61-4daa-bf6c-607c86d59c97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1929, 16, 19)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08Dq1l0CjmmO",
        "outputId": "96168050-46a5-4653-8069-4041a65ee932",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Première fenêtre de la variable 0\n",
        "print(x_train[0,:,0])\n",
        "\n",
        "# Deuxième fenêtre de la variable 0\n",
        "print(x_train[1,:,0])\n"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.1210604 2.116377  2.1123915]\n",
            "[2.116377  2.1123915 2.1110697]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rgeeq1GkOOR"
      },
      "source": [
        "# Toutes les fenêtres de la variable 0\n",
        "print(x_train[:,:,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAk4Q_VoiAnj"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XP9pVs_o-G7K",
        "outputId": "e21f2d87-f4b1-4700-ca0c-7699fe3b9566",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Première valeur de la sortie\n",
        "y_train[:,:,0]"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.1115928],\n",
              "       [2.1109197],\n",
              "       [2.1131437],\n",
              "       ...,\n",
              "       [2.603084 ],\n",
              "       [2.60296  ],\n",
              "       [2.602588 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVPcKZoxHBuy"
      },
      "source": [
        "**1. One-hot Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7tuSZfqHXGX",
        "outputId": "5452b0ad-47ba-4a3d-a81f-ab1da4f8265a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, KBinsDiscretizer\n",
        "\n",
        "en = KBinsDiscretizer(n_bins=int(mu+1), encode='onehot').fit(y_train[:,0])\n",
        "y_train_OneHot = en.transform(y_train[:,:,0])\n",
        "y_train_OneHot = np.array(y_train_OneHot.todense())\n",
        "y_train_OneHot.shape"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1929, 256)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcFLwCiR5cSl",
        "outputId": "f0b96772-f373-4b1a-f860-78db690df1d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_val_OneHot = en.transform(y_val[:,:,0])\n",
        "y_val_OneHot = np.array(y_val_OneHot.todense())\n",
        "y_val_OneHot"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPpqMaoG6pqP",
        "outputId": "19d776a7-6e2e-439a-d6ae-abeb372bef92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "decode = en.inverse_transform(y_train_OneHot)\n",
        "decode"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.11996815],\n",
              "       [2.11996815],\n",
              "       [2.10349416],\n",
              "       ...,\n",
              "       [2.60360725],\n",
              "       [2.60360725],\n",
              "       [2.60275722]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKYBt_Wt6zxF",
        "outputId": "f27a7868-6641-49be-873e-f569e3611bba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        }
      },
      "source": [
        "# Affiche la série X\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "\n",
        "ax.plot(X_Avec_Prix_norm.index[:temps_separation].values,serie_entrainement_X[0], label=\"X_Ent\")\n",
        "ax.plot(X_Avec_Prix_norm.index[temps_separation:].values,serie_test_X[0], label=\"X_Val\")\n",
        "\n",
        "ax.plot(X_Avec_Prix_norm.index[taille_fenetre:temps_separation].values,decode[:,0], label=\"X_Ent\")\n",
        "\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABEAAAAFwCAYAAAC4vQ5FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeXRV1dnH8e+5NzfzPEEggYQoowgIAiKCE4qKOCtawaGVFq1FKlZea1ttrdbSKrVqrVhUnHACB8QBFRBUQNCEKUxhhoSEJGSe7r3n/eOEhJCRTDckv89aWeecffbZ57ksYCVP9t6PYZomIiIiIiIiIiIdmc3TAYiIiIiIiIiItDYlQERERERERESkw1MCREREREREREQ6PCVARERERERERKTDUwJERERERERERDo8JUBEREREREREpMPz8tSLIyMjzfj4eE+9XkREREREREQ6mPXr1x8xTTOqtnseS4DEx8ezbt06T71eRERERERERDoYwzD21nVPS2BEREREREREpMNTAkREREREREREOjwlQERERERERESkw/PYHiC1KS8v58CBA5SUlHg6FI/w9fUlNjYWh8Ph6VBEREREREREOpR2lQA5cOAAQUFBxMfHYxiGp8NpU6ZpkpWVxYEDB0hISPB0OCIiIiIiIiIdSrtaAlNSUkJERESnS34AGIZBREREp539IiIiIiIiItKa2lUCBOiUyY9jOvNnFxEREREREWlN7S4BIiIiIiIiIiLS0hpMgBiG4WsYxlrDMJINw9hsGMajtfTxMQzjbcMwdhqGscYwjPjWCLa17d+/n4SEBLKzswHIyckhISGBPXv21Oi7Z88e/Pz8GDx4cOXX/Pnz6x3/gw8+YMuWLa0RuoiIiIiIiIjUozEzQEqBC03THAQMBsYbhjHyhD4/B3JM0zwNeBp4smXDbBtxcXFMmzaNWbNmATBr1iymTp1KfHx8rf0TExNJSkqq/JoyZUq94ysBIiIiIiIiIuIZDVaBMU3TBAoqLh0VX+YJ3a4CHqk4fw941jAMo+LZJnn0481sOZTX1Mdr1b9bMH+6ckC9fWbMmMHQoUOZM2cOq1at4tlnnz3p9wQGBjJ9+nQWL16Mn58fH374IampqXz00UesWLGCxx57jPfff5/ExMSmfhQREREREREROQmN2gPEMAy7YRhJQAaw1DTNNSd06Q7sBzBN0wnkAhG1jDPVMIx1hmGsy8zMbF7krcThcDB79mxmzJjBnDlzcDgcdfZNTU2ttgRm5cqVABQWFjJy5EiSk5MZM2YMc+fOZdSoUUycOJHZs2eTlJSk5IeIiIiIiIhIG2pwBgiAaZouYLBhGKHAIsMwzjBNc9PJvsw0zReBFwGGDRtW7+yQhmZqtKZPP/2UmJgYNm3axLhx4+rsd2wJzIm8vb2ZMGECAEOHDmXp0qWtFquIiIiIiIjISTu6D5LfhnN/A14+no6mTZxUFRjTNI8Cy4DxJ9w6CMQBGIbhBYQAWS0RYFtLSkpi6dKlrF69mqeffpq0tLSTHsPhcFSWtLXb7TidzpYOU0RERERERKTplv4Rlj0G3z3j6UjaTGOqwERVzPzAMAw/YByw9YRuHwG3VZxfD3zdnP0/PMU0TaZNm8acOXPo0aMHDzzwADNnzmyx8YOCgsjPz2+x8UREREREREQAcLutr8YwTdizyjr/9t+Qn956cbUjjZkBEgMsMwxjA/AD1h4giw3D+LNhGBMr+vwPiDAMYyfwW2BW64TbuubOnUuPHj0ql73cfffdpKSksGLFilr7n7gHyDPP1J85mzRpErNnz2bIkCGkpqa2ePwiIiIiIiLSSb1/JzwRW/f94+corPwHFGZC3wlQmguHam7t0BEZnpqoMWzYMHPdunXV2lJSUujXr59H4mkv9GcgIiIiIiIiJ+2REOt428ew9zs4dzo4/KzExw8vwbLHYcQvodf5MO9Sq+/Pl8L/xsFVz8GQWz0VeYsyDGO9aZrDarvXqE1QRURERERERKSdKjxuC85Xr7SOIbFWUiPlY1hSsbXD8iesL4BffQvhCRXPt88qrS3tpDZB7Yw2btxYbZnL4MGDGTFihKfDEhERERERkc7C7YJXJsDKf9Z+P/Xrmm2r/2PtCZLyEfiFwy+O6+MbAl0GgHeAdb3t05aPuR3SDJAGDBw4sNZStyIiIiIiIiJtYt/3sGel9XXe/dXvuV2w/HEI7AIFh6220b+FVU/Bzi9h6ydw5o0QOxT+cAS+fw76XA4VlUsB2L8GNrxj9evANANEREREREREpD3L3lV1fmKllz2rrPvjn4AuAyFuBAy4xrr35g1QXgwjplnXdgeMvg+ielc9f/sS6/jFHyB9E7x7O2R1zKIdSoCIiIiIiIiItGc5e6vOS45Wv5f6NdgccPqlcNfXcPsnEN2/6n6XARDdt+6x48+FCXOgIN3aLHXzInD4t2z87YQSICIiIiIiIiLtWebWqvMTEyBZOyG8F/gEgpe3NcvD7gUXP2rdD4tvePzoikqkyQsgPBGCY1ok7PZGCRARERERERGR9qokz9rLIyTOui4+IQGSux9Ce9R87lji44xrG35HYBfr6CyGyN719z2FKQFynP3795OQkEB2djYAOTk5JCQksGfPnhp9e/XqxbZt26q13XfffTz55JN1jh8fH8+RI0daNGYRERERERHpwLZ/Bs4SGH6XdV2cYx33fg9H90H2HgiNq/lc/6usUrdnXNfwO4K6Vp2HxDY75PZKCZDjxMXFMW3aNGbNmgXArFmzmDp1KvHx8TX6Tpo0iQULFlReu91u3nvvPSZNmtRW4YqIiIiIiEhHd2AdOALgtIut69evtfYEeXk8zBkIpblw2riazxkGdD2jce9w+FWdB0Q2P+Z2qv2Wwf10FqRvbNkxuw6Ey/5Wb5cZM2YwdOhQ5syZw6pVq3j22Wdr7XfzzTdz00038ac//QmAb775hp49e9KzZ0+uvvpq9u/fT0lJCdOnT2fq1Kkt+zlERERERESk48veBWv/C7Fng39EVfuGt6vOvXyh9/jmv+vyf8D3z9aeTOkg2m8CxEMcDgezZ89m/PjxfPHFFzgcjlr7DRw4EJvNRnJyMoMGDWLBggXcfPPNAMybN4/w8HCKi4s5++yzue6664iIiKh1HBEREREREZEaXOXwzBDrPGaQtU/HiGmw5j+Q9EZVP59gsLXA4o7hd1Uts+mg2m8CpIGZGq3p008/JSYmhk2bNjFuXN3Zr5tvvpkFCxYwYMAAPvjgAx591Npl95lnnmHRokWAta/Ijh07lAARERERERGRxinKhi8erro+59fWkpZLHrMSIDl7IKgbYMLE2lctSE3tNwHiIUlJSSxdupTVq1czevRoJk2aRExM7SWAJk2axCWXXMLYsWM588wz6dKlC8uXL+fLL7/k+++/x9/fn/PPP5+SkpI2/hQiIiIiIiJyynr3dti9wjqfuRMCo6xz+3E/wkckwu2L2zy0U5k2QT2OaZpMmzaNOXPm0KNHDx544AFmzpxZZ//ExEQiIyOZNWtW5fKX3NxcwsLC8Pf3Z+vWraxevbqtwhcREREREZGOID/NOl70x6rkxzHXzrWOrrK2jakDUALkOHPnzqVHjx6Vy17uvvtuUlJSWLFiRZ3P3HzzzWzdupVrr7VqK48fPx6n00m/fv2YNWsWI0eObJPYRUREREREpIMIT4SQODjv/pr3up1lHXuOatuYOgDDNE2PvHjYsGHmunXrqrWlpKTQr18/j8TTXujPQEREREREpJN7+XLreMeS2u8f3gyRfaoviREADMNYb5rmsNruaQaIiIiIiIiISHtSmmdVd6lLlwFKfjSB/sQasHHjRiZPnlytzcfHhzVr1ngoIhEREREREenQSvIgKsjTUXQ4SoA0YODAgSQlJXk6DBEREREREeksSvPBRwmQlqYlMCIiIiIiIiKtKflt2PBO4/q6XVCSC36hrRtTJ6QZICIiIiIiIiKtpTgHFk21zvtcDj6B9ff/6F4wXRDas/Vj62Q0A0RERERERESktaQlV53n7AGXs/7+SW9Yx9C4Vgups1ICRERERERERKS1ZG6vOn/hXPjPOXX3dburzqP6tV5MnZQSIMfZv38/CQkJZGdnA5CTk0NCQgJ79uyp0XfPnj34+fkxePDgyq/58+fXO/4HH3zAli1bWiN0ERERERERaY+yd1W/PrIdyotr77v9U+t42WwIjmnduDohJUCOExcXx7Rp05g1axYAs2bNYurUqcTHx9faPzExkaSkpMqvKVOm1Du+EiAiIiIiIiKdTGFGzbZ/D4P1r9RsXzcP7D7Q78pWD6szareboD659km2Zm9t0TH7hvflweEP1ttnxowZDB06lDlz5rBq1SqeffbZk35PYGAg06dPZ/Hixfj5+fHhhx+SmprKRx99xIoVK3jsscd4//33SUxMbOpHERERERERkVNBQQYExUB+WlVb3gH4eDoMvb2qzTRh32o4a7Jmf7QSzQA5gcPhYPbs2cyYMYM5c+bgcDjq7JuamlptCczKlSsBKCwsZOTIkSQnJzNmzBjmzp3LqFGjmDhxIrNnzyYpKUnJDxERERERkc6gMBPCe9V+zzSto8sJeYegrACi+rZdbJ1Mu50B0tBMjdb06aefEhMTw6ZNmxg3blyd/Y4tgTmRt7c3EyZMAGDo0KEsXbq01WIVERERERGRdqwgA7oPg73fWtcX/B7SN0LKR1CUBQGR8Fg0eFeUx60rWSLNphkgJ0hKSmLp0qWsXr2ap59+mrS0tIYfOoHD4cAwDADsdjtOZwNljkRERERERKTjcZVDcXb1kra9L4Uzb7TOcw9Ys0BMF5TmWm2aAdJqlAA5jmmaTJs2jTlz5tCjRw8eeOABZs6c2WLjBwUFkZ+f32LjiYiIiIiISDvldsNHv7HOA6Lg5gUwYzPEDILg7lb7wfWQ9EbVMzYHhHRv+1g7CSVAjjN37lx69OhRuezl7rvvJiUlhRUrVtTa/8Q9QJ555pl6x580aRKzZ89myJAhpKamtnj8IiIiIiIi0k5kbIbkN63zwGjocxmExFrXx46f/BY+vKfqmUlvtm2MnYxhHtt0pY0NGzbMXLduXbW2lJQU+vXr55F42gv9GYiIiIiIiHQAe7+Dly+zzu/8AnqMqLpnmvBoaPX+N72u8rctwDCM9aZpDqvtnmaAiIiIiIiIiLS0wsyq86g+1e8ZBgR2qd7mF976MXVy7bYKTHuxceNGJk+eXK3Nx8eHNWvWeCgiERERERERafeOJUBueQf8Qmvev+NTKDwC8y6xrrsObLvYOiklQBowcODAWkvdioiIiIiIiNSpIAMwIPHC2u9HJFpfty+xyuH6BrdpeJ1Ru0uAmKZZWUK2s/HUfiwiIiIiIiLSwrJSrfK3dkf9/eLPbZt4pH3tAeLr60tWVlanTASYpklWVha+vr6eDkVERERERESaK2sHRJzu6SjkOO1qBkhsbCwHDhwgMzOz4c4dkK+vL7GxsZ4OQ0RERERERJojIwXSNsDYBz0diRynXSVAHA4HCQkJng5DREREREREpOm2fGQdz/6FZ+OQatrVEhgRERERERGRU97+1dD1DAiM8nQkchwlQERERERERERaSloypH6t/T/aISVARERERERERFrKa9dYR5W1bXeUABERERERERFpCW4XFGVZ54Nu8WwsUoMSICIiIiIiIiItobCiounl/4AeIzwbi9SgBIiIiIiIiIhIS8g7ZB2Du3s2DqmVEiAiIiIiIiIiLSFnj3UMjfNoGFI7JUBEREREREREWkJ2qnUM7+XZOKRWXp4OQEREREREROSUZprgLIWMrRDUDbwDPB2R1EIJEBEREREREZHmeGcKpHxknceN9GwsUqcGl8AYhhFnGMYywzC2GIax2TCM6bX0CTEM42PDMJIr+tzROuGKiIiIiIiItCMZKVXJD4Dhd3kuFqlXY2aAOIH7TdP80TCMIGC9YRhLTdPcclyfe4AtpmleaRhGFLDNMIw3TNMsa42gRURERERERNqFVXPAyxcmPA3R/aHbYE9HJHVoMAFimmYakFZxnm8YRgrQHTg+AWICQYZhGEAgkI2VOBERERERERHpuNKSIPFCGHyLpyORBpzUHiCGYcQDQ4A1J9x6FvgIOAQEATeZpulugfhERERERERE2q+SPPAP93QU0giNLoNrGEYg8D5wn2maeSfcvhRIAroBg4FnDcMIrmWMqYZhrDMMY11mZmYzwhYRERERERFpB0pywTfU01FIIzQqAWIYhgMr+fGGaZoLa+lyB7DQtOwEdgN9T+xkmuaLpmkOM01zWFRUVHPiFhEREREREfEsVzmUF4JviKcjkUZoTBUYA/gfkGKa5lN1dNsHXFTRvwvQB9jVUkGKiIiIiIiItDslFYsjlAA5JTRmD5BzgcnARsMwkiraHgJ6AJim+QLwF+AVwzA2AgbwoGmaR1ohXhEREREREZH2oeSodVQC5JTQmCowq7CSGvX1OQRc0lJBiYiIiIiIiLR7eQetY2C0Z+OQRmn0JqgiIiIirW1vViG/ey+ZknKXp0MRERGpbueXsHkRFGRAcY7Vtr+iQGrMYM/FJY12UmVwRURERFrTP77YzsfJhxjTO4oJZ3bzdDgiItLZ7f4G1s6FcX+G16+rao84Ha56Fpb/DWLPVhncU4QSICIiItJu+HpZk1M3HshVAkRERDzH7YZFU2Hju9Z13Ijq97N2wGezwO2E619u+/ikSbQERkRERNqNIwWlAHyXmuXhSEREpFM79GNV8gNgxxdg94aHM2Hmjoo+P0GPURAa55kY5aQpASIiIiIetyNnB1csvIKfzFkEJP6d3T6PsCd3T41+X+39il99+au2D1BERDoP04SP77POT7vYOu5eAV3OAC9va8NT31CrPairZ2KUJlECRERERDzuuaTn2Je/D5c9E9MVgOFzmJ8Ob6zR777l9/HtwW8pdZV6IEoREekUDv4IhzdC9AAY/2RVe/ezqs5/8xPEjYTRM9o+PmkyJUBERETE4wyMyvMe9isAOJSXXWf/vNK8Vo9JREQ6qcObrOM1L0BAZFV7t+MSIP7h8PPPIebMto1NmkUJEBEREfE4l2lWng+JSQTgcOHROvvnlua2ekwiItJJZW4FLz9ryYtfaFV7/GjPxSQtQgkQERER8bg9R4oqz89L6IXpdnC4oO4ZID/sP9AWYYmISGeUkQJRvcFW8ePyNS/CmAcgrKdn45JmUwJEREREPOrNdZvZm1012+OC03uC259Dedk43U7SC9NrPLPhUFpbhigiIp2BacKGd+HgeojqV9U+6Ca48GHPxSUtRgkQERER8RjTNHli8yTcfimVbX7ePngbAeSU5vHEmicY9944CssLqz2XUaQyuSIi0sJ2r4CFv4DSPIgd5ulopBUoASIiIiIek55ftZmpL9EsuWYJAA7Dn3J3AR/s/BA4btNT0w5AZnHNWSEiIiJN4nKC2w2bF1nX/pHQ/yrPxiStwsvTAYiIiEjntfHw7srzcK9exAXHAeBtCyTfvQ+X2yp3u+/oEfy9AsFwAZBZtq3tgxURkY7p+RFQlA3F2TDoZqv6i3RImgEiIiIiHrMja3/luY8toPLc2+aLy1a1CWpafg6XvD+u8rrAtpX9edoIVUREmslZClk7reQHwFm3eTYeaVVKgIiIiIjH5JYWVJ6XOEsrz92UVOuXXpBDkdPaByTSGArAt/s2t0GEIiLSoeVWJNPt3nD9POh5jmfjkValBIiIiIh4TEFZVfnbYlfVRqdl7qJq/T7dsqvy/ML4UQBsztiNiIhIs+TssY6TP4AzrvNoKNL6lAARERERjykqr0p0DOjSvfLc5g6t1m9n+buV50O7J2C6HXyw/zkGvjqQeZvmtX6gIiLSMR1LgIT19GgY0jaUABERERGPKXZaS10eHv4oT138h8r20KKbT+hpYJp2fG0hXNbrYuyuyMo7c9bPaYtQRUSkIzq611r+EhTj6UikDSgBIiIiIh5T7CzGNA2u6301/g7/yvYZFw3EXRYBgN0diuGVj2G4GBo6AcMwCHKEV/b1Pm7zVBERkUZxu2HNi5D0FoTEgc3u6YikDSgBIiIiIh5T4iwB04GXvfq3JBf27UKMXy8Auvj1wDBMAPpGxwIwIrZPZV+bO7CNohURkQ6hrAi+ehQ+fQAKMyDxAk9HJG1ECRARERHxmBJXMYbpXeu9N67+J3PGzGVY1LmVbb8ccgMAfx7zO1665CUMtz8+tpA2iVVERDqIBTfDtxXLJ4dPhXOnezYeaTNeng5AREREOq8yd0mdCZAugWF0CRzJgfwDcAD87AH4efsAEOAIYETMCHycp1Fmy2nLkEVE5FRVlA0r/wm7llvXd3wKPUd5NCRpW5oBIiIiIh5T5irFhk+9fXqGWRuehvnWnOnhYwug3CzgP8n/YXvO9laJUaQjennTq/x0OKla24KtC3hizROYpumhqERa2YJb4Ptnodf5MHOnkh+dkBIgIiIi4jGlZiF2fOvt0z+iP0Oih/DrIb+ucc/H7k85uTyf9DxTltzWWmGKdDhPrf8HUz6bXK3tr2v+yptb3+Rg/mEPRSXSitxuSEuG3uNhyocQGOXpiMQDtARGREREPKbElYevLbLePtH+0cy/bH6t93y9fMHlBqDQWdDi8Yl0RCXlpTXanG5n5fmu7Axig7u2ZUgire/oXigvgj6XezoS8SDNABERERGPKSefQEdwk5/39/JrwWhEOofDBbmV5+XucgAyCjMr2/bnZjQ4RpmrjOySbIqdxS0foHQ+Wz6Ep/pDXlrrvSNzq3WM7t9675B2TwkQERER8Qi3243bKCDUO6zJY/g7/FswIpHOIT3/aOX5Fe9dT6nTxebMfZVth/KPNDjGyDdHMvbtsVy18MZWiVE6uOKjsGcVLJpmbUj6zhTIOwjf/B3crqp+P7wEW5e0zDsztljHqD7195MOTUtgRERExCMO5OZg2FyE+zU9ARLgqD4DpNzp5Ko3/0zfiNN46oopdT6XW1SOw8vA37vqW6GScuubbl+HvcnxiJwKMgqrZoCkFe9i2DN/wOmzHa9Aq23z4YP1Pr8lY2/lzJG04j1sPbKbvpEJrRavdCC7VkBoHLwwBsryrbbkN6vur5tnfY37M/QcDZ/cb7U/kltzrJO151sITwTfps86lFOfZoCIiIiIR7y3+TsAzorp2+QxgnyqzwD5v6Wvst9cxNIjs+t97pxXr2HsvOmV13tzMhj66lhG/29qk2MROVVkFp3ww2TEx3gFbgPA7fRnfcErbDy8s87nl+/eCFBZwvrXS/7ROoFKx1KcA/MnwjNDrOSHX3jVvZ+9D7P2QfQA63rpH+GlC6vu56ef/PtcTlgxG+ZfBe/9HFK/gv5XNe8zyClPCRARERHxiDe2vAVuH24aOLbJYwT7BFS7/jxjTuV5en5Orc84XU7s/nsp8fumsu3nn8zC5sil1G+tSoBKh3ekYgbIyLCba9y7o+99AHy09fs6nz+QZ1WJmXvhAuyuKA6b3/BhyretEKl0KPvWVJ0HdYMHdsKkt+Cyv8NpF4FvCNyxBO783DoP7wVXPW/1z9x28u/7dg4sewwO/QSb3ofg7nBOzWpi0rkoASIiIiJtbkP6Hkq9NzEi4mqCfQKbPI6fl1VC1+YKrXFv7YHttT6z8fDuGm1HylIrz7cfacJvGkVOIVuzrH8DY3sMq9be3a8fvzjrakzTxrbs1NoeBeBwxYapfaK68c6VbwDwyY5VrRStdAi7v4G3brLOz7geLv872OzQ93IY8UswDOueXyj0GAn3bYRfrYLEC6z2Iyf8f15aAKvmwOoXqtpczup9dn4Fkb3hwb0wc4c1XkBE63w+OWUoASIiIiJt7qtdP2IYJpcmXNCscYrKrQoUYV69Ktti/awp1MmHd9T6zLzkhZXnWYXFHMzLxGXPxtdljfHjobqn/ouc6g7mZfFD7mvYXKGcE3dmZXuUbzc+uHY+oX5+2J2RHCjYQ5mrjLSC6lU5TNPkh6PvgNuHUL8AekfFgMuPtMLDFJU6mf/Des2ikpqSKvb5uOZFuP5/0O/K+vv7hoB3AATFgJevVcL2mIJM+PdZ8OWf4LMHIS0ZkhfAY9HWUpe8Q7B7JexfDb3HW8mVwCjwD6/7fdJpKAEiIiIibe5ArjWFvl9Uj2aNM6RbPAAX9BjD4IBbuSLmft6c8BIAu3MO1OhfUFrCsvQFlddrD2zlF5/MAuC8rpcDsCN7X43nRDqKh796EQw3F3W/jp6hUZXtL42bj2/FjKoge3dyyg/wqyWPcsn7l7Du0NbKfj8c2I5plBNkVG166iCM7JJMrl7wf8zecjtf7NhY7Z2vJi/iktfuZdKb/1ZypLPK2QM9z4VBN53cc4YBIbFwdH9V24a3oeCwtVGqYYe3J8OiX4Lpgk3vwVP94NUJYLphwNUt+jHk1KcqMCIiItKmfvf5XL7IsNZ1nxbRpVljXdFnJP26fExCSE+MY1OoAdz+pBXWXMqy+fA+DJuT030msKN0Me9v+5QDZWvxccXx4HmTWLrwWfbl1UyciHQUGcXp4ArkqUvvq9bePbjqt+Mx/vGkFCfxQ/ZHANz/5WOsmPI6AJ/sWA3AH895qLJ/kFckR8syybMlAbDu0GZ2HE1hTI+hnNn1NJ7+cQ4uWzZp7uVMXLCLa/uNJunwDn47Ygo9Q7u16udtF9I3QeTp4OXj6Ug8J2cPJF7YYLdahcRaM0DcLqss7urnocsZcO50q7LLjs8hsAvcthi+eNi67jHKWl7TfWiLfgw59SkBIiIiIm3qs4PzoKLSrK/Du9nj9QqNr9HmSzg5pZk12lOOWLM7zu8xiu3bPic5eyUY8KuB0+kSFAKuQNKLDjU7JpH2qqD8KF5m1b47hunANMrx8XJUtk0ZeDUP/bCo8jrblYLT5cTL7sX69CRw+3Bh4sDK+138Ysk2N1ReL9j7OAD/3WJj/eQfcBpHibYNJdO9nj1lX/JU8pcAfL3oPXyMMF65bC5ndIlvrY/sWWkb4L/nWTMVpidBaPNmvZ2SyoshPw3C4pv2fNwIWPGkVc1lz0rwCYZr/mvdu/zv8F0cnDYOonrDTa9BWaGWu0idtCMvoXoAACAASURBVARGRERE2tbxMzVaSYgjmkJ3Ro32nVkHARjSLREfdywlhjXbo09kHADeZgQ5pYdbPT4RTyl25eFtC668/ujqxcwZM7danyv6Dq48DzEHg62Mi9+8DYADxSkEG4l4e1X9HjXAYSVUvN1dq7/McPPEN69hGG4GR47gk6u/xHRZy2wm93oYbKWUGulMXvxrCkpLWvRztgu7VljJD7CWZzwzBB4JsX5Ab2umaSUiPOFoxbLCsIT6+9Vl9Aw4/RIr+REWD/f+CAkVf65h8XDFP6HPeOvay0fJD6mXEiAiIiLSpgzcAAwKa94GqPXpEXg6puMwaXlHq7WvTV8Pbm+GduvFqJiq9w/s2hOAQK8oitw1Z460hDkrVvHsyrpLi4q0hTIzD397SOV1fGg3LkoYWa2PzVb1I8LFPS4GIMu9gae+fxOn/RB9wwZV6//guXcQYxvNh9e8zazBT5Hodx639/orAMv2W+WmE8Ni6RHShbkXLGT22Qv53Xk3MSXB6uP02stvPn2q5T+sJ7mcsHiGdR7YBfpNBHdFlZKFUyH57baNJ3kB/LUrZB9XBctZWrPfir/DvwZbZWczt8HWJVbypDmOvbOpM0AcfnDLO/C73fCbJGtDU5EmUgJERERE2kxq9gHctkISfS9g3hX/aLX3DIjsj2GYrDmQUq09o2wbYba++Hv78ocxt1e2h/oFABDu0wWnkd3iGzWm5+fxvz3T+O+uqeSWFLXo2CKN5Xa7cRp5hHg3/jfkvzx7QuX5y9ufwDBMLoqvnjDpGxXLF5P/Q2xoOD8bNI4PbnyeGedOwHQ7yDJ/BGBi33MAOCchjvH9TwfggTETSZqcBKaDnbmbm/vx2pc1L0B2KvS/Gu5eDQOuqbq3dTEsmgqFWW0Xz7Yl1jHlY+u47HF4vDss/VP1fsv+Cjm74bnh1teCm2HXsqr7uQfh01lQ1sj/x5xlsOl9sHtDVJ+mx28Y1syONphBKB2bEiAiIiLSZr5KTQbg6tOuwdve/P0/6hITGAnA4YKcau0u4yih3tEARAeG0df3Sq6JfaDyfrhvONicPP7VZyQfrLmJalNsPpLCKz99Unn91oavW2RckZO17cghDHsJPYMbvw9FTFAEY7teTQA9K9uu7DuynicsNpsNu1mRWGQYcaGRtfaz2+xEGkPJde2v9X5dvtm7nnNfv5Qvd1mbsq45tI7UnF0nNUaLc7utkqx7v4MtH0DXM+HGV60f3PtfDde+BP4RVf1Tv2q72Fzl1jH3ADx9hrWnhrsc1r1csTymBN6+tfozPUdbx9eugefPga8fg/d/Dmv+YyU1TlScU7Pti4dh4zsw8AbwDa55X6SNaRNUERERaTO7KkrTntnKGx52CQoFIKu4aglMQWkJ2IsI9636Qezdmx6v9lyUXwTkwoKDv+Ot1Hg2/fLjJr0/v7SY8W/+HH9fk/SyTQCYph1Mg+dS/o/0okM8cv7UJo0t0hRZRUf52/dW9aUBkac12P/VS98kr6wAgGcv/QvbM9N5+OsXGBN/BkE+/o165/1nPcyuo/t5+Lw76u0X4RtNZtG6Ro0J1kyW6V89iNN+mPtX3A8ri3FTjt0dwqpbviLQx0PVVr74vVWh5JhR91ad22xw5g0w8HqrPOtfu8LhTcCNbRNbxhbrmJYEuRXJpoE3wMZ3Yd9q637Kx3D+Q5Awxtp41GaDJQ/A2het+8fGAPjxVRg0yfoMEafBJzNhwwLAgN7j4Zx7rFK1P8yF+PPgyn+1zecUaYASICIiItJmDhakY5o2+kZ3b9X3xARZU/w/37WCh8beAsDOrDQAov3rXj/eJbBqaYDhu6fJ7/86dQN5to3klVW1BdKLhOA+bCpYzPt7/80jKAEibWfyh//H/rJVmG4HE/ud02D/s7oOrHbdO6or79z0yEm9c8rgSxvVL9g7CKPESUFZMYHefg32zy0pwmm3Nit22/Iq2122XJ5b8zEPjrn+pOJsMSmLIbQn9L7UShqceVPNPoZhVYQJ7wVHdlpt5SVW//4Tm75PRn2Ksq0ysgAH11vHSW9CZG8rAfLyeAjsaiUyzn+w+rOX/R2i+8G3z8AFD1nlbMFaKvOXWmb12L1h+6fWF1gbn056A+yOmn1FPEAJEBEREWkzmcXpGK5g/L1bb/kLQLegMACybd+xNyeLnmERbMuyfusZF9y1zueCvKv/Zvtg7hG+253O3755m5uG9WDGiNtxNOIb+bnJr1ddFJ8Gfjs5I/wsnr/8dwx9/TMMt28TPpVI0+0vSgYvuK3XY3St+PfRXoT4WJuyHsrNpndUw8nR7GKrisqIsJsI9wsmLiCRX5w9juGvjeX13Y9y/YDRJEbU/e+8Rbjd1l4e3c+CkFjIT4fcfXDp49bsh7EPQkDty34Aaz+MQ0nW+aqnrCUpO5fCbU2bdVar0gLY+SW8a1XwwbBXbcQaNxICIqxZKt/9GwrSofeUmmMYBgy70/o6xjStjUm//ZeV5Dn4IwTHwNUvWMtqyoth3TwIiII+l4NvSM1xRTxECRARERFpM7llmfgYEQ13bKYQ36pExvubv2HSoJF8e2AtAGd1613nc+NOO4t/bQwk0W8sqWWfsGx3MnPWvEFZyA+8tgOKy0z+1MDSlVV7UthbthwAtzOI2WMfJz3HyeTh/XF42Tkj8Eo2FnxImdNZrZSoSHN8tnUr247s5d5R46pVcQFIzcoAr3zOi7iDB8Ze7qEI6xbmFwRAWkFOoxIgOcXW0pxovxgeH/fzynZ/WzRFFHD14nFM7P4b/nrxXa0TcHkxzDkTCjMgtAf86lvYb/3/Quxw61hf8gOgxyjY8iGsedFKfgAc2dGycS79g5WIOOb0S6yZGeGJVvID4JLHYNNCyDsIZ05q3LiGYSVOzvl1LZuSeoN3AIz9XYt8BJGWpk1QRUREpM0UmVkEezXwg0ELMI77pnxf3iEuXXgJyzJexTQNhnbrVedzPcOi2Hjn99w/0vqN6c7sA7i9qsriphzZ1+C7P9hqVUx48YKFfP+zFVzerx93jhqIw8sOQFxQLIbh5txXb+DBz+fVN5RIoz30zR94KXUmg14ZzbAXp7D5cNXf1ZV7rH1oBnVpRhWOVhThZ80QOFxYfRNNp8vJgo1f1ajKlFtqzQA5cbnM38c+SjCnY5ZH8NHBZ/j14n/jcrlbPuCN71rJD4Cj++BvcbD9c2v5R8yZjRuj35XgCIBPKzZhDkuA/DQoK2y5OI8lVPwj4YZXIKhiVswZ11Xvd8MrcON8iD/35MZXRRY5BSkBIiIiIm3C5XbhsuUQ4dOlTd639Dqr2kpeWdUeAX5mHL6OhpffnBbRDYA9ufsps1dVp0gr3NvgsynZmzCdQQzvnkiwb83lMuN6WfsvlHjtZMnBF/nP2o+58LU7KHc6GxxbpC4ue7Z1Ys+n1OcnnlhVlVz7Kd0qB31ujzM8EVqDogKsJTkZBdnkFOdSUJHg+O3n/+avP97HzC//US0JcrRiBkigd0C1ccYmDObb2xby/lULAFiR9SJPrHyj5QPe8611nJ4M/SZa50mvQ7ch4NXIDVhDusMtVpxc+xJc8HvrPPdAy8Vp2KwZKb9LtcrwXvB7mLwIzp9VvV/ccOh/Vcu9V6QdUwJERERE2sTi7d9iGC56hjS+BGdzdA2MwnTb2ZG7ubLtlt4/r+eJ454Nsso1rs97G8NwcV3sQwS7hpLj2l3nMxn5BWxJT+dQyXaCjV7Y7bV/m3V+Qn9Cyy6HshiwF/J8ykNkutfx8ba1J/HpTt7irWvZmnGQQ/mHW/U94hkuIx8fZ2Ll9ZGSjMrznUdTwe3DgOi2+bd3ss7u3gfTNFh7KInz376EcxaMZF9uBt+nrwDgi0PzOe+1aykoLWRD2j525lhJyWCfgFrH6xPVlekDHwXg633LWi7Q7V9A9m7I3mVVNgmLt2ZPdK2Y9dF7/MmNlzAGHkqzqsMEVmzOXJhZ/zMnoyADAqOrrgOjIPFCsNlb7h0ipxgtPBUREZFWN2Xho/yU/x4A58Y1cop4CzBMP46SXHk9JKbh8p9QfQmN9VwvMooGsTJ7Pdsz0+gdFVPjmSvfmUaRVxLYoX/oxXWO7fCys/KuJ3n9p294csM9le1f7lrDtQNGNSq+k5Wef5T/W1OV/Ll/4FPcfta4VnmXtI0yVxnedms2U1ZhPoa9mMGh53B29Exe2PJXCsxcAErKy9hX8gNhjt41/l63FwnhUfi44q3/IypCvGLheLCXE2WcQ7m7jKOs5/GVb/LxwWcqnwuppxzvL866lrc2f0KGey2f71zHpacNa16QrnJ484aq6yGTraPNDrcvhs0f1F71pSHHNl4OaI0EyGHoMbLlxhPpADQDRERERFpVRkEeP+YtxO4OJtZ7OON7n9Vm77aZ1h4Bfu5ePDf2dc7v1fjkS5ytarPInqHRjIy1yoI+u+61Gn2XbFtvJT8A07Rx7/CGNxO8dcgYhgbcSZjzfHD5caCg4f1FmurN5Oq/BZ+/6e1We5e0vlsXPszQ+efw1oavefr7N7n3038CcHp4PL88ZzR+9hCKXLk8s/p9/rRsHngd5ape13o46vrFB1SV3e3ldRXYygF48qIH+PyWF8EZUi35ARDiW/sMkGNmDLeSfm9vWtr8ALNSq84dAdaSkmN8Q2DobeBoRnWnYwmQLR81fYzjlRVCcbZVoUZEKmkGiIiIiLSa4vIyrnj7TgwvN3f1ncU9I69s0/d7GX6UAV18ExgTP+iknl0y+UkGvroEgITwaGJDw5m9AVamfQZUr3Dw4TZrv5GLY27lmtMvZ1BMfKPe8cr1MwAYOu8KskrTTiq+k7E5czsA3R0jOFqWRaYzBdM02+2MAKlf8tGvwV7G4z9Nr2wLME/nt+dYm1sGeoWQb2xm7rZHALA5u3DPyImeCLXRnp/wO+b/NIRz487inJ4J3P1JCJH+oZzd/XQAHh/1FL9f/RtMW9UmoSF+gfWOOaHPKB7+tgs7c1OaH2DGluMGfhpOu6j5Yx7Pv6Iqy+aFcOHDEJFYf/+GHK3Yuyi0Z/PGEelgNANEREREWs2izasp8doGwFX9TrLCQAtwGNYMkBDv0GaNE+IbQKR/CH39rqTclkWps6za/aySI5hub54a9zvGJAysY5S6hTniyHfvq9zo8c1NSxgx/0IufusGxr91Owdzs5sV/4HC/RiuQD675SWGRp4HXnlWaVQ55bjdbkxbUY32S2KvxVFRVvmsrlWbnbpLo7j3jD/i69Xw5r+e1CUwhAfOu55R8b0wDIP/THiAv1xYVcb2yn7D+fG2Vfx+yHOVbT1DG64oFWjrQoHzCAD5JaX89euPKXO6Tj7AjIokytTlMPCG+no2jc0O51YktNKSmj9eUsXmr6FxzR9LpANpMAFiGEacYRjLDMPYYhjGZsMwptfR73zDMJIq+qxo+VBFRESkvXO63LjdVdUaNmfuBODsgLuJDQlv83hsFZNdAxz1T5WvS0Jg9VkjiSGJGIab5PTqm6EeLc3G7g5u8oyKfmFnYNrz+dOyl/nxUCpPrH+QIjOTw2VbOVi2nt9//UKTxj0mo3Q3AbbuAAyK7gvA1MWPNWtM8YwDeVkYRtUP8H19riOeyUwfWVXa9C8X/IobEu7hseEv8N2tn/OLEWM8EWqL87J5cV58VXIn0j+kwWdCvKMoN3Jwupyc99YlLNj/EP/94VPmrv+IUfOvZPG276v1L3M6+eOXr1NQWgLu4xIl+9dAeKJV6cXWSr9DPrsi4dPcUri7lsPaudDtLIgZ3OywRDqSxiyBcQL3m6b5o2EYQcB6wzCWmqZZOQ/MMIxQ4HlgvGma+wzDiK5rMBEREem4hrw0ER97ANOH3cXkwReTenQXpmnn+asaV32lxVUkJAId9U+Vr8vCa17BZVb9EJQY3h3SYfuRg4T5BnN6pLUZaqErB2+j6bNM7hg8kWVfzmXR/qdZtPd5sIHh9sPmCsPplUZ6YXqTxn1p7TL+tWUmeJXRL+QWAG46cwzPbPIh01xPucuFw964ihBHS3Jx2LwJ8PZrUizSMnZmVV8q1S+qJ3++6I5qbQ67gz+O+VVbhtVmugefXCI10jeafeUFfL0rCZfNmkm1Nm09m3JX4LRl8sjK2Uzos7Cy/1PffcCig0+y46MtvJX9LXQ5A4bdCbtXVJWqbS3Hyvo2JwHidsPbU6xyvDe83Lx9SUQ6oAbTl6Zpppmm+WPFeT6QAnQ/odstwELTNPdV9NOcShERkU7m3U3fgu9eSh1b+HvyDPJLSjhcfAC7KxJfh2em39sqvtUJ9mlaAsTL5oWP3afyuluwNeV+bvKrXPvJJSzZtg6AEvdR/G1NT4CcFduDv5w9tyLoUi4I/w3Jt69hzW1L8DVjyS070qRxX9r0XzCs5Tr3nH0jACG+gdyQ8BvwyufjrY0rvfvNrhTOe3s01713b5PikJaz7tDWatc9Qrp6KBLP+b9Bz/H48Jcb1fe0sAQA/vXDfABMt4Ok/Pdw2jLxc51OqWMHY+c+xO6sHMqdLlbtWw/ApqKPmVOym/Kk1+GDaeATDCOntc4HOuZkEiDrX4XnRsKPJ2zKnJ0Kpblw6V+tMr0iUs1Jzd8yDCMeGAKsOeFWbyDMMIzlhmGsNwxjSsuEJyIiIqeKZ9ZXX6bx/pZvyXMdJNBWs2Rs27FmgLTUrIW4igRINj8C8OmO7wBwGrmE+jS8H0F9rhkwnDMCrqKb1yieuHQyhmHg47ATaA+n2J3TpDFLzGxC6M/cCxcwtHvVpoqTz7wU0zT4cFvD1TFKyl3MXPZHAA6Wr8HpcjcpFmkZaw79gOn2ZurpT9LX/womDewYy1tOxi2Dx3Blv8aVtb17xARw+7OvfBk4g/nnufPo7TeOK7vfw5e3vIG/qzfZ3h9z3QeTOe+Na9nrWgzOMPzKBvFxQD+8ovrAkW0w5FbwCWrdD2b3BptX4xIgG96GzBRY+1/ruiAT3r0DFlsbK9NtSOvFKXIKa3QVGMMwAoH3gftM08yrZZyhwEWAH/C9YRirTdPcfsIYU4GpAD169GhO3CIiItLOHHXuJsjen3zDWiW7Pm0TZcYRujnaruztiVq6yknPsKhq16syFzH8laVgKyHKv3kJEIC3rq+5L0e4bzRHnDtwuU3stpP7PC4jlwifMxgZN6Bae6+ILvi6Evkx/13+uDyCP59/T63PL0haz1/W3YfNcRTD7YdpK2Z3TganR3a+WQftxaHi3fi4Y7l31OXcy+UNP9DJRfgHMTjkMpLy3yfGeyiX9h7Mpb2r9sVYfuvbPPTVC3yZ8V/KAcP0YskN7xAb3JUyZxlG+gb4eDqMaIMlRYZhzQJpKAHidkP6Jus8fSPkp8Pnv7cqyFgDQWSfVg1V5FTVqBkghmE4sJIfb5imubCWLgeAz03TLDRN8wjwDVCj1pxpmi+apjnMNM1hUVFRNQYRERGRU9OBo9nglcvA8GEkT04GVwApOclgOOni38VjcRkVM0DcZsvMWgjxrb6ZqtN2hGJjDwCxQa3zOWMDe2DYi0jNOnxSzx0tLgJ7EZG+tSdmbu1j/UD31Z5v6hxjbvI8bI6jAIyOtjbZPJjbtOU40jIKzYOEO2I9HcYp5fr+FwJwY/+La9zz8/bi6ct+jbezFwBDQ64nNthK8Hl7eUPsMJj2LYS1UTlZ78CGEyCpX1nLXHpdYF3/sw9seg8CKn6+MgywN/r33CKdSmOqwBjA/4AU0zSfqqPbh8BowzC8DMPwB0Zg7RUiIiIincCmjD0AxIfEYbPZ8CaSzLIdAHQP9lwC5K4zf4Hp8ubKPqNbbMwYH2s2hbu0KrHgLg/mij5nt9g7jtc73PrBbObSf+J2m0x5918Mn3cV+47W3HLt4a/+y8j549l2ZDdJaXsAiA2ufQnSfaMvJcQcRLH7xIm9lneT15LBN/i5T+PdyxdzZpRVfeNQfs2SvJ9tS+IfqxbicrnJLiyrcV9aRn5pAaatgG4BKm16Mq7qO4ZlNy7jF0Mn1tmnZ+DpAMQEN38mV7N4B0B5LQmQ/HRwlsKeVfDG9VbbhQ9X7zPxWYg9G372XuvHKXKKakxq8FxgMrDRMIxjRakfAnoAmKb5gmmaKYZhfAZsANzAS6ZpbmqNgEVERKT92Zl9EIBeYdY+6YH2KLLZC0BiWDePxTV5yBgmD1nfomN+MWkBGXklPL32ZRYffJ4B/tfy8Kjfckb3hktyNsWkgRfw321woHgTr/64nJ+KXgI7fLLtB6aNuAKAp1bP550d8yl0W7NEXv7pi8qZNwOjT6tz7ECvMPLKdtdof3X918zeMBPDBqO7jaVvVE+2ZlrVRw4XZlXruz0znZnf3YlhK+fdrYspcvxAFKMZEJXIrvzNpBXvZkqfX7MnbyffHPqS6xLuYmT3wVx4Wu8aS5QKSgtxm26CfVt5r4VT1I4sqxpQTJAKLp6sSL/6Exvzr36UP6yI5L6RN7RRRHVw+FefAeJ2wZs3ws4vYdS9VhIEYPzfrNkpM3dY9wfdAr0vhT7jPRO3yCmiwQSIaZqrOLaDWP39ZgOzWyIoERERObXsPXoIgL5R1m+mI3y6kF3xffoZXdpo6ngbig725dHzfw4rSpkxcgrRAa2T/ACIDAgixj6GNL7hqc2/qWxPzdlfeT5/6wu4jPzK6w2Zm4kNtK6Hd+9b59ihPmEccBbgcruw2+y8sOYzvj+0mh/z3sewweCAn/HPS6YD0C0oAoCv963gnhFXcs0795NVksZZUaMwbOUAFDl+ACCTVSzPXGW9xIBXUl7AAJz2wyzY+xgL9oL5rY1I2yBGdz+Pxy66iyOFBVzx9hSKbLt5dMTfuajXcEJ8Wu/P9VSUmm39O+uuBEiLC/QJ4OlLZnk6DPDyBWdJ1XXufiv5AZCxFTJSrNK8xyrSBEbD1OVtHaXIKUuLw0RERKTZduXuxnQ76BtpzQDpGtCVHRUJkMEx8Z4LrBV5e3nzxEX3tcm7RnQdwQcHv8E0bYzrchdLD89lf7416+a3n8zDZeQzJOg67hx8DY+sfIb9JWvIK8vC5g6nR1jd+671Cklgc5GbwfOHEGL0J5fN1e7PvvSXlbM0eoVbM0r2lH3FnQv/xb7yr8EOK7NTME0b4eYIcmzfc07YrSSGJPLuzjf4zyX/4p+rX2Jz0SIAejrGASZlZX7kmD+S7UzlwwM/sfjlV3HZcsFuvfdPa3/LI98H8dHVHxIfrn3jjtl71Jrh07MTlr7tNLx8qidACo/tuWPAzoqqTSVH2zwskY5CCRARERFptrSi3fiYMfg4HAD0DOnOymzA7Y3N1qg916Uef75oMjNLbiTEzxeAIfM+ZEvBEn69JIhv05fidocxZ/xMwv0DuSptIvN2riWXDcT5jK133IfP/xkfv/UUGGZl8iPOPp6u/tH8avhEugaFVfaNDAjm3v5P8u8tD5Jc/ApuZyADgseQUrSEPn6X88SFv2Xz4QNcc4ZVfvPBsdY+BX2392ZzkTXGqO4jeGjsTZVjHinM56IFE3F5HcFw+3LPgEdJCI3n2R9fYHfpMj7c+j3TR9W9b0NnszvXmvXTN0qboHZYXr5VCY7vnoUvfm+dx5wJacnWefehnolNpANQAkREREROWrnLqqrisNvYn5tBHtvp5Xdh5f24io1PvdzhHomvozEMozL5AdDFuy8HXctZkfkaeEEvxzjC/QMBuHPoxby+/VmcFDJr1C/rHdff25uLuk7mq/TXALA7u7DktrpXNE89+3KiAgP5InUtQ6IHcdOZo5j34zDuHn41Pl4OetdS5e+XZ1/J8rQPiQ/sz+9GX1/tXmRAEEtv+pC0vFwGdava2HNQzKNcvHAZSYe3AEqAHLM5ez1GeTSnR3puY2FpZV4+Vft8HEt+AMQOtxIgAdFw46ueiU2kA1ACRERERE7apf/+iDJyWHXfHSza/D2Gzcn1fSZU3r+g1xD+vHIAM4ZP92CUHdeQ6EEcTFteee3tVfUtXYhvAOvvWNrosR4afTurF31Moeso3l72Bvtf028M1/QbU3k9Y1T9m0bGBIew/NZ367wfHRhMdGBwtbYuQWHgCiSjKK3BeDqLnJIcstwbiHZcUGPzWOlAjt8DJLI3HNlutY2+D3Z/A1f8A/zC6h9DROqkOakiIiJy0tL9nyE37CnWHtjC5kyrisj5iQMq78cEB7H13gXcOeIcT4XYod05zJpt09v/fABC7b2aPFZ0QDSfXPcBAJP6T2igd9vxIpACZ+0lejuj+7/8GwBnRerfVId2/AwQVzkMuBbu3wohsfDrtZAwpv7nRaRemgEiIiIiJ2V/bhZ2n0wA7vjsLjDt2Ly8iQvWZpVt5fTwXnx9w9eEeIcz+8t1/Gr04GaNF+EXwfIblxPm235+s+xtBFHiUgIE4MWk+fyQtQS305+fn3WFp8OR1nT8DJCibAiI0owPkRakGSAiIiLSKN/t3cLFr9/J9R9VLXmwOfLAcNLH/zJNy29jUf5ReHvZ+f34EUQE+jR7vAi/CGxG+/nW0M8eRImRzv82vozbdHs6HI85WlzEv5OtfVnsdhd9Y4IbeEJOacdmgLjKoTQX/CM8HZFIh/L/7N13eFRl2sfx7zMz6b1SQgfpVUDEQhEEBCzYy6qri73urrrq6rqya6+ra3ntBUTFhg0QkSZIB+kdQgvpvU457x/DDkZKEjLJkPD7XJcX5zznKfdRYjJ3nnL8fJcTERGR49otP91AunspJR7v7I/mtqEAtAk9k4kXPhrI0KQRinTE4LEV8OKK51m8Z02gw6k3lmXx2aavyDtwEsi0zSt8z27r8UCgwpL68r8ZICU53vtwbSQt4k9KgIiIiEi1eGyVlyOM73ktHlc4F3U8l9CgqjfPFKmJ4e36+a5Xpm0PYCT1a9X+jTy66GEu/vxuANZleN/9tSGTuanvRYEMTeqDIxQsDxSle++VABHxKyVAREREpEpF5d5N+XpEjPOVXdKrLz9cNIfrS1NcJwAAIABJREFU+p8eqLCkEbv71Gv46GzvaTZbc3cEOJr6sb94P9f8cCkA6a7lLE7dxfS0NwDo27xDIEOT+mI/sEXj0je9f2oJjIhfKQEiIiIiVdqVnwVAs8imDEq4jn/2fR2AlNgw7f0hdaZdQiKWJ5iM4uxAh1In8ktL+cv0/5BZlIfb7eGSr66r9Hz87LGUW/kk2rsRFhQaoCilXmVu8v654gPvn2GaASLiTzoFRkRERKq0J8+bAEkMi+OBwVcEOBo5UUSEOMATTFFFSaBD8bsNmdu4+KvrsAXnkjp9E/vLN1HgygCgd+QlrCqaAsYC4LkhTwQyVKlPA26GXycfvNcMEBG/UgJEREREqrSv0JsASY7UbyOlftkIpdRVGugw/GpF+iqeWvA2tuBcADYXz/c9W3bVMkIcITy9JILl+1dwZovBnNyibaBClfrWvDeMeR6++4v3PrJJYOMRaWSUABEREZEq7SnwbsjXOkY/jEv9shNMuadxJUCunX41AKGujpyWPJafcp4HoG/CEEIc3iON7zvlrwGLTwLMcWC50yk3HdwTRET8Ql9RIiIiUqWteTuwLEPf5icFOhQ5wdhNaKNKgOSXFfuuz2jdg+fO+iMb0kfTMTkeh00/mgvQbRykr4XB9wU6EpFGR5ugioiInAAemvc4p344nDm75tW47ezU+Swv+BjLGUNceEQdRCdyZEEmDJenLNBh+M3O3Ezf9YAWnbHZDN2aNSHIHqQNhcUrOBxGPQFhcYGORKTRUQJERETkBDB953cUe9L5ZP2MGrf9dstcABKc5/s7LJEqhdhCcVmNJwGSmuvdTyfSEcuwVsMCHI2IyIlFCRAREZFGrqC8gHKrAIB1mZt4Yf4P7MkrqHb7nXnpeCpimXrtnXUVosgRhdjDcNmyyS/PD3QofrGnwDsD5M4eE0gKTwpwNCIiJxYlQERERBq5mVvXeC/cYeR6NvHO9r9y+7cvVrv9/uJMgk0McRHBdRShyJElhMeArYKRU8YGOhS/yCjOAaBFdGKAIxEROfEoASIiItJIuT1u3l/3Pl9tnAVAt6hRvmeppSuP2rbcXc5zi19n0b5FFLCOuJCEOo1V5EgGtuoIQLE7L8CR+EdOmfc9UpQAERGpd0qAiIiINCIVLjdvLZ6HZVks27+CZ5c9y6qiKdg98YxuN8JXz+nYQXZx4RH7eWzeO7y38RVumHkDAC2im9V57CKHc3nXc33XHssTwEj8I+/AUp6UKCUVRUTqmxIgIiIijch9M97kPxtv49n5U/kldauvPMwWxZjOJ/vujc3NeV9ewrrMrYf04fF4mL7zu0pll3QZXXdBixxFUngSI5PvAuD1xdO4Z9bj/Jq+McBRHbvCijzwhBASpCVlIiL1TQkQERGRRiK9KI9ZWa8AsDhtOb9mbAIgjt7ce8pfSIgIp1vYJQxJuAWAAvdeHpv/f4f0c+lXt1BqS/Xdj2xyO+d2HFQPbyByeA8PuQrLMnyx9Utm7JnMH6ZfwpsrJx+1TUbh8XlyTLGzAOOJDHQYIiInJEegAxARERH/+PvMd33Xm/KXYCsuJcjdkXk3fOgr//jSfwDw5+/hx8zXqHBV/l2IZVlsKlwIwL29n2RfyXbuHTC+HqIXObKYsDCMJ5yM8i2+n15fWv04N/S54rD1P1yxgKfX3MzferzOH04+vR4jPbqMonwynFsJQgkQEZFA0AwQERGRRmJ93hIAYsxJELIPjz2XvsmnHrbuC6NvxeZKJK88p1L5mrS9AAQ523F1z9HcP/AO7DZ73QYuUg12wrEcv/n76j58EiGtMIunf/Ue2bxg7y+Vnr23fC7XffUYK9O21DqeLdl7KCo/8iyTYmcx5S4XMzevp9TpZE9+BmdPGYPbnk6n6AG1Hl9ERGpOM0BEREQaiUIrlRT7IN4cO4HRU4cA0CGu1RHrB5tYilw5TN+0kkmrf+L+QRfz3eblADww8G6MMfURtki1OIjATSYA7UIHs610Hr+kbmNg6/aV6p33+VVgqwDAUDl599yKxyA4jYdnZ/Dtlf85pjjSi7J5YM6TLM2eTtvg4Xx9xQuH1MkvK+LMycOwbCUAWPMjMY4iLGNjRJPxPD78pmMaW0REakcJEBERkUagxFmOZSsiKawpLWMTaB96FtvKfuKkhJQjtol0xJHrSuWfCx+j2LaJ+3/aBZ5gsByMOenwM0dEAqWcg7M//nHG3Vw782f+9fPLfN/6xUr1ysw+3/XOglRGvn8PT519Gyv3b4bgNAAyy3cfcxznTRlPic27efDekoMzSbZnZXLvT0+RXZpPXFiEN/nhDqNz1CB2FW+ihCJaOYbx/Ki7jnlsERGpHSVAREREGoHNWWkYY9EssgkAk8Y9yVsrpjKu65H3P4gJTiDLvZpSjwU2SCvdisuqINrRifDgsPoKXaR67AW+y74pHUi292NP2fJKVdILCyrd73bNAuDqmTN8ZeGurpSYXccUQomznGKzgyArlmhba3Icv3La++NIDmnL9qK1eBzpGMtGdqkL44pl7hXTiAuPxO1x88TcT7ju5FHHNK6IiPiH9gARERFpBDZn7QGgVXRTACKCw7jr1MuPuowlMSwRbOV4HN5lBeX2Xbgd++kR37/uAxapoQd6V57p0SHmJCxHHqk5B2eGLNnjnZFxVdsHaM7YQ/roFXElPRP7gb2IvQXZNY5hQeoGjHFzQasbGdl6DFiGIs8+tpXPxApK4/yWtzH9wtnc0ukppo77nLhw7z4ldpudh4ZeSUpMfI3HFBER/1ECREREpBHYkeud9t8uvnm12zSLSPZdW65o3/XYDmf5LzARP7my1zBu6fgS/+43EYCuSR0AWLp3s6/OhsztAHRP7sA3V/27UvuzmlzNxIsfoHOCd8+QiWumsiJtE49Mm8GKXfuPOG5qfiqL9i3BsiwW7l4NwMCWPXhw8BWs+eNqVl+3mCC8Xz8PDbqWlJhYbj11NG3jk4/Yp4iIBIaWwIiIiDQCewrTAeic1LLabVpENwHvoS90ixzN+rKPceaexpjOvesiRJFau3XgUN/1/2Y77SvM9JVty98BQJ9mHQh22Jl/2XxKXCUUlhfRNrYNAIPa9OK9bTBx6wtM3OrdwPSLXW355Px3+HHnHMafPJbw4FBfn1dMvYNCawd2wnFTgmXZOaN110pxTb/4S9weN2FBIXXy3iIi4h9KgIiIiDQCGcXpWJadNrFJ1W7TNr6Z7/qjix9g0tKLuKhPB53+Ig1Cyxjv3/U3tzxIqbOcvw2+mH1Fu7FckTSPjgMgNjSWWGLhNyfm9m/RniHxtzIn59WDhaE7uGyGN7kyf9cyplz6JAAey0Ohew/Y8CY/PA46BJ9PeHBwpViSIzTbQ0SkIVACREREpBHIKsvA7onBZqv+6taOvzkhxm63cc2pnesiNJE60TruYNJh4s5HsYfkklm2l1CaVpnEe2nszezMuZw28bHsLyjixqn/ISTYxcaS79lY+h093v+OcEcEr5/1AdicDIq/mceHX09RuYuU2Ki6fjUREakjSoCIiIg0cE/N+YYM6xfi7f1q1K5lTEIdRSRS9xLDDu5bY1mGD9Z9gGVctAjpW2VbYwxtE7yzRJrFRPHNNQ8B8NAPnZia9jgAJa5irvnhIgC6J7chJiyMGB2OJCLSoGkTVBERkQaswuVhYuqDAPRI6FOjtnabnVhHCqNbXFMXoYnUKZvNRrTVjfYhI+gccjmWIw/sRbSJaXPMff75zNFE2JLpGT2aU+LP95Wf0ba9HyIWEZFA0wwQERGRBmzO1u2+64t61GwGCMD8q6b7MxyRerXgjx8D8PKiL9i0yVvWo0mHY+4vISyGRVfP8t33eH8qAC0PbLgqIiINm2aAiIiINFAuj4t7Fl/ku++edFIAoxEJnDPbdvFdD27bzW/9No/wHisdExLjtz5FRCRwNANERESkgcovz8fCQ5CnKS+NeISk8OqfACPSmPRK7sytXf+B3Wanmx8TgZPGTGJP4R6djCQi0kgoASIiItJAZZXkAnBq3GWckXJGgKMRCRxjDLf0v8Tv/SaGJZIYluj3fkVEJDC0BEZERKSB2lOQBUDTyPgARyIiIiJy/FMCREREpIHaX+RNgCRF6DhbERERkaooASIiIlKHvt3+LT3e70GvD3px4dcXsq9on9/6Ti/yLoFpEqEZICIiIiJVUQJERESkDk3ZNAUAj+VhS+4W1mev91vf6cUZALSMTvZbnyIiIiKNlRIgIiIidckKqnSbXpzlt67TSzLwuCJIjor0W58iIiIijZVOgREREalDWzJzKt1vzNgHXWvez76CXKZunM+YTidT5Cpg1oYMlqb/Ap4YkqJC/BStiIiISOOlBIiIiEgdqvAUgw08FXEYWznbctIB2FeQzcq0bfRL6USTyJgq+7n5uwnsqPiRNzZE4KIYAFsw2MuaEh6sb+ciIiIiVdESGBERkTrkohRnQU+eP2MSDmJIzd8DwDmfj+H+RX/igs/+WK1+dpYuOdBfcaXyczuM8mu8IiIiIo2VEiAiIiJ1yE0x7eNSGNmlJW0iu5BvbWFzejYemzeRUWS2UuHyALBw30KGfjqUaTumVerD5XbhsRViuUN9ZX1D/srU875jwlnX1d/LiIiIiDRgSoCIiIj4idPt5s2lM7EsC4C9hfvAVkFiaBMAOsR0wthLuWnGvZXaLdy5A4CbZt5EVmkW9827r9LzPfnZGGMRRQdf2fPnXUS7uFYYY+rylUREREQaDSVARERE/OSO717ipfV/4dXF3wEwfesvAJzcpC8AieHxAGRZSwH494BXALhz1r0MfefuSn1dOulF3/X2vP0AdIjp4iuLD4+qi1cQERERabSqTIAYY1oaY2YbY9YbY9YZY+46St3+xhiXMeZi/4YpIiJy/NtZsNX7Z/5eAD5ZOw/LHcK5nfsAkBwR76v7f4O+ZnTHgWDZsUK3kGWfBUCwpzkA653vkVmcD8Du/AwA+qf0rJ8XEREREWmEqjMDxAX81bKsrsCpwG3GmEMO8DPG2IGngB/8G6KIiEhD4V2OUuEux+OxSCvbQLzjJNoleU95aRqZ4Kt5Wtu2BNmCcFixlXq4oO2VXN7yMYxx8+0m7wySfYWZAHROaAfA8Jbn1PmbiIiIiDQ2VZ6bZ1lWGpB24LrQGLMBSAHW/67qHcDnQH9/BykiItIQuK0KADJKMnh/1QwITqNn4jDf8+ZR3gSIZdl9ZbFBTchyZ/vuz2jTkUhHIh/vhv9b/xiD23X0JUA6JjZj+R+WYzcH24uIiIhI9dRoDxBjTBugD7D4d+UpwDjgNX8FJiIi0tAUu71LVvIrcpmy8WsAbuo3zve8fXwznHl9OCt6gq/s/855ks7Rp/HR6I84u/XZDGzRhy5JzQ/0l8f46XeQWZKNZdloGRNPsD0Yu00JEBEREZGaqnIGyP8YYyLxzvC427Ksgt89fhH4m2VZnqPtRm+MuRG4EaBVq1Y1j1ZEROQ4VuYuBgeUuoop9OwixNWJHskdfc+jQkNY8Kc3iQsP9pV1TGjLlHH/B8DzQ573Fv7mu3NeeR6W2YXNE6nEh4iIiEgtVCsBYowJwpv8mGRZ1heHqdIP+PhA8iMRGG2McVmW9dVvK1mW9QbwBkC/fv2s2gQuIiJyvHFapQBkWcvBQLy9yyF1EiJDqtVXRFAExc5iXFYpWSwnzN7Ur7GKiIiInGiqcwqMAd4GNliW9fzh6liW1dayrDaWZbUBPgNu/X3yQ0REpDFzuj14TGmlsha2Ucfc38RzJhJla4llnAAMSB5Rq/hERERETnTV2QPkdOBq4CxjzKoD/4w2xtxsjLm5juMTERFpEFKzizG2Mt+9Zdl4+vyzjrm/DnEd6BBxhu++Z0KfWsUnIiIicqKrzikwP/O/c/2qwbKsP9YmIBERkYZozua9GJvTdx/liCMlNqJWfTYJT4JC73WLaC2BEREREamNGp0CIyIiIof37o57ATBub9KjSXiTWvfZPCrZd906pvb9iYiIiJzIqn0KjIiIiBxZkTsdG+G4991GedAG7h92Wa37bB6V4LtuFh1T6/5ERERETmRKgIiIiNSSy+3GQxntg8/n738Yw/S1fTm1Rdda99sy5mACJDosqNb9iYiIiJzIlAARERGppVV794OxaJ+QTP828fRvE++XflvFHUyA2G3V3o5LRERERA5De4CIiIjU0rJdewHokpxcRc2aaRIR59f+RERERE5kSoCIiIjU0pr0PQC0iUv0a79Bdi17EREREfEXLYERERE5BiM+G0FacVqlsvgwzdgQEREROV4pASIiInIMfp/86BN2Az2Tevp9nMljJhMf6p89RUREREROZEqAiIiI1JLlCeL89hfisPn/22r3xO5+71NERETkRKQ9QERERGrLcnBmR//u/yEiIiIi/qUEiIiISA0VlpUfUpYcFRqASERERESkupQAERERqaG5W3dXujfGBCgSEREREakuJUBERERq6Pu1qZXuw4LsAYpERERERKpLCRAREZEa2Jy7mSWlz1Qqiw+LDlA0IiIiIlJdOgVGRESkBlamr6TctpdI04q2CTFUuCu48+Q7Ax2WiIiIiFRBCRAREZEayC8vBOC8pMd54JxeAY5GRERERKpLS2BERERqIDU3B8sy9ExJCnQoIiIiIlIDSoCIiIhU05o9+Xy2cht4QujdKi7Q4YiIiIhIDSgBIiIiUk0fLtqJsZUTGxpFSmxYoMMRERERkRrQHiAiIiLVtGZvAUkxkBgRE+hQRERERKSGlAARERGpQlZpFp+u/55tZZtokphORFBCoEMSERERkRpSAkRERKQKkzZM4q21bxHSFPJcMCj65ECHJCIiIiI1pASIiIhIFbJLswkmBrPvHn64exCxobGBDklEREREakgJEBERkSrklefhcUXQt3kK8WHxgQ5HRERERI6BToERERGpws7cTMrKQ+mRopkfIiIiIg2VEiAiItJo7C/ez8acjeSX5/utz2/Wr2Jbzl7CHdGM6dnUb/2KiIiISP3SEhgREWk0xk0dR5GzCIA1166pdX9ZpVk8uPRabMEeLuw4mg7JUbXuU0REREQCQzNARESkUSiuKPYlPwCcbifFzmJW7F+FZVk17u/XffuYMPsDwEPv8Ou5u98dfoxWREREROqbZoCIiEijsGj3lkr3c3es4601b7GuYC43dv4Hdwy4pMo+skqzSAxLJL8sn+umjcfp2A2eIB4dei0RQRF1FbqIiIiI1APNABERkUbhhy2rABjR7BoAvtw0k7V5CwD4euv0Kts/Pf9Thn46lA9Xf8U5n16J07Gblo6hfDx2Cu0SdfKLiIiISEOnBIiIiDR4ZU4Xs/Z+jc0dw6ODvUtV5mV9gLG5sDxB5FdkH7Gt0+Xm4in38OH2fwHwzPLHKLR2YcpOYvJFT9AtqX29vIOIiIiI1C0lQEREpEF7+JtF9HhuAuWOLYxudQWRIaGVnifYelHhKTxsW4/l4dF5r7OpZAY2dzxNzVA8phwsw9Nn3U9MqJa9iIiIiDQW2gNEREQatCk7XyK02RpsBHP/mX8AYEDyUBZnzCbUFkNsUDw57vWHbfv2mveYuvt1AOZf9T1RweHsL8wlOixEe36IiIiINDJKgIiISIO1IzsXR8Rmkuw9eW3UE8SExADw2ojnyCjNIC4kjlu/fR6rrJSi8nIiQ0IqtX9/1be+6+gQb8KjWbT2+xARERFpjLQERkREGqwbZo7H2MsZ1eF0OiW28pUH2YNIiUwhPCicpPB4jLHYkZNZqW1WaRb5lvfkmObh7eo1bhERERGpf0qAiIhIvSpzuikud9W6nwp3BellWwj2NOfWk685Yr3mUckA/G3qfN5dsMNX/u3GJQD0iBrNJ+e+X+t4REREROT4pgSIiIjUq+FvP8rJr9xR43b3zb2P6Tu9x9m6PC7O+3IcGIuBCZcRGRx5xHZtY1sAkBr0Ms+svoNSZzkA/139NAD/GnQPsaGxNY5HRERERBoW7QEiIiL1Kj/sS4LDYFdeNq1iE6rVZn9hFtN2TmPazmmMajOKH7cvYm/xLtylLRnee/BR256U4E2A2ByF4CjkvZU/MGPrIsrJJtbehvaJSbV+JxERERE5/mkGiIiI1JsyV5nveuGutdVqU+Is4ZKvr65U9t9lk7DcwYxO/Cfn9jj6/h0dEppWuv9o/adsc34NwB+73lStGERERESk4dMMEBERqTcvLfzad51RnFutNmd/dA0F7PHdnzpxKMXuLFoGjeTZi0+psn2I4+C3OuOJIM+2CoAmoa25tPvRZ4+IiIiISOOhGSAiIlJvZuz6znedVVp1AmTOtg0UsIk4e0dGJd0PQLE7C4BHB99a7XEfH/gy13b8M1E273IYnPH8cMnXRAVH1SB6EREREWnIlAAREZE6Z1kWX6zYRYZzLeHlAwDIKas6AfLhqlkAvDTsCYa3PQ3L8n7beqj/45zSqvpH157bcQj3DLye+ODmAMTbemGz6VugiIiIyIlES2BERKTOTduwgQfmvUBwXAUnN+nH/Jzl5JflHbWNZVksL5iCzYTSs2kHrCaGW/Z/zlUDWhEbHnxMcUSFREAFJIZVb/NVEREREWk89OsvERGpcxPXTSE4bhEOK4Y7TxsFnjAKnflHbbNi/zrcthySQ9pjMzbsNsNtQzscc/IDoFvkKDzOGHrFnnXMfYiIiIhIw6QZICIiUuf2FKdi9ySy8vrZANisMDKK8vF4LGw2A4DL7WLwpHEUuXPoEN2dzQXLwAZ/6f2I3+K49fSB5Oa/wl1DuvqtTxERERFpGDQDRERE6szs1J/p8X4Pcs1S4oJSfOWWZSh0LKfXhz3JKM4B4Ict6ymwduKxFbC5aCE4k2jNlYzo1Nlv8cRFBPPsJb2ICQvyW58iIiIi0jBoBoiIiNSZj1cv8F1f0+0PvmvLXoA5cP3jtlW0jI3iwblPQejBtg+ffheXdjunniIVERERkcZOM0BERKTO7C1MB6Ai9xSu7jXi4APjOlgnP5snF76FK2Qr7aO68eqwVxnY7DQu6DysvsMVERERkUZMM0BERKTO5JRlYzxNmHzhszjsB3Puxub0Xe8tzCCrLI0w90l8deHHAJzZ4sx6j1VEREREGjfNABERkTpRWFFIoX0FcSEJnNwqrtIzu7H7rmdlvk4Ju4gLalbfIYqIiIjICaTKBIgxpqUxZrYxZr0xZp0x5q7D1LnKGLPaGLPGGLPQGNOrbsIVEZGGYtHeFQC0juxyyLMPz/mQG3ve6Lt3lzWnZ9zQeotNRERERE481ZkB4gL+allWV+BU4DZjzO/PD9wBDLYsqwfwL+AN/4YpIiINzcq0bQCc2+biQ571SOrBHX3u8N2X7r2MP585qt5iExEREZETT5V7gFiWlQakHbguNMZsAFKA9b+ps/A3TRYBLfwcp4iINADb8rbxjwX/YH9RPlkl2Vg4GNaxQ5XtJl47kpTYsHqIUEREREROVDXaBNUY0wboAyw+SrU/AdOOPSQREWko5qUuZWdeGud1GsQj817ipz3fgb0EAHtFN05t0pf4iNAjtu9hHmbp/hX0b5NcXyGLiIiIyAnKWJZVvYrGRAJzgccsy/riCHWGAq8CZ1iWlX2Y5zcCNwK0atWqb2pq6rHGLSISUEUVRTyz+FWGtx7Gma36BjqcgOnxfg/vRclJEL4Fjyscm6OElJAefDHuA8JDjp5nL61wk1/qpGnMkZMkIiIiIiLVZYxZbllWv8M9q9YMEGNMEPA5MOkoyY+ewFvAOYdLfgBYlvUGB/YH6devX/UyLyIix6G//fgy8zI/YuqW6Sy/9kfs9sZ9qNYX6+fy3OK3sCwHTaIiuKH3pfRr1tv33ArZTaugM3ht7LO0ig/HGFOtfsOC7YQF26uuKCIiIiJSS1UmQIz3p9i3gQ2WZT1/hDqtgC+Aqy3L2uzfEEVEji95ZUXMzfgEY8Btz2R/UR4pMfGBDssvtubs4Odd67moyxCySwoodVbQJbk1Lyx7lQKbd+unwhL428K5vjZ/P/lFLug8hNAgJTJERERE5PhVnRkgpwNXA2uMMasOlD0ItAKwLOt14B9AAvDqgd/6uY405UREpKFbn74bY9xEejpTZNvIztyMRpEA2Zq7lQu/uRgLN7N2nsuq/G8AeLjvy+R5NtDaMZqBKafQNjaF19b/izzXLjpHn8rlPYYFOHIRERERkapV5xSYn4GjzmW2LGs8MN5fQYmIHM+25aQB0DyiHZtLN7KnIBPoHNigaqmovJhxX4/z3f8v+QHwr+V3gIErul3AVX0GAnBhj8/ZWbCT1tGt6z1WEREREZFj0bgXrYuI+NnW3G08vfouADrHdwRgX2GW73lJhYvSCndAYquNmVvXHLZ8YMJFANisMK7oNcBXHuoIpXN8Z8IcOrpWRERERBqGGh2DKyJyopu2Zbnv+tQW3fh6L6QVZVLiLMHpcTJk4sVQ0YJJF/2TrskHZ0d8tmEaH6yfyIdjXiUmNCYQoR/Vk0ueBBtc0eFmLuw8ip/3zKNLYkdOa34aryztxumtumCzKWcuIiIiIg2XfpoVEamBrKIiAHrFjOLsDidjWXam7X+ZAR8NYN6upbgc+3GFL+OyaWN5b8VMckvzeGbRGzy65D52FK1m8CcjeX/tJIqdxQF+k4P25OVTzE6i7c154LRb6ZzQnvG9ruP0lNMxxnD7KZfQp2n3QIcpIiIiIlIrSoCIiNRAZkkOAE8PfYhQRzBB7ia+Zy8tmgpAl6DrsCzDO8tnMGryeD7Y9DIAYc7euCnm2eVPMnDicHq9M4g/fDah/l/iN77euJBRXw7F2Nzc0/fv1T6+VkRERESkodESGBGRGsguzcHyBNE0KhqAKEdTctkHwH7PHAA+vvzPdH/rW3JDZngbeUJ5tP9rXNi9H28unsPbG5+lglKc9gx+zZ+JZT1cb4mHKetmM2vHItIK8mgRk8DKrEUYm5O6KUGfAAAgAElEQVRuYRczrsugeolBRERERCQQlAAREamBgoo8jCcSm82bsAi1hYPn4PNQd1tsNsM17R9kbdYaejZvxt0DLyLYEQTADQOGcMOAIQDcPe1lZmW8waJdmxjYuuanyKxN38a69L2M6dSXyJCIo9a1LIufUhcwYdmdvrLtB/Zu7R1xJR9e/ECNxxcRERERaUiUABERqYEiVz5BRPru/zdz4+zkWzmjVQ96NW0PwP3DhgPDj9rXpV3G8GP6W0xa/wUDWz9Yozg2Z+3limkXgXHzyup2PHrG3zm9RV8q3E4iQ0J99b7Y8BO7i3awfP8qVubMAaBP7GieOOtubp1xP9uLV/DvYTfUaGwRERERkYZICRARkRoodRcQaos+pDzEHsSF3c6oUV8DWrXFOOPZnLOtRu2+WruCR5fcB3Y3VDQnN3g7d879E5bHgcHBoOTL2FeQw9aKbw5tbNl4ffSjhAeFMvn818gpy6FFVGKNxhcRERERaYiUABERqQGnVUhsUPNDyi2sGvdltxmig5qTWbaXhXtW8Oav71FREcIdfW/g1FYdj9juhWWv4rRlMLLJzYxofxqvrXiP+NBYUgu3k+5azbys9311Q9ytGd7sKjokNOHyXqcREmQnyOZdjhMeFE54UHiN4xYRERERaYiUABERqYLH8nDLtEfonzwQt62I6OAY37NzWl3CW1uXMLzN4GPqu2l4CpuKN3LfT0+Qb20EYPzM+bw06H26NU/kxu8fIKM4m8Epw7j39KsoqCgg172d5OCTeW7UbQCMPKmPr79v1+xgd8F+hndpSsvoZthtBxMeIiIiIiInMiVARESq8Ozc71iY+RULM7/C2KB5ZDPfs7vOHMIFXefSJvHom5AeSXJYMptLK3zJjzbhJ7OzZAV3LbwYLDsYN5YnlO/2vcZ3U17zNnJAp9iRh+1vbI+2QNtjikVEREREpDGzBToAEZHj3SfrZgPQMeQ8WjtGMGHoeN8zY8wxJz8AmkY28V1PHvk931zyPq3CDszoMG7u6voCX439gWBiAQg20bQNHsljw2495jFFRERERE5EmgEiInIEKzNWcs20ayASEoJb8/nlj/l9jDB7GACtHSPp3rQlAGe26cqkDSu5o88djO/pPUlmWNuBTNsxjbdHvULv5N5+j0NEREREpLFTAkRE5AjeWfOO7/rUpjU74aW6bhtwHmvSU3l6xMFZJVd1uYpwRzjjOozzlf19wN/pk9yHXkm96iQOEREREZHGzlhWzU8u8Id+/fpZy5YtC8jYIiLVMXzStaS7VgDw3sgP6dtUMy9ERERERI5nxpjllmX1O9wz7QEiInIYBWVO9uWV+e6TwuMDGI2IiIiIiNSWEiAiIoexaX8hnt/MkIsJiTlKbREREREROd4pAXIM7pi8kvs/X31I+S/7fuGlFS+RVZoVgKhExJ++2fo99pAM331UcFQAoxERERERkdpSAuQYzdqYwe/3T1myfwlvrnmTH3b+EKCoRMRfvt33X0xQLgB9kvtgM/rfpYiIiIhIQ6ZTYI7BmScl8s2v+3joq7X8uiePwR2TsNts3DH0Nt5e8zY5ZTmBDlFEasHlceG0iokqHc0vtzwV6HBERERERMQPlAA5BqO6N2XCN+uZtHgXAGv3FgDw0qwtxHaOILtUCRCRhiy/PB+ApIi4AEciIiIiIiL+ojndxyA6NIivbjuNmwa34+Ur+vjKEyKCqagIY0NGWgCjE/EPy7KYvGYG83asC3QotWZZFi/+/C1L9x18l625W/lq61eHnbGVXepd+tI8KrHeYhQRERERkbqlGSDHqENyFA+c0wXLsigud9EuKZJ+rePo/dZ/2ZOfGejwRGplya6t3Pbjnymz78RyxvFg/wlszNrFhGHXVKr3yqJpdEtuwZB2PQIUafU8MfczJqdO4P0tSUwa8y5dklpx9be3UuRJI8HejcSQ5hQ5Cxne6mzuOeNStmV7Nz9tHZcU4MhFRERERMRflACpJWMMl5/SynffJKwp6c6G/xtzadg+WrmQV5ZPZPYfXyLYUfMv89dXTKHMvhMAE5TLE6vuAODCtEH0btYGgJ93ruf1TffBJui2chQfjXsKm+34m1T2xboFTN7xFNjAZcvksmljKz3Pdq8jq2gTAO9vW0S5q5zlmcsA6JKUUu/xioiIiIhI3Tj+Pq00cB1iO2PZ89mWvT/QocgJ7ImV91EQNJ/NWftq3NbpcbI+dyl2V3NeHfRppWer07bhsTyUOEuYsv5HX/m6oul8vXHxMcX6/soZXPHFX8ktzWd52hoenP0sbo/7mPr6vfUZe/nHkjuw8NApfGSlZ8GeZOZeuoDbujzBK4MmM/+yBRhXEh+nPs2Wkp9oaRvDuZ1P9kscIiIiIiISeJoB4mfdkzrwcw4s3buZX9O30qdpB9rGN611v+UuJx+smsH4vmMwxvghUmnMDAYLyC4tqnHbgR+eRbktj/6xVzCwVSeCK7pT4dgENieL9v3K86v/jtsU++pf1/E+3t38NI8t/hcXdP2+xuM9v/LfeOx5jP/WyeaS2QCkflnEpIv+WeO+/sfpdnL3jOdYnbkOY3Py4hkfMqhNN2ZsO48xJ52Jx/JgMzaMMdx8ysEZIS8OepPJa36kVVQzHhx2LnabvtZERERERBoLJUD8rHVsMwB25O7lo52P43A3YeX1P1bRqmq3f/cci/Im4bEMN/UfU+v+pHEz2LCA9KLcGrVLK8yhnDwAnh11Gw67jeU3TKaovIyBH/dnfva78JucQNugc7h7wFW8u/lpymy7KXc5CXEEVXu85+dPxWP3jve/5AfArwVfszPnVtrEJ7M+LZMuTRMpd5djsBHiCAZg8qqFTN38E++P+xsr92/l7VWf0TyyGTlFLn5O/wZXkPeUpmRbf4a17w3A2I6DALAb+2HjOeuk9px1Uvtqxy8iIiIiIg2HEiB+1iHBO9tj6f7lALjs6X7pd1fhbsCbWBGpkuVd3ZZZXLMEyBVf3AfA+JP+TXxYjK88MiQUy+PA2Fy0sp3Ly2P+Qtu4BN9spGGJtzEr6xU2Ze1hxd5U4sMiOa9rv6OOlVaYy7tbHgM7dIsYw7qi6YxpdjcDUrrx8LLx3DLtUaJDQ1hfNBOHuwkuezrRdKBNZE/WFMzEshUC0O+jTw52euBAF5stmvahQxnS+lSu731Bjf4diIiIiIhI46QEiJ+1i/MmQLaUzfCVZRTlkxwZc6Qm1WLw/sa6zF1Wq37kxGAObO+TU5pfo3Y5rm3ggEt7DDrk2YRTXmZ91nYeGnzNIc86JbZiVhaszdjJc2vvBOC8rmuOOtZt0x8Beyk3d3qCWweMocT5MBHBEQA8taQTe1zz4MAKnv8lEgvYyuqirdjdLUiw9STPlUYFOXSOHMyV3c7n590riAgJ5pEh12C3HX6Wh4iIiIiInJiUAPGzw524cfO3T/DF5U9WKssrKWX4hzdze78/8ce+h37Y/D0LDwDbc/ewJ6+IFrGR/glYGqVjSYB4PB489mw6hIykWVTcIc8v7HYGF3LGYdt2TGgJwOasXb6yVXv30qVpIiH2kEPqz9m+ni0ls8Edxc39R2OM8SU/ALrFnsbigk2kBPfl5ZGPkBTWlPBgB3/85C2Cg8v4z9gbiA0Lx+n2YACH3fu+47r3r/b7ioiIiIjIiUUJkDqw4LLFXP7FX+gY04vV2cvYUjKXCperUnJkQep6ykNX8NzaFVzWYxVhwUf/bXWxKw8M7HBOY8THa1h/87S6fg1pwIzx/l3LKs2qdpvzPnoQYyzaRLep8Xht45IBWJ2xwVd29Y+jAJhzyQISwqOxLMu3ZObVZR9hWYb3RkzEbj/0MKr/jr2DTelX06tFfKVNfydfdVOlekGHaSsiIiIiInI4+vRQB6JDw/n+ytd5ccwtDG4xBOxFnDLpVNZmbvXV2ZC9w3d9yuTeDHzzJtILC4/YZ6k7z3dtD9uDy+1mye5NvLfyBwC25OymsLzU/y8jDZLHqgBgZcGX7M2reh+QCpebnRWzsFe05uHB19Z4vJaxiQBsKf/ukGeTf52HZVn0fPWPXP/Za+wpSGNj8SxirJ70a9HusP2FBdvp3TJBJx6JiIiIiIjfKAFSx0Z0GACAm3L+/tN/fOXbc3dVqlcUvJDbv3/miP04KSDInYLl8f5m/7F5E/nTTxfz3Oq/MuD9MVz4zWjOmDSCR+e+wbqMbXXwJtKQeIx3rxhjc3HxlzcdsZ7T7ebxOZ/wy64tGHsZo9qMJSEiqsbjBdkPzmCyu5L5dMTPzLxwAQD/t/kBrpjyL4hcwdLiVznnyxFYtiKu6HxljccRERERERE5VkqA1LEBLTqQzOkApBavxrIsAHYUbAN3FCOa3MT4jg9j98Swo3jlYftwuT14bAV0iu7HL1f8Aq4Yfth1cAlMCd5kiseex2c7X2b89/fW8VvJ8c5jyn3XRbYN7MrPOGy9537+ksmp/+bPc/4MQL9mXWs99tJrf6BLsxiaRkXTJngYAOtKp1Sqc07zG7l94OhajyUiIiIiIlJdSoDUMZvNxqxrX+eMuBtx23NYn5nK0A+vZY9rHhG04rlRt3PXwEs5KWIwZbbdlDjLD+njyblfYGxuksITiQoNJTmoGwWs8z037lhmjJvH9Sf9g1BXJ4rMFvYWZNfna8pxxO1xY2zlxLgH0DPMu5xl8q9zD1t3R773eGWnfQ8AZ7Tufszjdo46nW4xgwhyHJwN8uWlz/H56Jn0jxzPHZ1eYe4l83mw79M8Nfz2Yx5HRERERETkWCgBUk/+d0rGt5sWk+VZAUCn2D6+592TOmGMm+83LSG7pPKeDQvS5gFwU/+x3nZxBz+kPjPwQ1ZfP5/m0XH8+bRLuKrz1QDM2r6s7l5GjmvZJd69ZLolduHJEdcBsKtg7yH1Zmyfx8Lcd3z3HmcUTaPij3ncKRe+zscXvFKpzGG30zGpKe9cdBc3njqI+PBYruh+jvb2EBERERGReqcESD3pEN8cgOVpawGIM914afQdvudD2vYE4NHltzLs4/Nxe9y+Z5nlqUR4OtItqQMAp6WcDICpaMHZ7XtWGue0Vt7kyDO/3sN5H99Bau7hlz5I45VZXABAZHAESQf28yiqKD6k3ivLPqx0f323I+8VIiIiIiIi0tApAVJPuiSnAJBatBmAvw34CzGh4b7ng1r3Iskc2DDVnst5n1+N0+Pk3Ml3Um7fTpvILr66l/YcyMi4fzPj0s8OOUK0f4t2NLMNAmBH+Rz+NPXIG6tK45Rd4k2ARAdHEhoUjOUJosRVdEi93FJv2exL5vDp2E/568Can/4iIiIiIiLSUCgBUk/axDbBsgzFVioAHRKaV3pujOGna95i2rg5AOwqWcPDP73OzorZeFxR3DfwT766wQ4bz553Ps1iIg4ZxxjDD1e/wpIrVhFr9WW/Zx7FFToe90SScyCxERMSCYCxQtlY+i0zd/7E/XP+yerM1QAUuNNIZACJ4Ql0SehyxP5EREREREQaAyVA6onD7sDmicY4isETRru4Joet1yI6gRi6AfDd7g+xPA4WXPkjJ7doW6PxwoLtjGk7FmMvYcbW5dzw9RMMfe820ouyav0ucnzbV+hd9pQU4d3Pw+DdlPQvc+/iu9TPuWna/bjcLty2XJLCmgUsThERERERkfqkBEg9shMGQBP7KQTZg45Yb/plH+BxRoOtlGBXG2LDwo9Y92iGtesNwCNLb2FR7kdkmXn86ZtHjqkvaRimb13Eaxv/DkCvpu0AsOx5leoUWbsZMulijPGQEqkEiIiIiIiInBiUAKlHTuOdfXFBh/OOWi8yNJiLWt6DM68vN/W6+ZjH65vSDtxhlcpSK+bx2uLpFJW7sCzrmPuW49MLS1/zXXdKPHJyI9/aBkDP5E51HpOIiIiIiMjxwBHoAE4kCaVXs79sMzdcNazKuhNGjmPCyHG1Gs9msxFOe0pYS/fI0disUFYXf8GrG+/llQ2GFvahTL/6P7UaQ44zrhgAKrLPJMjhXfri3H8RHlsh95zdk7zyQn7avpzOCR0pLC/i2pOHBjJaERERERGReqMESD36/JrbKCh1EnLgg2l96Nu0K/Mz19IuMY7HBt/Pe7/24LlVj2KMxR7XPEoqKggPDq63eBqiMqcbp9tDVOiRly0dL3LLswm2p/DTDS/4yubecj8VLg8t471Lqe4bEKjoREREREREAkdLYOpRYmQI7ZIi63XM87r2B6B5TBQA1/a8yPfM2FzM3r6mzmN4d+27XPDVBXU+Tl0Z9cEDDHj30Nk4S9KW0G9iP7JKj4+NZTdmplIatJrksCSSokJ85U2iQ33JDxERERERkROVEiCN3IjWI5hw2gSu73494D0m980Rb/LHjvcAsDMvrU7Hr3B5eH7582zL30Z6cXqdjlVXsh3TsIensjsvt1L5f1e9Srm7nJXpKwMUWWVT1s0BYHTbsYENRERERERE5DikBEgjZzM2xp00jjDHwc1QT212Kv2b9gNgf1EOe3JL6mz8FxZ87bs+75M7mTD7gzobqy6UVFT4rq/58mHcbg8Ai3ev9yU+vt+4PiCx/dbW7DQ+TX0WyxPEjX0vqrqBiIiIiIjICUYJkBNUy5hEAL5MfYWRnw/nifnvHLbe3qK9tRpn8f6FvusS+3qm7HqmQZ0+M3/nRt91lm0u329ZCsBDc18A402G/Jj+Fs/9PIW3l8zH4/EEJM57Z74EgOWMJ8ShrX1ERERERER+TwmQE1RKdAKWZTCOEmxBhXyyZdIhH94/WTuTUZ+P4u0VU2vc/zMLJvLQT6+wu2Qdoa5OBFvxvmc78/bXOv76smD3agD+0v0FLMvGc0teYcKPX5FWsZIWjsE8f/p7ALy3bQIvbriVa778V0DizCjbDsA9vQMzvoiIiIiIyPFOCZATVLDDgTHemRh2Zwvc9gymb11Wqc5P270f/r/bsqBGfZdWuPlg61NM3f06ZbZddIzpwYxLv+DiVvcCsHzfFj+8Qd3bmZ3Hrxm/YnmCuKLnIGKsnmRbK5my92GMvZw/dLuEszv05cLWt9EiaCDh7s78WvQZs7bU75KYGVsWU2DW0yX0Iq47ZWC9ji0iIiIiItJQVJkAMca0NMbMNsasN8asM8bcdZg6xhjzkjFmqzFmtTHm5LoJV/wpjBQAJpz2byyPnbdXflHpeXhQKADl7vIa9btq78EZHpZl47b+l5AYnsApzXsCsDFrB88teptyV836rU+5pcWMnXo22yumE261IzQomE8ufJE2QcPpFX4lj/Z9l6t6Dwbg0SE3M+3KN/jgvGcB+O/y93hx0cQ6X+ozY9M6rvviOf6x4F9Ylp0nz769TscTERERERFpyKqzWYAL+KtlWSuMMVHAcmPMTMuyfvtr7nOAkw78MwB47cCfchybceknLE9bz/B2fXl8cQdSi38/c8H7Ad5ZwwTI/F3ezUFbOAbx1rmPkhLt3W+ke5OWAHyy05soKC6Dfwz5Uy3ewP9KnU4+X7uY7LJMjL0MgGu6jAegRUwC31z5whHbdkpsDcDW8mls3TSNtze+wAcjP6ZPs5PqJNZHFj5KsW0DGOgffRXt4hPrZBwREREREZHGoMoEiGVZaUDagetCY8wGIAX47afl84EPLO+vvBcZY2KNMc0OtJXjVFxYDMPbeZdMhNtjyHGmVnpeUF4IQJmnuEb9rslcA8DEcf8mITzOV54SHVepXmp+5WNxv98yH7fl5tyOQ2o0nj89PudTvtr3JACWO4S5l80lISKi2u1bhvZhd9lKXEWdcERu4m+zJzDjig8wxvg1TsuyKPPkgQ3CbYm8c8Hf/Nq/iIiIiIhIY1Oj4yKMMW2APsDi3z1KAXb/5n7PgTIlQBqIMEckblfl43ALnd4ESL6r+ifBuD0Wq7IXYg9KqpT8ALDZKq+42l2YSlZpFolhiYz7/Hq2FnlPWDmj5XziwmKP5TVqbU3Wat91vL1bjZIfAJ+Oe5USVwnJ4ckMfG8cac5VfL5hDhd3HerXOD/6dT5uRxr94y7g2RF/9nuCRUREREREpLGp9iaoxphI4HPgbsuyCo5lMGPMjcaYZcaYZZmZmcfShdSRyKAoLFNaad+KEmcRAB5HJplFh/4nn7H5V85+/88UlR9cInP7N69C6E7i7Ydf9vFQ/3/zzJkvEeHpQpp7IUM/Hcr/LZvqS34AvLXiG3+91lF9uPwXTnnzOmZvXwVATkkh20uWEOJJYXDSlbw79oka9xkZHElyeDIAE8e+CcDs7Sv9FzTe2R8vr3oG3FE8O/w+4kPjq24kIiIiIiJygqvWDBBjTBDe5Mcky7K+OEyVvUDL39y3OFBWiWVZbwBvAPTr169ud4iUGokKjsLYXOSXlRIbFg5AvjPb93xrzn6SIqMrtfnrwmswxsNP21azdN8GruhxFiuz5+DxRPPKuf887DiXdT0fgEfmvOIr+++6hwC4qt29TNr8Jt9tm8m9p19drbjTi3Kx22zEhUZjt9VsFsQHaz+lNHgZd86/mog53fDY8/DYs7m1+7Nc329Ejfo6nDZx8ViWjZyyvCPW+X7zAppHJ9G7aceq410xm/fXfUSJO5tis5M+sZcRHx5V6zhFREREREROBFUmQIx3bv3bwAbLsp4/QrWvgduNMR/j3fw0X/t/NCwxId7kxt6CHF8CpNCdhmWCMbYKdubuZ2Cryh/SjfEA8Pel1wMwLfULysmhWUhXujRpdtTx7jr5z7y47DXCgoLIYRnGE8Y9p13Joj1r2Fr2I5lFBYckXP7H6XZyy3dP0iaqPZ/teAu3PROPM5Z/nfo8F3bvz668TO754T8MbNGTWwecT4g95LD9ZLs2+74Ciu3rAIi3+vsl+QFgt9swngjyKw6fAPl4zU88tsJ7qNLIpjfz7MjbDqlT5qzAsgyfb5zF06vvxxg34KBfzOW8OOqQA5lERERERETkCKozA+R04GpgjTFm1YGyB4FWAJZlvQ58D4wGtgIlwHX+D1XqUmyoN9mQUZRLtyYtKC4vx2XLJoZuFLCGLTm7OO2da/nbqbdzftf+lDkrDumj3LHt/9u7+yArq/uA49/fvbvL5WV5f1eDuDGoIZGiQhUU4ihvkSFpgyKZquAYUyXodJymSTvRqmPQlGTSWJuYiYmmFcWiYjKahM5kVDqRLFCIgqUiGlkEtgblZZFdlj39415elgC6YXfvevf7mdm5957n3vv8Fp7fc579zXnOAaB/buAH7m/2qHHMHjUOgD/sfY89DfWUZcv4y7Omct/vfsHnF9/Kc7MfoDKXY83WjdxfvYi7PnMTQyr7MefJhazdu5gV7wJZoLEPmfJ3uX3VXO6truIADdRnNvPqa0v50YZvMnHAXO6/4ivN9r9u+2YaspsZ02s2M8+ezIj+p/CzDb/lqk+NP7l/yKOUpR7saHiL+sb9dCkrb7bte6t+CIVBK7/c9n3GvjyKK84aTa4sx676vcx/7lusem8pkWkEIALmVN3NDedPpjKXa9U4JUmSJKnUfeAcICml5SmlSCl9OqU0qvDzbErp+4XiBynv5pRSVUrpUymllW0fulpTZUUPAHbW51d8+cyjM4lo4rwB+VVinnjrXnZnV3Pnb+4AYH3t4TucujaO5L6xP6Vf08XkDpzBledMa9G++3XrzbDegwD44rkTGZodz85sNXe/8AgAC5b/hBXvLmbOM//Ais2vsaZuUbPP/+rKZ5g08GYGx8XszbxOfWYzoytnMaH/XLqnKp7/w4M8uvbXAOzeV8+itc9z/XPzgWDuqM8xdcRozug3iFsums7gyuYTt56spux71MXrTHvsSzSlJpZtXM2Sdcupa9jHrqZNnFY+kR9MeJqUsiyoXsCYf7uYsQ9PY+aSW1i9awmRaaRrGsaEfnP5wYSn+ZvxMyx+SJIkSdKfoEWrwKh09azIr3ayc19+4tP3M28AcOMFM1jx8+XszeZXPT5APU+9soo9DfkVYm74xN3MGzudTCbD1LMeOOk4MpkMz83+F879yYU8u+VBqlYOoaZuEwTU7P8v5i3bRmSb+O74RWzfvZPf1LzMkMreLJz6ZQCeWLuKmj1vc8uFnyWTybBj7w1MeHwi31wzn9d3zmPppv+gPrZBwCX9rmX86WefdMwn8tlTr+dnW+6n9sBKrnnqH1m78+eQaeCOlUAWrqiaxEWnV/HJ7tNZv/dpAN6nhvcba6hMI7jmnOv48pgr2jRGSZIkSeoMLIAIgF65fAFkV8Me6hvzq7oMiAv45MCPcfu4v+WrL10HwIGy7Xxj1XWQMhAwemjVHy1ve7IymQzl0YP9mVq+t+7rEFDRNISGzFb2ZTdCynBp1UgAri7cRnPQzHPPA8479Lpvtx6cUnEBWxqXs/iN+yFgYLqUb0+Zz7mDq1o17mO557Ibmb/nC1y+ZCJrdz8JGTgjN4FdDTs5s/cIbho7HYDHZ97Fj1dfQqKR/Q1d+O/atXxnyk10rTj2/CWSJEmSpJaxACIAenbJ3wKzu76OjTtqABjV/yIAxg0bAS8d9YHCBKgjB36sTeLZn6lt9vqeCd/gthf/GoCvjV7You96eMa9LHxxKZHdQ99uldx24Wyy2dYt2pzI4B794EB3yNYxsvt0Fn3hnmO+b87oy494dWn7BCdJkiRJnYQFEAHQJ5dfTnXP/jo2vPMWAKf1HApAr1y+ODKwy3Bq699o9rneXY+9UsvJumDgeKprl7Pyi6uoyJYTEdz2IvStOJXZn76sRd81qEdP7pv64ZbVbStZunKAOkYO/ODlbiVJkiRJrc8CiADo0zVf5Hhi9essrn6brkNh9NDhh7a/cNUL5Mpy1O2vo3t5d7bVbaOyorLN4nng8u+wt3EvXcoqDrU9f9XzdC3r2mb7bEuDevTm7fff4eyBJ14eWJIkSZLUNiyACDhcACnrtolEQAouGvbxw9tz+dVRDhYghvca/sdf0opyZTlyZc1XO+mb69um+2xLQ3r24u33YTAyjbYAAAW6SURBVHBl2xWNJEmSJEnH134TIahD61ZRTlNDP8oq/4fyylep6v1xyrPlxQ6rZMwaMQto+8KRJEmSJOnYHAEiALqUZajbdCuR2QfA4jv/osgRlZYpw6cw+fTJRESxQ5EkSZKkTskCiADyf5incvrmuvP4jRdSka344A+pRSx+SJIkSVLxWADRIdV/fxmVuTJy5dlihyJJkiRJUquyAKJDBlR2KXYIkiRJkiS1CSdBlSRJkiRJJc8CiCRJkiRJKnkWQCRJkiRJUsmzACJJkiRJkkqeBRBJkiRJklTyLIBIkiRJkqSSZwFEkiRJkiSVPAsgkiRJkiSp5FkAkSRJkiRJJc8CiCRJkiRJKnkWQCRJkiRJUsmzACJJkiRJkkqeBRBJkiRJklTyIqVUnB1H/B/w+6LsvHX0B94pdhBSB2E+SM2ZE9Jh5oPUnDkhHdYW+TAspTTgWBuKVgD5qIuIlSml84sdh9QRmA9Sc+aEdJj5IDVnTkiHtXc+eAuMJEmSJEkqeRZAJEmSJElSybMA8qd7sNgBSB2I+SA1Z05Ih5kPUnPmhHRYu+aDc4BIkiRJkqSS5wgQSZIkSZJU8iyAtFBETImIDRGxMSL+rtjxSO0lIt6MiJcjYk1ErCy09Y2IZRHxWuGxT6E9IuKfC3nyu4gYXdzopZMTEQ9FRG1EvHJEW4uP/4i4tvD+1yLi2mL8LlJrOE5O3BERWwr9xJqImHbEtq8VcmJDREw+ot3rKn3kRcRpEfHriFgfEesi4pZCu/2EOp0T5EOH6CO8BaYFIiIL/C9wOVADVANXp5TWFzUwqR1ExJvA+Smld45ouw/YkVJaUDgp9UkpfbVwQvsKMA0YC3w3pTS2GHFLrSEiLgH2AI+klEYW2lp0/EdEX2AlcD6QgFXAeSmld4vwK0kn5Tg5cQewJ6X0T0e99xxgETAGGAr8J/CJwmavq/SRFxFDgCEppdURUUn+/P454DrsJ9TJnCAfrqQD9BGOAGmZMcDGlNKmlFID8Bgwo8gxScU0A3i48Pxh8ie3g+2PpLyXgN6Fk6H0kZRSegHYcVRzS4//ycCylNKOwsXsMmBK20cvtb7j5MTxzAAeSynVp5TeADaSv6byukolIaW0NaW0uvB8N/AqcAr2E+qETpAPx9OufYQFkJY5Bdh8xOsaTvyfKZWSBPwqIlZFxJcKbYNSSlsLz7cBgwrPzRV1Bi09/s0LdQbzCkP6Hzo43B9zQp1IRJwO/BmwAvsJdXJH5QN0gD7CAoikD2t8Smk0MBW4uTD8+ZCUv5/Oe+rUKXn8SwD8K1AFjAK2AguLG47UviKiB7AEuDWltOvIbfYT6myOkQ8doo+wANIyW4DTjnh9aqFNKnkppS2Fx1rgKfLD0rYfvLWl8FhbeLu5os6gpce/eaGSllLanlI6kFJqAn5Ivp8Ac0KdQESUk/9j799TSk8Wmu0n1CkdKx86Sh9hAaRlqoEzI2J4RFQAs4BnihyT1OYionthEiMiojswCXiF/PF/cIbya4GlhefPANcUZjn/c2DnEUNApVLR0uP/l8CkiOhTGPY5qdAmlYSj5nr6PPl+AvI5MSsiukTEcOBM4Ld4XaUSEREB/Ah4NaX07SM22U+o0zlePnSUPqLsZL+gM0kpNUbEPPInoizwUEppXZHDktrDIOCp/PmMMuDRlNIvIqIaWBwR1wO/Jz+7M8Cz5Gc23wjsBea0f8hS64mIRcBEoH9E1AC3AwtowfGfUtoREXeR79AB7kwpfdhJJKUO5Tg5MTEiRpEf5v8mcCNASmldRCwG1gONwM0ppQOF7/G6SqVgHPBXwMsRsabQ9nXsJ9Q5HS8fru4IfYTL4EqSJEmSpJLnLTCSJEmSJKnkWQCRJEmSJEklzwKIJEmSJEkqeRZAJEmSJElSybMAIkmSJEmSSp4FEEmSJEmSVPIsgEiSJEmSpJJnAUSSJEmSJJW8/wdZDqQXbKQ9QwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVVTuw0xYXt8"
      },
      "source": [
        "# Création du modèle type Wavenet Multivarié Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB6dP-d8YXt9"
      },
      "source": [
        "**1. Classes de dilatations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzpdbYrbYXt-"
      },
      "source": [
        "from keras.layers import Conv1D\n",
        "from keras.layers import Conv1D\n",
        "from keras.utils.conv_utils import conv_output_length\n",
        "from keras import layers\n",
        "from keras.regularizers import l2\n",
        "import keras.backend as K\n",
        "from keras.engine import Input\n",
        "from keras.engine import Model"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNMpYRuEYXt_"
      },
      "source": [
        "class CausalDilatedConv1D(Conv1D):\n",
        "    def __init__(self, nb_filter, filter_length, init='glorot_uniform', activation=None, weights=None,\n",
        "                 border_mode='valid', subsample_length=1, atrous_rate=1, W_regularizer=None, b_regularizer=None,\n",
        "                 activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True, causal=False, **kwargs):\n",
        "        super(CausalDilatedConv1D, self).__init__(nb_filter, filter_length, weights=weights, activation=activation, \n",
        "                padding=border_mode, strides=subsample_length, dilation_rate=atrous_rate, kernel_regularizer=W_regularizer, \n",
        "                bias_regularizer=b_regularizer, activity_regularizer=activity_regularizer, kernel_constraint=W_constraint, \n",
        "                bias_constraint=b_constraint, use_bias=bias, **kwargs)\n",
        "        self.causal = causal\n",
        "        self.nb_filter = nb_filter\n",
        "        self.atrous_rate = atrous_rate\n",
        "        self.filter_length = filter_length\n",
        "        self.subsample_length = subsample_length\n",
        "        self.border_mode = border_mode\n",
        "        if self.causal and border_mode != 'valid':\n",
        "            raise ValueError(\"Causal mode dictates border_mode=valid.\")\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        input_length = input_shape[1]\n",
        "        if self.causal:\n",
        "            input_length += self.atrous_rate * (self.filter_length - 1)\n",
        "        length = conv_output_length(input_length, self.filter_length, self.border_mode, self.strides[0], dilation=self.atrous_rate)\n",
        "        return (input_shape[0], length, self.nb_filter)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        if self.causal:\n",
        "            x = K.temporal_padding(x, padding=(self.atrous_rate * (self.filter_length - 1), 0))\n",
        "        return super(CausalDilatedConv1D, self).call(x)"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93bV8CcwYXuA"
      },
      "source": [
        "def _compute_receptive_field(dilation_depth, stacks):\n",
        "  return stacks * (2 ** dilation_depth * 2) - (stacks - 1)"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSZKN-OS5r1e"
      },
      "source": [
        "**2. Modèle finance - Single Exo Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAmgCh3_YXuB"
      },
      "source": [
        "def build_model_residual_block(x, i, s,nb_filters, dim_filters, use_bias,res_l2):\n",
        "        original_x = x\n",
        "        selu_out = CausalDilatedConv1D(nb_filters, dim_filters, atrous_rate=2 ** i, border_mode='valid', causal=True, bias=True, name='dilated_conv_%d_selu_s%d' % (2 ** i, s), activation='selu',\n",
        "                                       W_regularizer=l2(res_l2))(x)\n",
        " \n",
        "        res_x = layers.Conv1D(1, 1, padding='same', use_bias=use_bias, kernel_regularizer=l2(res_l2))(x)\n",
        "        skip_x = layers.Conv1D(1, 1, padding='same', use_bias=use_bias, kernel_regularizer=l2(res_l2))(x)\n",
        "        res_x = layers.Add()([original_x, res_x])\n",
        "        return res_x, skip_x\n",
        "\n",
        "def build_model_couche_condition(x,y,Nbr_Inputs, output_bins, nb_filters, dim_filters, use_bias,res_l2):\n",
        "        main_1st_exo = CausalDilatedConv1D(nb_filters, dim_filters, atrous_rate=1, border_mode='valid', causal=True, bias=True, name='dilated_conv_main_1st',\n",
        "                                           W_regularizer=l2(res_l2))(x)\n",
        "\n",
        "        exo_dil_layers = []\n",
        "        exo_res_layers = []\n",
        "        for index_cond in range(0,Nbr_Inputs-1):\n",
        "          exo_dil_layers.append(CausalDilatedConv1D(nb_filters, dim_filters, atrous_rate=1, border_mode='valid', causal=True, bias=True, name='dilated_conv_exos_1st_%d'%(index_cond),\n",
        "                                                W_regularizer=l2(res_l2))(tf.expand_dims(y[:,:,index_cond],-1)))\n",
        "          \n",
        "          exo_selu = layers.Activation('selu')(exo_dil_layers[index_cond])\n",
        "          exo_selu = layers.Conv1D(1, 1, padding='same', use_bias=use_bias)(exo_selu)\n",
        "          exo_res_layers.append(layers.Add()([exo_selu,tf.expand_dims(y[:,:,index_cond],-1)]))\n",
        "        \n",
        "        exo_res = layers.Add()(exo_res_layers)\n",
        "        exo_sum = layers.Add()(exo_dil_layers)\n",
        "        main_sum = layers.Add()([exo_sum,main_1st_exo])\n",
        "\n",
        "        main_sum = layers.Activation('selu')(main_sum)\n",
        "        main_skip = layers.Conv1D(1, 1, padding='same', use_bias=use_bias)(main_sum)\n",
        "        main_res = layers.Add()([main_skip,x])\n",
        "\n",
        "        return  exo_res, main_res, main_skip\n",
        "\n",
        "\n",
        "def build_model(fragment_length, nb_filters, dim_filters, Nbr_Inputs, output_bins, dilation_depth, stacks, use_skip_connections, use_bias, res_l2, final_l2, n_filter_quant=32,n_filters_input=32,drop=0.0):\n",
        "        # input = (1914, taille_fentre, 22)\n",
        "        input_shape = Input(shape=(fragment_length, Nbr_Inputs), name='input_part')\n",
        "        \n",
        "        x_input = tf.expand_dims(input_shape[:,:,0],-1)         # x_input = (1914, taille_fentre, 1)\n",
        "        conditions_input = input_shape[:,:,1:]                  # conditions_input = (1914, taille_fentre, 21)\n",
        "\n",
        "        # Pre-process main serie\n",
        "        out = CausalDilatedConv1D(2, 1, atrous_rate=1, border_mode='valid', causal=True, bias=True, name='dilated_preprocess')(x_input)\n",
        "\n",
        "        # Couche de condition\n",
        "        res, main, skip = build_model_couche_condition(out,conditions_input,Nbr_Inputs,output_bins,nb_filters,dim_filters,use_bias,res_l2)\n",
        "\n",
        "        out = layers.Add()([main,res])\n",
        "        \n",
        "        for s in range(stacks):\n",
        "          for i in range(1, dilation_depth + 1):\n",
        "            out, skip_out = build_model_residual_block(out, i, s, nb_filters, dim_filters, use_bias, res_l2)\n",
        "\n",
        "        out = layers.Conv1D(1, 1, padding='same', kernel_regularizer=l2(final_l2))(main)\n",
        "\n",
        "        model = Model(input_shape,out)\n",
        "        return model"
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWiy4ikQ5vHy"
      },
      "source": [
        "**3. Modèle WaveNEt**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXcBR1Gw5xqo"
      },
      "source": [
        "def build_model_residual_block_Sans_Conditions(x, y, i, s,nb_filters, dim_filters, use_bias,res_l2):\n",
        "        original_x = x\n",
        "        tanh_out = CausalDilatedConv1D(nb_filters, dim_filters, atrous_rate=2 ** i, border_mode='valid', causal=True, bias=True, name='dilated_conv_%d_tanh_s%d' % (2 ** i, s), activation='tanh',\n",
        "                                       W_regularizer=l2(res_l2))(x)\n",
        "        sigmo_out = CausalDilatedConv1D(nb_filters, dim_filters, atrous_rate=2 ** i, border_mode='valid', causal=True, bias=True, name='dilated_conv_%d_sigmo_s%d' % (2 ** i, s), activation='sigmoid',\n",
        "                                       W_regularizer=l2(res_l2))(x)\n",
        "        x = layers.Multiply()([tanh_out, sigmo_out])\n",
        "\n",
        "        res_x = layers.Conv1D(nb_filters, 1, padding='same', use_bias=use_bias, kernel_regularizer=l2(res_l2))(x)\n",
        "        skip_x = layers.Conv1D(nb_filters, 1, padding='same', use_bias=use_bias, kernel_regularizer=l2(res_l2))(x)\n",
        "        res_x = layers.Add()([original_x, res_x])\n",
        "        return res_x, skip_x\n",
        "\n",
        "\n",
        "def build_model_residual_block_Avec_Conditions(x, y, i, s,nb_filters, dim_filters, use_bias,res_l2):\n",
        "        # x = (1914, taille_fentre, 1)\n",
        "        # y = (1914, taille_fentre, 21)\n",
        "\n",
        "        original_x = x\n",
        "\n",
        "        # Convolution des conditions\n",
        "        conv_y1 = []\n",
        "        conv_y2 = []\n",
        "        for index_cond in range(0,y.shape[2]):\n",
        "          conv_y1.append(layers.Conv1D(nb_filters, 1, padding='same', use_bias=use_bias,name='conv_y1_%d_%d_%d' %(i,s,index_cond))(tf.expand_dims(y[:,:,index_cond],-1)))\n",
        "          conv_y2.append(layers.Conv1D(nb_filters, 1, padding='same', use_bias=use_bias,name='conv_y2_%d_%d_%d' %(i,s,index_cond))(tf.expand_dims(y[:,:,index_cond],-1)))\n",
        "        sum_conv_y1 = layers.Add()(conv_y1)\n",
        "        sum_conv_y2 = layers.Add()(conv_y2)\n",
        "\n",
        "        # Convolution dilattée #1\n",
        "        tanh_1 = CausalDilatedConv1D(nb_filters, dim_filters, atrous_rate=2 ** i, border_mode='valid', causal=True, bias=True, name='dilated_conv_%d_tanh_s%d' % (2 ** i, s), activation='tanh',\n",
        "                                       W_regularizer=l2(res_l2))(layers.Add()([sum_conv_y1,x]))\n",
        "        sigmo_1 = CausalDilatedConv1D(nb_filters, dim_filters, atrous_rate=2 ** i, border_mode='valid', causal=True, bias=True, name='dilated_conv_%d_sigmo_s%d' % (2 ** i, s), activation='sigmoid',\n",
        "                                       W_regularizer=l2(res_l2))(layers.Add()([sum_conv_y2,x]))\n",
        "        x = layers.Multiply()([tanh_1, sigmo_1])\n",
        "\n",
        "        # Calcul des résidus et du skip\n",
        "        res_x = layers.Conv1D(nb_filters, 1, padding='same', use_bias=use_bias, kernel_regularizer=l2(res_l2))(x)\n",
        "        skip_x = layers.Conv1D(nb_filters, 1, padding='same', use_bias=use_bias, kernel_regularizer=l2(res_l2))(x)\n",
        "        res_x = layers.Add()([original_x, res_x])\n",
        "        return res_x, skip_x\n",
        "\n",
        "def build_model(fragment_length, nb_filters, dim_filters, Nbr_Inputs, output_bins, dilation_depth, stacks, use_skip_connections, use_bias, res_l2, final_l2, n_filter_quant=32,n_filters_input=32,drop=0.0):\n",
        "        # input = (1914, taille_fentre, 22)\n",
        "        input_shape = Input(shape=(fragment_length, Nbr_Inputs), name='input_part')\n",
        "        \n",
        "        x_input = tf.expand_dims(input_shape[:,:,0],-1)         # x_input = (1914, taille_fentre, 1)\n",
        "        conditions_input = input_shape[:,:,1:]                  # conditions_input = (1914, taille_fentre, 21)\n",
        "\n",
        "\n",
        "        # Pre-process\n",
        "        out = CausalDilatedConv1D(n_filters_input, 1, atrous_rate=1, border_mode='valid', causal=True, bias=True, name='dilated_preprocess')(x_input)\n",
        "\n",
        "        # Dilated layers\n",
        "        skip_connections = []\n",
        "        for s in range(stacks):\n",
        "            for i in range(1, dilation_depth + 1):\n",
        "                out, skip_out = build_model_residual_block_Avec_Conditions(out, conditions_input, i, s, nb_filters, dim_filters, use_bias, res_l2)\n",
        "                skip_connections.append(skip_out)\n",
        "\n",
        "        # Ajoute les skipped connexion\n",
        "        if use_skip_connections:\n",
        "            out = layers.Add()(skip_connections)\n",
        "        \n",
        "        # Post-process 1 : Combien de filtres vont contribuer à quantifier les sorties\n",
        "        out = layers.Activation('relu')(out)\n",
        "        out = layers.Conv1D(n_filter_quant, 1, padding='same', kernel_regularizer=l2(final_l2))(out)\n",
        "\n",
        "        # Post-process 2 : Nombre de quantifications\n",
        "        out = layers.Activation('relu')(out)\n",
        "        out = layers.Conv1D(output_bins, 1, padding='same')(out)\n",
        "\n",
        "        # Softmax\n",
        "#        out = layers.Flatten()(out)\n",
        "        out = layers.Dropout(drop)(out)\n",
        "#        out = layers.Dense(output_bins,activation=\"softmax\")(out)\n",
        "        out = layers.Dense(output_bins,activation=\"softmax\")(out[:,fragment_length-1,:])\n",
        "#        out = layers.Activation('softmax')(out)\n",
        "\n",
        "        model = Model(input_shape,out)\n",
        "        return model\n"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-i75DMlNYXuD"
      },
      "source": [
        "**2. Construction du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EX9vTKthYXuE",
        "outputId": "ce75631c-81e9-45e7-9f29-e23e431e5f70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def compute_receptive_field_(dilation_depth, nb_stacks):\n",
        "    receptive_field = nb_stacks * (2 ** dilation_depth * 2) - (nb_stacks - 1)\n",
        "    return receptive_field\n",
        "\n",
        "n_filters_input = 64 #mu+1\n",
        "\n",
        "nb_filters = 64 #mu+1\n",
        "dim_filters = 2\n",
        "Nbr_Inputs = x_train.shape[2]\n",
        "\n",
        "n_filter_quant = 64    # mu+1\n",
        "\n",
        "output_bins = mu+1\n",
        "\n",
        "\n",
        "drop = 0.09\n",
        "\n",
        "dilation_depth = 3\n",
        "nb_stacks = 1\n",
        "use_skip_connections = True\n",
        "use_bias = False\n",
        "res_l2 = 0.001\n",
        "final_l2 = 0.001\n",
        "\n",
        "fragment_length = compute_receptive_field_(dilation_depth, nb_stacks)\n",
        "fragment_length\n",
        "\n",
        "model = build_model(fragment_length, nb_filters, dim_filters, Nbr_Inputs, output_bins, dilation_depth, nb_stacks, use_skip_connections, use_bias, res_l2, final_l2, n_filter_quant,n_filters_input,drop)\n",
        "model.summary()"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_33\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_part (InputLayer)         [(None, 16, 19)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_3784 ( (None, 16, 18)       0           input_part[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_3785 ( (None, 16)           0           tf.__operators__.getitem_3784[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_3787 ( (None, 16)           0           tf.__operators__.getitem_3784[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_3789 ( (None, 16)           0           tf.__operators__.getitem_3784[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_3791 ( (None, 16)           0           tf.__operators__.getitem_3784[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_3793 ( (None, 16)           0           tf.__operators__.getitem_3784[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_3795 ( (None, 16)           0           tf.__operators__.getitem_3784[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_3797 ( (None, 16)           0           tf.__operators__.getitem_3784[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_3799 ( (None, 16)           0           tf.__operators__.getitem_3784[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_3801 ( (None, 16)           0           tf.__operators__.getitem_3784[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_3803 ( (None, 16)           0           tf.__operators__.getitem_3784[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_3805 ( (None, 16)           0           tf.__operators__.getitem_3784[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_3807 ( (None, 16)           0           tf.__operators__.getitem_3784[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_3809 ( (None, 16)           0           tf.__operators__.getitem_3784[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_3811 ( (None, 16)           0           tf.__operators__.getitem_3784[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_3813 ( (None, 16)           0           tf.__operators__.getitem_3784[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_3815 ( (None, 16)           0           tf.__operators__.getitem_3784[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_3817 ( (None, 16)           0           tf.__operators__.getitem_3784[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_3819 ( (None, 16)           0           tf.__operators__.getitem_3784[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_3783 ( (None, 16)           0           input_part[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims_3721 (TFOpLambda (None, 16, 1)        0           tf.__operators__.getitem_3785[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims_3723 (TFOpLambda (None, 16, 1)        0           tf.__operators__.getitem_3787[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims_3725 (TFOpLambda (None, 16, 1)        0           tf.__operators__.getitem_3789[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims_3727 (TFOpLambda (None, 16, 1)        0           tf.__operators__.getitem_3791[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims_3729 (TFOpLambda (None, 16, 1)        0           tf.__operators__.getitem_3793[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims_3731 (TFOpLambda (None, 16, 1)        0           tf.__operators__.getitem_3795[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims_3733 (TFOpLambda (None, 16, 1)        0           tf.__operators__.getitem_3797[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims_3735 (TFOpLambda (None, 16, 1)        0           tf.__operators__.getitem_3799[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims_3737 (TFOpLambda (None, 16, 1)        0           tf.__operators__.getitem_3801[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims_3739 (TFOpLambda (None, 16, 1)        0           tf.__operators__.getitem_3803[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims_3741 (TFOpLambda (None, 16, 1)        0           tf.__operators__.getitem_3805[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims_3743 (TFOpLambda (None, 16, 1)        0           tf.__operators__.getitem_3807[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims_3745 (TFOpLambda (None, 16, 1)        0           tf.__operators__.getitem_3809[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims_3747 (TFOpLambda (None, 16, 1)        0           tf.__operators__.getitem_3811[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims_3749 (TFOpLambda (None, 16, 1)        0           tf.__operators__.getitem_3813[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims_3751 (TFOpLambda (None, 16, 1)        0           tf.__operators__.getitem_3815[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims_3753 (TFOpLambda (None, 16, 1)        0           tf.__operators__.getitem_3817[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims_3755 (TFOpLambda (None, 16, 1)        0           tf.__operators__.getitem_3819[0][\n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims_3720 (TFOpLambda (None, 16, 1)        0           tf.__operators__.getitem_3783[0][\n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_exos_1st_0 (Causal (None, 16, 8)        24          tf.expand_dims_3721[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_exos_1st_1 (Causal (None, 16, 8)        24          tf.expand_dims_3723[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_exos_1st_2 (Causal (None, 16, 8)        24          tf.expand_dims_3725[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_exos_1st_3 (Causal (None, 16, 8)        24          tf.expand_dims_3727[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_exos_1st_4 (Causal (None, 16, 8)        24          tf.expand_dims_3729[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_exos_1st_5 (Causal (None, 16, 8)        24          tf.expand_dims_3731[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_exos_1st_6 (Causal (None, 16, 8)        24          tf.expand_dims_3733[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_exos_1st_7 (Causal (None, 16, 8)        24          tf.expand_dims_3735[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_exos_1st_8 (Causal (None, 16, 8)        24          tf.expand_dims_3737[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_exos_1st_9 (Causal (None, 16, 8)        24          tf.expand_dims_3739[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_exos_1st_10 (Causa (None, 16, 8)        24          tf.expand_dims_3741[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_exos_1st_11 (Causa (None, 16, 8)        24          tf.expand_dims_3743[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_exos_1st_12 (Causa (None, 16, 8)        24          tf.expand_dims_3745[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_exos_1st_13 (Causa (None, 16, 8)        24          tf.expand_dims_3747[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_exos_1st_14 (Causa (None, 16, 8)        24          tf.expand_dims_3749[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_exos_1st_15 (Causa (None, 16, 8)        24          tf.expand_dims_3751[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_exos_1st_16 (Causa (None, 16, 8)        24          tf.expand_dims_3753[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_exos_1st_17 (Causa (None, 16, 8)        24          tf.expand_dims_3755[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dilated_preprocess (CausalDilat (None, 16, 2)        4           tf.expand_dims_3720[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "add_1069 (Add)                  (None, 16, 8)        0           dilated_conv_exos_1st_0[0][0]    \n",
            "                                                                 dilated_conv_exos_1st_1[0][0]    \n",
            "                                                                 dilated_conv_exos_1st_2[0][0]    \n",
            "                                                                 dilated_conv_exos_1st_3[0][0]    \n",
            "                                                                 dilated_conv_exos_1st_4[0][0]    \n",
            "                                                                 dilated_conv_exos_1st_5[0][0]    \n",
            "                                                                 dilated_conv_exos_1st_6[0][0]    \n",
            "                                                                 dilated_conv_exos_1st_7[0][0]    \n",
            "                                                                 dilated_conv_exos_1st_8[0][0]    \n",
            "                                                                 dilated_conv_exos_1st_9[0][0]    \n",
            "                                                                 dilated_conv_exos_1st_10[0][0]   \n",
            "                                                                 dilated_conv_exos_1st_11[0][0]   \n",
            "                                                                 dilated_conv_exos_1st_12[0][0]   \n",
            "                                                                 dilated_conv_exos_1st_13[0][0]   \n",
            "                                                                 dilated_conv_exos_1st_14[0][0]   \n",
            "                                                                 dilated_conv_exos_1st_15[0][0]   \n",
            "                                                                 dilated_conv_exos_1st_16[0][0]   \n",
            "                                                                 dilated_conv_exos_1st_17[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_main_1st (CausalDi (None, 16, 8)        40          dilated_preprocess[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "add_1070 (Add)                  (None, 16, 8)        0           add_1069[0][0]                   \n",
            "                                                                 dilated_conv_main_1st[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_432 (Activation)     (None, 16, 8)        0           add_1070[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_685 (Conv1D)             (None, 16, 1)        8           activation_432[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_1071 (Add)                  (None, 16, 2)        0           conv1d_685[0][0]                 \n",
            "                                                                 dilated_preprocess[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_692 (Conv1D)             (None, 16, 1)        3           add_1071[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 487\n",
            "Trainable params: 487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIJ-LmYkBEsZ"
      },
      "source": [
        "from keras.callbacks import *\n",
        "\n",
        "\n",
        "mcp_save = ModelCheckpoint('LSTM_cls_interval3.hdf5', save_best_only=True, monitor='val_loss', mode='max')\n",
        "earlyStopping = EarlyStopping(monitor='val_loss', patience=50, verbose=1, mode='max')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOVQPlXyBOL4"
      },
      "source": [
        "model.fit(x=x_train,y=y_train,validation_data=(x_val,y_val),epochs=5000, shuffle=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgGc6g4dVYJQ"
      },
      "source": [
        "model.predict(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufsIFFXvYkiW"
      },
      "source": [
        "# Entrainement du modèle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyEdiKMmYkiX"
      },
      "source": [
        "**1. Optimisation de l'apprentissage**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOFjr4yXYkiX"
      },
      "source": [
        "Pour accélérer le traitement des données, nous n'allons pas utiliser l'intégralité des données pendant la mise à jour du gradient, comme cela a été fait jusqu'à présent (en utilisant le dataset).  \n",
        "Cette fois-ci, nous allons forcer les mises à jour du gradient à se produire de manière moins fréquente en attribuant la valeur du batch_size à prendre en compte lors de la regression du modèle.  \n",
        "Pour cela, on utilise l'argument \"batch_size\" dans la méthode fit. En précisant un batch_size=1000, cela signifie que :\n",
        " - Sur notre total de 56000 échantillons, 56 seront utilisés pour les calculs du gradient\n",
        " - Il y aura également 56 itérations à chaque période.\n",
        "  \n",
        "    \n",
        "    \n",
        "Si nous avions pris le dataset comme entrée, nous aurions eu :\n",
        "- Un total de 56000 échantillons également\n",
        "- Chaque période aurait également pris 56 itérations pour se compléter\n",
        "- Mais 1000 échantillons auraient été utilisés pour le calcul du gradient, au lieu de 56 avec la méthode utilisée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU0a-eQ9YkiY"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam()\n",
        "#optimiseur=tf.keras.optimizers.SGD()\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=optimiseur, metrics=[\"accuracy\"])\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='accuracy', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(x=x_train,y=y_train_OneHot,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint],batch_size=512)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHchQmnoYkiZ"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 20])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc5-yJEqYkia"
      },
      "source": [
        "# Chargement des poids sauvegardés\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fx8H8AHKYkia"
      },
      "source": [
        "**2. Entrainement du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZWWRL2WYkib",
        "outputId": "5da4bfe9-0561-414e-b464-1985301e63c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "max_periodes = 5000\n",
        "\n",
        "# Classe permettant d'arrêter l'entrainement si la variation\n",
        "# devient plus petite qu'une valeur à choisir sur un nombre\n",
        "# de périodes à choisir\n",
        "class StopTrain(keras.callbacks.Callback):\n",
        "    def __init__(self, delta=0.01,periodes=100, term=\"loss\", logs={}):\n",
        "      self.n_periodes = 0\n",
        "      self.periodes = periodes\n",
        "      self.loss_1 = 100\n",
        "      self.delta = delta\n",
        "      self.term = term\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "      diff_loss = abs(self.loss_1 - logs[self.term])\n",
        "      self.loss_1 = logs[self.term]\n",
        "      if (diff_loss < self.delta):\n",
        "        self.n_periodes = self.n_periodes + 1\n",
        "      else:\n",
        "        self.n_periodes = 0\n",
        "      if (self.n_periodes == self.periodes):\n",
        "        print(\"Arrêt de l'entrainement...\")\n",
        "        self.model.stop_training = True\n",
        "\n",
        "  \n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.001,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0.01)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "#optimiseur=tf.keras.optimizers.SGD(learning_rate=lr_schedule,momentum=0.9)\n",
        "\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=optimiseur, metrics=[\"accuracy\"])\n",
        "\n",
        "# Entraine le modèle, avec une réduction des calculs du gradient\n",
        "historique = model.fit(x=x_train,y=y_train_OneHot,validation_data=(x_val,y_val_OneHot), epochs=max_periodes,verbose=1, callbacks=[CheckPoint],batch_size=64)\n",
        "\n",
        "# Entraine le modèle sans réduction de calculs\n",
        "#historique = model.fit(dataset,validation_data=dataset_val, epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=1e-3,periodes = 20, term=\"My_MSE\")])\n",
        "\n",
        "files.download('poids_train.hdf5')\n",
        "\n"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mLe flux de sortie a été tronqué et ne contient que les 5000 dernières lignes.\u001b[0m\n",
            "Epoch 3751/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9752 - accuracy: 0.1553 - val_loss: 9.4520 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03751: loss did not improve from 2.94934\n",
            "Epoch 3752/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9913 - accuracy: 0.1569 - val_loss: 9.4552 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03752: loss did not improve from 2.94934\n",
            "Epoch 3753/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9562 - accuracy: 0.1592 - val_loss: 9.4676 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03753: loss did not improve from 2.94934\n",
            "Epoch 3754/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9606 - accuracy: 0.1573 - val_loss: 9.4998 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03754: loss did not improve from 2.94934\n",
            "Epoch 3755/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9720 - accuracy: 0.1675 - val_loss: 9.5174 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03755: loss did not improve from 2.94934\n",
            "Epoch 3756/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9286 - accuracy: 0.1691 - val_loss: 9.4623 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03756: loss did not improve from 2.94934\n",
            "Epoch 3757/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9245 - accuracy: 0.1611 - val_loss: 9.4671 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03757: loss did not improve from 2.94934\n",
            "Epoch 3758/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9674 - accuracy: 0.1603 - val_loss: 9.4513 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03758: loss did not improve from 2.94934\n",
            "Epoch 3759/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 3.0104 - accuracy: 0.1564 - val_loss: 9.4714 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03759: loss did not improve from 2.94934\n",
            "Epoch 3760/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9616 - accuracy: 0.1637 - val_loss: 9.4699 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03760: loss did not improve from 2.94934\n",
            "Epoch 3761/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9593 - accuracy: 0.1737 - val_loss: 9.5169 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03761: loss did not improve from 2.94934\n",
            "Epoch 3762/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9687 - accuracy: 0.1503 - val_loss: 9.4644 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03762: loss did not improve from 2.94934\n",
            "Epoch 3763/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9459 - accuracy: 0.1633 - val_loss: 9.4643 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03763: loss did not improve from 2.94934\n",
            "Epoch 3764/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9919 - accuracy: 0.1571 - val_loss: 9.4689 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03764: loss did not improve from 2.94934\n",
            "Epoch 3765/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9792 - accuracy: 0.1630 - val_loss: 9.4523 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03765: loss did not improve from 2.94934\n",
            "Epoch 3766/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9350 - accuracy: 0.1708 - val_loss: 9.4528 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03766: loss did not improve from 2.94934\n",
            "Epoch 3767/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9754 - accuracy: 0.1617 - val_loss: 9.4833 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03767: loss did not improve from 2.94934\n",
            "Epoch 3768/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9715 - accuracy: 0.1537 - val_loss: 9.4255 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03768: loss did not improve from 2.94934\n",
            "Epoch 3769/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9325 - accuracy: 0.1570 - val_loss: 9.4833 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03769: loss did not improve from 2.94934\n",
            "Epoch 3770/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9443 - accuracy: 0.1637 - val_loss: 9.4696 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03770: loss did not improve from 2.94934\n",
            "Epoch 3771/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9637 - accuracy: 0.1749 - val_loss: 9.4939 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03771: loss did not improve from 2.94934\n",
            "Epoch 3772/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 3.0032 - accuracy: 0.1589 - val_loss: 9.4828 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03772: loss did not improve from 2.94934\n",
            "Epoch 3773/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9967 - accuracy: 0.1563 - val_loss: 9.4882 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03773: loss did not improve from 2.94934\n",
            "Epoch 3774/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 3.0048 - accuracy: 0.1417 - val_loss: 9.4489 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03774: loss did not improve from 2.94934\n",
            "Epoch 3775/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9855 - accuracy: 0.1604 - val_loss: 9.4451 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03775: loss did not improve from 2.94934\n",
            "Epoch 3776/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9757 - accuracy: 0.1527 - val_loss: 9.3920 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03776: loss did not improve from 2.94934\n",
            "Epoch 3777/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9619 - accuracy: 0.1700 - val_loss: 9.5055 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03777: loss did not improve from 2.94934\n",
            "Epoch 3778/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9845 - accuracy: 0.1444 - val_loss: 9.5086 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03778: loss did not improve from 2.94934\n",
            "Epoch 3779/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 3.0187 - accuracy: 0.1598 - val_loss: 9.5092 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03779: loss did not improve from 2.94934\n",
            "Epoch 3780/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9355 - accuracy: 0.1630 - val_loss: 9.4528 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03780: loss did not improve from 2.94934\n",
            "Epoch 3781/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9762 - accuracy: 0.1575 - val_loss: 9.4886 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03781: loss did not improve from 2.94934\n",
            "Epoch 3782/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9745 - accuracy: 0.1524 - val_loss: 9.5151 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03782: loss did not improve from 2.94934\n",
            "Epoch 3783/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9630 - accuracy: 0.1721 - val_loss: 9.4921 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03783: loss did not improve from 2.94934\n",
            "Epoch 3784/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9087 - accuracy: 0.1517 - val_loss: 9.5087 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03784: loss did not improve from 2.94934\n",
            "Epoch 3785/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9644 - accuracy: 0.1648 - val_loss: 9.4260 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03785: loss did not improve from 2.94934\n",
            "Epoch 3786/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9368 - accuracy: 0.1611 - val_loss: 9.5017 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03786: loss did not improve from 2.94934\n",
            "Epoch 3787/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9764 - accuracy: 0.1673 - val_loss: 9.4741 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03787: loss did not improve from 2.94934\n",
            "Epoch 3788/5000\n",
            "31/31 [==============================] - 1s 40ms/step - loss: 2.9414 - accuracy: 0.1546 - val_loss: 9.4894 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03788: loss did not improve from 2.94934\n",
            "Epoch 3789/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9795 - accuracy: 0.1451 - val_loss: 9.4691 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03789: loss did not improve from 2.94934\n",
            "Epoch 3790/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9706 - accuracy: 0.1471 - val_loss: 9.4572 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03790: loss did not improve from 2.94934\n",
            "Epoch 3791/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9571 - accuracy: 0.1578 - val_loss: 9.4413 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03791: loss did not improve from 2.94934\n",
            "Epoch 3792/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9706 - accuracy: 0.1484 - val_loss: 9.4957 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03792: loss did not improve from 2.94934\n",
            "Epoch 3793/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9845 - accuracy: 0.1574 - val_loss: 9.4933 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03793: loss did not improve from 2.94934\n",
            "Epoch 3794/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9953 - accuracy: 0.1577 - val_loss: 9.4928 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03794: loss did not improve from 2.94934\n",
            "Epoch 3795/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9646 - accuracy: 0.1499 - val_loss: 9.4897 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03795: loss did not improve from 2.94934\n",
            "Epoch 3796/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9613 - accuracy: 0.1520 - val_loss: 9.4761 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03796: loss did not improve from 2.94934\n",
            "Epoch 3797/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9769 - accuracy: 0.1549 - val_loss: 9.5155 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03797: loss did not improve from 2.94934\n",
            "Epoch 3798/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9427 - accuracy: 0.1572 - val_loss: 9.4228 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 03798: loss did not improve from 2.94934\n",
            "Epoch 3799/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9656 - accuracy: 0.1624 - val_loss: 9.4664 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 03799: loss did not improve from 2.94934\n",
            "Epoch 3800/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9330 - accuracy: 0.1557 - val_loss: 9.4437 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 03800: loss did not improve from 2.94934\n",
            "Epoch 3801/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9976 - accuracy: 0.1534 - val_loss: 9.5030 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03801: loss did not improve from 2.94934\n",
            "Epoch 3802/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9274 - accuracy: 0.1826 - val_loss: 9.4760 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03802: loss did not improve from 2.94934\n",
            "Epoch 3803/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9692 - accuracy: 0.1529 - val_loss: 9.4451 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 03803: loss did not improve from 2.94934\n",
            "Epoch 3804/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9935 - accuracy: 0.1497 - val_loss: 9.4856 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03804: loss did not improve from 2.94934\n",
            "Epoch 3805/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9823 - accuracy: 0.1675 - val_loss: 9.5220 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03805: loss did not improve from 2.94934\n",
            "Epoch 3806/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9739 - accuracy: 0.1568 - val_loss: 9.4548 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03806: loss did not improve from 2.94934\n",
            "Epoch 3807/5000\n",
            "31/31 [==============================] - 1s 40ms/step - loss: 2.9618 - accuracy: 0.1717 - val_loss: 9.4786 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03807: loss did not improve from 2.94934\n",
            "Epoch 3808/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9709 - accuracy: 0.1583 - val_loss: 9.4927 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03808: loss did not improve from 2.94934\n",
            "Epoch 3809/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9517 - accuracy: 0.1688 - val_loss: 9.4810 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 03809: loss did not improve from 2.94934\n",
            "Epoch 3810/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9475 - accuracy: 0.1616 - val_loss: 9.5324 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03810: loss did not improve from 2.94934\n",
            "Epoch 3811/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9925 - accuracy: 0.1522 - val_loss: 9.5292 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03811: loss did not improve from 2.94934\n",
            "Epoch 3812/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9796 - accuracy: 0.1541 - val_loss: 9.5163 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03812: loss did not improve from 2.94934\n",
            "Epoch 3813/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9429 - accuracy: 0.1549 - val_loss: 9.4716 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03813: loss did not improve from 2.94934\n",
            "Epoch 3814/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9640 - accuracy: 0.1441 - val_loss: 9.4738 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03814: loss did not improve from 2.94934\n",
            "Epoch 3815/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9337 - accuracy: 0.1581 - val_loss: 9.5042 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03815: loss did not improve from 2.94934\n",
            "Epoch 3816/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9814 - accuracy: 0.1467 - val_loss: 9.4179 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03816: loss did not improve from 2.94934\n",
            "Epoch 3817/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9576 - accuracy: 0.1542 - val_loss: 9.5284 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03817: loss did not improve from 2.94934\n",
            "Epoch 3818/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.8945 - accuracy: 0.1690 - val_loss: 9.5059 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03818: loss did not improve from 2.94934\n",
            "Epoch 3819/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9210 - accuracy: 0.1650 - val_loss: 9.4985 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03819: loss did not improve from 2.94934\n",
            "Epoch 3820/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9530 - accuracy: 0.1536 - val_loss: 9.4572 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03820: loss did not improve from 2.94934\n",
            "Epoch 3821/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9419 - accuracy: 0.1603 - val_loss: 9.4618 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03821: loss did not improve from 2.94934\n",
            "Epoch 3822/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9414 - accuracy: 0.1857 - val_loss: 9.4567 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03822: loss improved from 2.94934 to 2.94281, saving model to poids_train.hdf5\n",
            "Epoch 3823/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9488 - accuracy: 0.1598 - val_loss: 9.4483 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03823: loss did not improve from 2.94281\n",
            "Epoch 3824/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9379 - accuracy: 0.1697 - val_loss: 9.4977 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03824: loss did not improve from 2.94281\n",
            "Epoch 3825/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9814 - accuracy: 0.1585 - val_loss: 9.4479 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03825: loss did not improve from 2.94281\n",
            "Epoch 3826/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9349 - accuracy: 0.1805 - val_loss: 9.4913 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03826: loss did not improve from 2.94281\n",
            "Epoch 3827/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 3.0075 - accuracy: 0.1490 - val_loss: 9.4959 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03827: loss did not improve from 2.94281\n",
            "Epoch 3828/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9574 - accuracy: 0.1512 - val_loss: 9.4368 - val_accuracy: 0.0127\n",
            "\n",
            "Epoch 03828: loss did not improve from 2.94281\n",
            "Epoch 3829/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9330 - accuracy: 0.1632 - val_loss: 9.5283 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03829: loss did not improve from 2.94281\n",
            "Epoch 3830/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9489 - accuracy: 0.1734 - val_loss: 9.4863 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03830: loss did not improve from 2.94281\n",
            "Epoch 3831/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9759 - accuracy: 0.1595 - val_loss: 9.4429 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03831: loss did not improve from 2.94281\n",
            "Epoch 3832/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9518 - accuracy: 0.1648 - val_loss: 9.5123 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03832: loss did not improve from 2.94281\n",
            "Epoch 3833/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9437 - accuracy: 0.1609 - val_loss: 9.4918 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03833: loss did not improve from 2.94281\n",
            "Epoch 3834/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 3.0023 - accuracy: 0.1582 - val_loss: 9.5204 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03834: loss did not improve from 2.94281\n",
            "Epoch 3835/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9979 - accuracy: 0.1600 - val_loss: 9.4462 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03835: loss did not improve from 2.94281\n",
            "Epoch 3836/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9442 - accuracy: 0.1669 - val_loss: 9.4681 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03836: loss did not improve from 2.94281\n",
            "Epoch 3837/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9662 - accuracy: 0.1613 - val_loss: 9.5199 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03837: loss did not improve from 2.94281\n",
            "Epoch 3838/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9933 - accuracy: 0.1536 - val_loss: 9.4978 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03838: loss did not improve from 2.94281\n",
            "Epoch 3839/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9869 - accuracy: 0.1556 - val_loss: 9.4873 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03839: loss did not improve from 2.94281\n",
            "Epoch 3840/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 3.0057 - accuracy: 0.1503 - val_loss: 9.4904 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03840: loss did not improve from 2.94281\n",
            "Epoch 3841/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9417 - accuracy: 0.1603 - val_loss: 9.5078 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03841: loss did not improve from 2.94281\n",
            "Epoch 3842/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9764 - accuracy: 0.1734 - val_loss: 9.5537 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03842: loss did not improve from 2.94281\n",
            "Epoch 3843/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9858 - accuracy: 0.1488 - val_loss: 9.4776 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03843: loss did not improve from 2.94281\n",
            "Epoch 3844/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9575 - accuracy: 0.1740 - val_loss: 9.5055 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03844: loss did not improve from 2.94281\n",
            "Epoch 3845/5000\n",
            "31/31 [==============================] - 1s 40ms/step - loss: 2.9787 - accuracy: 0.1730 - val_loss: 9.5210 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03845: loss did not improve from 2.94281\n",
            "Epoch 3846/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9543 - accuracy: 0.1679 - val_loss: 9.5866 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03846: loss did not improve from 2.94281\n",
            "Epoch 3847/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9881 - accuracy: 0.1715 - val_loss: 9.5629 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03847: loss did not improve from 2.94281\n",
            "Epoch 3848/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9686 - accuracy: 0.1530 - val_loss: 9.5337 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03848: loss did not improve from 2.94281\n",
            "Epoch 3849/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9418 - accuracy: 0.1679 - val_loss: 9.5690 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03849: loss did not improve from 2.94281\n",
            "Epoch 3850/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9919 - accuracy: 0.1563 - val_loss: 9.5457 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03850: loss did not improve from 2.94281\n",
            "Epoch 3851/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9746 - accuracy: 0.1538 - val_loss: 9.5049 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03851: loss did not improve from 2.94281\n",
            "Epoch 3852/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9848 - accuracy: 0.1504 - val_loss: 9.4538 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03852: loss did not improve from 2.94281\n",
            "Epoch 3853/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9573 - accuracy: 0.1639 - val_loss: 9.5386 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03853: loss did not improve from 2.94281\n",
            "Epoch 3854/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9865 - accuracy: 0.1583 - val_loss: 9.5055 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03854: loss did not improve from 2.94281\n",
            "Epoch 3855/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9935 - accuracy: 0.1536 - val_loss: 9.4945 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03855: loss did not improve from 2.94281\n",
            "Epoch 3856/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9872 - accuracy: 0.1677 - val_loss: 9.5083 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03856: loss did not improve from 2.94281\n",
            "Epoch 3857/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9591 - accuracy: 0.1483 - val_loss: 9.5895 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03857: loss did not improve from 2.94281\n",
            "Epoch 3858/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9494 - accuracy: 0.1537 - val_loss: 9.5751 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03858: loss did not improve from 2.94281\n",
            "Epoch 3859/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9185 - accuracy: 0.1792 - val_loss: 9.5063 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03859: loss did not improve from 2.94281\n",
            "Epoch 3860/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9611 - accuracy: 0.1530 - val_loss: 9.5556 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03860: loss did not improve from 2.94281\n",
            "Epoch 3861/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9751 - accuracy: 0.1489 - val_loss: 9.5298 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03861: loss did not improve from 2.94281\n",
            "Epoch 3862/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9674 - accuracy: 0.1669 - val_loss: 9.5404 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03862: loss did not improve from 2.94281\n",
            "Epoch 3863/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9369 - accuracy: 0.1487 - val_loss: 9.5538 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03863: loss did not improve from 2.94281\n",
            "Epoch 3864/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9900 - accuracy: 0.1384 - val_loss: 9.5547 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03864: loss did not improve from 2.94281\n",
            "Epoch 3865/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9788 - accuracy: 0.1637 - val_loss: 9.5050 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03865: loss did not improve from 2.94281\n",
            "Epoch 3866/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9749 - accuracy: 0.1481 - val_loss: 9.4964 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03866: loss did not improve from 2.94281\n",
            "Epoch 3867/5000\n",
            "31/31 [==============================] - 1s 40ms/step - loss: 2.9687 - accuracy: 0.1659 - val_loss: 9.5143 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03867: loss did not improve from 2.94281\n",
            "Epoch 3868/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9407 - accuracy: 0.1582 - val_loss: 9.5924 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03868: loss did not improve from 2.94281\n",
            "Epoch 3869/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9888 - accuracy: 0.1579 - val_loss: 9.5579 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03869: loss did not improve from 2.94281\n",
            "Epoch 3870/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9734 - accuracy: 0.1451 - val_loss: 9.5260 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03870: loss did not improve from 2.94281\n",
            "Epoch 3871/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9588 - accuracy: 0.1703 - val_loss: 9.5113 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03871: loss did not improve from 2.94281\n",
            "Epoch 3872/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9771 - accuracy: 0.1502 - val_loss: 9.5018 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03872: loss did not improve from 2.94281\n",
            "Epoch 3873/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9584 - accuracy: 0.1601 - val_loss: 9.5176 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03873: loss did not improve from 2.94281\n",
            "Epoch 3874/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9453 - accuracy: 0.1695 - val_loss: 9.5429 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03874: loss did not improve from 2.94281\n",
            "Epoch 3875/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9837 - accuracy: 0.1456 - val_loss: 9.5035 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03875: loss did not improve from 2.94281\n",
            "Epoch 3876/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9860 - accuracy: 0.1518 - val_loss: 9.5117 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03876: loss did not improve from 2.94281\n",
            "Epoch 3877/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9513 - accuracy: 0.1599 - val_loss: 9.4997 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03877: loss did not improve from 2.94281\n",
            "Epoch 3878/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9765 - accuracy: 0.1607 - val_loss: 9.5448 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03878: loss did not improve from 2.94281\n",
            "Epoch 3879/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9521 - accuracy: 0.1672 - val_loss: 9.5299 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03879: loss did not improve from 2.94281\n",
            "Epoch 3880/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9418 - accuracy: 0.1612 - val_loss: 9.5272 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03880: loss did not improve from 2.94281\n",
            "Epoch 3881/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9126 - accuracy: 0.1808 - val_loss: 9.4853 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03881: loss did not improve from 2.94281\n",
            "Epoch 3882/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9694 - accuracy: 0.1484 - val_loss: 9.5304 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03882: loss did not improve from 2.94281\n",
            "Epoch 3883/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9536 - accuracy: 0.1607 - val_loss: 9.5433 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03883: loss did not improve from 2.94281\n",
            "Epoch 3884/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9311 - accuracy: 0.1840 - val_loss: 9.4885 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03884: loss did not improve from 2.94281\n",
            "Epoch 3885/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9827 - accuracy: 0.1611 - val_loss: 9.5318 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03885: loss did not improve from 2.94281\n",
            "Epoch 3886/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9863 - accuracy: 0.1596 - val_loss: 9.4900 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03886: loss did not improve from 2.94281\n",
            "Epoch 3887/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9736 - accuracy: 0.1531 - val_loss: 9.5358 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03887: loss did not improve from 2.94281\n",
            "Epoch 3888/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9695 - accuracy: 0.1570 - val_loss: 9.5531 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03888: loss did not improve from 2.94281\n",
            "Epoch 3889/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9387 - accuracy: 0.1745 - val_loss: 9.5401 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03889: loss did not improve from 2.94281\n",
            "Epoch 3890/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9810 - accuracy: 0.1575 - val_loss: 9.4842 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03890: loss did not improve from 2.94281\n",
            "Epoch 3891/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9624 - accuracy: 0.1631 - val_loss: 9.5152 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03891: loss did not improve from 2.94281\n",
            "Epoch 3892/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9520 - accuracy: 0.1643 - val_loss: 9.4918 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03892: loss did not improve from 2.94281\n",
            "Epoch 3893/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9475 - accuracy: 0.1415 - val_loss: 9.5194 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03893: loss did not improve from 2.94281\n",
            "Epoch 3894/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9731 - accuracy: 0.1456 - val_loss: 9.5132 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03894: loss did not improve from 2.94281\n",
            "Epoch 3895/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 3.0002 - accuracy: 0.1550 - val_loss: 9.5495 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03895: loss did not improve from 2.94281\n",
            "Epoch 3896/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 3.0023 - accuracy: 0.1267 - val_loss: 9.4917 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03896: loss did not improve from 2.94281\n",
            "Epoch 3897/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9220 - accuracy: 0.1507 - val_loss: 9.5055 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03897: loss did not improve from 2.94281\n",
            "Epoch 3898/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9730 - accuracy: 0.1555 - val_loss: 9.4872 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03898: loss did not improve from 2.94281\n",
            "Epoch 3899/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9807 - accuracy: 0.1646 - val_loss: 9.5067 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03899: loss did not improve from 2.94281\n",
            "Epoch 3900/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9607 - accuracy: 0.1655 - val_loss: 9.5400 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03900: loss did not improve from 2.94281\n",
            "Epoch 3901/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9434 - accuracy: 0.1466 - val_loss: 9.5491 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03901: loss did not improve from 2.94281\n",
            "Epoch 3902/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9588 - accuracy: 0.1719 - val_loss: 9.5395 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03902: loss did not improve from 2.94281\n",
            "Epoch 3903/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9742 - accuracy: 0.1439 - val_loss: 9.4861 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03903: loss did not improve from 2.94281\n",
            "Epoch 3904/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9665 - accuracy: 0.1556 - val_loss: 9.5243 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03904: loss did not improve from 2.94281\n",
            "Epoch 3905/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9460 - accuracy: 0.1506 - val_loss: 9.5146 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03905: loss did not improve from 2.94281\n",
            "Epoch 3906/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9703 - accuracy: 0.1629 - val_loss: 9.5131 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03906: loss did not improve from 2.94281\n",
            "Epoch 3907/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9370 - accuracy: 0.1827 - val_loss: 9.5254 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03907: loss did not improve from 2.94281\n",
            "Epoch 3908/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9633 - accuracy: 0.1521 - val_loss: 9.5021 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03908: loss did not improve from 2.94281\n",
            "Epoch 3909/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9185 - accuracy: 0.1773 - val_loss: 9.5851 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03909: loss did not improve from 2.94281\n",
            "Epoch 3910/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9838 - accuracy: 0.1606 - val_loss: 9.4709 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03910: loss did not improve from 2.94281\n",
            "Epoch 3911/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9411 - accuracy: 0.1728 - val_loss: 9.5519 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03911: loss did not improve from 2.94281\n",
            "Epoch 3912/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 3.0091 - accuracy: 0.1612 - val_loss: 9.5473 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03912: loss did not improve from 2.94281\n",
            "Epoch 3913/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9773 - accuracy: 0.1730 - val_loss: 9.4782 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03913: loss did not improve from 2.94281\n",
            "Epoch 3914/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9871 - accuracy: 0.1555 - val_loss: 9.5252 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03914: loss did not improve from 2.94281\n",
            "Epoch 3915/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9517 - accuracy: 0.1574 - val_loss: 9.5232 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03915: loss did not improve from 2.94281\n",
            "Epoch 3916/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9562 - accuracy: 0.1716 - val_loss: 9.5245 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03916: loss did not improve from 2.94281\n",
            "Epoch 3917/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9313 - accuracy: 0.1761 - val_loss: 9.5412 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03917: loss did not improve from 2.94281\n",
            "Epoch 3918/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.8938 - accuracy: 0.1761 - val_loss: 9.5327 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03918: loss did not improve from 2.94281\n",
            "Epoch 3919/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9459 - accuracy: 0.1721 - val_loss: 9.4995 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03919: loss did not improve from 2.94281\n",
            "Epoch 3920/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9528 - accuracy: 0.1730 - val_loss: 9.5880 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03920: loss did not improve from 2.94281\n",
            "Epoch 3921/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9480 - accuracy: 0.1661 - val_loss: 9.5249 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03921: loss did not improve from 2.94281\n",
            "Epoch 3922/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9657 - accuracy: 0.1616 - val_loss: 9.5409 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03922: loss did not improve from 2.94281\n",
            "Epoch 3923/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9352 - accuracy: 0.1731 - val_loss: 9.5383 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03923: loss did not improve from 2.94281\n",
            "Epoch 3924/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9362 - accuracy: 0.1615 - val_loss: 9.5431 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03924: loss did not improve from 2.94281\n",
            "Epoch 3925/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9837 - accuracy: 0.1577 - val_loss: 9.5088 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03925: loss did not improve from 2.94281\n",
            "Epoch 3926/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9598 - accuracy: 0.1588 - val_loss: 9.5667 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03926: loss did not improve from 2.94281\n",
            "Epoch 3927/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9602 - accuracy: 0.1701 - val_loss: 9.5256 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03927: loss did not improve from 2.94281\n",
            "Epoch 3928/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9663 - accuracy: 0.1621 - val_loss: 9.5336 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03928: loss did not improve from 2.94281\n",
            "Epoch 3929/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9261 - accuracy: 0.1459 - val_loss: 9.5464 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03929: loss did not improve from 2.94281\n",
            "Epoch 3930/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9792 - accuracy: 0.1587 - val_loss: 9.5937 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03930: loss did not improve from 2.94281\n",
            "Epoch 3931/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9346 - accuracy: 0.1744 - val_loss: 9.5202 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03931: loss did not improve from 2.94281\n",
            "Epoch 3932/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9766 - accuracy: 0.1607 - val_loss: 9.5167 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03932: loss did not improve from 2.94281\n",
            "Epoch 3933/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9850 - accuracy: 0.1472 - val_loss: 9.5470 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03933: loss did not improve from 2.94281\n",
            "Epoch 3934/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9467 - accuracy: 0.1706 - val_loss: 9.5235 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03934: loss did not improve from 2.94281\n",
            "Epoch 3935/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9414 - accuracy: 0.1773 - val_loss: 9.5358 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03935: loss did not improve from 2.94281\n",
            "Epoch 3936/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9634 - accuracy: 0.1749 - val_loss: 9.5945 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03936: loss did not improve from 2.94281\n",
            "Epoch 3937/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9642 - accuracy: 0.1541 - val_loss: 9.5700 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03937: loss did not improve from 2.94281\n",
            "Epoch 3938/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9528 - accuracy: 0.1591 - val_loss: 9.5330 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03938: loss did not improve from 2.94281\n",
            "Epoch 3939/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9795 - accuracy: 0.1521 - val_loss: 9.5394 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03939: loss did not improve from 2.94281\n",
            "Epoch 3940/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9288 - accuracy: 0.1593 - val_loss: 9.5152 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03940: loss did not improve from 2.94281\n",
            "Epoch 3941/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9970 - accuracy: 0.1547 - val_loss: 9.4897 - val_accuracy: 0.0149\n",
            "\n",
            "Epoch 03941: loss did not improve from 2.94281\n",
            "Epoch 3942/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9454 - accuracy: 0.1733 - val_loss: 9.5567 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03942: loss did not improve from 2.94281\n",
            "Epoch 3943/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9665 - accuracy: 0.1570 - val_loss: 9.5212 - val_accuracy: 0.0127\n",
            "\n",
            "Epoch 03943: loss did not improve from 2.94281\n",
            "Epoch 3944/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9835 - accuracy: 0.1601 - val_loss: 9.4655 - val_accuracy: 0.0127\n",
            "\n",
            "Epoch 03944: loss did not improve from 2.94281\n",
            "Epoch 3945/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9476 - accuracy: 0.1589 - val_loss: 9.5084 - val_accuracy: 0.0149\n",
            "\n",
            "Epoch 03945: loss did not improve from 2.94281\n",
            "Epoch 3946/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9904 - accuracy: 0.1446 - val_loss: 9.5374 - val_accuracy: 0.0149\n",
            "\n",
            "Epoch 03946: loss did not improve from 2.94281\n",
            "Epoch 3947/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9901 - accuracy: 0.1547 - val_loss: 9.4675 - val_accuracy: 0.0149\n",
            "\n",
            "Epoch 03947: loss did not improve from 2.94281\n",
            "Epoch 3948/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9784 - accuracy: 0.1683 - val_loss: 9.5661 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03948: loss did not improve from 2.94281\n",
            "Epoch 3949/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9649 - accuracy: 0.1579 - val_loss: 9.5320 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03949: loss did not improve from 2.94281\n",
            "Epoch 3950/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9621 - accuracy: 0.1545 - val_loss: 9.5457 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03950: loss did not improve from 2.94281\n",
            "Epoch 3951/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9938 - accuracy: 0.1744 - val_loss: 9.5318 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03951: loss did not improve from 2.94281\n",
            "Epoch 3952/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9338 - accuracy: 0.1635 - val_loss: 9.4842 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03952: loss did not improve from 2.94281\n",
            "Epoch 3953/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9702 - accuracy: 0.1460 - val_loss: 9.5400 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03953: loss did not improve from 2.94281\n",
            "Epoch 3954/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9601 - accuracy: 0.1562 - val_loss: 9.5498 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03954: loss did not improve from 2.94281\n",
            "Epoch 3955/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9576 - accuracy: 0.1503 - val_loss: 9.4929 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03955: loss did not improve from 2.94281\n",
            "Epoch 3956/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9709 - accuracy: 0.1481 - val_loss: 9.5189 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03956: loss did not improve from 2.94281\n",
            "Epoch 3957/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9543 - accuracy: 0.1646 - val_loss: 9.5366 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03957: loss did not improve from 2.94281\n",
            "Epoch 3958/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9444 - accuracy: 0.1703 - val_loss: 9.5517 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03958: loss did not improve from 2.94281\n",
            "Epoch 3959/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9321 - accuracy: 0.1686 - val_loss: 9.5171 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03959: loss did not improve from 2.94281\n",
            "Epoch 3960/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9736 - accuracy: 0.1640 - val_loss: 9.5339 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03960: loss did not improve from 2.94281\n",
            "Epoch 3961/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9870 - accuracy: 0.1743 - val_loss: 9.5495 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03961: loss did not improve from 2.94281\n",
            "Epoch 3962/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9713 - accuracy: 0.1580 - val_loss: 9.5391 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03962: loss did not improve from 2.94281\n",
            "Epoch 3963/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9538 - accuracy: 0.1576 - val_loss: 9.5383 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03963: loss did not improve from 2.94281\n",
            "Epoch 3964/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9372 - accuracy: 0.1593 - val_loss: 9.5105 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03964: loss did not improve from 2.94281\n",
            "Epoch 3965/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9507 - accuracy: 0.1561 - val_loss: 9.5761 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03965: loss did not improve from 2.94281\n",
            "Epoch 3966/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9480 - accuracy: 0.1622 - val_loss: 9.5344 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03966: loss did not improve from 2.94281\n",
            "Epoch 3967/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9786 - accuracy: 0.1519 - val_loss: 9.5821 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03967: loss did not improve from 2.94281\n",
            "Epoch 3968/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9284 - accuracy: 0.1578 - val_loss: 9.5110 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03968: loss did not improve from 2.94281\n",
            "Epoch 3969/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9740 - accuracy: 0.1503 - val_loss: 9.5306 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03969: loss did not improve from 2.94281\n",
            "Epoch 3970/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9722 - accuracy: 0.1615 - val_loss: 9.5451 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03970: loss did not improve from 2.94281\n",
            "Epoch 3971/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9547 - accuracy: 0.1580 - val_loss: 9.4697 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03971: loss did not improve from 2.94281\n",
            "Epoch 3972/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9940 - accuracy: 0.1643 - val_loss: 9.5629 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03972: loss did not improve from 2.94281\n",
            "Epoch 3973/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9682 - accuracy: 0.1545 - val_loss: 9.5447 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03973: loss did not improve from 2.94281\n",
            "Epoch 3974/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9757 - accuracy: 0.1547 - val_loss: 9.5437 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03974: loss did not improve from 2.94281\n",
            "Epoch 3975/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9669 - accuracy: 0.1618 - val_loss: 9.5370 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03975: loss did not improve from 2.94281\n",
            "Epoch 3976/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 3.0113 - accuracy: 0.1441 - val_loss: 9.5216 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03976: loss did not improve from 2.94281\n",
            "Epoch 3977/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9710 - accuracy: 0.1559 - val_loss: 9.6173 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03977: loss did not improve from 2.94281\n",
            "Epoch 3978/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9746 - accuracy: 0.1612 - val_loss: 9.5524 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03978: loss did not improve from 2.94281\n",
            "Epoch 3979/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9518 - accuracy: 0.1725 - val_loss: 9.5477 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03979: loss did not improve from 2.94281\n",
            "Epoch 3980/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 3.0060 - accuracy: 0.1447 - val_loss: 9.5139 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03980: loss did not improve from 2.94281\n",
            "Epoch 3981/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9613 - accuracy: 0.1663 - val_loss: 9.5708 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03981: loss did not improve from 2.94281\n",
            "Epoch 3982/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9855 - accuracy: 0.1524 - val_loss: 9.5206 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03982: loss did not improve from 2.94281\n",
            "Epoch 3983/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9731 - accuracy: 0.1576 - val_loss: 9.6147 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03983: loss did not improve from 2.94281\n",
            "Epoch 3984/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9542 - accuracy: 0.1663 - val_loss: 9.5449 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03984: loss did not improve from 2.94281\n",
            "Epoch 3985/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9581 - accuracy: 0.1626 - val_loss: 9.5938 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03985: loss did not improve from 2.94281\n",
            "Epoch 3986/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9714 - accuracy: 0.1514 - val_loss: 9.5539 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03986: loss did not improve from 2.94281\n",
            "Epoch 3987/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9436 - accuracy: 0.1606 - val_loss: 9.5426 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03987: loss did not improve from 2.94281\n",
            "Epoch 3988/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9799 - accuracy: 0.1484 - val_loss: 9.5103 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03988: loss did not improve from 2.94281\n",
            "Epoch 3989/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9596 - accuracy: 0.1592 - val_loss: 9.5454 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03989: loss did not improve from 2.94281\n",
            "Epoch 3990/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9589 - accuracy: 0.1506 - val_loss: 9.5444 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03990: loss did not improve from 2.94281\n",
            "Epoch 3991/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9981 - accuracy: 0.1599 - val_loss: 9.4990 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 03991: loss did not improve from 2.94281\n",
            "Epoch 3992/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9831 - accuracy: 0.1497 - val_loss: 9.6393 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 03992: loss did not improve from 2.94281\n",
            "Epoch 3993/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9300 - accuracy: 0.1632 - val_loss: 9.5625 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03993: loss did not improve from 2.94281\n",
            "Epoch 3994/5000\n",
            "31/31 [==============================] - 1s 36ms/step - loss: 2.9847 - accuracy: 0.1501 - val_loss: 9.5386 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03994: loss did not improve from 2.94281\n",
            "Epoch 3995/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9339 - accuracy: 0.1513 - val_loss: 9.5931 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 03995: loss did not improve from 2.94281\n",
            "Epoch 3996/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9386 - accuracy: 0.1686 - val_loss: 9.5917 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 03996: loss did not improve from 2.94281\n",
            "Epoch 3997/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9477 - accuracy: 0.1602 - val_loss: 9.5297 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03997: loss did not improve from 2.94281\n",
            "Epoch 3998/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9883 - accuracy: 0.1594 - val_loss: 9.6011 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03998: loss did not improve from 2.94281\n",
            "Epoch 3999/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9399 - accuracy: 0.1673 - val_loss: 9.5829 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 03999: loss did not improve from 2.94281\n",
            "Epoch 4000/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9850 - accuracy: 0.1861 - val_loss: 9.5966 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04000: loss did not improve from 2.94281\n",
            "Epoch 4001/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9710 - accuracy: 0.1494 - val_loss: 9.5492 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04001: loss did not improve from 2.94281\n",
            "Epoch 4002/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9487 - accuracy: 0.1649 - val_loss: 9.6371 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04002: loss did not improve from 2.94281\n",
            "Epoch 4003/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9974 - accuracy: 0.1469 - val_loss: 9.5449 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04003: loss did not improve from 2.94281\n",
            "Epoch 4004/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9262 - accuracy: 0.1737 - val_loss: 9.5608 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04004: loss did not improve from 2.94281\n",
            "Epoch 4005/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9303 - accuracy: 0.1768 - val_loss: 9.5802 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04005: loss did not improve from 2.94281\n",
            "Epoch 4006/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9582 - accuracy: 0.1557 - val_loss: 9.6404 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04006: loss did not improve from 2.94281\n",
            "Epoch 4007/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9631 - accuracy: 0.1587 - val_loss: 9.5990 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04007: loss did not improve from 2.94281\n",
            "Epoch 4008/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9470 - accuracy: 0.1676 - val_loss: 9.5293 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04008: loss did not improve from 2.94281\n",
            "Epoch 4009/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9438 - accuracy: 0.1824 - val_loss: 9.5915 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04009: loss did not improve from 2.94281\n",
            "Epoch 4010/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9893 - accuracy: 0.1440 - val_loss: 9.6203 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04010: loss did not improve from 2.94281\n",
            "Epoch 4011/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9590 - accuracy: 0.1572 - val_loss: 9.5813 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04011: loss did not improve from 2.94281\n",
            "Epoch 4012/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9330 - accuracy: 0.1798 - val_loss: 9.5982 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04012: loss did not improve from 2.94281\n",
            "Epoch 4013/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9496 - accuracy: 0.1869 - val_loss: 9.5370 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04013: loss did not improve from 2.94281\n",
            "Epoch 4014/5000\n",
            "31/31 [==============================] - 1s 40ms/step - loss: 2.9067 - accuracy: 0.1854 - val_loss: 9.5492 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04014: loss did not improve from 2.94281\n",
            "Epoch 4015/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9876 - accuracy: 0.1458 - val_loss: 9.5275 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04015: loss did not improve from 2.94281\n",
            "Epoch 4016/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9719 - accuracy: 0.1521 - val_loss: 9.5642 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04016: loss did not improve from 2.94281\n",
            "Epoch 4017/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9543 - accuracy: 0.1592 - val_loss: 9.5801 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04017: loss did not improve from 2.94281\n",
            "Epoch 4018/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9755 - accuracy: 0.1648 - val_loss: 9.5683 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04018: loss did not improve from 2.94281\n",
            "Epoch 4019/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9385 - accuracy: 0.1701 - val_loss: 9.6044 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04019: loss did not improve from 2.94281\n",
            "Epoch 4020/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9785 - accuracy: 0.1668 - val_loss: 9.5447 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04020: loss did not improve from 2.94281\n",
            "Epoch 4021/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9423 - accuracy: 0.1721 - val_loss: 9.5782 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04021: loss did not improve from 2.94281\n",
            "Epoch 4022/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9563 - accuracy: 0.1695 - val_loss: 9.5931 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04022: loss did not improve from 2.94281\n",
            "Epoch 4023/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9645 - accuracy: 0.1590 - val_loss: 9.5881 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04023: loss did not improve from 2.94281\n",
            "Epoch 4024/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9966 - accuracy: 0.1476 - val_loss: 9.6009 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04024: loss did not improve from 2.94281\n",
            "Epoch 4025/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9442 - accuracy: 0.1535 - val_loss: 9.5809 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04025: loss did not improve from 2.94281\n",
            "Epoch 4026/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9599 - accuracy: 0.1701 - val_loss: 9.5973 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04026: loss did not improve from 2.94281\n",
            "Epoch 4027/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 3.0299 - accuracy: 0.1578 - val_loss: 9.5601 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04027: loss did not improve from 2.94281\n",
            "Epoch 4028/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9701 - accuracy: 0.1594 - val_loss: 9.5607 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04028: loss did not improve from 2.94281\n",
            "Epoch 4029/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9386 - accuracy: 0.1606 - val_loss: 9.5460 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04029: loss did not improve from 2.94281\n",
            "Epoch 4030/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9406 - accuracy: 0.1737 - val_loss: 9.6145 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04030: loss did not improve from 2.94281\n",
            "Epoch 4031/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9346 - accuracy: 0.1744 - val_loss: 9.5706 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04031: loss did not improve from 2.94281\n",
            "Epoch 4032/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9273 - accuracy: 0.1548 - val_loss: 9.6099 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04032: loss did not improve from 2.94281\n",
            "Epoch 4033/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9169 - accuracy: 0.1637 - val_loss: 9.5405 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04033: loss did not improve from 2.94281\n",
            "Epoch 4034/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9360 - accuracy: 0.1696 - val_loss: 9.5710 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04034: loss did not improve from 2.94281\n",
            "Epoch 4035/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9520 - accuracy: 0.1657 - val_loss: 9.5584 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04035: loss did not improve from 2.94281\n",
            "Epoch 4036/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 3.0299 - accuracy: 0.1370 - val_loss: 9.5997 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04036: loss did not improve from 2.94281\n",
            "Epoch 4037/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9313 - accuracy: 0.1775 - val_loss: 9.5901 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04037: loss did not improve from 2.94281\n",
            "Epoch 4038/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9829 - accuracy: 0.1500 - val_loss: 9.5760 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04038: loss did not improve from 2.94281\n",
            "Epoch 4039/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9688 - accuracy: 0.1650 - val_loss: 9.5953 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04039: loss did not improve from 2.94281\n",
            "Epoch 4040/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9812 - accuracy: 0.1588 - val_loss: 9.6009 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04040: loss did not improve from 2.94281\n",
            "Epoch 4041/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9413 - accuracy: 0.1609 - val_loss: 9.5614 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04041: loss did not improve from 2.94281\n",
            "Epoch 4042/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9781 - accuracy: 0.1480 - val_loss: 9.6036 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04042: loss did not improve from 2.94281\n",
            "Epoch 4043/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9626 - accuracy: 0.1644 - val_loss: 9.6263 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04043: loss did not improve from 2.94281\n",
            "Epoch 4044/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9611 - accuracy: 0.1491 - val_loss: 9.6629 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04044: loss did not improve from 2.94281\n",
            "Epoch 4045/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9329 - accuracy: 0.1594 - val_loss: 9.6135 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04045: loss did not improve from 2.94281\n",
            "Epoch 4046/5000\n",
            "31/31 [==============================] - 1s 40ms/step - loss: 2.9994 - accuracy: 0.1407 - val_loss: 9.5839 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04046: loss did not improve from 2.94281\n",
            "Epoch 4047/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9617 - accuracy: 0.1689 - val_loss: 9.6209 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04047: loss did not improve from 2.94281\n",
            "Epoch 4048/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9478 - accuracy: 0.1556 - val_loss: 9.5240 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04048: loss did not improve from 2.94281\n",
            "Epoch 4049/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9245 - accuracy: 0.1739 - val_loss: 9.6214 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04049: loss did not improve from 2.94281\n",
            "Epoch 4050/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9823 - accuracy: 0.1489 - val_loss: 9.6127 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04050: loss did not improve from 2.94281\n",
            "Epoch 4051/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9212 - accuracy: 0.1741 - val_loss: 9.5934 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04051: loss did not improve from 2.94281\n",
            "Epoch 4052/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9281 - accuracy: 0.1636 - val_loss: 9.6037 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04052: loss did not improve from 2.94281\n",
            "Epoch 4053/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9424 - accuracy: 0.1627 - val_loss: 9.6539 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04053: loss did not improve from 2.94281\n",
            "Epoch 4054/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9491 - accuracy: 0.1676 - val_loss: 9.5629 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04054: loss did not improve from 2.94281\n",
            "Epoch 4055/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9531 - accuracy: 0.1564 - val_loss: 9.6408 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04055: loss did not improve from 2.94281\n",
            "Epoch 4056/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9542 - accuracy: 0.1740 - val_loss: 9.5714 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04056: loss did not improve from 2.94281\n",
            "Epoch 4057/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9657 - accuracy: 0.1592 - val_loss: 9.5504 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04057: loss did not improve from 2.94281\n",
            "Epoch 4058/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9135 - accuracy: 0.1794 - val_loss: 9.6418 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04058: loss did not improve from 2.94281\n",
            "Epoch 4059/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9496 - accuracy: 0.1662 - val_loss: 9.6172 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04059: loss did not improve from 2.94281\n",
            "Epoch 4060/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9477 - accuracy: 0.1455 - val_loss: 9.6472 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04060: loss did not improve from 2.94281\n",
            "Epoch 4061/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9213 - accuracy: 0.1706 - val_loss: 9.6177 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04061: loss did not improve from 2.94281\n",
            "Epoch 4062/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9621 - accuracy: 0.1599 - val_loss: 9.5537 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04062: loss did not improve from 2.94281\n",
            "Epoch 4063/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9426 - accuracy: 0.1584 - val_loss: 9.6120 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04063: loss did not improve from 2.94281\n",
            "Epoch 4064/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 3.0108 - accuracy: 0.1391 - val_loss: 9.6112 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04064: loss did not improve from 2.94281\n",
            "Epoch 4065/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9470 - accuracy: 0.1604 - val_loss: 9.6045 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04065: loss did not improve from 2.94281\n",
            "Epoch 4066/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9839 - accuracy: 0.1533 - val_loss: 9.5703 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04066: loss did not improve from 2.94281\n",
            "Epoch 4067/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9748 - accuracy: 0.1693 - val_loss: 9.5872 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04067: loss did not improve from 2.94281\n",
            "Epoch 4068/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9126 - accuracy: 0.1761 - val_loss: 9.6504 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04068: loss improved from 2.94281 to 2.93714, saving model to poids_train.hdf5\n",
            "Epoch 4069/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9480 - accuracy: 0.1651 - val_loss: 9.5869 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04069: loss did not improve from 2.93714\n",
            "Epoch 4070/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9723 - accuracy: 0.1661 - val_loss: 9.6123 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04070: loss did not improve from 2.93714\n",
            "Epoch 4071/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9219 - accuracy: 0.1721 - val_loss: 9.6466 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04071: loss did not improve from 2.93714\n",
            "Epoch 4072/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9776 - accuracy: 0.1580 - val_loss: 9.6297 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04072: loss did not improve from 2.93714\n",
            "Epoch 4073/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9616 - accuracy: 0.1657 - val_loss: 9.5844 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04073: loss did not improve from 2.93714\n",
            "Epoch 4074/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9762 - accuracy: 0.1682 - val_loss: 9.5392 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04074: loss did not improve from 2.93714\n",
            "Epoch 4075/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9704 - accuracy: 0.1420 - val_loss: 9.5646 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04075: loss did not improve from 2.93714\n",
            "Epoch 4076/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9657 - accuracy: 0.1545 - val_loss: 9.6048 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04076: loss did not improve from 2.93714\n",
            "Epoch 4077/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9580 - accuracy: 0.1506 - val_loss: 9.5921 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04077: loss did not improve from 2.93714\n",
            "Epoch 4078/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9505 - accuracy: 0.1676 - val_loss: 9.5412 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04078: loss did not improve from 2.93714\n",
            "Epoch 4079/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9484 - accuracy: 0.1679 - val_loss: 9.6007 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04079: loss did not improve from 2.93714\n",
            "Epoch 4080/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9825 - accuracy: 0.1537 - val_loss: 9.5643 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04080: loss did not improve from 2.93714\n",
            "Epoch 4081/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9653 - accuracy: 0.1584 - val_loss: 9.6026 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04081: loss did not improve from 2.93714\n",
            "Epoch 4082/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9696 - accuracy: 0.1638 - val_loss: 9.5770 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04082: loss did not improve from 2.93714\n",
            "Epoch 4083/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9720 - accuracy: 0.1545 - val_loss: 9.6372 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04083: loss did not improve from 2.93714\n",
            "Epoch 4084/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9770 - accuracy: 0.1538 - val_loss: 9.5962 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04084: loss did not improve from 2.93714\n",
            "Epoch 4085/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9887 - accuracy: 0.1541 - val_loss: 9.5643 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04085: loss did not improve from 2.93714\n",
            "Epoch 4086/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9459 - accuracy: 0.1656 - val_loss: 9.5829 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04086: loss did not improve from 2.93714\n",
            "Epoch 4087/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9732 - accuracy: 0.1612 - val_loss: 9.5903 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04087: loss improved from 2.93714 to 2.93593, saving model to poids_train.hdf5\n",
            "Epoch 4088/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9630 - accuracy: 0.1523 - val_loss: 9.5608 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04088: loss did not improve from 2.93593\n",
            "Epoch 4089/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9651 - accuracy: 0.1694 - val_loss: 9.5568 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04089: loss did not improve from 2.93593\n",
            "Epoch 4090/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9632 - accuracy: 0.1575 - val_loss: 9.5394 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04090: loss did not improve from 2.93593\n",
            "Epoch 4091/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9573 - accuracy: 0.1537 - val_loss: 9.6129 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04091: loss did not improve from 2.93593\n",
            "Epoch 4092/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9535 - accuracy: 0.1512 - val_loss: 9.5970 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04092: loss did not improve from 2.93593\n",
            "Epoch 4093/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 3.0042 - accuracy: 0.1402 - val_loss: 9.5693 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04093: loss did not improve from 2.93593\n",
            "Epoch 4094/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9747 - accuracy: 0.1471 - val_loss: 9.5756 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04094: loss did not improve from 2.93593\n",
            "Epoch 4095/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9318 - accuracy: 0.1601 - val_loss: 9.5293 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04095: loss did not improve from 2.93593\n",
            "Epoch 4096/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9334 - accuracy: 0.1695 - val_loss: 9.6812 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04096: loss did not improve from 2.93593\n",
            "Epoch 4097/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9595 - accuracy: 0.1598 - val_loss: 9.5246 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04097: loss did not improve from 2.93593\n",
            "Epoch 4098/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9345 - accuracy: 0.1554 - val_loss: 9.5937 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04098: loss did not improve from 2.93593\n",
            "Epoch 4099/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9590 - accuracy: 0.1551 - val_loss: 9.6018 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04099: loss did not improve from 2.93593\n",
            "Epoch 4100/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9527 - accuracy: 0.1657 - val_loss: 9.5635 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04100: loss did not improve from 2.93593\n",
            "Epoch 4101/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9586 - accuracy: 0.1583 - val_loss: 9.5540 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04101: loss did not improve from 2.93593\n",
            "Epoch 4102/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9961 - accuracy: 0.1478 - val_loss: 9.6016 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04102: loss did not improve from 2.93593\n",
            "Epoch 4103/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9176 - accuracy: 0.1690 - val_loss: 9.5518 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04103: loss did not improve from 2.93593\n",
            "Epoch 4104/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9259 - accuracy: 0.1677 - val_loss: 9.6294 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04104: loss did not improve from 2.93593\n",
            "Epoch 4105/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9771 - accuracy: 0.1585 - val_loss: 9.5679 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04105: loss did not improve from 2.93593\n",
            "Epoch 4106/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9585 - accuracy: 0.1448 - val_loss: 9.5584 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04106: loss did not improve from 2.93593\n",
            "Epoch 4107/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9758 - accuracy: 0.1549 - val_loss: 9.6153 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04107: loss did not improve from 2.93593\n",
            "Epoch 4108/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9500 - accuracy: 0.1576 - val_loss: 9.5736 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04108: loss did not improve from 2.93593\n",
            "Epoch 4109/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9298 - accuracy: 0.1630 - val_loss: 9.5723 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04109: loss did not improve from 2.93593\n",
            "Epoch 4110/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9524 - accuracy: 0.1756 - val_loss: 9.6244 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04110: loss did not improve from 2.93593\n",
            "Epoch 4111/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9444 - accuracy: 0.1585 - val_loss: 9.5756 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04111: loss did not improve from 2.93593\n",
            "Epoch 4112/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9828 - accuracy: 0.1490 - val_loss: 9.5718 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04112: loss did not improve from 2.93593\n",
            "Epoch 4113/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9592 - accuracy: 0.1496 - val_loss: 9.5874 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04113: loss did not improve from 2.93593\n",
            "Epoch 4114/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9257 - accuracy: 0.1658 - val_loss: 9.6054 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04114: loss did not improve from 2.93593\n",
            "Epoch 4115/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9570 - accuracy: 0.1618 - val_loss: 9.5835 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04115: loss did not improve from 2.93593\n",
            "Epoch 4116/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9759 - accuracy: 0.1498 - val_loss: 9.5896 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04116: loss did not improve from 2.93593\n",
            "Epoch 4117/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 3.0030 - accuracy: 0.1392 - val_loss: 9.6037 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04117: loss did not improve from 2.93593\n",
            "Epoch 4118/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9445 - accuracy: 0.1678 - val_loss: 9.6174 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04118: loss did not improve from 2.93593\n",
            "Epoch 4119/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 3.0091 - accuracy: 0.1522 - val_loss: 9.6012 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04119: loss did not improve from 2.93593\n",
            "Epoch 4120/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9733 - accuracy: 0.1488 - val_loss: 9.5536 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04120: loss did not improve from 2.93593\n",
            "Epoch 4121/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9592 - accuracy: 0.1588 - val_loss: 9.6216 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04121: loss did not improve from 2.93593\n",
            "Epoch 4122/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9660 - accuracy: 0.1606 - val_loss: 9.5826 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04122: loss did not improve from 2.93593\n",
            "Epoch 4123/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9835 - accuracy: 0.1530 - val_loss: 9.5436 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04123: loss did not improve from 2.93593\n",
            "Epoch 4124/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9178 - accuracy: 0.1743 - val_loss: 9.6532 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04124: loss did not improve from 2.93593\n",
            "Epoch 4125/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9648 - accuracy: 0.1668 - val_loss: 9.5600 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04125: loss did not improve from 2.93593\n",
            "Epoch 4126/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9646 - accuracy: 0.1559 - val_loss: 9.5707 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04126: loss did not improve from 2.93593\n",
            "Epoch 4127/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9493 - accuracy: 0.1698 - val_loss: 9.6264 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04127: loss did not improve from 2.93593\n",
            "Epoch 4128/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9527 - accuracy: 0.1556 - val_loss: 9.5798 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04128: loss did not improve from 2.93593\n",
            "Epoch 4129/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9610 - accuracy: 0.1514 - val_loss: 9.5856 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04129: loss did not improve from 2.93593\n",
            "Epoch 4130/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9889 - accuracy: 0.1585 - val_loss: 9.6356 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04130: loss did not improve from 2.93593\n",
            "Epoch 4131/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9738 - accuracy: 0.1634 - val_loss: 9.6012 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04131: loss did not improve from 2.93593\n",
            "Epoch 4132/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 3.0006 - accuracy: 0.1374 - val_loss: 9.5815 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04132: loss did not improve from 2.93593\n",
            "Epoch 4133/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9166 - accuracy: 0.1730 - val_loss: 9.5988 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04133: loss did not improve from 2.93593\n",
            "Epoch 4134/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9189 - accuracy: 0.1673 - val_loss: 9.5294 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04134: loss did not improve from 2.93593\n",
            "Epoch 4135/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9402 - accuracy: 0.1588 - val_loss: 9.6095 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04135: loss did not improve from 2.93593\n",
            "Epoch 4136/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9706 - accuracy: 0.1638 - val_loss: 9.6301 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04136: loss did not improve from 2.93593\n",
            "Epoch 4137/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9712 - accuracy: 0.1514 - val_loss: 9.5850 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04137: loss did not improve from 2.93593\n",
            "Epoch 4138/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9203 - accuracy: 0.1738 - val_loss: 9.6291 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04138: loss did not improve from 2.93593\n",
            "Epoch 4139/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9328 - accuracy: 0.1546 - val_loss: 9.5837 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04139: loss did not improve from 2.93593\n",
            "Epoch 4140/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9853 - accuracy: 0.1502 - val_loss: 9.6191 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04140: loss did not improve from 2.93593\n",
            "Epoch 4141/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9362 - accuracy: 0.1684 - val_loss: 9.5315 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04141: loss did not improve from 2.93593\n",
            "Epoch 4142/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9706 - accuracy: 0.1580 - val_loss: 9.5728 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04142: loss did not improve from 2.93593\n",
            "Epoch 4143/5000\n",
            "31/31 [==============================] - 1s 40ms/step - loss: 2.9345 - accuracy: 0.1574 - val_loss: 9.6252 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04143: loss did not improve from 2.93593\n",
            "Epoch 4144/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9847 - accuracy: 0.1477 - val_loss: 9.5657 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04144: loss did not improve from 2.93593\n",
            "Epoch 4145/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9747 - accuracy: 0.1544 - val_loss: 9.5964 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04145: loss did not improve from 2.93593\n",
            "Epoch 4146/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9673 - accuracy: 0.1567 - val_loss: 9.6020 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04146: loss did not improve from 2.93593\n",
            "Epoch 4147/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9591 - accuracy: 0.1474 - val_loss: 9.5894 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04147: loss did not improve from 2.93593\n",
            "Epoch 4148/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9751 - accuracy: 0.1639 - val_loss: 9.6437 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04148: loss did not improve from 2.93593\n",
            "Epoch 4149/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9555 - accuracy: 0.1550 - val_loss: 9.5839 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04149: loss did not improve from 2.93593\n",
            "Epoch 4150/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9437 - accuracy: 0.1757 - val_loss: 9.6263 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04150: loss did not improve from 2.93593\n",
            "Epoch 4151/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9337 - accuracy: 0.1617 - val_loss: 9.6059 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04151: loss did not improve from 2.93593\n",
            "Epoch 4152/5000\n",
            "31/31 [==============================] - 1s 40ms/step - loss: 2.9515 - accuracy: 0.1819 - val_loss: 9.5839 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04152: loss did not improve from 2.93593\n",
            "Epoch 4153/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9416 - accuracy: 0.1705 - val_loss: 9.6064 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04153: loss did not improve from 2.93593\n",
            "Epoch 4154/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9148 - accuracy: 0.1537 - val_loss: 9.6059 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04154: loss did not improve from 2.93593\n",
            "Epoch 4155/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9745 - accuracy: 0.1349 - val_loss: 9.6793 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04155: loss did not improve from 2.93593\n",
            "Epoch 4156/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9400 - accuracy: 0.1757 - val_loss: 9.6178 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04156: loss did not improve from 2.93593\n",
            "Epoch 4157/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9790 - accuracy: 0.1472 - val_loss: 9.6412 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04157: loss did not improve from 2.93593\n",
            "Epoch 4158/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9655 - accuracy: 0.1740 - val_loss: 9.6260 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04158: loss did not improve from 2.93593\n",
            "Epoch 4159/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9293 - accuracy: 0.1680 - val_loss: 9.6096 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04159: loss did not improve from 2.93593\n",
            "Epoch 4160/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9551 - accuracy: 0.1632 - val_loss: 9.5683 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04160: loss did not improve from 2.93593\n",
            "Epoch 4161/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9869 - accuracy: 0.1517 - val_loss: 9.5613 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04161: loss did not improve from 2.93593\n",
            "Epoch 4162/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9675 - accuracy: 0.1539 - val_loss: 9.5552 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04162: loss did not improve from 2.93593\n",
            "Epoch 4163/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9693 - accuracy: 0.1696 - val_loss: 9.5429 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04163: loss did not improve from 2.93593\n",
            "Epoch 4164/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9336 - accuracy: 0.1729 - val_loss: 9.5727 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04164: loss did not improve from 2.93593\n",
            "Epoch 4165/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9340 - accuracy: 0.1608 - val_loss: 9.6220 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04165: loss did not improve from 2.93593\n",
            "Epoch 4166/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9572 - accuracy: 0.1433 - val_loss: 9.5837 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04166: loss did not improve from 2.93593\n",
            "Epoch 4167/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9714 - accuracy: 0.1675 - val_loss: 9.5720 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04167: loss did not improve from 2.93593\n",
            "Epoch 4168/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9437 - accuracy: 0.1565 - val_loss: 9.6164 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04168: loss did not improve from 2.93593\n",
            "Epoch 4169/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9366 - accuracy: 0.1745 - val_loss: 9.6495 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04169: loss did not improve from 2.93593\n",
            "Epoch 4170/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9485 - accuracy: 0.1589 - val_loss: 9.6272 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04170: loss did not improve from 2.93593\n",
            "Epoch 4171/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9197 - accuracy: 0.1623 - val_loss: 9.6582 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04171: loss did not improve from 2.93593\n",
            "Epoch 4172/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9650 - accuracy: 0.1741 - val_loss: 9.5985 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04172: loss did not improve from 2.93593\n",
            "Epoch 4173/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 3.0032 - accuracy: 0.1448 - val_loss: 9.6333 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04173: loss did not improve from 2.93593\n",
            "Epoch 4174/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9273 - accuracy: 0.1580 - val_loss: 9.6102 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04174: loss did not improve from 2.93593\n",
            "Epoch 4175/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9782 - accuracy: 0.1597 - val_loss: 9.6710 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04175: loss did not improve from 2.93593\n",
            "Epoch 4176/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9606 - accuracy: 0.1666 - val_loss: 9.6058 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04176: loss did not improve from 2.93593\n",
            "Epoch 4177/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9416 - accuracy: 0.1734 - val_loss: 9.6974 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04177: loss did not improve from 2.93593\n",
            "Epoch 4178/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9468 - accuracy: 0.1717 - val_loss: 9.6022 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04178: loss did not improve from 2.93593\n",
            "Epoch 4179/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 3.0282 - accuracy: 0.1451 - val_loss: 9.6238 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04179: loss did not improve from 2.93593\n",
            "Epoch 4180/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9323 - accuracy: 0.1604 - val_loss: 9.6341 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04180: loss did not improve from 2.93593\n",
            "Epoch 4181/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9610 - accuracy: 0.1540 - val_loss: 9.6379 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04181: loss did not improve from 2.93593\n",
            "Epoch 4182/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9539 - accuracy: 0.1608 - val_loss: 9.6442 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04182: loss did not improve from 2.93593\n",
            "Epoch 4183/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9000 - accuracy: 0.1695 - val_loss: 9.6397 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04183: loss did not improve from 2.93593\n",
            "Epoch 4184/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9556 - accuracy: 0.1731 - val_loss: 9.6431 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04184: loss did not improve from 2.93593\n",
            "Epoch 4185/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9583 - accuracy: 0.1476 - val_loss: 9.6331 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04185: loss did not improve from 2.93593\n",
            "Epoch 4186/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9685 - accuracy: 0.1503 - val_loss: 9.6563 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04186: loss did not improve from 2.93593\n",
            "Epoch 4187/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9490 - accuracy: 0.1531 - val_loss: 9.6470 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04187: loss did not improve from 2.93593\n",
            "Epoch 4188/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9516 - accuracy: 0.1719 - val_loss: 9.6495 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04188: loss did not improve from 2.93593\n",
            "Epoch 4189/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9329 - accuracy: 0.1750 - val_loss: 9.6164 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04189: loss did not improve from 2.93593\n",
            "Epoch 4190/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9527 - accuracy: 0.1833 - val_loss: 9.6358 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04190: loss did not improve from 2.93593\n",
            "Epoch 4191/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9378 - accuracy: 0.1538 - val_loss: 9.6286 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04191: loss did not improve from 2.93593\n",
            "Epoch 4192/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9320 - accuracy: 0.1622 - val_loss: 9.6172 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04192: loss did not improve from 2.93593\n",
            "Epoch 4193/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9566 - accuracy: 0.1624 - val_loss: 9.6681 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04193: loss did not improve from 2.93593\n",
            "Epoch 4194/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9355 - accuracy: 0.1816 - val_loss: 9.6502 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04194: loss did not improve from 2.93593\n",
            "Epoch 4195/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9533 - accuracy: 0.1468 - val_loss: 9.6465 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04195: loss did not improve from 2.93593\n",
            "Epoch 4196/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9659 - accuracy: 0.1660 - val_loss: 9.6710 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04196: loss did not improve from 2.93593\n",
            "Epoch 4197/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9672 - accuracy: 0.1658 - val_loss: 9.6878 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04197: loss did not improve from 2.93593\n",
            "Epoch 4198/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9618 - accuracy: 0.1492 - val_loss: 9.6069 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04198: loss did not improve from 2.93593\n",
            "Epoch 4199/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9529 - accuracy: 0.1573 - val_loss: 9.6525 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04199: loss did not improve from 2.93593\n",
            "Epoch 4200/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9681 - accuracy: 0.1434 - val_loss: 9.6715 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04200: loss did not improve from 2.93593\n",
            "Epoch 4201/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9627 - accuracy: 0.1582 - val_loss: 9.6653 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04201: loss did not improve from 2.93593\n",
            "Epoch 4202/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9457 - accuracy: 0.1565 - val_loss: 9.6700 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04202: loss did not improve from 2.93593\n",
            "Epoch 4203/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9502 - accuracy: 0.1689 - val_loss: 9.6146 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04203: loss did not improve from 2.93593\n",
            "Epoch 4204/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9924 - accuracy: 0.1510 - val_loss: 9.6676 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04204: loss did not improve from 2.93593\n",
            "Epoch 4205/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9538 - accuracy: 0.1675 - val_loss: 9.6713 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04205: loss did not improve from 2.93593\n",
            "Epoch 4206/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9402 - accuracy: 0.1583 - val_loss: 9.6705 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04206: loss did not improve from 2.93593\n",
            "Epoch 4207/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9280 - accuracy: 0.1790 - val_loss: 9.6524 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04207: loss did not improve from 2.93593\n",
            "Epoch 4208/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9554 - accuracy: 0.1540 - val_loss: 9.5941 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04208: loss did not improve from 2.93593\n",
            "Epoch 4209/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9793 - accuracy: 0.1709 - val_loss: 9.6229 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04209: loss did not improve from 2.93593\n",
            "Epoch 4210/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9460 - accuracy: 0.1700 - val_loss: 9.6651 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04210: loss did not improve from 2.93593\n",
            "Epoch 4211/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9416 - accuracy: 0.1775 - val_loss: 9.6665 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04211: loss did not improve from 2.93593\n",
            "Epoch 4212/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9685 - accuracy: 0.1623 - val_loss: 9.6486 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04212: loss did not improve from 2.93593\n",
            "Epoch 4213/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9358 - accuracy: 0.1631 - val_loss: 9.6265 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04213: loss did not improve from 2.93593\n",
            "Epoch 4214/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9368 - accuracy: 0.1544 - val_loss: 9.6506 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04214: loss did not improve from 2.93593\n",
            "Epoch 4215/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9827 - accuracy: 0.1413 - val_loss: 9.6789 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04215: loss did not improve from 2.93593\n",
            "Epoch 4216/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9830 - accuracy: 0.1618 - val_loss: 9.5978 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04216: loss did not improve from 2.93593\n",
            "Epoch 4217/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9715 - accuracy: 0.1531 - val_loss: 9.6571 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04217: loss did not improve from 2.93593\n",
            "Epoch 4218/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9243 - accuracy: 0.1511 - val_loss: 9.6732 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04218: loss did not improve from 2.93593\n",
            "Epoch 4219/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9392 - accuracy: 0.1740 - val_loss: 9.5864 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04219: loss did not improve from 2.93593\n",
            "Epoch 4220/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9210 - accuracy: 0.1776 - val_loss: 9.6501 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04220: loss did not improve from 2.93593\n",
            "Epoch 4221/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9880 - accuracy: 0.1586 - val_loss: 9.6079 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04221: loss did not improve from 2.93593\n",
            "Epoch 4222/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9643 - accuracy: 0.1679 - val_loss: 9.7123 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04222: loss did not improve from 2.93593\n",
            "Epoch 4223/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9223 - accuracy: 0.1691 - val_loss: 9.6229 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04223: loss did not improve from 2.93593\n",
            "Epoch 4224/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9315 - accuracy: 0.1719 - val_loss: 9.6419 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04224: loss did not improve from 2.93593\n",
            "Epoch 4225/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9420 - accuracy: 0.1614 - val_loss: 9.6580 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04225: loss did not improve from 2.93593\n",
            "Epoch 4226/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9757 - accuracy: 0.1390 - val_loss: 9.6673 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04226: loss did not improve from 2.93593\n",
            "Epoch 4227/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9885 - accuracy: 0.1489 - val_loss: 9.6975 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04227: loss did not improve from 2.93593\n",
            "Epoch 4228/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9541 - accuracy: 0.1617 - val_loss: 9.6494 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04228: loss did not improve from 2.93593\n",
            "Epoch 4229/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9533 - accuracy: 0.1521 - val_loss: 9.6787 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04229: loss did not improve from 2.93593\n",
            "Epoch 4230/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9716 - accuracy: 0.1557 - val_loss: 9.6086 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04230: loss did not improve from 2.93593\n",
            "Epoch 4231/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9683 - accuracy: 0.1404 - val_loss: 9.5862 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04231: loss did not improve from 2.93593\n",
            "Epoch 4232/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9882 - accuracy: 0.1437 - val_loss: 9.6379 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04232: loss did not improve from 2.93593\n",
            "Epoch 4233/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9475 - accuracy: 0.1534 - val_loss: 9.6732 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04233: loss did not improve from 2.93593\n",
            "Epoch 4234/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9473 - accuracy: 0.1498 - val_loss: 9.6132 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04234: loss did not improve from 2.93593\n",
            "Epoch 4235/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9357 - accuracy: 0.1722 - val_loss: 9.6308 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04235: loss did not improve from 2.93593\n",
            "Epoch 4236/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9473 - accuracy: 0.1625 - val_loss: 9.6046 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04236: loss did not improve from 2.93593\n",
            "Epoch 4237/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9522 - accuracy: 0.1636 - val_loss: 9.6671 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04237: loss did not improve from 2.93593\n",
            "Epoch 4238/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9461 - accuracy: 0.1617 - val_loss: 9.5921 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04238: loss did not improve from 2.93593\n",
            "Epoch 4239/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9807 - accuracy: 0.1438 - val_loss: 9.6899 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04239: loss did not improve from 2.93593\n",
            "Epoch 4240/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9618 - accuracy: 0.1616 - val_loss: 9.6686 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04240: loss did not improve from 2.93593\n",
            "Epoch 4241/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9658 - accuracy: 0.1477 - val_loss: 9.6692 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04241: loss did not improve from 2.93593\n",
            "Epoch 4242/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9500 - accuracy: 0.1594 - val_loss: 9.6716 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04242: loss did not improve from 2.93593\n",
            "Epoch 4243/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9506 - accuracy: 0.1781 - val_loss: 9.6331 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04243: loss did not improve from 2.93593\n",
            "Epoch 4244/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9399 - accuracy: 0.1625 - val_loss: 9.6368 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04244: loss did not improve from 2.93593\n",
            "Epoch 4245/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9515 - accuracy: 0.1619 - val_loss: 9.6180 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04245: loss did not improve from 2.93593\n",
            "Epoch 4246/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9753 - accuracy: 0.1570 - val_loss: 9.6681 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04246: loss did not improve from 2.93593\n",
            "Epoch 4247/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9688 - accuracy: 0.1627 - val_loss: 9.6011 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04247: loss did not improve from 2.93593\n",
            "Epoch 4248/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9458 - accuracy: 0.1646 - val_loss: 9.5973 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04248: loss did not improve from 2.93593\n",
            "Epoch 4249/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9717 - accuracy: 0.1636 - val_loss: 9.6572 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04249: loss did not improve from 2.93593\n",
            "Epoch 4250/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9529 - accuracy: 0.1686 - val_loss: 9.6365 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04250: loss did not improve from 2.93593\n",
            "Epoch 4251/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9768 - accuracy: 0.1669 - val_loss: 9.6552 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04251: loss did not improve from 2.93593\n",
            "Epoch 4252/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9662 - accuracy: 0.1597 - val_loss: 9.6810 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04252: loss did not improve from 2.93593\n",
            "Epoch 4253/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9906 - accuracy: 0.1693 - val_loss: 9.6791 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04253: loss did not improve from 2.93593\n",
            "Epoch 4254/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9534 - accuracy: 0.1627 - val_loss: 9.6763 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04254: loss did not improve from 2.93593\n",
            "Epoch 4255/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9450 - accuracy: 0.1632 - val_loss: 9.6264 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04255: loss did not improve from 2.93593\n",
            "Epoch 4256/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9537 - accuracy: 0.1480 - val_loss: 9.6646 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04256: loss did not improve from 2.93593\n",
            "Epoch 4257/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9344 - accuracy: 0.1810 - val_loss: 9.6813 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04257: loss did not improve from 2.93593\n",
            "Epoch 4258/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9698 - accuracy: 0.1595 - val_loss: 9.6617 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04258: loss did not improve from 2.93593\n",
            "Epoch 4259/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9339 - accuracy: 0.1777 - val_loss: 9.6359 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04259: loss did not improve from 2.93593\n",
            "Epoch 4260/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9178 - accuracy: 0.1678 - val_loss: 9.6709 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04260: loss did not improve from 2.93593\n",
            "Epoch 4261/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9388 - accuracy: 0.1553 - val_loss: 9.6136 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04261: loss did not improve from 2.93593\n",
            "Epoch 4262/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9586 - accuracy: 0.1659 - val_loss: 9.6611 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04262: loss did not improve from 2.93593\n",
            "Epoch 4263/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9572 - accuracy: 0.1551 - val_loss: 9.6648 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04263: loss did not improve from 2.93593\n",
            "Epoch 4264/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9573 - accuracy: 0.1680 - val_loss: 9.6641 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04264: loss did not improve from 2.93593\n",
            "Epoch 4265/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9543 - accuracy: 0.1428 - val_loss: 9.6190 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04265: loss did not improve from 2.93593\n",
            "Epoch 4266/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9420 - accuracy: 0.1511 - val_loss: 9.6557 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04266: loss did not improve from 2.93593\n",
            "Epoch 4267/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9478 - accuracy: 0.1614 - val_loss: 9.6899 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04267: loss did not improve from 2.93593\n",
            "Epoch 4268/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9348 - accuracy: 0.1713 - val_loss: 9.6780 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04268: loss did not improve from 2.93593\n",
            "Epoch 4269/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9253 - accuracy: 0.1702 - val_loss: 9.7205 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04269: loss did not improve from 2.93593\n",
            "Epoch 4270/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9756 - accuracy: 0.1588 - val_loss: 9.7080 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04270: loss did not improve from 2.93593\n",
            "Epoch 4271/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9519 - accuracy: 0.1708 - val_loss: 9.6594 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04271: loss did not improve from 2.93593\n",
            "Epoch 4272/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9679 - accuracy: 0.1563 - val_loss: 9.6439 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04272: loss did not improve from 2.93593\n",
            "Epoch 4273/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9295 - accuracy: 0.1759 - val_loss: 9.6452 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04273: loss did not improve from 2.93593\n",
            "Epoch 4274/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9484 - accuracy: 0.1513 - val_loss: 9.6391 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04274: loss did not improve from 2.93593\n",
            "Epoch 4275/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9487 - accuracy: 0.1642 - val_loss: 9.6667 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04275: loss did not improve from 2.93593\n",
            "Epoch 4276/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9279 - accuracy: 0.1760 - val_loss: 9.6724 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04276: loss did not improve from 2.93593\n",
            "Epoch 4277/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9479 - accuracy: 0.1477 - val_loss: 9.6703 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04277: loss did not improve from 2.93593\n",
            "Epoch 4278/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9601 - accuracy: 0.1455 - val_loss: 9.6452 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04278: loss did not improve from 2.93593\n",
            "Epoch 4279/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9286 - accuracy: 0.1814 - val_loss: 9.6372 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04279: loss did not improve from 2.93593\n",
            "Epoch 4280/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9264 - accuracy: 0.1822 - val_loss: 9.6589 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04280: loss did not improve from 2.93593\n",
            "Epoch 4281/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9363 - accuracy: 0.1517 - val_loss: 9.6268 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04281: loss did not improve from 2.93593\n",
            "Epoch 4282/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9710 - accuracy: 0.1614 - val_loss: 9.7217 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04282: loss did not improve from 2.93593\n",
            "Epoch 4283/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9678 - accuracy: 0.1748 - val_loss: 9.6634 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04283: loss did not improve from 2.93593\n",
            "Epoch 4284/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9241 - accuracy: 0.1576 - val_loss: 9.6589 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04284: loss did not improve from 2.93593\n",
            "Epoch 4285/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9077 - accuracy: 0.1740 - val_loss: 9.6887 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04285: loss did not improve from 2.93593\n",
            "Epoch 4286/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9777 - accuracy: 0.1575 - val_loss: 9.6737 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04286: loss did not improve from 2.93593\n",
            "Epoch 4287/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9368 - accuracy: 0.1505 - val_loss: 9.6596 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04287: loss did not improve from 2.93593\n",
            "Epoch 4288/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9556 - accuracy: 0.1601 - val_loss: 9.6704 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04288: loss did not improve from 2.93593\n",
            "Epoch 4289/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9570 - accuracy: 0.1611 - val_loss: 9.6986 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04289: loss did not improve from 2.93593\n",
            "Epoch 4290/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9781 - accuracy: 0.1638 - val_loss: 9.6536 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04290: loss did not improve from 2.93593\n",
            "Epoch 4291/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9390 - accuracy: 0.1682 - val_loss: 9.6176 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04291: loss did not improve from 2.93593\n",
            "Epoch 4292/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9166 - accuracy: 0.1780 - val_loss: 9.6393 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04292: loss did not improve from 2.93593\n",
            "Epoch 4293/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9304 - accuracy: 0.1657 - val_loss: 9.6677 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04293: loss did not improve from 2.93593\n",
            "Epoch 4294/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9907 - accuracy: 0.1672 - val_loss: 9.7014 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04294: loss did not improve from 2.93593\n",
            "Epoch 4295/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9528 - accuracy: 0.1582 - val_loss: 9.7373 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04295: loss did not improve from 2.93593\n",
            "Epoch 4296/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9528 - accuracy: 0.1541 - val_loss: 9.6212 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04296: loss did not improve from 2.93593\n",
            "Epoch 4297/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9245 - accuracy: 0.1841 - val_loss: 9.6834 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04297: loss did not improve from 2.93593\n",
            "Epoch 4298/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9559 - accuracy: 0.1593 - val_loss: 9.7482 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04298: loss did not improve from 2.93593\n",
            "Epoch 4299/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9522 - accuracy: 0.1670 - val_loss: 9.6441 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04299: loss did not improve from 2.93593\n",
            "Epoch 4300/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9575 - accuracy: 0.1678 - val_loss: 9.6619 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04300: loss did not improve from 2.93593\n",
            "Epoch 4301/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9749 - accuracy: 0.1490 - val_loss: 9.6909 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04301: loss did not improve from 2.93593\n",
            "Epoch 4302/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9524 - accuracy: 0.1501 - val_loss: 9.6847 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04302: loss did not improve from 2.93593\n",
            "Epoch 4303/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9573 - accuracy: 0.1438 - val_loss: 9.6440 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04303: loss did not improve from 2.93593\n",
            "Epoch 4304/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9658 - accuracy: 0.1645 - val_loss: 9.6871 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04304: loss did not improve from 2.93593\n",
            "Epoch 4305/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9815 - accuracy: 0.1427 - val_loss: 9.7112 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04305: loss did not improve from 2.93593\n",
            "Epoch 4306/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9551 - accuracy: 0.1606 - val_loss: 9.6698 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04306: loss did not improve from 2.93593\n",
            "Epoch 4307/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9347 - accuracy: 0.1626 - val_loss: 9.5931 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04307: loss did not improve from 2.93593\n",
            "Epoch 4308/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9358 - accuracy: 0.1628 - val_loss: 9.6796 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04308: loss did not improve from 2.93593\n",
            "Epoch 4309/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9542 - accuracy: 0.1498 - val_loss: 9.6918 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04309: loss did not improve from 2.93593\n",
            "Epoch 4310/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9522 - accuracy: 0.1683 - val_loss: 9.6796 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04310: loss did not improve from 2.93593\n",
            "Epoch 4311/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9331 - accuracy: 0.1698 - val_loss: 9.6773 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04311: loss did not improve from 2.93593\n",
            "Epoch 4312/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9452 - accuracy: 0.1809 - val_loss: 9.6661 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04312: loss did not improve from 2.93593\n",
            "Epoch 4313/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9696 - accuracy: 0.1604 - val_loss: 9.7035 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04313: loss did not improve from 2.93593\n",
            "Epoch 4314/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9394 - accuracy: 0.1536 - val_loss: 9.6486 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04314: loss did not improve from 2.93593\n",
            "Epoch 4315/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9132 - accuracy: 0.1704 - val_loss: 9.6688 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04315: loss did not improve from 2.93593\n",
            "Epoch 4316/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9580 - accuracy: 0.1676 - val_loss: 9.6743 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04316: loss did not improve from 2.93593\n",
            "Epoch 4317/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9609 - accuracy: 0.1619 - val_loss: 9.6186 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04317: loss did not improve from 2.93593\n",
            "Epoch 4318/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9905 - accuracy: 0.1515 - val_loss: 9.6700 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04318: loss did not improve from 2.93593\n",
            "Epoch 4319/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9404 - accuracy: 0.1803 - val_loss: 9.6633 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04319: loss did not improve from 2.93593\n",
            "Epoch 4320/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9568 - accuracy: 0.1686 - val_loss: 9.6806 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04320: loss did not improve from 2.93593\n",
            "Epoch 4321/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9475 - accuracy: 0.1716 - val_loss: 9.7102 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04321: loss did not improve from 2.93593\n",
            "Epoch 4322/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9790 - accuracy: 0.1428 - val_loss: 9.7028 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04322: loss did not improve from 2.93593\n",
            "Epoch 4323/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9391 - accuracy: 0.1602 - val_loss: 9.6635 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04323: loss did not improve from 2.93593\n",
            "Epoch 4324/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9329 - accuracy: 0.1618 - val_loss: 9.6594 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04324: loss did not improve from 2.93593\n",
            "Epoch 4325/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9569 - accuracy: 0.1598 - val_loss: 9.6541 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04325: loss did not improve from 2.93593\n",
            "Epoch 4326/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9480 - accuracy: 0.1619 - val_loss: 9.6719 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04326: loss did not improve from 2.93593\n",
            "Epoch 4327/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9428 - accuracy: 0.1388 - val_loss: 9.6998 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04327: loss did not improve from 2.93593\n",
            "Epoch 4328/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9484 - accuracy: 0.1652 - val_loss: 9.6147 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04328: loss did not improve from 2.93593\n",
            "Epoch 4329/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9555 - accuracy: 0.1683 - val_loss: 9.6891 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04329: loss did not improve from 2.93593\n",
            "Epoch 4330/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9293 - accuracy: 0.1687 - val_loss: 9.7079 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04330: loss did not improve from 2.93593\n",
            "Epoch 4331/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9880 - accuracy: 0.1613 - val_loss: 9.6920 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04331: loss did not improve from 2.93593\n",
            "Epoch 4332/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9266 - accuracy: 0.1688 - val_loss: 9.6332 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04332: loss did not improve from 2.93593\n",
            "Epoch 4333/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9693 - accuracy: 0.1752 - val_loss: 9.6757 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04333: loss did not improve from 2.93593\n",
            "Epoch 4334/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9211 - accuracy: 0.1602 - val_loss: 9.6534 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04334: loss did not improve from 2.93593\n",
            "Epoch 4335/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9712 - accuracy: 0.1552 - val_loss: 9.6947 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04335: loss did not improve from 2.93593\n",
            "Epoch 4336/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9602 - accuracy: 0.1582 - val_loss: 9.7234 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04336: loss did not improve from 2.93593\n",
            "Epoch 4337/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9553 - accuracy: 0.1640 - val_loss: 9.6581 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04337: loss did not improve from 2.93593\n",
            "Epoch 4338/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9616 - accuracy: 0.1640 - val_loss: 9.6609 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04338: loss did not improve from 2.93593\n",
            "Epoch 4339/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9316 - accuracy: 0.1709 - val_loss: 9.7500 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04339: loss did not improve from 2.93593\n",
            "Epoch 4340/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9945 - accuracy: 0.1535 - val_loss: 9.6621 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04340: loss did not improve from 2.93593\n",
            "Epoch 4341/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9682 - accuracy: 0.1617 - val_loss: 9.7225 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04341: loss did not improve from 2.93593\n",
            "Epoch 4342/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9496 - accuracy: 0.1632 - val_loss: 9.7168 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04342: loss did not improve from 2.93593\n",
            "Epoch 4343/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9915 - accuracy: 0.1480 - val_loss: 9.6837 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04343: loss did not improve from 2.93593\n",
            "Epoch 4344/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9450 - accuracy: 0.1501 - val_loss: 9.6774 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04344: loss did not improve from 2.93593\n",
            "Epoch 4345/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9667 - accuracy: 0.1664 - val_loss: 9.7039 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04345: loss did not improve from 2.93593\n",
            "Epoch 4346/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9103 - accuracy: 0.1651 - val_loss: 9.6608 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04346: loss did not improve from 2.93593\n",
            "Epoch 4347/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9322 - accuracy: 0.1740 - val_loss: 9.6572 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04347: loss did not improve from 2.93593\n",
            "Epoch 4348/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9545 - accuracy: 0.1652 - val_loss: 9.6516 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04348: loss did not improve from 2.93593\n",
            "Epoch 4349/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9791 - accuracy: 0.1595 - val_loss: 9.6718 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04349: loss did not improve from 2.93593\n",
            "Epoch 4350/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.8974 - accuracy: 0.1649 - val_loss: 9.7573 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04350: loss did not improve from 2.93593\n",
            "Epoch 4351/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9729 - accuracy: 0.1624 - val_loss: 9.6725 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04351: loss did not improve from 2.93593\n",
            "Epoch 4352/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9687 - accuracy: 0.1574 - val_loss: 9.6695 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04352: loss did not improve from 2.93593\n",
            "Epoch 4353/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9535 - accuracy: 0.1709 - val_loss: 9.6926 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04353: loss did not improve from 2.93593\n",
            "Epoch 4354/5000\n",
            "31/31 [==============================] - 1s 40ms/step - loss: 2.9025 - accuracy: 0.1784 - val_loss: 9.6780 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04354: loss did not improve from 2.93593\n",
            "Epoch 4355/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9231 - accuracy: 0.1782 - val_loss: 9.6788 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04355: loss did not improve from 2.93593\n",
            "Epoch 4356/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9395 - accuracy: 0.1715 - val_loss: 9.6826 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04356: loss did not improve from 2.93593\n",
            "Epoch 4357/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9502 - accuracy: 0.1595 - val_loss: 9.7339 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04357: loss did not improve from 2.93593\n",
            "Epoch 4358/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9319 - accuracy: 0.1600 - val_loss: 9.7025 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04358: loss did not improve from 2.93593\n",
            "Epoch 4359/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9765 - accuracy: 0.1498 - val_loss: 9.6554 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04359: loss did not improve from 2.93593\n",
            "Epoch 4360/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9689 - accuracy: 0.1690 - val_loss: 9.6965 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04360: loss did not improve from 2.93593\n",
            "Epoch 4361/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9543 - accuracy: 0.1577 - val_loss: 9.6574 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04361: loss did not improve from 2.93593\n",
            "Epoch 4362/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9185 - accuracy: 0.1688 - val_loss: 9.6902 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04362: loss did not improve from 2.93593\n",
            "Epoch 4363/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9879 - accuracy: 0.1685 - val_loss: 9.6606 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04363: loss did not improve from 2.93593\n",
            "Epoch 4364/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9850 - accuracy: 0.1476 - val_loss: 9.6644 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04364: loss did not improve from 2.93593\n",
            "Epoch 4365/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9447 - accuracy: 0.1647 - val_loss: 9.7171 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04365: loss did not improve from 2.93593\n",
            "Epoch 4366/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9825 - accuracy: 0.1597 - val_loss: 9.6920 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04366: loss did not improve from 2.93593\n",
            "Epoch 4367/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9575 - accuracy: 0.1698 - val_loss: 9.6971 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04367: loss did not improve from 2.93593\n",
            "Epoch 4368/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9597 - accuracy: 0.1571 - val_loss: 9.6827 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04368: loss did not improve from 2.93593\n",
            "Epoch 4369/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9601 - accuracy: 0.1570 - val_loss: 9.7329 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04369: loss did not improve from 2.93593\n",
            "Epoch 4370/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9405 - accuracy: 0.1631 - val_loss: 9.6595 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04370: loss did not improve from 2.93593\n",
            "Epoch 4371/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9882 - accuracy: 0.1518 - val_loss: 9.6442 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04371: loss did not improve from 2.93593\n",
            "Epoch 4372/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9752 - accuracy: 0.1548 - val_loss: 9.6712 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04372: loss did not improve from 2.93593\n",
            "Epoch 4373/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9510 - accuracy: 0.1594 - val_loss: 9.7241 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04373: loss did not improve from 2.93593\n",
            "Epoch 4374/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9533 - accuracy: 0.1596 - val_loss: 9.6785 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04374: loss did not improve from 2.93593\n",
            "Epoch 4375/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9283 - accuracy: 0.1647 - val_loss: 9.6653 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04375: loss did not improve from 2.93593\n",
            "Epoch 4376/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9781 - accuracy: 0.1502 - val_loss: 9.6686 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04376: loss did not improve from 2.93593\n",
            "Epoch 4377/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9852 - accuracy: 0.1677 - val_loss: 9.7131 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04377: loss did not improve from 2.93593\n",
            "Epoch 4378/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9679 - accuracy: 0.1484 - val_loss: 9.6967 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04378: loss did not improve from 2.93593\n",
            "Epoch 4379/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9294 - accuracy: 0.1851 - val_loss: 9.6772 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04379: loss did not improve from 2.93593\n",
            "Epoch 4380/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9708 - accuracy: 0.1547 - val_loss: 9.6987 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04380: loss did not improve from 2.93593\n",
            "Epoch 4381/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9784 - accuracy: 0.1545 - val_loss: 9.6968 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04381: loss did not improve from 2.93593\n",
            "Epoch 4382/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9695 - accuracy: 0.1662 - val_loss: 9.6832 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04382: loss did not improve from 2.93593\n",
            "Epoch 4383/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9535 - accuracy: 0.1622 - val_loss: 9.6779 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04383: loss did not improve from 2.93593\n",
            "Epoch 4384/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9270 - accuracy: 0.1590 - val_loss: 9.7121 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04384: loss did not improve from 2.93593\n",
            "Epoch 4385/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9466 - accuracy: 0.1674 - val_loss: 9.6960 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04385: loss did not improve from 2.93593\n",
            "Epoch 4386/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9488 - accuracy: 0.1706 - val_loss: 9.7141 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04386: loss did not improve from 2.93593\n",
            "Epoch 4387/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9518 - accuracy: 0.1582 - val_loss: 9.6631 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04387: loss did not improve from 2.93593\n",
            "Epoch 4388/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9179 - accuracy: 0.1630 - val_loss: 9.6478 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04388: loss did not improve from 2.93593\n",
            "Epoch 4389/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9329 - accuracy: 0.1654 - val_loss: 9.6782 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04389: loss did not improve from 2.93593\n",
            "Epoch 4390/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9661 - accuracy: 0.1510 - val_loss: 9.6764 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04390: loss did not improve from 2.93593\n",
            "Epoch 4391/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9252 - accuracy: 0.1592 - val_loss: 9.6347 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04391: loss did not improve from 2.93593\n",
            "Epoch 4392/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9580 - accuracy: 0.1652 - val_loss: 9.6811 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04392: loss did not improve from 2.93593\n",
            "Epoch 4393/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9709 - accuracy: 0.1584 - val_loss: 9.6922 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04393: loss did not improve from 2.93593\n",
            "Epoch 4394/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9757 - accuracy: 0.1516 - val_loss: 9.6914 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04394: loss did not improve from 2.93593\n",
            "Epoch 4395/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9142 - accuracy: 0.1810 - val_loss: 9.6344 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04395: loss did not improve from 2.93593\n",
            "Epoch 4396/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9452 - accuracy: 0.1635 - val_loss: 9.7204 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04396: loss did not improve from 2.93593\n",
            "Epoch 4397/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9660 - accuracy: 0.1622 - val_loss: 9.7202 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04397: loss did not improve from 2.93593\n",
            "Epoch 4398/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9579 - accuracy: 0.1632 - val_loss: 9.6864 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04398: loss did not improve from 2.93593\n",
            "Epoch 4399/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9597 - accuracy: 0.1455 - val_loss: 9.7293 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04399: loss did not improve from 2.93593\n",
            "Epoch 4400/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9224 - accuracy: 0.1659 - val_loss: 9.6599 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04400: loss did not improve from 2.93593\n",
            "Epoch 4401/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9658 - accuracy: 0.1587 - val_loss: 9.6967 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04401: loss did not improve from 2.93593\n",
            "Epoch 4402/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9368 - accuracy: 0.1812 - val_loss: 9.7807 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04402: loss did not improve from 2.93593\n",
            "Epoch 4403/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9253 - accuracy: 0.1655 - val_loss: 9.7145 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04403: loss did not improve from 2.93593\n",
            "Epoch 4404/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9009 - accuracy: 0.1778 - val_loss: 9.6984 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04404: loss improved from 2.93593 to 2.93574, saving model to poids_train.hdf5\n",
            "Epoch 4405/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9718 - accuracy: 0.1602 - val_loss: 9.7074 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04405: loss did not improve from 2.93574\n",
            "Epoch 4406/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9696 - accuracy: 0.1581 - val_loss: 9.7181 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04406: loss did not improve from 2.93574\n",
            "Epoch 4407/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9905 - accuracy: 0.1607 - val_loss: 9.6852 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04407: loss did not improve from 2.93574\n",
            "Epoch 4408/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9550 - accuracy: 0.1640 - val_loss: 9.6616 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04408: loss did not improve from 2.93574\n",
            "Epoch 4409/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9423 - accuracy: 0.1625 - val_loss: 9.6865 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04409: loss did not improve from 2.93574\n",
            "Epoch 4410/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9550 - accuracy: 0.1628 - val_loss: 9.7229 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04410: loss did not improve from 2.93574\n",
            "Epoch 4411/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9618 - accuracy: 0.1555 - val_loss: 9.6543 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04411: loss did not improve from 2.93574\n",
            "Epoch 4412/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9313 - accuracy: 0.1661 - val_loss: 9.7033 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04412: loss did not improve from 2.93574\n",
            "Epoch 4413/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9731 - accuracy: 0.1617 - val_loss: 9.6745 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04413: loss did not improve from 2.93574\n",
            "Epoch 4414/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9382 - accuracy: 0.1744 - val_loss: 9.6744 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04414: loss did not improve from 2.93574\n",
            "Epoch 4415/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9494 - accuracy: 0.1511 - val_loss: 9.7435 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04415: loss did not improve from 2.93574\n",
            "Epoch 4416/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9699 - accuracy: 0.1518 - val_loss: 9.6852 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04416: loss did not improve from 2.93574\n",
            "Epoch 4417/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9760 - accuracy: 0.1583 - val_loss: 9.6561 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04417: loss did not improve from 2.93574\n",
            "Epoch 4418/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9334 - accuracy: 0.1518 - val_loss: 9.6834 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04418: loss did not improve from 2.93574\n",
            "Epoch 4419/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9456 - accuracy: 0.1629 - val_loss: 9.6835 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04419: loss did not improve from 2.93574\n",
            "Epoch 4420/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9768 - accuracy: 0.1558 - val_loss: 9.6807 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04420: loss did not improve from 2.93574\n",
            "Epoch 4421/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9969 - accuracy: 0.1610 - val_loss: 9.6984 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04421: loss did not improve from 2.93574\n",
            "Epoch 4422/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9453 - accuracy: 0.1604 - val_loss: 9.7066 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04422: loss improved from 2.93574 to 2.93539, saving model to poids_train.hdf5\n",
            "Epoch 4423/5000\n",
            "31/31 [==============================] - 1s 40ms/step - loss: 2.9784 - accuracy: 0.1447 - val_loss: 9.7283 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04423: loss did not improve from 2.93539\n",
            "Epoch 4424/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9729 - accuracy: 0.1687 - val_loss: 9.6794 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04424: loss did not improve from 2.93539\n",
            "Epoch 4425/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9813 - accuracy: 0.1424 - val_loss: 9.6656 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04425: loss did not improve from 2.93539\n",
            "Epoch 4426/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9515 - accuracy: 0.1641 - val_loss: 9.6662 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04426: loss did not improve from 2.93539\n",
            "Epoch 4427/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9925 - accuracy: 0.1551 - val_loss: 9.6763 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04427: loss did not improve from 2.93539\n",
            "Epoch 4428/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9359 - accuracy: 0.1851 - val_loss: 9.6797 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04428: loss did not improve from 2.93539\n",
            "Epoch 4429/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9388 - accuracy: 0.1727 - val_loss: 9.6938 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04429: loss did not improve from 2.93539\n",
            "Epoch 4430/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9272 - accuracy: 0.1549 - val_loss: 9.6848 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04430: loss did not improve from 2.93539\n",
            "Epoch 4431/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9349 - accuracy: 0.1562 - val_loss: 9.6991 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04431: loss did not improve from 2.93539\n",
            "Epoch 4432/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9291 - accuracy: 0.1419 - val_loss: 9.7745 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04432: loss did not improve from 2.93539\n",
            "Epoch 4433/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9553 - accuracy: 0.1526 - val_loss: 9.7548 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04433: loss did not improve from 2.93539\n",
            "Epoch 4434/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9289 - accuracy: 0.1755 - val_loss: 9.6783 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04434: loss did not improve from 2.93539\n",
            "Epoch 4435/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9809 - accuracy: 0.1743 - val_loss: 9.6774 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04435: loss did not improve from 2.93539\n",
            "Epoch 4436/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9689 - accuracy: 0.1599 - val_loss: 9.7250 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04436: loss did not improve from 2.93539\n",
            "Epoch 4437/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9360 - accuracy: 0.1595 - val_loss: 9.6966 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04437: loss did not improve from 2.93539\n",
            "Epoch 4438/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9516 - accuracy: 0.1665 - val_loss: 9.6560 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04438: loss did not improve from 2.93539\n",
            "Epoch 4439/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9433 - accuracy: 0.1499 - val_loss: 9.6984 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04439: loss did not improve from 2.93539\n",
            "Epoch 4440/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9352 - accuracy: 0.1571 - val_loss: 9.7153 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04440: loss did not improve from 2.93539\n",
            "Epoch 4441/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9606 - accuracy: 0.1598 - val_loss: 9.7258 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04441: loss did not improve from 2.93539\n",
            "Epoch 4442/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9414 - accuracy: 0.1549 - val_loss: 9.6638 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04442: loss did not improve from 2.93539\n",
            "Epoch 4443/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9590 - accuracy: 0.1620 - val_loss: 9.7147 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04443: loss did not improve from 2.93539\n",
            "Epoch 4444/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9564 - accuracy: 0.1746 - val_loss: 9.6654 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04444: loss did not improve from 2.93539\n",
            "Epoch 4445/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9448 - accuracy: 0.1609 - val_loss: 9.7382 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04445: loss did not improve from 2.93539\n",
            "Epoch 4446/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9639 - accuracy: 0.1525 - val_loss: 9.6945 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04446: loss did not improve from 2.93539\n",
            "Epoch 4447/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9317 - accuracy: 0.1735 - val_loss: 9.7264 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04447: loss did not improve from 2.93539\n",
            "Epoch 4448/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.8955 - accuracy: 0.1735 - val_loss: 9.6729 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04448: loss did not improve from 2.93539\n",
            "Epoch 4449/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9558 - accuracy: 0.1660 - val_loss: 9.6989 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04449: loss did not improve from 2.93539\n",
            "Epoch 4450/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9574 - accuracy: 0.1694 - val_loss: 9.7148 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04450: loss did not improve from 2.93539\n",
            "Epoch 4451/5000\n",
            "31/31 [==============================] - 1s 40ms/step - loss: 2.9485 - accuracy: 0.1685 - val_loss: 9.6958 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04451: loss did not improve from 2.93539\n",
            "Epoch 4452/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9360 - accuracy: 0.1510 - val_loss: 9.7313 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04452: loss did not improve from 2.93539\n",
            "Epoch 4453/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9410 - accuracy: 0.1635 - val_loss: 9.6800 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04453: loss did not improve from 2.93539\n",
            "Epoch 4454/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9184 - accuracy: 0.1637 - val_loss: 9.7076 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04454: loss did not improve from 2.93539\n",
            "Epoch 4455/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9534 - accuracy: 0.1564 - val_loss: 9.7236 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04455: loss did not improve from 2.93539\n",
            "Epoch 4456/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9428 - accuracy: 0.1771 - val_loss: 9.7271 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04456: loss did not improve from 2.93539\n",
            "Epoch 4457/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9387 - accuracy: 0.1624 - val_loss: 9.7093 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04457: loss did not improve from 2.93539\n",
            "Epoch 4458/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9281 - accuracy: 0.1530 - val_loss: 9.6998 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04458: loss did not improve from 2.93539\n",
            "Epoch 4459/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9554 - accuracy: 0.1599 - val_loss: 9.6927 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04459: loss did not improve from 2.93539\n",
            "Epoch 4460/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9305 - accuracy: 0.1886 - val_loss: 9.7290 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04460: loss did not improve from 2.93539\n",
            "Epoch 4461/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9600 - accuracy: 0.1596 - val_loss: 9.7060 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04461: loss did not improve from 2.93539\n",
            "Epoch 4462/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9759 - accuracy: 0.1652 - val_loss: 9.7212 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04462: loss did not improve from 2.93539\n",
            "Epoch 4463/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9599 - accuracy: 0.1647 - val_loss: 9.7156 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04463: loss did not improve from 2.93539\n",
            "Epoch 4464/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9598 - accuracy: 0.1608 - val_loss: 9.7327 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04464: loss did not improve from 2.93539\n",
            "Epoch 4465/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9399 - accuracy: 0.1533 - val_loss: 9.6856 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04465: loss did not improve from 2.93539\n",
            "Epoch 4466/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9471 - accuracy: 0.1668 - val_loss: 9.7092 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04466: loss did not improve from 2.93539\n",
            "Epoch 4467/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9783 - accuracy: 0.1613 - val_loss: 9.6969 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04467: loss did not improve from 2.93539\n",
            "Epoch 4468/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9461 - accuracy: 0.1606 - val_loss: 9.7207 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04468: loss did not improve from 2.93539\n",
            "Epoch 4469/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9393 - accuracy: 0.1594 - val_loss: 9.7590 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04469: loss did not improve from 2.93539\n",
            "Epoch 4470/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9647 - accuracy: 0.1579 - val_loss: 9.6687 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04470: loss did not improve from 2.93539\n",
            "Epoch 4471/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9388 - accuracy: 0.1629 - val_loss: 9.7526 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04471: loss did not improve from 2.93539\n",
            "Epoch 4472/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9523 - accuracy: 0.1688 - val_loss: 9.6729 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04472: loss did not improve from 2.93539\n",
            "Epoch 4473/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9927 - accuracy: 0.1432 - val_loss: 9.7618 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04473: loss did not improve from 2.93539\n",
            "Epoch 4474/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9477 - accuracy: 0.1667 - val_loss: 9.7211 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04474: loss did not improve from 2.93539\n",
            "Epoch 4475/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9434 - accuracy: 0.1741 - val_loss: 9.7203 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04475: loss did not improve from 2.93539\n",
            "Epoch 4476/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9216 - accuracy: 0.1652 - val_loss: 9.7025 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04476: loss did not improve from 2.93539\n",
            "Epoch 4477/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9343 - accuracy: 0.1664 - val_loss: 9.6848 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04477: loss did not improve from 2.93539\n",
            "Epoch 4478/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9244 - accuracy: 0.1773 - val_loss: 9.7244 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04478: loss did not improve from 2.93539\n",
            "Epoch 4479/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9407 - accuracy: 0.1450 - val_loss: 9.7574 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04479: loss did not improve from 2.93539\n",
            "Epoch 4480/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9301 - accuracy: 0.1766 - val_loss: 9.7323 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04480: loss did not improve from 2.93539\n",
            "Epoch 4481/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9583 - accuracy: 0.1510 - val_loss: 9.7684 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04481: loss did not improve from 2.93539\n",
            "Epoch 4482/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9330 - accuracy: 0.1516 - val_loss: 9.6852 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04482: loss did not improve from 2.93539\n",
            "Epoch 4483/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9628 - accuracy: 0.1609 - val_loss: 9.7200 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04483: loss did not improve from 2.93539\n",
            "Epoch 4484/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9856 - accuracy: 0.1559 - val_loss: 9.7355 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04484: loss did not improve from 2.93539\n",
            "Epoch 4485/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9514 - accuracy: 0.1488 - val_loss: 9.7269 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04485: loss did not improve from 2.93539\n",
            "Epoch 4486/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9577 - accuracy: 0.1599 - val_loss: 9.7232 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04486: loss did not improve from 2.93539\n",
            "Epoch 4487/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9734 - accuracy: 0.1466 - val_loss: 9.7189 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04487: loss did not improve from 2.93539\n",
            "Epoch 4488/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9785 - accuracy: 0.1602 - val_loss: 9.7124 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04488: loss did not improve from 2.93539\n",
            "Epoch 4489/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9237 - accuracy: 0.1793 - val_loss: 9.7287 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04489: loss did not improve from 2.93539\n",
            "Epoch 4490/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9325 - accuracy: 0.1683 - val_loss: 9.6794 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04490: loss did not improve from 2.93539\n",
            "Epoch 4491/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9217 - accuracy: 0.1707 - val_loss: 9.7112 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04491: loss did not improve from 2.93539\n",
            "Epoch 4492/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9434 - accuracy: 0.1709 - val_loss: 9.7183 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04492: loss did not improve from 2.93539\n",
            "Epoch 4493/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9669 - accuracy: 0.1614 - val_loss: 9.7243 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04493: loss did not improve from 2.93539\n",
            "Epoch 4494/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9140 - accuracy: 0.1668 - val_loss: 9.7860 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04494: loss did not improve from 2.93539\n",
            "Epoch 4495/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9142 - accuracy: 0.1675 - val_loss: 9.7583 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04495: loss did not improve from 2.93539\n",
            "Epoch 4496/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9575 - accuracy: 0.1560 - val_loss: 9.7552 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04496: loss did not improve from 2.93539\n",
            "Epoch 4497/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9360 - accuracy: 0.1617 - val_loss: 9.7668 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04497: loss did not improve from 2.93539\n",
            "Epoch 4498/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9521 - accuracy: 0.1561 - val_loss: 9.7964 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04498: loss did not improve from 2.93539\n",
            "Epoch 4499/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9488 - accuracy: 0.1493 - val_loss: 9.7106 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04499: loss did not improve from 2.93539\n",
            "Epoch 4500/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9876 - accuracy: 0.1552 - val_loss: 9.7812 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04500: loss did not improve from 2.93539\n",
            "Epoch 4501/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9368 - accuracy: 0.1559 - val_loss: 9.7436 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04501: loss did not improve from 2.93539\n",
            "Epoch 4502/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9528 - accuracy: 0.1640 - val_loss: 9.7549 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04502: loss did not improve from 2.93539\n",
            "Epoch 4503/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9360 - accuracy: 0.1457 - val_loss: 9.6862 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04503: loss did not improve from 2.93539\n",
            "Epoch 4504/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9118 - accuracy: 0.1756 - val_loss: 9.7227 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04504: loss did not improve from 2.93539\n",
            "Epoch 4505/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9383 - accuracy: 0.1593 - val_loss: 9.7531 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04505: loss did not improve from 2.93539\n",
            "Epoch 4506/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 3.0010 - accuracy: 0.1597 - val_loss: 9.7423 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04506: loss did not improve from 2.93539\n",
            "Epoch 4507/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9586 - accuracy: 0.1681 - val_loss: 9.7233 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04507: loss did not improve from 2.93539\n",
            "Epoch 4508/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9544 - accuracy: 0.1566 - val_loss: 9.7317 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04508: loss did not improve from 2.93539\n",
            "Epoch 4509/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9087 - accuracy: 0.1894 - val_loss: 9.7349 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04509: loss did not improve from 2.93539\n",
            "Epoch 4510/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9662 - accuracy: 0.1512 - val_loss: 9.7462 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04510: loss did not improve from 2.93539\n",
            "Epoch 4511/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9532 - accuracy: 0.1612 - val_loss: 9.7376 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04511: loss did not improve from 2.93539\n",
            "Epoch 4512/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9247 - accuracy: 0.1622 - val_loss: 9.7251 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04512: loss did not improve from 2.93539\n",
            "Epoch 4513/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9582 - accuracy: 0.1494 - val_loss: 9.7726 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04513: loss did not improve from 2.93539\n",
            "Epoch 4514/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9109 - accuracy: 0.1729 - val_loss: 9.7597 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04514: loss did not improve from 2.93539\n",
            "Epoch 4515/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9358 - accuracy: 0.1529 - val_loss: 9.7360 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04515: loss did not improve from 2.93539\n",
            "Epoch 4516/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9655 - accuracy: 0.1765 - val_loss: 9.7721 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04516: loss did not improve from 2.93539\n",
            "Epoch 4517/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9254 - accuracy: 0.1668 - val_loss: 9.7320 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04517: loss did not improve from 2.93539\n",
            "Epoch 4518/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9581 - accuracy: 0.1735 - val_loss: 9.7474 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04518: loss did not improve from 2.93539\n",
            "Epoch 4519/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9683 - accuracy: 0.1474 - val_loss: 9.7333 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04519: loss did not improve from 2.93539\n",
            "Epoch 4520/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9617 - accuracy: 0.1578 - val_loss: 9.7507 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04520: loss did not improve from 2.93539\n",
            "Epoch 4521/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9314 - accuracy: 0.1656 - val_loss: 9.7215 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04521: loss did not improve from 2.93539\n",
            "Epoch 4522/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9437 - accuracy: 0.1634 - val_loss: 9.7863 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04522: loss did not improve from 2.93539\n",
            "Epoch 4523/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9400 - accuracy: 0.1823 - val_loss: 9.7542 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04523: loss did not improve from 2.93539\n",
            "Epoch 4524/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9410 - accuracy: 0.1608 - val_loss: 9.7254 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04524: loss did not improve from 2.93539\n",
            "Epoch 4525/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9756 - accuracy: 0.1580 - val_loss: 9.7337 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04525: loss did not improve from 2.93539\n",
            "Epoch 4526/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9043 - accuracy: 0.1726 - val_loss: 9.7827 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04526: loss did not improve from 2.93539\n",
            "Epoch 4527/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9516 - accuracy: 0.1555 - val_loss: 9.7362 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04527: loss did not improve from 2.93539\n",
            "Epoch 4528/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9308 - accuracy: 0.1635 - val_loss: 9.7534 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04528: loss did not improve from 2.93539\n",
            "Epoch 4529/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9501 - accuracy: 0.1603 - val_loss: 9.7845 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04529: loss did not improve from 2.93539\n",
            "Epoch 4530/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9120 - accuracy: 0.1784 - val_loss: 9.8060 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04530: loss did not improve from 2.93539\n",
            "Epoch 4531/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9306 - accuracy: 0.1728 - val_loss: 9.7632 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04531: loss improved from 2.93539 to 2.93181, saving model to poids_train.hdf5\n",
            "Epoch 4532/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9371 - accuracy: 0.1678 - val_loss: 9.7265 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04532: loss did not improve from 2.93181\n",
            "Epoch 4533/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9347 - accuracy: 0.1663 - val_loss: 9.7510 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04533: loss did not improve from 2.93181\n",
            "Epoch 4534/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9701 - accuracy: 0.1450 - val_loss: 9.7208 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04534: loss did not improve from 2.93181\n",
            "Epoch 4535/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9326 - accuracy: 0.1580 - val_loss: 9.7903 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04535: loss did not improve from 2.93181\n",
            "Epoch 4536/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9745 - accuracy: 0.1447 - val_loss: 9.7620 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04536: loss did not improve from 2.93181\n",
            "Epoch 4537/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9154 - accuracy: 0.1707 - val_loss: 9.7222 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04537: loss did not improve from 2.93181\n",
            "Epoch 4538/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9156 - accuracy: 0.1612 - val_loss: 9.7635 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04538: loss did not improve from 2.93181\n",
            "Epoch 4539/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9520 - accuracy: 0.1678 - val_loss: 9.7580 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04539: loss did not improve from 2.93181\n",
            "Epoch 4540/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9573 - accuracy: 0.1499 - val_loss: 9.7263 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04540: loss did not improve from 2.93181\n",
            "Epoch 4541/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9614 - accuracy: 0.1592 - val_loss: 9.8106 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04541: loss did not improve from 2.93181\n",
            "Epoch 4542/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9411 - accuracy: 0.1648 - val_loss: 9.7579 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04542: loss did not improve from 2.93181\n",
            "Epoch 4543/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9020 - accuracy: 0.1836 - val_loss: 9.7814 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04543: loss improved from 2.93181 to 2.93037, saving model to poids_train.hdf5\n",
            "Epoch 4544/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9449 - accuracy: 0.1620 - val_loss: 9.7395 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04544: loss did not improve from 2.93037\n",
            "Epoch 4545/5000\n",
            "31/31 [==============================] - 1s 40ms/step - loss: 2.9470 - accuracy: 0.1696 - val_loss: 9.7440 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04545: loss did not improve from 2.93037\n",
            "Epoch 4546/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9057 - accuracy: 0.1801 - val_loss: 9.7257 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04546: loss did not improve from 2.93037\n",
            "Epoch 4547/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9388 - accuracy: 0.1631 - val_loss: 9.7392 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04547: loss did not improve from 2.93037\n",
            "Epoch 4548/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9583 - accuracy: 0.1512 - val_loss: 9.7089 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04548: loss did not improve from 2.93037\n",
            "Epoch 4549/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9696 - accuracy: 0.1562 - val_loss: 9.7352 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04549: loss did not improve from 2.93037\n",
            "Epoch 4550/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9506 - accuracy: 0.1576 - val_loss: 9.7738 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04550: loss did not improve from 2.93037\n",
            "Epoch 4551/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9177 - accuracy: 0.1706 - val_loss: 9.7455 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04551: loss did not improve from 2.93037\n",
            "Epoch 4552/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9634 - accuracy: 0.1609 - val_loss: 9.7773 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04552: loss did not improve from 2.93037\n",
            "Epoch 4553/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9673 - accuracy: 0.1549 - val_loss: 9.6887 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04553: loss did not improve from 2.93037\n",
            "Epoch 4554/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9287 - accuracy: 0.1607 - val_loss: 9.7770 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04554: loss did not improve from 2.93037\n",
            "Epoch 4555/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9658 - accuracy: 0.1659 - val_loss: 9.7303 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04555: loss did not improve from 2.93037\n",
            "Epoch 4556/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9391 - accuracy: 0.1604 - val_loss: 9.7874 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04556: loss did not improve from 2.93037\n",
            "Epoch 4557/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9569 - accuracy: 0.1640 - val_loss: 9.7721 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04557: loss did not improve from 2.93037\n",
            "Epoch 4558/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9437 - accuracy: 0.1675 - val_loss: 9.7862 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04558: loss did not improve from 2.93037\n",
            "Epoch 4559/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9397 - accuracy: 0.1664 - val_loss: 9.7556 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04559: loss did not improve from 2.93037\n",
            "Epoch 4560/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9507 - accuracy: 0.1636 - val_loss: 9.7239 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04560: loss did not improve from 2.93037\n",
            "Epoch 4561/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9266 - accuracy: 0.1573 - val_loss: 9.7811 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04561: loss did not improve from 2.93037\n",
            "Epoch 4562/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9379 - accuracy: 0.1725 - val_loss: 9.7623 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04562: loss did not improve from 2.93037\n",
            "Epoch 4563/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9687 - accuracy: 0.1644 - val_loss: 9.8008 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04563: loss did not improve from 2.93037\n",
            "Epoch 4564/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9615 - accuracy: 0.1431 - val_loss: 9.7658 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04564: loss did not improve from 2.93037\n",
            "Epoch 4565/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9686 - accuracy: 0.1596 - val_loss: 9.7921 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04565: loss did not improve from 2.93037\n",
            "Epoch 4566/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9469 - accuracy: 0.1552 - val_loss: 9.7647 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04566: loss did not improve from 2.93037\n",
            "Epoch 4567/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9339 - accuracy: 0.1639 - val_loss: 9.7172 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04567: loss did not improve from 2.93037\n",
            "Epoch 4568/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9446 - accuracy: 0.1519 - val_loss: 9.7423 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04568: loss did not improve from 2.93037\n",
            "Epoch 4569/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9474 - accuracy: 0.1616 - val_loss: 9.7308 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04569: loss did not improve from 2.93037\n",
            "Epoch 4570/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9418 - accuracy: 0.1744 - val_loss: 9.7818 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04570: loss did not improve from 2.93037\n",
            "Epoch 4571/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9445 - accuracy: 0.1586 - val_loss: 9.7443 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04571: loss did not improve from 2.93037\n",
            "Epoch 4572/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9050 - accuracy: 0.1630 - val_loss: 9.8322 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04572: loss did not improve from 2.93037\n",
            "Epoch 4573/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9239 - accuracy: 0.1805 - val_loss: 9.7549 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04573: loss did not improve from 2.93037\n",
            "Epoch 4574/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9240 - accuracy: 0.1914 - val_loss: 9.7708 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04574: loss did not improve from 2.93037\n",
            "Epoch 4575/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9739 - accuracy: 0.1514 - val_loss: 9.8191 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04575: loss did not improve from 2.93037\n",
            "Epoch 4576/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9477 - accuracy: 0.1827 - val_loss: 9.7477 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04576: loss did not improve from 2.93037\n",
            "Epoch 4577/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9570 - accuracy: 0.1662 - val_loss: 9.7791 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04577: loss did not improve from 2.93037\n",
            "Epoch 4578/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9047 - accuracy: 0.1703 - val_loss: 9.7562 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04578: loss did not improve from 2.93037\n",
            "Epoch 4579/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9771 - accuracy: 0.1567 - val_loss: 9.7604 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04579: loss did not improve from 2.93037\n",
            "Epoch 4580/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9357 - accuracy: 0.1715 - val_loss: 9.7713 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04580: loss did not improve from 2.93037\n",
            "Epoch 4581/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9280 - accuracy: 0.1592 - val_loss: 9.7868 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04581: loss did not improve from 2.93037\n",
            "Epoch 4582/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9117 - accuracy: 0.1620 - val_loss: 9.7778 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04582: loss did not improve from 2.93037\n",
            "Epoch 4583/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9534 - accuracy: 0.1597 - val_loss: 9.7446 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04583: loss did not improve from 2.93037\n",
            "Epoch 4584/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9661 - accuracy: 0.1614 - val_loss: 9.7395 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04584: loss did not improve from 2.93037\n",
            "Epoch 4585/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9289 - accuracy: 0.1533 - val_loss: 9.7551 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04585: loss did not improve from 2.93037\n",
            "Epoch 4586/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9510 - accuracy: 0.1566 - val_loss: 9.7494 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04586: loss did not improve from 2.93037\n",
            "Epoch 4587/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9389 - accuracy: 0.1722 - val_loss: 9.7672 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04587: loss did not improve from 2.93037\n",
            "Epoch 4588/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9613 - accuracy: 0.1734 - val_loss: 9.7778 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04588: loss did not improve from 2.93037\n",
            "Epoch 4589/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9487 - accuracy: 0.1680 - val_loss: 9.7809 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04589: loss did not improve from 2.93037\n",
            "Epoch 4590/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9433 - accuracy: 0.1677 - val_loss: 9.7111 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04590: loss did not improve from 2.93037\n",
            "Epoch 4591/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9563 - accuracy: 0.1544 - val_loss: 9.7511 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04591: loss did not improve from 2.93037\n",
            "Epoch 4592/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9492 - accuracy: 0.1551 - val_loss: 9.7855 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04592: loss did not improve from 2.93037\n",
            "Epoch 4593/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9478 - accuracy: 0.1585 - val_loss: 9.8008 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04593: loss did not improve from 2.93037\n",
            "Epoch 4594/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9460 - accuracy: 0.1508 - val_loss: 9.7674 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04594: loss did not improve from 2.93037\n",
            "Epoch 4595/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9590 - accuracy: 0.1594 - val_loss: 9.8218 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04595: loss did not improve from 2.93037\n",
            "Epoch 4596/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9247 - accuracy: 0.1707 - val_loss: 9.7579 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04596: loss did not improve from 2.93037\n",
            "Epoch 4597/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.8926 - accuracy: 0.1718 - val_loss: 9.7544 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04597: loss did not improve from 2.93037\n",
            "Epoch 4598/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9639 - accuracy: 0.1727 - val_loss: 9.7386 - val_accuracy: 0.0127\n",
            "\n",
            "Epoch 04598: loss did not improve from 2.93037\n",
            "Epoch 4599/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9279 - accuracy: 0.1585 - val_loss: 9.7299 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04599: loss did not improve from 2.93037\n",
            "Epoch 4600/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9535 - accuracy: 0.1722 - val_loss: 9.7623 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04600: loss did not improve from 2.93037\n",
            "Epoch 4601/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9384 - accuracy: 0.1665 - val_loss: 9.7882 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04601: loss did not improve from 2.93037\n",
            "Epoch 4602/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9724 - accuracy: 0.1549 - val_loss: 9.7755 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04602: loss did not improve from 2.93037\n",
            "Epoch 4603/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9127 - accuracy: 0.1626 - val_loss: 9.7828 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04603: loss did not improve from 2.93037\n",
            "Epoch 4604/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9261 - accuracy: 0.1554 - val_loss: 9.7818 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04604: loss did not improve from 2.93037\n",
            "Epoch 4605/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9027 - accuracy: 0.1740 - val_loss: 9.7607 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04605: loss did not improve from 2.93037\n",
            "Epoch 4606/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9336 - accuracy: 0.1734 - val_loss: 9.7504 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04606: loss did not improve from 2.93037\n",
            "Epoch 4607/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9392 - accuracy: 0.1552 - val_loss: 9.7505 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04607: loss did not improve from 2.93037\n",
            "Epoch 4608/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9621 - accuracy: 0.1560 - val_loss: 9.7806 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04608: loss did not improve from 2.93037\n",
            "Epoch 4609/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9334 - accuracy: 0.1647 - val_loss: 9.7698 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04609: loss did not improve from 2.93037\n",
            "Epoch 4610/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9151 - accuracy: 0.1650 - val_loss: 9.8160 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04610: loss improved from 2.93037 to 2.92799, saving model to poids_train.hdf5\n",
            "Epoch 4611/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9661 - accuracy: 0.1643 - val_loss: 9.7591 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04611: loss did not improve from 2.92799\n",
            "Epoch 4612/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9200 - accuracy: 0.1723 - val_loss: 9.7434 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04612: loss did not improve from 2.92799\n",
            "Epoch 4613/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9380 - accuracy: 0.1613 - val_loss: 9.7350 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04613: loss did not improve from 2.92799\n",
            "Epoch 4614/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9445 - accuracy: 0.1792 - val_loss: 9.7633 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04614: loss did not improve from 2.92799\n",
            "Epoch 4615/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9310 - accuracy: 0.1647 - val_loss: 9.7582 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04615: loss did not improve from 2.92799\n",
            "Epoch 4616/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9705 - accuracy: 0.1660 - val_loss: 9.7575 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04616: loss did not improve from 2.92799\n",
            "Epoch 4617/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9361 - accuracy: 0.1632 - val_loss: 9.7427 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04617: loss did not improve from 2.92799\n",
            "Epoch 4618/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9508 - accuracy: 0.1639 - val_loss: 9.7309 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04618: loss did not improve from 2.92799\n",
            "Epoch 4619/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9608 - accuracy: 0.1640 - val_loss: 9.7978 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04619: loss did not improve from 2.92799\n",
            "Epoch 4620/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9531 - accuracy: 0.1725 - val_loss: 9.7374 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04620: loss did not improve from 2.92799\n",
            "Epoch 4621/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9614 - accuracy: 0.1828 - val_loss: 9.7452 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04621: loss did not improve from 2.92799\n",
            "Epoch 4622/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9816 - accuracy: 0.1476 - val_loss: 9.7561 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04622: loss did not improve from 2.92799\n",
            "Epoch 4623/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9380 - accuracy: 0.1846 - val_loss: 9.7597 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04623: loss did not improve from 2.92799\n",
            "Epoch 4624/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9402 - accuracy: 0.1584 - val_loss: 9.7636 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04624: loss did not improve from 2.92799\n",
            "Epoch 4625/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9426 - accuracy: 0.1462 - val_loss: 9.7770 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04625: loss did not improve from 2.92799\n",
            "Epoch 4626/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9413 - accuracy: 0.1534 - val_loss: 9.7684 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04626: loss did not improve from 2.92799\n",
            "Epoch 4627/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9201 - accuracy: 0.1668 - val_loss: 9.7752 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04627: loss did not improve from 2.92799\n",
            "Epoch 4628/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9651 - accuracy: 0.1471 - val_loss: 9.7652 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04628: loss did not improve from 2.92799\n",
            "Epoch 4629/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9352 - accuracy: 0.1795 - val_loss: 9.7431 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04629: loss did not improve from 2.92799\n",
            "Epoch 4630/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9432 - accuracy: 0.1702 - val_loss: 9.7267 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04630: loss did not improve from 2.92799\n",
            "Epoch 4631/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9291 - accuracy: 0.1847 - val_loss: 9.7858 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04631: loss did not improve from 2.92799\n",
            "Epoch 4632/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9510 - accuracy: 0.1758 - val_loss: 9.8279 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04632: loss did not improve from 2.92799\n",
            "Epoch 4633/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9245 - accuracy: 0.1716 - val_loss: 9.7958 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04633: loss did not improve from 2.92799\n",
            "Epoch 4634/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9409 - accuracy: 0.1534 - val_loss: 9.7806 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04634: loss did not improve from 2.92799\n",
            "Epoch 4635/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9697 - accuracy: 0.1602 - val_loss: 9.7637 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04635: loss did not improve from 2.92799\n",
            "Epoch 4636/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9486 - accuracy: 0.1604 - val_loss: 9.7935 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04636: loss did not improve from 2.92799\n",
            "Epoch 4637/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9422 - accuracy: 0.1644 - val_loss: 9.7423 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04637: loss did not improve from 2.92799\n",
            "Epoch 4638/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9184 - accuracy: 0.1654 - val_loss: 9.8446 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04638: loss did not improve from 2.92799\n",
            "Epoch 4639/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9168 - accuracy: 0.1734 - val_loss: 9.8086 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04639: loss did not improve from 2.92799\n",
            "Epoch 4640/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9175 - accuracy: 0.1623 - val_loss: 9.7484 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04640: loss did not improve from 2.92799\n",
            "Epoch 4641/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9221 - accuracy: 0.1666 - val_loss: 9.7927 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04641: loss did not improve from 2.92799\n",
            "Epoch 4642/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9543 - accuracy: 0.1729 - val_loss: 9.7706 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04642: loss did not improve from 2.92799\n",
            "Epoch 4643/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9736 - accuracy: 0.1576 - val_loss: 9.7877 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04643: loss did not improve from 2.92799\n",
            "Epoch 4644/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9335 - accuracy: 0.1572 - val_loss: 9.7807 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04644: loss did not improve from 2.92799\n",
            "Epoch 4645/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9575 - accuracy: 0.1556 - val_loss: 9.8246 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04645: loss did not improve from 2.92799\n",
            "Epoch 4646/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9497 - accuracy: 0.1614 - val_loss: 9.7977 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04646: loss did not improve from 2.92799\n",
            "Epoch 4647/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9357 - accuracy: 0.1528 - val_loss: 9.7924 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04647: loss did not improve from 2.92799\n",
            "Epoch 4648/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9544 - accuracy: 0.1446 - val_loss: 9.7686 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04648: loss did not improve from 2.92799\n",
            "Epoch 4649/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9477 - accuracy: 0.1588 - val_loss: 9.7869 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04649: loss did not improve from 2.92799\n",
            "Epoch 4650/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9121 - accuracy: 0.1711 - val_loss: 9.7975 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04650: loss did not improve from 2.92799\n",
            "Epoch 4651/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9597 - accuracy: 0.1604 - val_loss: 9.8003 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04651: loss did not improve from 2.92799\n",
            "Epoch 4652/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9439 - accuracy: 0.1638 - val_loss: 9.7392 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04652: loss did not improve from 2.92799\n",
            "Epoch 4653/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9896 - accuracy: 0.1507 - val_loss: 9.8014 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04653: loss did not improve from 2.92799\n",
            "Epoch 4654/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9416 - accuracy: 0.1565 - val_loss: 9.7346 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04654: loss did not improve from 2.92799\n",
            "Epoch 4655/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9568 - accuracy: 0.1528 - val_loss: 9.7458 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04655: loss did not improve from 2.92799\n",
            "Epoch 4656/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9436 - accuracy: 0.1513 - val_loss: 9.8033 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04656: loss did not improve from 2.92799\n",
            "Epoch 4657/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9365 - accuracy: 0.1626 - val_loss: 9.7720 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04657: loss did not improve from 2.92799\n",
            "Epoch 4658/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9196 - accuracy: 0.1666 - val_loss: 9.7705 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04658: loss did not improve from 2.92799\n",
            "Epoch 4659/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9287 - accuracy: 0.1734 - val_loss: 9.8047 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04659: loss did not improve from 2.92799\n",
            "Epoch 4660/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9479 - accuracy: 0.1801 - val_loss: 9.8111 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04660: loss did not improve from 2.92799\n",
            "Epoch 4661/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9408 - accuracy: 0.1777 - val_loss: 9.7839 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04661: loss did not improve from 2.92799\n",
            "Epoch 4662/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9210 - accuracy: 0.1647 - val_loss: 9.7915 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04662: loss did not improve from 2.92799\n",
            "Epoch 4663/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9358 - accuracy: 0.1736 - val_loss: 9.7799 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04663: loss did not improve from 2.92799\n",
            "Epoch 4664/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9614 - accuracy: 0.1431 - val_loss: 9.7673 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04664: loss did not improve from 2.92799\n",
            "Epoch 4665/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9391 - accuracy: 0.1685 - val_loss: 9.8287 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04665: loss did not improve from 2.92799\n",
            "Epoch 4666/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9422 - accuracy: 0.1727 - val_loss: 9.7566 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04666: loss did not improve from 2.92799\n",
            "Epoch 4667/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9057 - accuracy: 0.1734 - val_loss: 9.8645 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04667: loss did not improve from 2.92799\n",
            "Epoch 4668/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9698 - accuracy: 0.1555 - val_loss: 9.7948 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04668: loss did not improve from 2.92799\n",
            "Epoch 4669/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9208 - accuracy: 0.1644 - val_loss: 9.7842 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04669: loss did not improve from 2.92799\n",
            "Epoch 4670/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9282 - accuracy: 0.1623 - val_loss: 9.8104 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04670: loss did not improve from 2.92799\n",
            "Epoch 4671/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9883 - accuracy: 0.1479 - val_loss: 9.7717 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04671: loss did not improve from 2.92799\n",
            "Epoch 4672/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9149 - accuracy: 0.1660 - val_loss: 9.8306 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04672: loss did not improve from 2.92799\n",
            "Epoch 4673/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9345 - accuracy: 0.1648 - val_loss: 9.7845 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04673: loss did not improve from 2.92799\n",
            "Epoch 4674/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9438 - accuracy: 0.1781 - val_loss: 9.7847 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04674: loss did not improve from 2.92799\n",
            "Epoch 4675/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9304 - accuracy: 0.1721 - val_loss: 9.7985 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04675: loss did not improve from 2.92799\n",
            "Epoch 4676/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9602 - accuracy: 0.1611 - val_loss: 9.7899 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04676: loss did not improve from 2.92799\n",
            "Epoch 4677/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9134 - accuracy: 0.1730 - val_loss: 9.8429 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04677: loss did not improve from 2.92799\n",
            "Epoch 4678/5000\n",
            "31/31 [==============================] - 1s 40ms/step - loss: 2.9439 - accuracy: 0.1391 - val_loss: 9.7726 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04678: loss did not improve from 2.92799\n",
            "Epoch 4679/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9711 - accuracy: 0.1452 - val_loss: 9.8178 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04679: loss did not improve from 2.92799\n",
            "Epoch 4680/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9418 - accuracy: 0.1664 - val_loss: 9.8840 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04680: loss did not improve from 2.92799\n",
            "Epoch 4681/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9510 - accuracy: 0.1621 - val_loss: 9.7458 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04681: loss did not improve from 2.92799\n",
            "Epoch 4682/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9438 - accuracy: 0.1474 - val_loss: 9.8123 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04682: loss did not improve from 2.92799\n",
            "Epoch 4683/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9556 - accuracy: 0.1512 - val_loss: 9.7871 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04683: loss did not improve from 2.92799\n",
            "Epoch 4684/5000\n",
            "31/31 [==============================] - 1s 40ms/step - loss: 2.9515 - accuracy: 0.1450 - val_loss: 9.8672 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04684: loss did not improve from 2.92799\n",
            "Epoch 4685/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9589 - accuracy: 0.1633 - val_loss: 9.7975 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04685: loss did not improve from 2.92799\n",
            "Epoch 4686/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9011 - accuracy: 0.1847 - val_loss: 9.7881 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04686: loss did not improve from 2.92799\n",
            "Epoch 4687/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9486 - accuracy: 0.1700 - val_loss: 9.8162 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04687: loss did not improve from 2.92799\n",
            "Epoch 4688/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9257 - accuracy: 0.1678 - val_loss: 9.8108 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04688: loss did not improve from 2.92799\n",
            "Epoch 4689/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9371 - accuracy: 0.1530 - val_loss: 9.8338 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04689: loss did not improve from 2.92799\n",
            "Epoch 4690/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.8881 - accuracy: 0.1690 - val_loss: 9.8243 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04690: loss did not improve from 2.92799\n",
            "Epoch 4691/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9127 - accuracy: 0.1638 - val_loss: 9.7617 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04691: loss did not improve from 2.92799\n",
            "Epoch 4692/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9561 - accuracy: 0.1705 - val_loss: 9.7827 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04692: loss did not improve from 2.92799\n",
            "Epoch 4693/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9255 - accuracy: 0.1817 - val_loss: 9.7451 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04693: loss did not improve from 2.92799\n",
            "Epoch 4694/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9139 - accuracy: 0.1720 - val_loss: 9.8288 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04694: loss did not improve from 2.92799\n",
            "Epoch 4695/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9299 - accuracy: 0.1735 - val_loss: 9.7906 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04695: loss did not improve from 2.92799\n",
            "Epoch 4696/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9484 - accuracy: 0.1595 - val_loss: 9.8361 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04696: loss did not improve from 2.92799\n",
            "Epoch 4697/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9382 - accuracy: 0.1661 - val_loss: 9.7656 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04697: loss did not improve from 2.92799\n",
            "Epoch 4698/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9413 - accuracy: 0.1706 - val_loss: 9.8316 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04698: loss did not improve from 2.92799\n",
            "Epoch 4699/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9111 - accuracy: 0.1777 - val_loss: 9.7633 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04699: loss did not improve from 2.92799\n",
            "Epoch 4700/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9588 - accuracy: 0.1663 - val_loss: 9.8264 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04700: loss did not improve from 2.92799\n",
            "Epoch 4701/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9342 - accuracy: 0.1605 - val_loss: 9.7952 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04701: loss did not improve from 2.92799\n",
            "Epoch 4702/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9291 - accuracy: 0.1592 - val_loss: 9.8191 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04702: loss did not improve from 2.92799\n",
            "Epoch 4703/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9479 - accuracy: 0.1576 - val_loss: 9.8322 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04703: loss did not improve from 2.92799\n",
            "Epoch 4704/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9470 - accuracy: 0.1488 - val_loss: 9.7628 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04704: loss did not improve from 2.92799\n",
            "Epoch 4705/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9571 - accuracy: 0.1563 - val_loss: 9.7815 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04705: loss did not improve from 2.92799\n",
            "Epoch 4706/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9632 - accuracy: 0.1503 - val_loss: 9.8335 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04706: loss did not improve from 2.92799\n",
            "Epoch 4707/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9411 - accuracy: 0.1572 - val_loss: 9.8031 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04707: loss did not improve from 2.92799\n",
            "Epoch 4708/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9629 - accuracy: 0.1704 - val_loss: 9.7873 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04708: loss did not improve from 2.92799\n",
            "Epoch 4709/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9027 - accuracy: 0.1823 - val_loss: 9.7843 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04709: loss did not improve from 2.92799\n",
            "Epoch 4710/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9450 - accuracy: 0.1698 - val_loss: 9.8636 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04710: loss did not improve from 2.92799\n",
            "Epoch 4711/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9371 - accuracy: 0.1741 - val_loss: 9.8198 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04711: loss did not improve from 2.92799\n",
            "Epoch 4712/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9607 - accuracy: 0.1570 - val_loss: 9.8070 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04712: loss did not improve from 2.92799\n",
            "Epoch 4713/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9438 - accuracy: 0.1741 - val_loss: 9.7860 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04713: loss did not improve from 2.92799\n",
            "Epoch 4714/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9606 - accuracy: 0.1499 - val_loss: 9.8103 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04714: loss did not improve from 2.92799\n",
            "Epoch 4715/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9442 - accuracy: 0.1672 - val_loss: 9.8087 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04715: loss did not improve from 2.92799\n",
            "Epoch 4716/5000\n",
            "31/31 [==============================] - 1s 40ms/step - loss: 2.9743 - accuracy: 0.1521 - val_loss: 9.7708 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04716: loss did not improve from 2.92799\n",
            "Epoch 4717/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9232 - accuracy: 0.1713 - val_loss: 9.8282 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04717: loss did not improve from 2.92799\n",
            "Epoch 4718/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9261 - accuracy: 0.1698 - val_loss: 9.7794 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04718: loss did not improve from 2.92799\n",
            "Epoch 4719/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9584 - accuracy: 0.1492 - val_loss: 9.8220 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04719: loss did not improve from 2.92799\n",
            "Epoch 4720/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9421 - accuracy: 0.1622 - val_loss: 9.8130 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04720: loss did not improve from 2.92799\n",
            "Epoch 4721/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9071 - accuracy: 0.1605 - val_loss: 9.7878 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04721: loss did not improve from 2.92799\n",
            "Epoch 4722/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9275 - accuracy: 0.1750 - val_loss: 9.7878 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04722: loss did not improve from 2.92799\n",
            "Epoch 4723/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9471 - accuracy: 0.1514 - val_loss: 9.8259 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04723: loss did not improve from 2.92799\n",
            "Epoch 4724/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9430 - accuracy: 0.1565 - val_loss: 9.7497 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04724: loss did not improve from 2.92799\n",
            "Epoch 4725/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9390 - accuracy: 0.1599 - val_loss: 9.7913 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04725: loss did not improve from 2.92799\n",
            "Epoch 4726/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9466 - accuracy: 0.1731 - val_loss: 9.8011 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04726: loss did not improve from 2.92799\n",
            "Epoch 4727/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.8972 - accuracy: 0.1754 - val_loss: 9.7947 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04727: loss did not improve from 2.92799\n",
            "Epoch 4728/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9352 - accuracy: 0.1599 - val_loss: 9.7850 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04728: loss did not improve from 2.92799\n",
            "Epoch 4729/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9590 - accuracy: 0.1777 - val_loss: 9.7579 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04729: loss did not improve from 2.92799\n",
            "Epoch 4730/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9391 - accuracy: 0.1516 - val_loss: 9.7747 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04730: loss did not improve from 2.92799\n",
            "Epoch 4731/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9702 - accuracy: 0.1603 - val_loss: 9.7610 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04731: loss did not improve from 2.92799\n",
            "Epoch 4732/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9284 - accuracy: 0.1766 - val_loss: 9.7837 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04732: loss did not improve from 2.92799\n",
            "Epoch 4733/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9301 - accuracy: 0.1474 - val_loss: 9.8108 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04733: loss did not improve from 2.92799\n",
            "Epoch 4734/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9502 - accuracy: 0.1659 - val_loss: 9.8124 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04734: loss did not improve from 2.92799\n",
            "Epoch 4735/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9437 - accuracy: 0.1493 - val_loss: 9.7868 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04735: loss did not improve from 2.92799\n",
            "Epoch 4736/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9219 - accuracy: 0.1641 - val_loss: 9.7967 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04736: loss improved from 2.92799 to 2.92442, saving model to poids_train.hdf5\n",
            "Epoch 4737/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9842 - accuracy: 0.1525 - val_loss: 9.8372 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04737: loss did not improve from 2.92442\n",
            "Epoch 4738/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9683 - accuracy: 0.1527 - val_loss: 9.7997 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04738: loss did not improve from 2.92442\n",
            "Epoch 4739/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9481 - accuracy: 0.1629 - val_loss: 9.8178 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04739: loss did not improve from 2.92442\n",
            "Epoch 4740/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9411 - accuracy: 0.1630 - val_loss: 9.8229 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04740: loss did not improve from 2.92442\n",
            "Epoch 4741/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9517 - accuracy: 0.1797 - val_loss: 9.8182 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04741: loss did not improve from 2.92442\n",
            "Epoch 4742/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9402 - accuracy: 0.1494 - val_loss: 9.8331 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04742: loss did not improve from 2.92442\n",
            "Epoch 4743/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9668 - accuracy: 0.1524 - val_loss: 9.8087 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04743: loss did not improve from 2.92442\n",
            "Epoch 4744/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9515 - accuracy: 0.1618 - val_loss: 9.8376 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04744: loss did not improve from 2.92442\n",
            "Epoch 4745/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9742 - accuracy: 0.1641 - val_loss: 9.8127 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04745: loss did not improve from 2.92442\n",
            "Epoch 4746/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.8978 - accuracy: 0.1674 - val_loss: 9.8504 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04746: loss did not improve from 2.92442\n",
            "Epoch 4747/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9351 - accuracy: 0.1772 - val_loss: 9.8285 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04747: loss did not improve from 2.92442\n",
            "Epoch 4748/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9543 - accuracy: 0.1664 - val_loss: 9.8115 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04748: loss did not improve from 2.92442\n",
            "Epoch 4749/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9524 - accuracy: 0.1483 - val_loss: 9.8291 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04749: loss did not improve from 2.92442\n",
            "Epoch 4750/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9458 - accuracy: 0.1646 - val_loss: 9.7843 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04750: loss did not improve from 2.92442\n",
            "Epoch 4751/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9571 - accuracy: 0.1522 - val_loss: 9.8393 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04751: loss did not improve from 2.92442\n",
            "Epoch 4752/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.8824 - accuracy: 0.1719 - val_loss: 9.8026 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04752: loss did not improve from 2.92442\n",
            "Epoch 4753/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9628 - accuracy: 0.1641 - val_loss: 9.8139 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04753: loss did not improve from 2.92442\n",
            "Epoch 4754/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9247 - accuracy: 0.1860 - val_loss: 9.8715 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04754: loss did not improve from 2.92442\n",
            "Epoch 4755/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9468 - accuracy: 0.1732 - val_loss: 9.8385 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04755: loss did not improve from 2.92442\n",
            "Epoch 4756/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9181 - accuracy: 0.1705 - val_loss: 9.7939 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04756: loss did not improve from 2.92442\n",
            "Epoch 4757/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9589 - accuracy: 0.1521 - val_loss: 9.8431 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04757: loss did not improve from 2.92442\n",
            "Epoch 4758/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9316 - accuracy: 0.1662 - val_loss: 9.8464 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04758: loss did not improve from 2.92442\n",
            "Epoch 4759/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9165 - accuracy: 0.1760 - val_loss: 9.8350 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04759: loss did not improve from 2.92442\n",
            "Epoch 4760/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9427 - accuracy: 0.1602 - val_loss: 9.8554 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04760: loss did not improve from 2.92442\n",
            "Epoch 4761/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9098 - accuracy: 0.1621 - val_loss: 9.8173 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04761: loss did not improve from 2.92442\n",
            "Epoch 4762/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9265 - accuracy: 0.1801 - val_loss: 9.8346 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04762: loss did not improve from 2.92442\n",
            "Epoch 4763/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9658 - accuracy: 0.1532 - val_loss: 9.8168 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04763: loss did not improve from 2.92442\n",
            "Epoch 4764/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9550 - accuracy: 0.1653 - val_loss: 9.8624 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04764: loss did not improve from 2.92442\n",
            "Epoch 4765/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9367 - accuracy: 0.1595 - val_loss: 9.7915 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04765: loss did not improve from 2.92442\n",
            "Epoch 4766/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9445 - accuracy: 0.1522 - val_loss: 9.8591 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04766: loss did not improve from 2.92442\n",
            "Epoch 4767/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9390 - accuracy: 0.1732 - val_loss: 9.7887 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04767: loss did not improve from 2.92442\n",
            "Epoch 4768/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9214 - accuracy: 0.1663 - val_loss: 9.8109 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04768: loss did not improve from 2.92442\n",
            "Epoch 4769/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9537 - accuracy: 0.1658 - val_loss: 9.8571 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04769: loss did not improve from 2.92442\n",
            "Epoch 4770/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9238 - accuracy: 0.1590 - val_loss: 9.8139 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04770: loss did not improve from 2.92442\n",
            "Epoch 4771/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9859 - accuracy: 0.1720 - val_loss: 9.8036 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04771: loss did not improve from 2.92442\n",
            "Epoch 4772/5000\n",
            "31/31 [==============================] - 1s 40ms/step - loss: 2.9588 - accuracy: 0.1648 - val_loss: 9.8388 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04772: loss did not improve from 2.92442\n",
            "Epoch 4773/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9405 - accuracy: 0.1493 - val_loss: 9.7984 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04773: loss did not improve from 2.92442\n",
            "Epoch 4774/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9160 - accuracy: 0.1715 - val_loss: 9.8514 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04774: loss did not improve from 2.92442\n",
            "Epoch 4775/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9301 - accuracy: 0.1838 - val_loss: 9.8306 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04775: loss did not improve from 2.92442\n",
            "Epoch 4776/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9537 - accuracy: 0.1494 - val_loss: 9.7832 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04776: loss did not improve from 2.92442\n",
            "Epoch 4777/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9468 - accuracy: 0.1673 - val_loss: 9.8209 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04777: loss did not improve from 2.92442\n",
            "Epoch 4778/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9269 - accuracy: 0.1764 - val_loss: 9.7673 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04778: loss did not improve from 2.92442\n",
            "Epoch 4779/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9175 - accuracy: 0.1700 - val_loss: 9.8017 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04779: loss did not improve from 2.92442\n",
            "Epoch 4780/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9322 - accuracy: 0.1668 - val_loss: 9.8301 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04780: loss did not improve from 2.92442\n",
            "Epoch 4781/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9690 - accuracy: 0.1600 - val_loss: 9.8609 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04781: loss did not improve from 2.92442\n",
            "Epoch 4782/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9020 - accuracy: 0.1577 - val_loss: 9.8137 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04782: loss did not improve from 2.92442\n",
            "Epoch 4783/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9276 - accuracy: 0.1614 - val_loss: 9.8251 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04783: loss did not improve from 2.92442\n",
            "Epoch 4784/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9200 - accuracy: 0.1654 - val_loss: 9.7949 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04784: loss did not improve from 2.92442\n",
            "Epoch 4785/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9434 - accuracy: 0.1584 - val_loss: 9.7935 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04785: loss did not improve from 2.92442\n",
            "Epoch 4786/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9217 - accuracy: 0.1685 - val_loss: 9.8182 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04786: loss did not improve from 2.92442\n",
            "Epoch 4787/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9291 - accuracy: 0.1748 - val_loss: 9.8188 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04787: loss did not improve from 2.92442\n",
            "Epoch 4788/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9259 - accuracy: 0.1705 - val_loss: 9.8010 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04788: loss did not improve from 2.92442\n",
            "Epoch 4789/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9700 - accuracy: 0.1514 - val_loss: 9.8491 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04789: loss did not improve from 2.92442\n",
            "Epoch 4790/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9428 - accuracy: 0.1642 - val_loss: 9.8407 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04790: loss did not improve from 2.92442\n",
            "Epoch 4791/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9295 - accuracy: 0.1670 - val_loss: 9.8152 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04791: loss did not improve from 2.92442\n",
            "Epoch 4792/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9854 - accuracy: 0.1515 - val_loss: 9.8247 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04792: loss did not improve from 2.92442\n",
            "Epoch 4793/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9136 - accuracy: 0.1680 - val_loss: 9.8089 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04793: loss did not improve from 2.92442\n",
            "Epoch 4794/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9290 - accuracy: 0.1576 - val_loss: 9.8435 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04794: loss did not improve from 2.92442\n",
            "Epoch 4795/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9011 - accuracy: 0.1719 - val_loss: 9.8462 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04795: loss did not improve from 2.92442\n",
            "Epoch 4796/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9606 - accuracy: 0.1503 - val_loss: 9.8183 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04796: loss did not improve from 2.92442\n",
            "Epoch 4797/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9163 - accuracy: 0.1733 - val_loss: 9.7835 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04797: loss did not improve from 2.92442\n",
            "Epoch 4798/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9458 - accuracy: 0.1589 - val_loss: 9.8830 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04798: loss did not improve from 2.92442\n",
            "Epoch 4799/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9533 - accuracy: 0.1679 - val_loss: 9.8246 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04799: loss did not improve from 2.92442\n",
            "Epoch 4800/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9463 - accuracy: 0.1644 - val_loss: 9.7936 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04800: loss did not improve from 2.92442\n",
            "Epoch 4801/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9411 - accuracy: 0.1618 - val_loss: 9.8180 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04801: loss did not improve from 2.92442\n",
            "Epoch 4802/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9351 - accuracy: 0.1785 - val_loss: 9.8189 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04802: loss did not improve from 2.92442\n",
            "Epoch 4803/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9442 - accuracy: 0.1681 - val_loss: 9.8495 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04803: loss did not improve from 2.92442\n",
            "Epoch 4804/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9682 - accuracy: 0.1635 - val_loss: 9.8148 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04804: loss did not improve from 2.92442\n",
            "Epoch 4805/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9102 - accuracy: 0.1662 - val_loss: 9.8294 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04805: loss did not improve from 2.92442\n",
            "Epoch 4806/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.8919 - accuracy: 0.1704 - val_loss: 9.8360 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04806: loss did not improve from 2.92442\n",
            "Epoch 4807/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9758 - accuracy: 0.1508 - val_loss: 9.8341 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04807: loss did not improve from 2.92442\n",
            "Epoch 4808/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9559 - accuracy: 0.1851 - val_loss: 9.8261 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04808: loss did not improve from 2.92442\n",
            "Epoch 4809/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9210 - accuracy: 0.1734 - val_loss: 9.8361 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04809: loss did not improve from 2.92442\n",
            "Epoch 4810/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9251 - accuracy: 0.1575 - val_loss: 9.8254 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04810: loss did not improve from 2.92442\n",
            "Epoch 4811/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9227 - accuracy: 0.1746 - val_loss: 9.8673 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04811: loss did not improve from 2.92442\n",
            "Epoch 4812/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9159 - accuracy: 0.1679 - val_loss: 9.8171 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04812: loss did not improve from 2.92442\n",
            "Epoch 4813/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9260 - accuracy: 0.1647 - val_loss: 9.8218 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04813: loss did not improve from 2.92442\n",
            "Epoch 4814/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9461 - accuracy: 0.1551 - val_loss: 9.8246 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04814: loss did not improve from 2.92442\n",
            "Epoch 4815/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9672 - accuracy: 0.1519 - val_loss: 9.8233 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04815: loss did not improve from 2.92442\n",
            "Epoch 4816/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9102 - accuracy: 0.1706 - val_loss: 9.8378 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04816: loss did not improve from 2.92442\n",
            "Epoch 4817/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9171 - accuracy: 0.1645 - val_loss: 9.8534 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04817: loss did not improve from 2.92442\n",
            "Epoch 4818/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9513 - accuracy: 0.1648 - val_loss: 9.7863 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04818: loss did not improve from 2.92442\n",
            "Epoch 4819/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9059 - accuracy: 0.1599 - val_loss: 9.8253 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04819: loss did not improve from 2.92442\n",
            "Epoch 4820/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9571 - accuracy: 0.1590 - val_loss: 9.8698 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04820: loss did not improve from 2.92442\n",
            "Epoch 4821/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9147 - accuracy: 0.1614 - val_loss: 9.8339 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04821: loss did not improve from 2.92442\n",
            "Epoch 4822/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9569 - accuracy: 0.1716 - val_loss: 9.8275 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04822: loss did not improve from 2.92442\n",
            "Epoch 4823/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9281 - accuracy: 0.1746 - val_loss: 9.8357 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04823: loss did not improve from 2.92442\n",
            "Epoch 4824/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9127 - accuracy: 0.1766 - val_loss: 9.7664 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04824: loss did not improve from 2.92442\n",
            "Epoch 4825/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 3.0010 - accuracy: 0.1551 - val_loss: 9.8564 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04825: loss did not improve from 2.92442\n",
            "Epoch 4826/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9165 - accuracy: 0.1849 - val_loss: 9.8676 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04826: loss did not improve from 2.92442\n",
            "Epoch 4827/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9177 - accuracy: 0.1617 - val_loss: 9.8679 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04827: loss did not improve from 2.92442\n",
            "Epoch 4828/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9671 - accuracy: 0.1578 - val_loss: 9.8407 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04828: loss did not improve from 2.92442\n",
            "Epoch 4829/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9279 - accuracy: 0.1706 - val_loss: 9.8517 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04829: loss did not improve from 2.92442\n",
            "Epoch 4830/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9454 - accuracy: 0.1623 - val_loss: 9.8146 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04830: loss did not improve from 2.92442\n",
            "Epoch 4831/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9346 - accuracy: 0.1704 - val_loss: 9.8307 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04831: loss did not improve from 2.92442\n",
            "Epoch 4832/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9375 - accuracy: 0.1620 - val_loss: 9.8616 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04832: loss did not improve from 2.92442\n",
            "Epoch 4833/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.8897 - accuracy: 0.1591 - val_loss: 9.8693 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04833: loss did not improve from 2.92442\n",
            "Epoch 4834/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9537 - accuracy: 0.1536 - val_loss: 9.8291 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04834: loss did not improve from 2.92442\n",
            "Epoch 4835/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9433 - accuracy: 0.1688 - val_loss: 9.8470 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04835: loss did not improve from 2.92442\n",
            "Epoch 4836/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9165 - accuracy: 0.1720 - val_loss: 9.8581 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04836: loss did not improve from 2.92442\n",
            "Epoch 4837/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9347 - accuracy: 0.1657 - val_loss: 9.8590 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04837: loss did not improve from 2.92442\n",
            "Epoch 4838/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9363 - accuracy: 0.1740 - val_loss: 9.8103 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04838: loss did not improve from 2.92442\n",
            "Epoch 4839/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9575 - accuracy: 0.1667 - val_loss: 9.8279 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04839: loss did not improve from 2.92442\n",
            "Epoch 4840/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9702 - accuracy: 0.1663 - val_loss: 9.8244 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04840: loss did not improve from 2.92442\n",
            "Epoch 4841/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9466 - accuracy: 0.1613 - val_loss: 9.8229 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04841: loss did not improve from 2.92442\n",
            "Epoch 4842/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9422 - accuracy: 0.1617 - val_loss: 9.8086 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04842: loss did not improve from 2.92442\n",
            "Epoch 4843/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9533 - accuracy: 0.1602 - val_loss: 9.8315 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04843: loss did not improve from 2.92442\n",
            "Epoch 4844/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9518 - accuracy: 0.1730 - val_loss: 9.7848 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04844: loss did not improve from 2.92442\n",
            "Epoch 4845/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9484 - accuracy: 0.1657 - val_loss: 9.8327 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04845: loss did not improve from 2.92442\n",
            "Epoch 4846/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9345 - accuracy: 0.1697 - val_loss: 9.8026 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04846: loss did not improve from 2.92442\n",
            "Epoch 4847/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9180 - accuracy: 0.1739 - val_loss: 9.7665 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04847: loss did not improve from 2.92442\n",
            "Epoch 4848/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9332 - accuracy: 0.1586 - val_loss: 9.8548 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04848: loss did not improve from 2.92442\n",
            "Epoch 4849/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9344 - accuracy: 0.1670 - val_loss: 9.8083 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04849: loss did not improve from 2.92442\n",
            "Epoch 4850/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9665 - accuracy: 0.1618 - val_loss: 9.8488 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04850: loss did not improve from 2.92442\n",
            "Epoch 4851/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9138 - accuracy: 0.1835 - val_loss: 9.8548 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04851: loss did not improve from 2.92442\n",
            "Epoch 4852/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9385 - accuracy: 0.1522 - val_loss: 9.8421 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04852: loss did not improve from 2.92442\n",
            "Epoch 4853/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9840 - accuracy: 0.1425 - val_loss: 9.8829 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04853: loss did not improve from 2.92442\n",
            "Epoch 4854/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9696 - accuracy: 0.1453 - val_loss: 9.8584 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04854: loss did not improve from 2.92442\n",
            "Epoch 4855/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9649 - accuracy: 0.1492 - val_loss: 9.8386 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04855: loss did not improve from 2.92442\n",
            "Epoch 4856/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9484 - accuracy: 0.1534 - val_loss: 9.8840 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04856: loss did not improve from 2.92442\n",
            "Epoch 4857/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9368 - accuracy: 0.1872 - val_loss: 9.8259 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04857: loss did not improve from 2.92442\n",
            "Epoch 4858/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9755 - accuracy: 0.1591 - val_loss: 9.8947 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04858: loss did not improve from 2.92442\n",
            "Epoch 4859/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9075 - accuracy: 0.1683 - val_loss: 9.8534 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04859: loss did not improve from 2.92442\n",
            "Epoch 4860/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9483 - accuracy: 0.1700 - val_loss: 9.8460 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04860: loss did not improve from 2.92442\n",
            "Epoch 4861/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9554 - accuracy: 0.1770 - val_loss: 9.8428 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04861: loss did not improve from 2.92442\n",
            "Epoch 4862/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9632 - accuracy: 0.1445 - val_loss: 9.8647 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04862: loss did not improve from 2.92442\n",
            "Epoch 4863/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9386 - accuracy: 0.1672 - val_loss: 9.8742 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04863: loss did not improve from 2.92442\n",
            "Epoch 4864/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9063 - accuracy: 0.1766 - val_loss: 9.8352 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04864: loss did not improve from 2.92442\n",
            "Epoch 4865/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9287 - accuracy: 0.1584 - val_loss: 9.8281 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04865: loss did not improve from 2.92442\n",
            "Epoch 4866/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9463 - accuracy: 0.1811 - val_loss: 9.8274 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04866: loss did not improve from 2.92442\n",
            "Epoch 4867/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9619 - accuracy: 0.1655 - val_loss: 9.8852 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04867: loss did not improve from 2.92442\n",
            "Epoch 4868/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9563 - accuracy: 0.1480 - val_loss: 9.8850 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04868: loss did not improve from 2.92442\n",
            "Epoch 4869/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9358 - accuracy: 0.1550 - val_loss: 9.8399 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04869: loss did not improve from 2.92442\n",
            "Epoch 4870/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9392 - accuracy: 0.1525 - val_loss: 9.8168 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04870: loss did not improve from 2.92442\n",
            "Epoch 4871/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9647 - accuracy: 0.1731 - val_loss: 9.8689 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04871: loss did not improve from 2.92442\n",
            "Epoch 4872/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9175 - accuracy: 0.1794 - val_loss: 9.8448 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04872: loss did not improve from 2.92442\n",
            "Epoch 4873/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9547 - accuracy: 0.1452 - val_loss: 9.8607 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04873: loss did not improve from 2.92442\n",
            "Epoch 4874/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9362 - accuracy: 0.1668 - val_loss: 9.8570 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04874: loss did not improve from 2.92442\n",
            "Epoch 4875/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9634 - accuracy: 0.1707 - val_loss: 9.8754 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04875: loss did not improve from 2.92442\n",
            "Epoch 4876/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9287 - accuracy: 0.1674 - val_loss: 9.7934 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04876: loss did not improve from 2.92442\n",
            "Epoch 4877/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9438 - accuracy: 0.1657 - val_loss: 9.8938 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04877: loss did not improve from 2.92442\n",
            "Epoch 4878/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9717 - accuracy: 0.1581 - val_loss: 9.8683 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04878: loss did not improve from 2.92442\n",
            "Epoch 4879/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9322 - accuracy: 0.1599 - val_loss: 9.8540 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04879: loss did not improve from 2.92442\n",
            "Epoch 4880/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9306 - accuracy: 0.1632 - val_loss: 9.8309 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04880: loss did not improve from 2.92442\n",
            "Epoch 4881/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9618 - accuracy: 0.1624 - val_loss: 9.8012 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04881: loss did not improve from 2.92442\n",
            "Epoch 4882/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9232 - accuracy: 0.1762 - val_loss: 9.8878 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04882: loss did not improve from 2.92442\n",
            "Epoch 4883/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9231 - accuracy: 0.1662 - val_loss: 9.8649 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04883: loss did not improve from 2.92442\n",
            "Epoch 4884/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9318 - accuracy: 0.1638 - val_loss: 9.8596 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04884: loss did not improve from 2.92442\n",
            "Epoch 4885/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9656 - accuracy: 0.1735 - val_loss: 9.9063 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04885: loss did not improve from 2.92442\n",
            "Epoch 4886/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9691 - accuracy: 0.1478 - val_loss: 9.8400 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04886: loss did not improve from 2.92442\n",
            "Epoch 4887/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9385 - accuracy: 0.1609 - val_loss: 9.9019 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04887: loss did not improve from 2.92442\n",
            "Epoch 4888/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9696 - accuracy: 0.1414 - val_loss: 9.8782 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04888: loss did not improve from 2.92442\n",
            "Epoch 4889/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9581 - accuracy: 0.1530 - val_loss: 9.8471 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04889: loss did not improve from 2.92442\n",
            "Epoch 4890/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9315 - accuracy: 0.1609 - val_loss: 9.8275 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04890: loss did not improve from 2.92442\n",
            "Epoch 4891/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9272 - accuracy: 0.1705 - val_loss: 9.8280 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04891: loss did not improve from 2.92442\n",
            "Epoch 4892/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9223 - accuracy: 0.1681 - val_loss: 9.8823 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04892: loss did not improve from 2.92442\n",
            "Epoch 4893/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9545 - accuracy: 0.1608 - val_loss: 9.8313 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04893: loss did not improve from 2.92442\n",
            "Epoch 4894/5000\n",
            "31/31 [==============================] - 1s 40ms/step - loss: 2.9241 - accuracy: 0.1592 - val_loss: 9.8136 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04894: loss did not improve from 2.92442\n",
            "Epoch 4895/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9512 - accuracy: 0.1610 - val_loss: 9.8550 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04895: loss did not improve from 2.92442\n",
            "Epoch 4896/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9390 - accuracy: 0.1558 - val_loss: 9.8169 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04896: loss did not improve from 2.92442\n",
            "Epoch 4897/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9403 - accuracy: 0.1613 - val_loss: 9.8079 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04897: loss did not improve from 2.92442\n",
            "Epoch 4898/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9098 - accuracy: 0.1716 - val_loss: 9.8628 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04898: loss did not improve from 2.92442\n",
            "Epoch 4899/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9247 - accuracy: 0.1557 - val_loss: 9.8833 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04899: loss did not improve from 2.92442\n",
            "Epoch 4900/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9320 - accuracy: 0.1691 - val_loss: 9.8446 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04900: loss did not improve from 2.92442\n",
            "Epoch 4901/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9598 - accuracy: 0.1648 - val_loss: 9.8540 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04901: loss did not improve from 2.92442\n",
            "Epoch 4902/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9309 - accuracy: 0.1650 - val_loss: 9.8308 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04902: loss did not improve from 2.92442\n",
            "Epoch 4903/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9014 - accuracy: 0.1594 - val_loss: 9.8818 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04903: loss did not improve from 2.92442\n",
            "Epoch 4904/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9489 - accuracy: 0.1703 - val_loss: 9.8661 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04904: loss did not improve from 2.92442\n",
            "Epoch 4905/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9569 - accuracy: 0.1719 - val_loss: 9.9063 - val_accuracy: 0.0042\n",
            "\n",
            "Epoch 04905: loss did not improve from 2.92442\n",
            "Epoch 4906/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9266 - accuracy: 0.1583 - val_loss: 9.8734 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04906: loss did not improve from 2.92442\n",
            "Epoch 4907/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9413 - accuracy: 0.1606 - val_loss: 9.8875 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04907: loss did not improve from 2.92442\n",
            "Epoch 4908/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9185 - accuracy: 0.1633 - val_loss: 9.8660 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04908: loss did not improve from 2.92442\n",
            "Epoch 4909/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9545 - accuracy: 0.1600 - val_loss: 9.8510 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04909: loss did not improve from 2.92442\n",
            "Epoch 4910/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9094 - accuracy: 0.1538 - val_loss: 9.8575 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04910: loss did not improve from 2.92442\n",
            "Epoch 4911/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9478 - accuracy: 0.1637 - val_loss: 9.8016 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04911: loss did not improve from 2.92442\n",
            "Epoch 4912/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9722 - accuracy: 0.1600 - val_loss: 9.8601 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04912: loss did not improve from 2.92442\n",
            "Epoch 4913/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9030 - accuracy: 0.1763 - val_loss: 9.8442 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04913: loss did not improve from 2.92442\n",
            "Epoch 4914/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.8953 - accuracy: 0.1761 - val_loss: 9.8856 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04914: loss did not improve from 2.92442\n",
            "Epoch 4915/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9604 - accuracy: 0.1583 - val_loss: 9.8570 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04915: loss did not improve from 2.92442\n",
            "Epoch 4916/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9546 - accuracy: 0.1547 - val_loss: 9.8952 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04916: loss did not improve from 2.92442\n",
            "Epoch 4917/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9205 - accuracy: 0.1872 - val_loss: 9.8455 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04917: loss did not improve from 2.92442\n",
            "Epoch 4918/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9129 - accuracy: 0.1670 - val_loss: 9.9483 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04918: loss did not improve from 2.92442\n",
            "Epoch 4919/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9605 - accuracy: 0.1571 - val_loss: 9.8656 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04919: loss did not improve from 2.92442\n",
            "Epoch 4920/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9102 - accuracy: 0.1749 - val_loss: 9.8890 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04920: loss did not improve from 2.92442\n",
            "Epoch 4921/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9349 - accuracy: 0.1699 - val_loss: 9.8838 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04921: loss did not improve from 2.92442\n",
            "Epoch 4922/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9103 - accuracy: 0.1833 - val_loss: 9.9298 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04922: loss did not improve from 2.92442\n",
            "Epoch 4923/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9581 - accuracy: 0.1471 - val_loss: 9.9107 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04923: loss did not improve from 2.92442\n",
            "Epoch 4924/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9451 - accuracy: 0.1576 - val_loss: 9.8236 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04924: loss did not improve from 2.92442\n",
            "Epoch 4925/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9487 - accuracy: 0.1558 - val_loss: 9.8616 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04925: loss did not improve from 2.92442\n",
            "Epoch 4926/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9387 - accuracy: 0.1596 - val_loss: 9.8559 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04926: loss did not improve from 2.92442\n",
            "Epoch 4927/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9183 - accuracy: 0.1539 - val_loss: 9.8427 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04927: loss did not improve from 2.92442\n",
            "Epoch 4928/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9446 - accuracy: 0.1551 - val_loss: 9.8825 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04928: loss did not improve from 2.92442\n",
            "Epoch 4929/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9017 - accuracy: 0.1773 - val_loss: 9.9015 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04929: loss improved from 2.92442 to 2.92101, saving model to poids_train.hdf5\n",
            "Epoch 4930/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9466 - accuracy: 0.1544 - val_loss: 9.8706 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04930: loss did not improve from 2.92101\n",
            "Epoch 4931/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9161 - accuracy: 0.1621 - val_loss: 9.8683 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04931: loss did not improve from 2.92101\n",
            "Epoch 4932/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9372 - accuracy: 0.1653 - val_loss: 9.8826 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04932: loss did not improve from 2.92101\n",
            "Epoch 4933/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9333 - accuracy: 0.1544 - val_loss: 9.8798 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04933: loss did not improve from 2.92101\n",
            "Epoch 4934/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9396 - accuracy: 0.1635 - val_loss: 9.9430 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04934: loss did not improve from 2.92101\n",
            "Epoch 4935/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9771 - accuracy: 0.1548 - val_loss: 9.8772 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04935: loss did not improve from 2.92101\n",
            "Epoch 4936/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9199 - accuracy: 0.1652 - val_loss: 9.8869 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04936: loss did not improve from 2.92101\n",
            "Epoch 4937/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9448 - accuracy: 0.1523 - val_loss: 9.7893 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04937: loss did not improve from 2.92101\n",
            "Epoch 4938/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9417 - accuracy: 0.1670 - val_loss: 9.8658 - val_accuracy: 0.0127\n",
            "\n",
            "Epoch 04938: loss did not improve from 2.92101\n",
            "Epoch 4939/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.8830 - accuracy: 0.1709 - val_loss: 9.8846 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04939: loss did not improve from 2.92101\n",
            "Epoch 4940/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9203 - accuracy: 0.1546 - val_loss: 9.8877 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04940: loss did not improve from 2.92101\n",
            "Epoch 4941/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9393 - accuracy: 0.1709 - val_loss: 9.8472 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04941: loss did not improve from 2.92101\n",
            "Epoch 4942/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9335 - accuracy: 0.1842 - val_loss: 9.8991 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04942: loss did not improve from 2.92101\n",
            "Epoch 4943/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9397 - accuracy: 0.1555 - val_loss: 9.9337 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04943: loss did not improve from 2.92101\n",
            "Epoch 4944/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9470 - accuracy: 0.1545 - val_loss: 9.8601 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04944: loss did not improve from 2.92101\n",
            "Epoch 4945/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9329 - accuracy: 0.1523 - val_loss: 9.8674 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04945: loss did not improve from 2.92101\n",
            "Epoch 4946/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9185 - accuracy: 0.1514 - val_loss: 9.8640 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04946: loss did not improve from 2.92101\n",
            "Epoch 4947/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9300 - accuracy: 0.1711 - val_loss: 9.8642 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04947: loss did not improve from 2.92101\n",
            "Epoch 4948/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9651 - accuracy: 0.1625 - val_loss: 9.9133 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04948: loss did not improve from 2.92101\n",
            "Epoch 4949/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9311 - accuracy: 0.1731 - val_loss: 9.8997 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04949: loss did not improve from 2.92101\n",
            "Epoch 4950/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9261 - accuracy: 0.1581 - val_loss: 9.8643 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04950: loss did not improve from 2.92101\n",
            "Epoch 4951/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9017 - accuracy: 0.1900 - val_loss: 9.8510 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04951: loss did not improve from 2.92101\n",
            "Epoch 4952/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9104 - accuracy: 0.1762 - val_loss: 9.9571 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04952: loss did not improve from 2.92101\n",
            "Epoch 4953/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9545 - accuracy: 0.1518 - val_loss: 9.8704 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04953: loss did not improve from 2.92101\n",
            "Epoch 4954/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9332 - accuracy: 0.1813 - val_loss: 9.8846 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04954: loss did not improve from 2.92101\n",
            "Epoch 4955/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9371 - accuracy: 0.1521 - val_loss: 9.9093 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04955: loss did not improve from 2.92101\n",
            "Epoch 4956/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9523 - accuracy: 0.1701 - val_loss: 9.9025 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04956: loss did not improve from 2.92101\n",
            "Epoch 4957/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9204 - accuracy: 0.1701 - val_loss: 9.8810 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04957: loss improved from 2.92101 to 2.91917, saving model to poids_train.hdf5\n",
            "Epoch 4958/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9130 - accuracy: 0.1833 - val_loss: 9.9067 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04958: loss did not improve from 2.91917\n",
            "Epoch 4959/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9232 - accuracy: 0.1585 - val_loss: 9.9198 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04959: loss did not improve from 2.91917\n",
            "Epoch 4960/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9454 - accuracy: 0.1519 - val_loss: 9.8987 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04960: loss did not improve from 2.91917\n",
            "Epoch 4961/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9305 - accuracy: 0.1733 - val_loss: 9.9077 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04961: loss did not improve from 2.91917\n",
            "Epoch 4962/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9556 - accuracy: 0.1650 - val_loss: 9.8989 - val_accuracy: 0.0064\n",
            "\n",
            "Epoch 04962: loss did not improve from 2.91917\n",
            "Epoch 4963/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9250 - accuracy: 0.1671 - val_loss: 9.8601 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04963: loss did not improve from 2.91917\n",
            "Epoch 4964/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9371 - accuracy: 0.1648 - val_loss: 9.8731 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04964: loss did not improve from 2.91917\n",
            "Epoch 4965/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9771 - accuracy: 0.1525 - val_loss: 9.8443 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04965: loss did not improve from 2.91917\n",
            "Epoch 4966/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9167 - accuracy: 0.1641 - val_loss: 9.9165 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04966: loss did not improve from 2.91917\n",
            "Epoch 4967/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9474 - accuracy: 0.1835 - val_loss: 9.8613 - val_accuracy: 0.0127\n",
            "\n",
            "Epoch 04967: loss did not improve from 2.91917\n",
            "Epoch 4968/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9449 - accuracy: 0.1702 - val_loss: 9.8915 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04968: loss did not improve from 2.91917\n",
            "Epoch 4969/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9047 - accuracy: 0.1537 - val_loss: 9.8566 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04969: loss did not improve from 2.91917\n",
            "Epoch 4970/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9363 - accuracy: 0.1671 - val_loss: 9.9108 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04970: loss did not improve from 2.91917\n",
            "Epoch 4971/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9144 - accuracy: 0.1647 - val_loss: 9.8764 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04971: loss did not improve from 2.91917\n",
            "Epoch 4972/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9461 - accuracy: 0.1612 - val_loss: 9.8982 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04972: loss did not improve from 2.91917\n",
            "Epoch 4973/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9570 - accuracy: 0.1592 - val_loss: 9.8756 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04973: loss did not improve from 2.91917\n",
            "Epoch 4974/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9814 - accuracy: 0.1429 - val_loss: 9.8884 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04974: loss did not improve from 2.91917\n",
            "Epoch 4975/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9379 - accuracy: 0.1542 - val_loss: 9.8846 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04975: loss did not improve from 2.91917\n",
            "Epoch 4976/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9384 - accuracy: 0.1512 - val_loss: 9.8688 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04976: loss did not improve from 2.91917\n",
            "Epoch 4977/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9254 - accuracy: 0.1575 - val_loss: 9.8744 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04977: loss did not improve from 2.91917\n",
            "Epoch 4978/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9336 - accuracy: 0.1837 - val_loss: 9.8185 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04978: loss did not improve from 2.91917\n",
            "Epoch 4979/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9720 - accuracy: 0.1388 - val_loss: 9.8570 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04979: loss did not improve from 2.91917\n",
            "Epoch 4980/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9544 - accuracy: 0.1603 - val_loss: 9.8672 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04980: loss did not improve from 2.91917\n",
            "Epoch 4981/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9435 - accuracy: 0.1577 - val_loss: 9.9010 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04981: loss did not improve from 2.91917\n",
            "Epoch 4982/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9389 - accuracy: 0.1665 - val_loss: 9.8734 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04982: loss did not improve from 2.91917\n",
            "Epoch 4983/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9353 - accuracy: 0.1718 - val_loss: 9.8897 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04983: loss did not improve from 2.91917\n",
            "Epoch 4984/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9253 - accuracy: 0.1506 - val_loss: 9.8899 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04984: loss did not improve from 2.91917\n",
            "Epoch 4985/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9500 - accuracy: 0.1615 - val_loss: 9.8847 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04985: loss did not improve from 2.91917\n",
            "Epoch 4986/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9391 - accuracy: 0.1546 - val_loss: 9.9430 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04986: loss did not improve from 2.91917\n",
            "Epoch 4987/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9218 - accuracy: 0.1571 - val_loss: 9.9095 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04987: loss did not improve from 2.91917\n",
            "Epoch 4988/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9707 - accuracy: 0.1596 - val_loss: 9.9172 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04988: loss did not improve from 2.91917\n",
            "Epoch 4989/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9616 - accuracy: 0.1581 - val_loss: 9.8801 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04989: loss did not improve from 2.91917\n",
            "Epoch 4990/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9373 - accuracy: 0.1625 - val_loss: 9.8835 - val_accuracy: 0.0127\n",
            "\n",
            "Epoch 04990: loss did not improve from 2.91917\n",
            "Epoch 4991/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9682 - accuracy: 0.1478 - val_loss: 9.9332 - val_accuracy: 0.0085\n",
            "\n",
            "Epoch 04991: loss did not improve from 2.91917\n",
            "Epoch 4992/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9151 - accuracy: 0.1677 - val_loss: 9.9198 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04992: loss did not improve from 2.91917\n",
            "Epoch 4993/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9629 - accuracy: 0.1505 - val_loss: 9.8592 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04993: loss did not improve from 2.91917\n",
            "Epoch 4994/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9353 - accuracy: 0.1670 - val_loss: 9.8352 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04994: loss did not improve from 2.91917\n",
            "Epoch 4995/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9583 - accuracy: 0.1562 - val_loss: 9.8520 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04995: loss did not improve from 2.91917\n",
            "Epoch 4996/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9376 - accuracy: 0.1605 - val_loss: 9.9239 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04996: loss did not improve from 2.91917\n",
            "Epoch 4997/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9602 - accuracy: 0.1420 - val_loss: 9.8623 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04997: loss did not improve from 2.91917\n",
            "Epoch 4998/5000\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 2.9391 - accuracy: 0.1522 - val_loss: 9.9065 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04998: loss did not improve from 2.91917\n",
            "Epoch 4999/5000\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 2.9524 - accuracy: 0.1611 - val_loss: 9.9482 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 04999: loss did not improve from 2.91917\n",
            "Epoch 5000/5000\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 2.9251 - accuracy: 0.1619 - val_loss: 9.9038 - val_accuracy: 0.0106\n",
            "\n",
            "Epoch 05000: loss did not improve from 2.91917\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_4a352d94-7552-494c-8e21-d29607a367d0\", \"poids_train.hdf5\", 847024)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EKaVqPCYkid"
      },
      "source": [
        "# Télécharge les résultats d'entrainement du modèle\n",
        "! wget --no-check-certificate --content-disposition \"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/Bitcoin/poids_train_BitWave_One_All.hdf5?raw=true\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMUbH3CaYkid"
      },
      "source": [
        "model.load_weights(\"poids_train.hdf5\")\n",
        "#model.load_weights(\"poids_train_BitWave_One_All.hdf5\")"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idMYWg8mYkie"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6763fLQ6Ykif"
      },
      "source": [
        "# Evaluation du modèle\n",
        "\n",
        "model.evaluate(dataset)\n",
        "model.evaluate(dataset_val)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj3qvyK6Ykir"
      },
      "source": [
        "**3. Prédictions \"single step\"**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iOJE_lAYkir",
        "outputId": "7ce5d5d8-4179-4cb9-9f6a-4ce5938fb86d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Création des instants d'entrainement et de validation\n",
        "y_train_timing = X_Avec_Prix.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_train)]\n",
        "y_val_timing = X_Avec_Prix.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_val)]\n",
        "\n",
        "# Calcul des prédictions\n",
        "pred_ent = model.predict(x_train, verbose=1)\n",
        "pred_val = model.predict(x_val, verbose=1)"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "61/61 [==============================] - 3s 13ms/step\n",
            "15/15 [==============================] - 0s 12ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfUyGJ0RYkir",
        "outputId": "1e70696e-5119-4b8d-aab8-da76b008d8fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbes originales\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index,y=serie_entrainement_X[0],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index[temps_separation:],y=serie_test_X[0],line=dict(color='blue', width=1)))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index[taille_fenetre+horizon-1:],y=en.inverse_transform(pred_ent)[:,0],line=dict(color='green', width=1)))\n",
        "\n",
        "# Courbes des prédictions de validations\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index[temps_separation+taille_fenetre+horizon-1:],y=en.inverse_transform(pred_val)[:,0],line=dict(color='red', width=1)))\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"5e41caa9-78e7-4bf4-88cb-deb56f925a08\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"5e41caa9-78e7-4bf4-88cb-deb56f925a08\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '5e41caa9-78e7-4bf4-88cb-deb56f925a08',\n",
              "                        [{\"line\": {\"color\": \"blue\", \"width\": 1}, \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431], \"y\": [2.1209771633148193, 2.1159462928771973, 2.1124393939971924, 2.1115927696228027, 2.110919713973999, 2.1131436824798584, 2.1143698692321777, 2.113212823867798, 2.113311529159546, 2.1088881492614746, 2.107494592666626, 2.113555669784546, 2.1109886169433594, 2.1097490787506104, 2.110935926437378, 2.111395835876465, 2.11073637008667, 2.1079421043395996, 2.1035571098327637, 2.0963573455810547, 2.0824339389801025, 2.084240436553955, 2.080282211303711, 2.079901933670044, 2.0831167697906494, 2.0921969413757324, 2.0847558975219727, 2.081796646118164, 2.08042573928833, 2.0756335258483887, 2.0692970752716064, 2.0701093673706055, 2.072471857070923, 2.069432258605957, 2.0643999576568604, 2.053560972213745, 2.0371553897857666, 2.0409162044525146, 2.0441625118255615, 2.0499889850616455, 2.06303334236145, 2.0619163513183594, 2.0604710578918457, 2.0620477199554443, 2.0699048042297363, 2.0807151794433594, 2.0769553184509277, 2.073476791381836, 2.0702672004699707, 2.0727458000183105, 2.0744729042053223, 2.0716121196746826, 2.0724143981933594, 2.071852922439575, 2.064392566680908, 2.0586190223693848, 2.055227518081665, 2.0563580989837646, 2.05713152885437, 2.0571887493133545, 2.0530500411987305, 2.049334764480591, 2.0511789321899414, 2.044595241546631, 2.041966676712036, 2.0438907146453857, 2.0427989959716797, 2.0488669872283936, 2.0516774654388428, 2.0530741214752197, 2.0521469116210938, 2.055636167526245, 2.0636556148529053, 2.0640976428985596, 2.077482223510742, 2.0922045707702637, 2.078559160232544, 2.0737359523773193, 2.072591781616211, 2.0784752368927, 2.071026563644409, 2.0698843002319336, 2.06367826461792, 2.056516170501709, 2.0590312480926514, 2.0600273609161377, 2.0681028366088867, 2.072064161300659, 2.066581964492798, 2.066084861755371, 2.0654425621032715, 2.069911003112793, 2.069502830505371, 2.0703604221343994, 2.0711519718170166, 2.0699098110198975, 2.067739725112915, 2.0671427249908447, 2.0681095123291016, 2.0681514739990234, 2.0652048587799072, 2.0570967197418213, 2.055330991744995, 2.056659460067749, 2.0561273097991943, 2.0544047355651855, 2.05476713180542, 2.0542314052581787, 2.047889471054077, 2.0413033962249756, 2.035353899002075, 2.0351686477661133, 2.0421741008758545, 2.041745901107788, 2.0440359115600586, 2.0465798377990723, 2.044642448425293, 2.0389909744262695, 2.0412955284118652, 2.0388243198394775, 2.036949872970581, 2.036595582962036, 2.0349392890930176, 2.0353896617889404, 2.037055492401123, 2.036322593688965, 2.0300447940826416, 2.012089729309082, 2.010279893875122, 2.012930154800415, 2.020021915435791, 2.0202503204345703, 2.0178840160369873, 2.015592336654663, 2.0109877586364746, 2.008253812789917, 1.987237811088562, 1.9525816440582275, 1.9600549936294556, 1.9650799036026, 1.955323576927185, 1.9600480794906616, 1.9651377201080322, 1.9641269445419312, 1.9675160646438599, 1.9817863702774048, 1.979788899421692, 1.9855011701583862, 1.993942141532898, 2.0163071155548096, 2.00295352935791, 1.9970200061798096, 1.9818555116653442, 1.9819104671478271, 1.9783365726470947, 1.9707187414169312, 1.9774528741836548, 1.9852607250213623, 1.9762667417526245, 1.9739629030227661, 1.9719148874282837, 1.976710319519043, 1.9759397506713867, 1.9727411270141602, 1.970465898513794, 1.972446322441101, 1.971423864364624, 1.981304407119751, 1.9922271966934204, 1.9951894283294678, 1.9835023880004883, 1.9866864681243896, 1.9873042106628418, 1.9860221147537231, 1.9898918867111206, 1.9907243251800537, 1.988559365272522, 1.9847184419631958, 1.9862000942230225, 1.9853945970535278, 1.9846068620681763, 1.995270848274231, 1.996963620185852, 1.99608314037323, 2.0038089752197266, 2.012446165084839, 2.0147690773010254, 2.009230375289917, 2.010789632797241, 2.011077642440796, 2.0123565196990967, 2.016716480255127, 2.0241501331329346, 2.0242371559143066, 2.024437665939331, 2.0224123001098633, 2.017611265182495, 2.017948865890503, 2.0222933292388916, 2.0210530757904053, 2.011298656463623, 2.000087022781372, 2.0025887489318848, 2.0016539096832275, 2.0033349990844727, 2.0070276260375977, 1.9994280338287354, 1.9911373853683472, 1.994271993637085, 1.9931318759918213, 1.9959344863891602, 1.9914710521697998, 1.991471767425537, 1.9915961027145386, 1.9900084733963013, 1.9937387704849243, 1.9975769519805908, 1.996949315071106, 1.9985917806625366, 2.000469446182251, 1.9972132444381714, 1.9939100742340088, 1.9903382062911987, 1.9860531091690063, 1.9845465421676636, 1.9846221208572388, 1.981749176979065, 1.9719886779785156, 1.9726251363754272, 1.9772460460662842, 1.9753799438476562, 1.9739766120910645, 1.9752522706985474, 1.9750628471374512, 1.9768036603927612, 1.9840888977050781, 1.983366847038269, 1.981235146522522, 1.9787403345108032, 1.970980167388916, 1.9728007316589355, 1.9764810800552368, 1.9761048555374146, 1.9812040328979492, 1.9839192628860474, 1.9833892583847046, 1.9872831106185913, 1.9877365827560425, 1.984294056892395, 1.9835691452026367, 1.9816986322402954, 1.9876877069473267, 1.9896320104599, 1.988684892654419, 1.9887573719024658, 1.9887768030166626, 1.9893076419830322, 1.9849063158035278, 1.9855589866638184, 1.9853352308273315, 1.9852501153945923, 1.9847078323364258, 1.9823042154312134, 1.9825184345245361, 1.9839117527008057, 1.9854013919830322, 1.9876607656478882, 1.9875479936599731, 1.9866200685501099, 1.985672950744629, 1.9857268333435059, 1.9855589866638184, 1.985305666923523, 1.9832953214645386, 1.9817763566970825, 1.9778556823730469, 1.9753944873809814, 1.9769178628921509, 1.9763898849487305, 1.9757375717163086, 1.9761425256729126, 1.9759124517440796, 1.9768483638763428, 1.979570746421814, 1.9793968200683594, 1.97982656955719, 1.9804290533065796, 1.9806348085403442, 1.9827510118484497, 1.9843415021896362, 1.9890178442001343, 1.9956443309783936, 1.9937924146652222, 1.9931726455688477, 1.9898995161056519, 1.9901878833770752, 1.9918413162231445, 1.991385579109192, 1.990057349205017, 1.9895353317260742, 1.989698886871338, 1.992396354675293, 1.9948323965072632, 1.99710214138031, 2.003916025161743, 2.0021309852600098, 2.000398874282837, 1.9993681907653809, 2.000519275665283, 2.0051941871643066, 2.010190486907959, 2.0073256492614746, 2.0079357624053955, 2.0088860988616943, 2.0162720680236816, 2.0226452350616455, 2.030449867248535, 2.0241103172302246, 2.0219476222991943, 2.0219457149505615, 2.0169081687927246, 2.013629674911499, 2.013817548751831, 2.012098789215088, 2.0129380226135254, 2.014521360397339, 2.013218641281128, 2.0135562419891357, 2.0172061920166016, 2.0208899974823, 2.0222747325897217, 2.0226030349731445, 2.0243091583251953, 2.022284507751465, 2.020381212234497, 2.0184388160705566, 2.016087055206299, 2.0156610012054443, 2.017118215560913, 2.017751932144165, 2.0171194076538086, 2.0148823261260986, 2.014223337173462, 2.0105886459350586, 2.004384994506836, 2.0051982402801514, 2.0058393478393555, 2.0073344707489014, 2.004882335662842, 2.0054168701171875, 2.005338668823242, 2.0013763904571533, 2.000598430633545, 1.997733473777771, 1.9813488721847534, 1.9822330474853516, 1.9826699495315552, 1.979864239692688, 1.9794236421585083, 1.9718787670135498, 1.9667049646377563, 1.9762194156646729, 1.9765187501907349, 1.9781591892242432, 1.9800752401351929, 1.9797160625457764, 1.9790339469909668, 1.9785642623901367, 1.9784212112426758, 1.9781030416488647, 1.9792433977127075, 1.981493592262268, 1.987445592880249, 1.9880563020706177, 1.9901907444000244, 1.98800528049469, 1.9865573644638062, 1.9875954389572144, 1.9862357378005981, 1.98202383518219, 1.9801543951034546, 1.9801026582717896, 1.9789652824401855, 1.9805206060409546, 1.982353687286377, 1.9812242984771729, 1.980718970298767, 1.9784204959869385, 1.9789526462554932, 1.9803000688552856, 1.9817304611206055, 1.9840879440307617, 1.9831321239471436, 1.98263680934906, 1.9854851961135864, 1.986101508140564, 1.985288143157959, 1.9853131771087646, 1.9859416484832764, 1.9864377975463867, 1.9867498874664307, 1.9866971969604492, 1.989999532699585, 1.9915696382522583, 1.9904181957244873, 1.9903647899627686, 1.9910666942596436, 1.9919726848602295, 1.992578387260437, 1.9931377172470093, 1.9959437847137451, 1.9982287883758545, 2.0020012855529785, 2.0083200931549072, 2.0066986083984375, 2.004692316055298, 2.007028341293335, 2.0076324939727783, 2.0101685523986816, 2.0132603645324707, 2.0152838230133057, 2.021477222442627, 2.0182740688323975, 2.0230278968811035, 2.0286498069763184, 2.033557653427124, 2.042201280593872, 2.040627956390381, 2.0374739170074463, 2.0481953620910645, 2.072484254837036, 2.1007325649261475, 2.0804824829101562, 2.0667850971221924, 2.072240114212036, 2.0715689659118652, 2.0681066513061523, 2.0636422634124756, 2.0371365547180176, 2.0438220500946045, 2.047088146209717, 2.0469610691070557, 2.044048547744751, 2.04073429107666, 2.047530174255371, 2.0470755100250244, 2.0455405712127686, 2.0394747257232666, 2.042229413986206, 2.0418460369110107, 2.0411458015441895, 2.03987979888916, 2.0408647060394287, 2.0517828464508057, 2.058751106262207, 2.058175802230835, 2.0610175132751465, 2.0687496662139893, 2.0643439292907715, 2.0585880279541016, 2.061739444732666, 2.0607662200927734, 2.068037986755371, 2.0769131183624268, 2.0770227909088135, 2.077660083770752, 2.0875327587127686, 2.086660861968994, 2.0936810970306396, 2.0936646461486816, 2.095219612121582, 2.0975732803344727, 2.1037001609802246, 2.1036128997802734, 2.1030538082122803, 2.104740619659424, 2.1059932708740234, 2.1024715900421143, 2.0959250926971436, 2.0965747833251953, 2.0973167419433594, 2.102658271789551, 2.1028170585632324, 2.0921573638916016, 2.0873982906341553, 2.090333938598633, 2.0907511711120605, 2.092395067214966, 2.090888023376465, 2.093764543533325, 2.0941736698150635, 2.0925140380859375, 2.093581199645996, 2.093757390975952, 2.0927481651306152, 2.1001932621002197, 2.1029512882232666, 2.1009819507598877, 2.0987634658813477, 2.0998823642730713, 2.0992212295532227, 2.0921952724456787, 2.092966318130493, 2.0815603733062744, 2.0663623809814453, 2.0726206302642822, 2.071945905685425, 2.0721681118011475, 2.076566219329834, 2.086057424545288, 2.0768537521362305, 2.0746490955352783, 2.0781092643737793, 2.078080177307129, 2.076655149459839, 2.077169895172119, 2.07234263420105, 2.0686137676239014, 2.0696983337402344, 2.0694446563720703, 2.0672895908355713, 2.067796230316162, 2.0667850971221924, 2.0697431564331055, 2.0742199420928955, 2.0692334175109863, 2.069369316101074, 2.069074869155884, 2.067939281463623, 2.070380210876465, 2.0709996223449707, 2.071924924850464, 2.075131416320801, 2.0797789096832275, 2.0818488597869873, 2.081928253173828, 2.086322546005249, 2.088895320892334, 2.0888774394989014, 2.0928542613983154, 2.0963776111602783, 2.0957586765289307, 2.0917742252349854, 2.0890867710113525, 2.0899877548217773, 2.090397357940674, 2.093763589859009, 2.0924534797668457, 2.095634937286377, 2.0947704315185547, 2.0934793949127197, 2.089143753051758, 2.088386297225952, 2.0815675258636475, 2.0817596912384033, 2.0841360092163086, 2.0854010581970215, 2.085082530975342, 2.085895538330078, 2.0883948802948, 2.0856966972351074, 2.085442543029785, 2.085993528366089, 2.086754560470581, 2.086609363555908, 2.0878167152404785, 2.0855791568756104, 2.083141803741455, 2.084350109100342, 2.0841219425201416, 2.0865418910980225, 2.087280035018921, 2.086773157119751, 2.0865731239318848, 2.086958646774292, 2.0883350372314453, 2.090054512023926, 2.0876920223236084, 2.0854814052581787, 2.086409330368042, 2.086484670639038, 2.087775230407715, 2.0877063274383545, 2.087959051132202, 2.088993787765503, 2.0891306400299072, 2.0885791778564453, 2.0887115001678467, 2.087254762649536, 2.0880990028381348, 2.088606595993042, 2.090186834335327, 2.0901219844818115, 2.0899581909179688, 2.0913333892822266, 2.092710018157959, 2.0919058322906494, 2.0916903018951416, 2.0922811031341553, 2.0955512523651123, 2.098278522491455, 2.0991954803466797, 2.099348306655884, 2.1020588874816895, 2.103950262069702, 2.106196641921997, 2.1044764518737793, 2.0987155437469482, 2.1007204055786133, 2.1017494201660156, 2.1009998321533203, 2.099503755569458, 2.099550485610962, 2.0998072624206543, 2.0996737480163574, 2.1016039848327637, 2.104191541671753, 2.104036569595337, 2.104236364364624, 2.1029112339019775, 2.10205078125, 2.101428747177124, 2.102712392807007, 2.1030211448669434, 2.1033971309661865, 2.1033315658569336, 2.1023993492126465, 2.1025938987731934, 2.100815534591675, 2.0975914001464844, 2.097811460494995, 2.097484827041626, 2.0976483821868896, 2.0990395545959473, 2.1002542972564697, 2.1013567447662354, 2.1096689701080322, 2.1189000606536865, 2.1279101371765137, 2.1315901279449463, 2.1323599815368652, 2.1312739849090576, 2.132082939147949, 2.1378540992736816, 2.1440138816833496, 2.144869565963745, 2.1469504833221436, 2.1466591358184814, 2.145073413848877, 2.1449966430664062, 2.144609212875366, 2.146643877029419, 2.1641006469726562, 2.177285671234131, 2.1770143508911133, 2.176176071166992, 2.189492702484131, 2.1918892860412598, 2.1926963329315186, 2.1939289569854736, 2.192580223083496, 2.1759843826293945, 2.1673154830932617, 2.149538278579712, 2.168015956878662, 2.172605514526367, 2.1647143363952637, 2.1639373302459717, 2.1666548252105713, 2.1627767086029053, 2.167644739151001, 2.173938035964966, 2.176480770111084, 2.1737232208251953, 2.172426700592041, 2.1727354526519775, 2.1729886531829834, 2.1647961139678955, 2.1658413410186768, 2.167219877243042, 2.1660311222076416, 2.166311264038086, 2.168336868286133, 2.169969081878662, 2.1689035892486572, 2.1697819232940674, 2.1702773571014404, 2.1721134185791016, 2.1741740703582764, 2.1725146770477295, 2.1720998287200928, 2.1707661151885986, 2.169954538345337, 2.168346405029297, 2.1690733432769775, 2.1690220832824707, 2.16787052154541, 2.168156862258911, 2.168666362762451, 2.168774127960205, 2.168783664703369, 2.1634297370910645, 2.158940076828003, 2.1514627933502197, 2.137921094894409, 2.144538402557373, 2.1446025371551514, 2.1453616619110107, 2.14968204498291, 2.1495707035064697, 2.148674726486206, 2.149742603302002, 2.1505348682403564, 2.1489434242248535, 2.148867607116699, 2.1456210613250732, 2.143332004547119, 2.144278049468994, 2.145068645477295, 2.1448681354522705, 2.145094633102417, 2.1460700035095215, 2.147078514099121, 2.147599697113037, 2.148075580596924, 2.1470141410827637, 2.1454296112060547, 2.146249294281006, 2.1449666023254395, 2.1443328857421875, 2.145188331604004, 2.145768880844116, 2.1450538635253906, 2.14461612701416, 2.144649028778076, 2.1467134952545166, 2.154158115386963, 2.1545820236206055, 2.154927968978882, 2.1558847427368164, 2.159184217453003, 2.1595664024353027, 2.159942388534546, 2.158900022506714, 2.1550562381744385, 2.1556198596954346, 2.156010627746582, 2.1557915210723877, 2.1553893089294434, 2.155120849609375, 2.155277729034424, 2.1558566093444824, 2.155494213104248, 2.1522700786590576, 2.152250051498413, 2.152735471725464, 2.153907060623169, 2.1536788940429688, 2.1542882919311523, 2.154658079147339, 2.1543350219726562, 2.154289960861206, 2.154362678527832, 2.156338691711426, 2.156102418899536, 2.1560916900634766, 2.1556825637817383, 2.156019687652588, 2.1561951637268066, 2.156987428665161, 2.158005475997925, 2.157813787460327, 2.1577303409576416, 2.1621596813201904, 2.1637773513793945, 2.1633388996124268, 2.163553237915039, 2.164313793182373, 2.164478063583374, 2.1641738414764404, 2.163698673248291, 2.162454128265381, 2.1613988876342773, 2.1620497703552246, 2.1649651527404785, 2.1683459281921387, 2.1674866676330566, 2.1686151027679443, 2.1714653968811035, 2.1760714054107666, 2.1768839359283447, 2.182065725326538, 2.1819252967834473, 2.1803951263427734, 2.185807228088379, 2.186981678009033, 2.186800479888916, 2.180385112762451, 2.1813862323760986, 2.1829166412353516, 2.1816914081573486, 2.1821746826171875, 2.186852216720581, 2.1843271255493164, 2.1843788623809814, 2.1828839778900146, 2.1796982288360596, 2.1811277866363525, 2.183239221572876, 2.186190128326416, 2.19134259223938, 2.191331386566162, 2.1925716400146484, 2.1907970905303955, 2.1893537044525146, 2.1911723613739014, 2.1913130283355713, 2.190349817276001, 2.189491033554077, 2.1894426345825195, 2.188912868499756, 2.18891978263855, 2.188849687576294, 2.1901707649230957, 2.1925580501556396, 2.1973347663879395, 2.1965878009796143, 2.1964762210845947, 2.1943202018737793, 2.1945042610168457, 2.1956610679626465, 2.197098731994629, 2.1975314617156982, 2.198350191116333, 2.197251796722412, 2.1989052295684814, 2.199666738510132, 2.1994500160217285, 2.199448585510254, 2.1999881267547607, 2.201738119125366, 2.2019741535186768, 2.2023751735687256, 2.2024879455566406, 2.207244396209717, 2.216214418411255, 2.2249538898468018, 2.225569248199463, 2.2202811241149902, 2.2253854274749756, 2.2282137870788574, 2.235823392868042, 2.2387096881866455, 2.2359490394592285, 2.2362945079803467, 2.239708662033081, 2.246812343597412, 2.2480578422546387, 2.2580599784851074, 2.252955675125122, 2.231539487838745, 2.2189455032348633, 2.227769136428833, 2.2241501808166504, 2.225834846496582, 2.2142951488494873, 2.200509786605835, 2.205944776535034, 2.2104532718658447, 2.2083797454833984, 2.210514545440674, 2.2205581665039062, 2.2225759029388428, 2.224050283432007, 2.2244744300842285, 2.228969097137451, 2.2301533222198486, 2.229605197906494, 2.2272515296936035, 2.2248427867889404, 2.2268166542053223, 2.228928565979004, 2.229365587234497, 2.229107141494751, 2.2291369438171387, 2.233393430709839, 2.2393503189086914, 2.243029832839966, 2.246633529663086, 2.247525215148926, 2.247880458831787, 2.2480578422546387, 2.251743793487549, 2.2536447048187256, 2.248589038848877, 2.2401270866394043, 2.245558261871338, 2.245002269744873, 2.244051218032837, 2.245558261871338, 2.246096611022949, 2.2491185665130615, 2.252610445022583, 2.2548439502716064, 2.2536447048187256, 2.254159688949585, 2.2613842487335205, 2.26448655128479, 2.2686469554901123, 2.2745602130889893, 2.272249221801758, 2.2717835903167725, 2.2750189304351807, 2.276387929916382, 2.2783474922180176, 2.2833733558654785, 2.288121223449707, 2.2882633209228516, 2.2868387699127197, 2.2885468006134033, 2.2855470180511475, 2.2772951126098633, 2.2741005420684814, 2.2781975269317627, 2.2730228900909424, 2.277144193649292, 2.282203435897827, 2.2836647033691406, 2.2849700450897217, 2.2805821895599365, 2.2672176361083984, 2.2519173622131348, 2.247525215148926, 2.2519173622131348, 2.2603933811187744, 2.2558658123016357, 2.252610445022583, 2.244612455368042, 2.232858180999756, 2.240247964859009, 2.245558261871338, 2.252955675125122, 2.2513959407806396, 2.2515697479248047, 2.2550148963928223, 2.259397029876709, 2.260558843612671, 2.2673771381378174, 2.268963098526001, 2.2667386531829834, 2.272404193878174, 2.2762365341186523, 2.2760846614837646, 2.2769932746887207, 2.279095411300659, 2.2798402309417725, 2.2805821895599365, 2.2765395641326904, 2.2756285667419434, 2.2760846614837646, 2.2760846614837646, 2.276387929916382, 2.2805821895599365, 2.2804338932037354, 2.282789468765259, 2.2861223220825195, 2.2855470180511475, 2.2855470180511475, 2.2862658500671387, 2.2879793643951416, 2.29135799407959, 2.294811725616455, 2.295902967453003, 2.2971227169036865, 2.2973928451538086, 2.304403305053711, 2.3104002475738525, 2.3135106563568115, 2.3201704025268555, 2.3221800327301025, 2.321709156036377, 2.323000907897949, 2.3297767639160156, 2.3407094478607178, 2.342923402786255, 2.3510172367095947, 2.347271680831909, 2.3407094478607178, 2.3485965728759766, 2.3430283069610596, 2.3416616916656494, 2.3500125408172607, 2.3546864986419678, 2.3629374504089355, 2.3689322471618652, 2.374030113220215, 2.3829758167266846, 2.387860059738159, 2.4013118743896484, 2.415098190307617, 2.40116024017334, 2.3754355907440186, 2.3837294578552246, 2.387860059738159, 2.3918981552124023, 2.390132427215576, 2.4016904830932617, 2.4038710594177246, 2.4102585315704346, 2.4111220836639404, 2.416914939880371, 2.4321072101593018, 2.430502414703369, 2.4268553256988525, 2.4314029216766357, 2.4335711002349854, 2.436650514602661, 2.430631399154663, 2.4249372482299805, 2.4210383892059326, 2.40116024017334, 2.4070563316345215, 2.4144649505615234, 2.4149577617645264, 2.4146058559417725, 2.4212422370910645, 2.4246044158935547, 2.4225945472717285, 2.4248039722442627, 2.4210383892059326, 2.4150278568267822, 2.4082272052764893, 2.403571844100952, 2.4119815826416016, 2.413475751876831, 2.41119384765625, 2.4063942432403564, 2.4052112102508545, 2.4115521907806396, 2.416497230529785, 2.4149577617645264, 2.416566848754883, 2.4132628440856934, 2.41119384765625, 2.413120985031128, 2.406982898712158, 2.3984081745147705, 2.3984081745147705, 2.400247573852539, 2.3933300971984863, 2.3770039081573486, 2.3646175861358643, 2.3748221397399902, 2.3926949501037598, 2.395925998687744, 2.4081544876098633, 2.4242708683013916, 2.428492546081543, 2.428100824356079, 2.4271183013916016, 2.4190571308135986, 2.4104747772216797, 2.415308713912964, 2.4269211292266846, 2.4257993698120117, 2.4236693382263184, 2.4288182258605957, 2.42972731590271, 2.425335645675659, 2.4272496700286865, 2.4326815605163574, 2.4520509243011475, 2.4569053649902344, 2.460653305053711, 2.466872453689575, 2.4631989002227783, 2.465656280517578, 2.4716567993164062, 2.4848155975341797, 2.496856451034546, 2.5031707286834717, 2.503343105316162, 2.5028257369995117, 2.510309934616089, 2.5055253505706787, 2.499030113220215, 2.49951434135437, 2.49694561958313, 2.492337703704834, 2.5019168853759766, 2.503901958465576, 2.510847568511963, 2.509021520614624, 2.5101027488708496, 2.5087292194366455, 2.5151727199554443, 2.51967191696167, 2.5232152938842773, 2.5281968116760254, 2.523869752883911, 2.520260810852051, 2.5122873783111572, 2.5076396465301514, 2.5188047885894775, 2.520730495452881, 2.516618251800537, 2.509188413619995, 2.5048434734344482, 2.5054829120635986, 2.5068390369415283, 2.4930226802825928, 2.478405714035034, 2.4672935009002686, 2.4825594425201416, 2.479440927505493, 2.493295907974243, 2.494022846221924, 2.4931137561798096, 2.4862887859344482, 2.478257179260254, 2.4826560020446777, 2.481590747833252, 2.4872796535491943, 2.49174165725708, 2.497034788131714, 2.5038161277770996, 2.5019168853759766, 2.5076396465301514, 2.509021520614624, 2.513063907623291, 2.5091464519500732, 2.5065009593963623, 2.5067124366760254, 2.511054039001465, 2.510723829269409, 2.516578197479248, 2.522364854812622, 2.5288333892822266, 2.5277462005615234, 2.5379788875579834, 2.5560495853424072, 2.5581583976745605, 2.5567233562469482, 2.558253526687622, 2.556755304336548, 2.549070358276367, 2.557490110397339, 2.561650514602661, 2.57043719291687, 2.5656864643096924, 2.5644023418426514, 2.5576813220977783, 2.553948163986206, 2.5627686977386475, 2.5627686977386475, 2.560619354248047, 2.5645556449890137, 2.571767568588257, 2.5753262042999268, 2.58276629447937, 2.5949809551239014, 2.602015972137451, 2.6026127338409424, 2.6077041625976562, 2.6023643016815186, 2.5988247394561768, 2.6055431365966797, 2.6022651195526123, 2.594669818878174, 2.5839266777038574, 2.571561574935913, 2.5782198905944824, 2.585352897644043, 2.5978329181671143, 2.6074626445770264, 2.6163740158081055, 2.613241672515869, 2.616189956665039, 2.6225244998931885, 2.623985767364502, 2.6246020793914795, 2.6241180896759033, 2.623764991760254, 2.6301846504211426, 2.642077684402466, 2.6530280113220215, 2.657843828201294, 2.668713092803955, 2.660186290740967, 2.6644647121429443, 2.6770522594451904, 2.682617425918579, 2.6841087341308594, 2.6888184547424316, 2.7027359008789062, 2.7367374897003174, 2.748654842376709, 2.7373619079589844, 2.72798490524292, 2.7512319087982178, 2.755035400390625, 2.753235340118408, 2.750795841217041, 2.761620283126831, 2.769573450088501, 2.7797746658325195, 2.7746171951293945, 2.769425868988037, 2.754875421524048, 2.7484889030456543, 2.721322536468506, 2.7344095706939697, 2.71700382232666, 2.718668222427368, 2.7360517978668213, 2.74174165725708, 2.7252891063690186, 2.7284796237945557, 2.711374521255493, 2.713064432144165, 2.7146852016448975, 2.7199549674987793, 2.7337028980255127, 2.731966495513916, 2.74367094039917, 2.7536442279815674, 2.7530953884124756, 2.7390124797821045, 2.7338707447052, 2.7251129150390625, 2.719163179397583, 2.718994140625, 2.725552797317505, 2.718616008758545, 2.7200584411621094, 2.696929693222046, 2.6732499599456787, 2.6864166259765625, 2.686400890350342, 2.7011330127716064, 2.692896842956543, 2.680968999862671, 2.6741373538970947, 2.678980827331543, 2.6836342811584473, 2.6788344383239746, 2.6822028160095215, 2.6887259483337402, 2.683444023132324, 2.6738195419311523, 2.6630141735076904, 2.655785083770752, 2.636136531829834, 2.6430721282958984, 2.637737274169922, 2.613218307495117, 2.592082977294922, 2.6185007095336914, 2.625697612762451, 2.6266562938690186, 2.6349575519561768, 2.625216484069824, 2.634354829788208, 2.6342506408691406, 2.6423566341400146, 2.6573128700256348, 2.6613011360168457, 2.6726787090301514, 2.6736690998077393, 2.6758322715759277, 2.686260461807251, 2.6749868392944336, 2.6660261154174805, 2.661086082458496, 2.659806966781616, 2.6538147926330566, 2.6591548919677734, 2.6703405380249023, 2.6708853244781494, 2.6718857288360596, 2.6774790287017822, 2.683332920074463, 2.681835174560547, 2.686041831970215, 2.6782805919647217, 2.667815923690796, 2.6552109718322754, 2.63978910446167, 2.646237850189209, 2.6425557136535645, 2.6501901149749756, 2.644632577896118, 2.636979818344116, 2.622791051864624, 2.627241849899292, 2.6222352981567383, 2.6130309104919434, 2.627955198287964, 2.6339170932769775, 2.6412980556488037, 2.637389659881592, 2.631986618041992, 2.63885760307312, 2.632197618484497, 2.625260353088379, 2.619633674621582, 2.618659496307373, 2.6095290184020996, 2.59355092048645, 2.5965795516967773, 2.591397523880005, 2.5960397720336914, 2.6046125888824463, 2.598672389984131, 2.5901780128479004, 2.5873091220855713, 2.593759536743164, 2.5964510440826416, 2.5938117504119873, 2.590841770172119, 2.5936551094055176, 2.604857921600342, 2.6201975345611572, 2.620579957962036, 2.6253697872161865, 2.6223466396331787, 2.6217000484466553, 2.6219232082366943, 2.625391721725464, 2.6301846504211426, 2.6376147270202637, 2.6395061016082764, 2.6397488117218018, 2.6476552486419678, 2.6458282470703125, 2.6391212940216064, 2.6457695960998535, 2.646179437637329, 2.6484272480010986, 2.6472485065460205, 2.6413381099700928, 2.6438045501708984, 2.648850202560425, 2.6542067527770996, 2.6574230194091797, 2.6539828777313232, 2.648465633392334, 2.6468217372894287, 2.6454174518585205, 2.6469383239746094, 2.6356616020202637, 2.6296298503875732, 2.632344961166382, 2.6329543590545654, 2.6341464519500732, 2.626460552215576, 2.626591205596924, 2.622591018676758, 2.625916004180908, 2.628343105316162, 2.6304402351379395, 2.6258068084716797, 2.615821123123169, 2.609600782394409, 2.608546733856201, 2.6087868213653564, 2.6046371459960938, 2.602860927581787, 2.603775978088379, 2.607269287109375, 2.6087868213653564, 2.608402729034424, 2.6111228466033936, 2.613265037536621, 2.6106247901916504, 2.6086909770965576, 2.611217498779297, 2.613311767578125, 2.6119496822357178, 2.612020492553711, 2.5997602939605713, 2.589831829071045, 2.5900449752807617, 2.5814311504364014, 2.5829880237579346, 2.5844216346740723, 2.5820722579956055, 2.583513021469116, 2.58376145362854, 2.5890305042266846, 2.5879836082458496, 2.5895118713378906, 2.579268455505371, 2.5724732875823975, 2.56945538520813, 2.5747764110565186, 2.5750081539154053, 2.571944236755371, 2.571708917617798, 2.5661733150482178, 2.579268455505371, 2.5794379711151123, 2.58198881149292, 2.585707664489746, 2.58510684967041, 2.5853803157806396, 2.5844764709472656, 2.5856258869171143, 2.589618682861328, 2.5890841484069824, 2.5830156803131104, 2.5797762870788574, 2.575557231903076, 2.5756723880767822, 2.5755858421325684, 2.578134536743164, 2.5835959911346436, 2.5930540561676025, 2.606274366378784, 2.605933666229248, 2.606687545776367, 2.604809045791626, 2.6072933673858643, 2.6126790046691895, 2.6215436458587646, 2.6256539821624756, 2.624448299407959, 2.620490074157715, 2.6240298748016357, 2.6246020793914795, 2.62319016456604, 2.6184096336364746, 2.6102919578552246, 2.610719919204712, 2.6060311794281006, 2.602513551712036, 2.596168279647827, 2.5960912704467773, 2.5959367752075195, 2.581291437149048, 2.5797200202941895, 2.5801703929901123, 2.5739054679870605, 2.5773937702178955, 2.5785322189331055, 2.5700511932373047, 2.579127073287964, 2.57839035987854, 2.5818495750427246, 2.5814311504364014, 2.5801141262054443, 2.581515073776245, 2.5800859928131104, 2.58510684967041, 2.5810399055480957, 2.5843942165374756, 2.588870048522949, 2.5877950191497803, 2.588923454284668, 2.5958592891693115, 2.5976545810699463, 2.5945661067962646, 2.5957820415496826, 2.5988500118255615, 2.6022400856018066, 2.6027369499206543, 2.60429310798645, 2.5996339321136475, 2.580955743789673, 2.581542730331421, 2.579211950302124, 2.577165126800537, 2.577136516571045, 2.577279567718506, 2.576563835144043, 2.5811798572540283, 2.58276629447937, 2.583209276199341, 2.582322359085083, 2.5800015926361084, 2.577136516571045, 2.578418731689453, 2.58042311668396, 2.5871469974517822, 2.588413953781128, 2.58836030960083, 2.5865509510040283, 2.580451250076294, 2.581737995147705, 2.5833473205566406, 2.5875792503356934, 2.584449052810669, 2.5855987071990967, 2.5855987071990967, 2.5847508907318115, 2.582127809524536, 2.5848605632781982, 2.584887981414795, 2.585270881652832, 2.584778308868408, 2.5861432552337646, 2.586388111114502, 2.584805727005005, 2.5764777660369873, 2.576305389404297, 2.576592445373535, 2.5779354572296143, 2.585707664489746, 2.5864696502685547, 2.5855712890625, 2.585188865661621, 2.582960367202759, 2.583292245864868, 2.584613800048828, 2.5835959911346436, 2.5828771591186523, 2.583402633666992, 2.582627773284912, 2.5823779106140137, 2.582322359085083, 2.5821001529693604, 2.5799453258514404, 2.5778214931488037, 2.5777931213378906, 2.578674077987671, 2.5800578594207764, 2.579353094100952, 2.5801703929901123, 2.581207513809204, 2.581291437149048, 2.583871603012085, 2.582655668258667, 2.5805916786193848, 2.580451250076294, 2.5800859928131104, 2.580507516860962, 2.5799169540405273, 2.5728249549865723, 2.555856704711914, 2.556081533432007, 2.5547595024108887, 2.556755304336548, 2.545255422592163, 2.5214717388153076, 2.517376661300659, 2.5178935527801514, 2.5091049671173096, 2.5084784030914307, 2.4881248474121094, 2.491971254348755, 2.4846248626708984, 2.500260353088379, 2.5068390369415283, 2.500741481781006, 2.501221179962158, 2.503472089767456, 2.4951975345611572, 2.4941587448120117, 2.4904518127441406, 2.4836668968200684, 2.466503143310547, 2.468289613723755, 2.474052667617798, 2.4736979007720947, 2.4688117504119873, 2.4700586795806885, 2.4674510955810547, 2.460434913635254, 2.4579050540924072, 2.4602162837982178, 2.466608762741089, 2.4747602939605713, 2.4863831996917725, 2.4942946434020996, 2.4965438842773438, 2.491328001022339, 2.496767282485962, 2.502739429473877, 2.487138271331787, 2.4873266220092773, 2.4841463565826416, 2.4831862449645996, 2.4911439418792725, 2.4874205589294434, 2.485957145690918, 2.4830899238586426, 2.488826036453247, 2.4892454147338867, 2.4867610931396484, 2.4886391162872314, 2.4916040897369385, 2.4973466396331787, 2.4964096546173096, 2.4966330528259277, 2.4869496822357178, 2.4790964126586914, 2.478405714035034, 2.4760169982910156, 2.476217269897461, 2.4790472984313965, 2.4773147106170654, 2.477165460586548, 2.477860927581787, 2.4804210662841797, 2.4779105186462402, 2.47390079498291, 2.4746594429016113, 2.4756157398223877, 2.474810838699341, 2.4750123023986816, 2.476067066192627, 2.4749114513397217, 2.4695920944213867, 2.4673986434936523, 2.4690723419189453, 2.4688117504119873, 2.4686031341552734, 2.4688117504119873, 2.46855092048645, 2.467923402786255, 2.468132734298706, 2.4657092094421387, 2.4652843475341797, 2.4699552059173584, 2.4785044193267822, 2.478257179260254, 2.477860927581787, 2.476567268371582, 2.4768667221069336, 2.476067066192627, 2.476067066192627, 2.4768667221069336, 2.477015972137451, 2.485577344894409, 2.49206280708313, 2.492429256439209, 2.4923834800720215, 2.4931137561798096, 2.4952876567840576, 2.4959168434143066, 2.4868083000183105, 2.4868555068969727, 2.486666679382324, 2.4873266220092773, 2.4875147342681885, 2.486997127532959, 2.486666679382324, 2.4831862449645996, 2.4851016998291016, 2.488452196121216, 2.4897103309631348, 2.4902665615081787, 2.4911439418792725, 2.491189956665039, 2.4899423122406006, 2.489105701446533, 2.489105701446533, 2.4892454147338867, 2.4906365871429443, 2.4952876567840576, 2.4944756031036377, 2.494701623916626, 2.495152711868286, 2.4959616661071777, 2.496185779571533, 2.4949722290039062, 2.4950172901153564, 2.494520902633667, 2.4938414096832275, 2.491971254348755, 2.495647430419922, 2.4964544773101807, 2.4985885620117188, 2.4996023178100586, 2.4996023178100586, 2.501134157180786, 2.5220160484313965, 2.5362653732299805, 2.534318208694458, 2.5347166061401367, 2.5369813442230225, 2.540133237838745, 2.5431618690490723, 2.5423312187194824, 2.544468641281128, 2.5397112369537354, 2.5368025302886963, 2.5383336544036865, 2.538120985031128, 2.5397112369537354, 2.539006233215332, 2.5430235862731934, 2.5445716381073, 2.544400215148926, 2.5461063385009766, 2.545118808746338, 2.5461063385009766, 2.553427219390869, 2.5520195960998535, 2.5501694679260254, 2.5440568923950195, 2.54412579536438, 2.544400215148926, 2.543713092803955, 2.5445716381073, 2.5475945472717285, 2.5502028465270996, 2.559014081954956, 2.560462474822998, 2.5613386631011963, 2.5592668056488037, 2.5657474994659424, 2.5651371479034424, 2.5706746578216553, 2.577450752258301, 2.5908946990966797, 2.599961757659912, 2.6059579849243164, 2.619746446609497, 2.6210737228393555, 2.6198368072509766, 2.603182792663574, 2.6041948795318604, 2.6180226802825928, 2.617748975753784, 2.6186141967773438, 2.6173832416534424, 2.6131479740142822, 2.619452714920044, 2.620535135269165, 2.6223244667053223, 2.6365277767181396, 2.6355583667755127, 2.6340630054473877, 2.633812665939331, 2.627868890762329, 2.6318180561065674, 2.634354829788208, 2.6317334175109863, 2.6170170307159424, 2.6149423122406006, 2.614431381225586, 2.618523359298706, 2.6186141967773438, 2.61535906791687, 2.615682601928711, 2.6173832416534424, 2.620422601699829, 2.6234335899353027, 2.627458333969116, 2.635413408279419, 2.642873764038086, 2.6457109451293945, 2.6444554328918457, 2.6442387104034424, 2.647925615310669, 2.656541347503662, 2.6730148792266846, 2.6738028526306152, 2.6752192974090576, 2.682617425918579, 2.702906847000122, 2.6905035972595215, 2.6886026859283447, 2.6911277770996094, 2.6865413188934326, 2.6726112365722656, 2.6659910678863525, 2.6828880310058594, 2.6893715858459473, 2.6797432899475098, 2.6836183071136475, 2.682250738143921, 2.689540386199951, 2.700456142425537, 2.7025792598724365, 2.6868526935577393, 2.6867594718933105, 2.6842668056488037, 2.67526912689209, 2.6679024696350098, 2.666705846786499, 2.6525206565856934, 2.6612114906311035, 2.6685237884521484, 2.6719870567321777, 2.6704087257385254, 2.6683342456817627, 2.661731004714966, 2.6550254821777344, 2.6607625484466553, 2.656283378601074, 2.6553962230682373, 2.650989294052124, 2.6520681381225586, 2.651898145675659, 2.6575326919555664, 2.6617488861083984, 2.6687474250793457, 2.6739699840545654, 2.674370765686035, 2.688263416290283, 2.689678192138672, 2.688972234725952, 2.690274715423584, 2.69067120552063, 2.6870551109313965, 2.6839349269866943, 2.683950662612915, 2.6800665855407715, 2.6689538955688477, 2.660888433456421, 2.664623260498047, 2.6657464504241943, 2.665501356124878, 2.671004295349121, 2.6734344959259033, 2.6641650199890137, 2.6613011360168457, 2.664992570877075, 2.6630499362945557, 2.6621241569519043, 2.6664445400238037, 2.663670301437378, 2.6617846488952637, 2.6516337394714355, 2.6520681381225586, 2.6530468463897705, 2.653571605682373, 2.6590640544891357, 2.66934871673584, 2.670186996459961, 2.6703405380249023, 2.6716995239257812, 2.667781352996826, 2.668489456176758, 2.665921211242676, 2.66458797454834, 2.661731004714966, 2.664482593536377, 2.666078567504883, 2.6664795875549316, 2.6661832332611084, 2.665361166000366, 2.664482593536377, 2.6640238761901855, 2.6603844165802, 2.6636879444122314, 2.6613011360168457, 2.660348415374756, 2.6586098670959473, 2.650418758392334, 2.6302061080932617, 2.6257851123809814, 2.6212306022644043, 2.6238315105438232, 2.622079372406006, 2.6218340396881104, 2.6280629634857178, 2.62589430809021, 2.625675916671753, 2.623875379562378, 2.6229021549224854, 2.620377540588379, 2.6215436458587646, 2.6247777938842773, 2.6275014877319336, 2.632134437561035, 2.628901958465576, 2.627523183822632, 2.6280415058135986, 2.627068519592285, 2.6260905265808105, 2.6218340396881104, 2.621297597885132, 2.619565963745117, 2.6195433139801025, 2.620692253112793, 2.6252384185791016, 2.6246681213378906, 2.6142454147338867, 2.6073174476623535, 2.617131471633911, 2.647578001022339, 2.6492340564727783, 2.6503617763519287, 2.648869514465332, 2.6455740928649902, 2.6451430320739746, 2.6447503566741943, 2.6470158100128174, 2.646101474761963, 2.6468801498413086, 2.648273229598999, 2.648369550704956, 2.6466662883758545, 2.6415581703186035, 2.637880325317383, 2.63968825340271, 2.6378190517425537, 2.6367127895355225, 2.637143850326538, 2.635723829269409, 2.633080244064331, 2.6314585208892822, 2.632723569869995, 2.6300995349884033, 2.624558210372925, 2.623941659927368, 2.618295907974243, 2.605151891708374, 2.602389097213745, 2.6012423038482666, 2.5953691005706787, 2.6001381874084473, 2.6027369499206543, 2.60969614982605, 2.6106483936309814, 2.6124911308288574, 2.604980707168579, 2.603874683380127, 2.603973150253296, 2.6038498878479004, 2.604759693145752, 2.6068332195281982, 2.609720230102539, 2.6095528602600098, 2.6081862449645996, 2.604489803314209, 2.6026625633239746, 2.601442337036133, 2.602463722229004, 2.6011171340942383, 2.5992043018341064, 2.5981132984161377, 2.5921356678009033, 2.5906827449798584, 2.6004650592803955, 2.601292371749878, 2.601217031478882, 2.6029105186462402, 2.609074592590332, 2.605029344558716, 2.603084087371826, 2.6029601097106934]}, {\"line\": {\"color\": \"blue\", \"width\": 1}, \"type\": \"scatter\", \"x\": [1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431], \"y\": [2.602587938308716, 2.6050050258636475, 2.6076316833496094, 2.6056652069091797, 2.6033313274383545, 2.601966381072998, 2.5991790294647217, 2.6022651195526123, 2.6057140827178955, 2.6084506511688232, 2.6104824542999268, 2.618954658508301, 2.626068592071533, 2.618727922439575, 2.6185007095336914, 2.622702121734619, 2.6232786178588867, 2.623389482498169, 2.633164167404175, 2.636753797531128, 2.6349990367889404, 2.638613700866699, 2.6393847465515137, 2.6384103298187256, 2.634521245956421, 2.6345627307891846, 2.6349990367889404, 2.630291223526001, 2.629030466079712, 2.6277177333831787, 2.6302273273468018, 2.636054039001465, 2.6422369480133057, 2.6481189727783203, 2.648984670639038, 2.648446559906006, 2.64896559715271, 2.649061441421509, 2.648253917694092, 2.645867347717285, 2.6495022773742676, 2.654876947402954, 2.6565780639648438, 2.6570377349853516, 2.6617488861083984, 2.6590278148651123, 2.659680128097534, 2.666130781173706, 2.665395975112915, 2.665097951889038, 2.6629788875579834, 2.6587369441986084, 2.6551926136016846, 2.6573495864868164, 2.6618919372558594, 2.6531219482421875, 2.6545422077178955, 2.654132127761841, 2.6575326919555664, 2.655785083770752, 2.6513874530792236, 2.6425557136535645, 2.6372666358947754, 2.635268449783325, 2.634936809539795, 2.6324713230133057, 2.6362600326538086, 2.6370205879211426, 2.6365277767181396, 2.641738176345825, 2.643230676651001, 2.64225697517395, 2.6328284740448, 2.61742901802063, 2.6190452575683594, 2.61674165725708, 2.5879297256469727, 2.545016288757324, 2.549170732498169, 2.5463099479675293, 2.5354385375976562, 2.5416016578674316, 2.541705846786499, 2.5578722953796387, 2.576592445373535, 2.5730884075164795, 2.5721209049224854, 2.5710296630859375, 2.586116075515747, 2.5869033336639404, 2.5871200561523438, 2.5876331329345703, 2.5748634338378906, 2.571826696395874, 2.575960397720337, 2.581207513809204, 2.577336549758911, 2.5886287689208984, 2.591291666030884, 2.589991569519043, 2.5906827449798584, 2.598037004470825, 2.6037511825561523, 2.6030097007751465, 2.6031582355499268, 2.5954205989837646, 2.5919249057769775, 2.5945401191711426, 2.5892446041107178, 2.592660903930664, 2.5905234813690186, 2.5931589603424072, 2.5978329181671143, 2.5992298126220703, 2.5999367237091064, 2.5971436500549316, 2.592372179031372, 2.5955755710601807, 2.602811336517334, 2.608834981918335, 2.6093618869781494, 2.610363245010376, 2.613241672515869, 2.6137325763702393, 2.6244263648986816, 2.6388981342315674, 2.636733293533325, 2.638308525085449, 2.640716075897217, 2.6367948055267334, 2.6393239498138428, 2.6434288024902344, 2.649961233139038, 2.658755302429199, 2.6552109718322754, 2.6350820064544678, 2.6352269649505615, 2.6366920471191406, 2.6420376300811768, 2.65144419670105, 2.6513683795928955, 2.6492340564727783, 2.6533281803131104, 2.655285120010376, 2.654393196105957, 2.654337167739868, 2.647209882736206, 2.643982410430908, 2.6455347537994385, 2.644021987915039, 2.637798547744751, 2.638674736022949, 2.6419179439544678, 2.6475584506988525, 2.6503043174743652, 2.651198148727417, 2.6521060466766357, 2.6525392532348633, 2.6583364009857178, 2.652162551879883, 2.6549510955810547, 2.6558775901794434, 2.6539454460144043, 2.6537210941314697, 2.6553778648376465, 2.655118227005005, 2.6564862728118896, 2.654001474380493, 2.6496171951293945, 2.64994215965271, 2.649578809738159, 2.6464133262634277, 2.651008129119873, 2.650285482406616, 2.649655342102051, 2.648253917694092, 2.647751808166504, 2.648542642593384, 2.650818347930908, 2.6538147926330566, 2.650399684906006, 2.6464133262634277, 2.6455936431884766, 2.6441009044647217, 2.6429927349090576, 2.6442387104034424, 2.6448683738708496, 2.645691394805908, 2.645280361175537, 2.6437454223632812, 2.643982410430908, 2.6434485912323, 2.6460819244384766, 2.6471710205078125, 2.6486003398895264, 2.648369550704956, 2.645925760269165, 2.646763563156128, 2.6470158100128174, 2.6474812030792236, 2.646237850189209, 2.646354913711548, 2.6446127891540527, 2.6446127891540527, 2.6452410221099854, 2.6451821327209473, 2.645612955093384, 2.6479837894439697, 2.6490423679351807, 2.652162551879883, 2.652237892150879, 2.6534030437469482, 2.657276153564453, 2.667729616165161, 2.6772494316101074, 2.6790459156036377, 2.678084373474121, 2.6806795597076416, 2.686946153640747, 2.6829676628112793, 2.681627035140991, 2.6815309524536133, 2.684849977493286, 2.689448356628418, 2.6888644695281982, 2.688309669494629, 2.6881861686706543, 2.6915526390075684, 2.6876444816589355, 2.6849286556243896, 2.6862916946411133, 2.689417600631714, 2.691309928894043, 2.690929889678955, 2.693932294845581, 2.695674419403076, 2.690152406692505, 2.69012188911438, 2.6893868446350098, 2.686447858810425, 2.6874587535858154, 2.6891872882843018, 2.685791492462158, 2.6840457916259766, 2.6831741333007812, 2.684597969055176, 2.68566632270813, 2.687241554260254, 2.6884021759033203, 2.6912035942077637, 2.686821699142456, 2.678410768508911, 2.6672098636627197, 2.6652207374572754, 2.6639177799224854, 2.663191795349121, 2.6630499362945557, 2.663634777069092, 2.6663575172424316, 2.6657464504241943, 2.666966676712036, 2.667400598526001, 2.6692628860473633, 2.673769474029541, 2.6757993698120117, 2.6757826805114746, 2.676311492919922, 2.677757501602173, 2.6762619018554688, 2.6723246574401855, 2.6686270236968994, 2.668109655380249, 2.6681442260742188, 2.672105550765991, 2.672863721847534, 2.6728806495666504, 2.675584077835083, 2.673065423965454, 2.6733171939849854, 2.6735684871673584, 2.669656991958618, 2.670016288757324, 2.6709532737731934, 2.672628164291382, 2.6727795600891113, 2.6711912155151367, 2.673082113265991, 2.6772165298461914, 2.682856321334839, 2.683317184448242, 2.684645175933838, 2.684755563735962, 2.6840298175811768, 2.6841561794281006, 2.6834123134613037, 2.68306303024292, 2.684408664703369, 2.686743974685669, 2.690823554992676, 2.699718713760376, 2.7065846920013428, 2.7066822052001953, 2.707932472229004, 2.7083749771118164, 2.7085683345794678, 2.712498426437378, 2.7137627601623535, 2.711944103240967, 2.713507890701294, 2.717529773712158, 2.7179622650146484, 2.7157089710235596, 2.7158548831939697, 2.7196178436279297, 2.729808807373047, 2.740196943283081, 2.7375850677490234, 2.7356841564178467, 2.7379136085510254, 2.7372207641601562, 2.7402780055999756, 2.7441020011901855, 2.7482457160949707, 2.745523691177368, 2.7450172901153564, 2.7490739822387695, 2.7567121982574463, 2.7650415897369385, 2.7646377086639404, 2.769465208053589, 2.773195743560791, 2.770505428314209, 2.77063250541687, 2.774559736251831, 2.7766146659851074, 2.7602272033691406, 2.755897045135498, 2.7592713832855225, 2.766197919845581, 2.775075674057007, 2.778609037399292, 2.7755424976348877, 2.778646469116211, 2.777266025543213, 2.776008129119873, 2.777435779571533, 2.7779622077941895, 2.775580644607544, 2.768972396850586, 2.769376516342163, 2.7657361030578613, 2.770925760269165, 2.7771153450012207, 2.7775864601135254, 2.779327869415283, 2.785771608352661, 2.8072826862335205, 2.809471845626831, 2.8131165504455566, 2.815060615539551, 2.8124663829803467, 2.811119794845581, 2.8148159980773926, 2.8128535747528076, 2.818235158920288, 2.827307939529419, 2.839785099029541, 2.839231252670288, 2.837214708328247, 2.846759080886841, 2.851492166519165, 2.853874444961548, 2.862555742263794, 2.878046751022339, 2.869215250015259, 2.8713982105255127, 2.8854238986968994, 2.9011456966400146, 2.909304618835449, 2.9120171070098877, 2.909032106399536, 2.882613182067871, 2.8854546546936035, 2.88438081741333, 2.9033420085906982, 2.8986003398895264, 2.8955297470092773, 2.889622449874878, 2.8914883136749268, 2.895554304122925, 2.8870768547058105, 2.875396966934204, 2.869164228439331, 2.8719565868377686, 2.8713648319244385, 2.8775014877319336, 2.8703043460845947, 2.86529541015625, 2.8686368465423584, 2.8863425254821777, 2.8812575340270996, 2.8780198097229004, 2.879357099533081, 2.884567975997925, 2.8939921855926514, 2.89876389503479, 2.8991334438323975, 2.909367799758911, 2.9043233394622803, 2.9143590927124023, 2.9381165504455566, 2.934542655944824, 2.936400890350342, 2.9415905475616455, 2.94036602973938, 2.9452664852142334, 2.942584276199341, 2.946284770965576, 2.9537057876586914, 2.957228660583496, 2.9612045288085938, 2.9718103408813477, 2.97419810295105, 2.966796398162842, 2.9467861652374268, 2.949740171432495, 2.950470447540283, 2.9379851818084717, 2.939823627471924, 2.930784225463867, 2.9415602684020996, 2.945492744445801, 2.9519577026367188, 2.9483842849731445, 2.9420409202575684, 2.9453442096710205, 2.951197624206543, 2.9532835483551025, 2.9642746448516846, 2.96885085105896, 2.971784830093384, 2.9736216068267822, 2.979649305343628, 2.9848005771636963, 2.9744791984558105, 2.9687986373901367, 2.971377372741699, 2.9787418842315674, 2.9781272411346436, 2.979661703109741, 2.9750845432281494, 2.9734978675842285, 2.9666976928710938, 2.9681553840637207, 2.958012104034424, 2.9617135524749756, 2.968259811401367, 2.970404624938965, 2.973843574523926, 2.978346586227417, 2.97942852973938, 2.9799437522888184, 2.981158971786499, 2.980142593383789, 2.9763450622558594, 2.9775290489196777, 2.978689670562744, 2.974055767059326, 2.974684476852417, 2.9779136180877686, 2.982975482940674, 2.9827582836151123, 2.9837028980255127, 2.989724636077881, 2.993426561355591, 2.9918878078460693, 2.9885077476501465, 2.9872589111328125, 2.971203565597534, 2.9721786975860596, 2.9693777561187744, 2.969163179397583, 2.9637491703033447, 2.950105667114258, 2.9507768154144287, 2.9492578506469727, 2.9610073566436768]}, {\"line\": {\"color\": \"green\", \"width\": 1}, \"type\": \"scatter\", \"x\": [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431], \"y\": [2.0887667387723923, 2.0912162251770496, 2.0912162251770496, 2.0887667387723923, 2.0887667387723923, 2.0965179838240147, 2.0686159953475, 2.0686159953475, 2.0686159953475, 2.0923248268663883, 2.0923248268663883, 2.0686159953475, 2.07574175670743, 2.069478951394558, 2.069478951394558, 2.0735077001154423, 2.0630937218666077, 2.0630937218666077, 2.0630937218666077, 2.0630937218666077, 2.0777296870946884, 2.0777296870946884, 2.0630937218666077, 2.0630937218666077, 2.0630937218666077, 2.0630937218666077, 2.0630937218666077, 2.0630937218666077, 2.0630937218666077, 2.0630937218666077, 2.0735077001154423, 2.0777296870946884, 2.0735077001154423, 2.0735077001154423, 2.0735077001154423, 2.0735077001154423, 2.0735077001154423, 2.0735077001154423, 2.0735077001154423, 2.0735077001154423, 2.0735077001154423, 2.0735077001154423, 2.0523600205779076, 2.0523600205779076, 2.0523600205779076, 2.0523600205779076, 2.0523600205779076, 2.0523600205779076, 2.0523600205779076, 2.0735077001154423, 2.0523600205779076, 2.0523600205779076, 2.0523600205779076, 2.0523600205779076, 2.0523600205779076, 2.0523600205779076, 2.0523600205779076, 2.0523600205779076, 2.0523600205779076, 2.0523600205779076, 2.0735077001154423, 2.0735077001154423, 2.0735077001154423, 2.0735077001154423, 2.0735077001154423, 2.0735077001154423, 2.0735077001154423, 2.0735077001154423, 2.0735077001154423, 2.0735077001154423, 2.0735077001154423, 2.0735077001154423, 2.0735077001154423, 2.0711530447006226, 2.065528705716133, 2.0553237944841385, 2.038963157683611, 2.038963157683611, 2.038963157683611, 2.0455527156591415, 2.038963157683611, 2.0455527156591415, 2.0455527156591415, 2.038963157683611, 2.038963157683611, 2.0361831150949, 2.0361831150949, 2.0455527156591415, 2.0361831150949, 2.038963157683611, 2.0361831150949, 2.0361831150949, 2.0361831150949, 2.0361831150949, 2.0361831150949, 2.0361831150949, 2.0361831150949, 2.0361831150949, 2.0361831150949, 2.0361831150949, 2.0361831150949, 2.0361831150949, 2.0553237944841385, 2.0455527156591415, 2.038963157683611, 2.0455527156591415, 2.038963157683611, 2.038963157683611, 2.038963157683611, 2.038963157683611, 1.9598587527871132, 2.0297519341111183, 2.0297519341111183, 2.0297519341111183, 2.0297519341111183, 2.0297519341111183, 2.0297519341111183, 2.0297519341111183, 1.9598587527871132, 1.9598587527871132, 1.9598587527871132, 1.9598587527871132, 2.0297519341111183, 1.9598587527871132, 1.9598587527871132, 1.9598587527871132, 2.0034421421587467, 1.969576571136713, 1.969576571136713, 1.9735962990671396, 1.969576571136713, 1.969576571136713, 1.969576571136713, 1.9735962990671396, 1.9598587527871132, 1.9735962990671396, 1.9735962990671396, 1.9735962990671396, 1.9735962990671396, 1.9735962990671396, 1.969576571136713, 1.969576571136713, 1.969576571136713, 1.969576571136713, 1.969576571136713, 1.969576571136713, 1.969576571136713, 1.9947524704039097, 1.9947524704039097, 1.9947524704039097, 1.9897638466209173, 1.9897638466209173, 1.9897638466209173, 1.9897638466209173, 1.9947524704039097, 1.9947524704039097, 1.9897638466209173, 1.9947524704039097, 1.9897638466209173, 1.9897638466209173, 1.9897638466209173, 1.9897638466209173, 1.9897638466209173, 1.9897638466209173, 1.9947524704039097, 1.9947524704039097, 1.9897638466209173, 1.9897638466209173, 1.9897638466209173, 1.985023507848382, 1.9915718268603086, 1.9876525215804577, 2.007796235382557, 1.9986933171749115, 2.007796235382557, 2.0034421421587467, 2.0034421421587467, 1.9598587527871132, 1.9598587527871132, 2.0297519341111183, 2.0297519341111183, 2.0297519341111183, 2.021515369415283, 2.021515369415283, 1.9986933171749115, 2.021515369415283, 1.9598587527871132, 1.9598587527871132, 2.007796235382557, 1.9986933171749115, 1.9915718268603086, 1.9986933171749115, 1.9915718268603086, 1.9915718268603086, 2.0034421421587467, 2.0034421421587467, 1.9915718268603086, 1.9915718268603086, 1.9897638466209173, 1.9947524704039097, 1.9915718268603086, 1.9915718268603086, 2.007796235382557, 1.9598587527871132, 1.9986933171749115, 1.9986933171749115, 1.985023507848382, 1.985023507848382, 1.9876525215804577, 1.9876525215804577, 1.985023507848382, 1.969576571136713, 1.9735962990671396, 1.9735962990671396, 1.9735962990671396, 1.9735962990671396, 1.9735962990671396, 1.9735962990671396, 1.9735962990671396, 1.9735962990671396, 1.9735962990671396, 1.9735962990671396, 1.9735962990671396, 1.9735962990671396, 1.9735962990671396, 1.969576571136713, 1.969576571136713, 1.969576571136713, 1.969576571136713, 1.969576571136713, 1.9947524704039097, 2.007796235382557, 1.9986933171749115, 1.9986933171749115, 1.9887300059199333, 1.9986933171749115, 1.9986933171749115, 2.021515369415283, 1.9986933171749115, 2.021515369415283, 2.021515369415283, 2.021515369415283, 1.9986933171749115, 1.9855172168463469, 1.9826968405395746, 1.9826968405395746, 1.9826968405395746, 1.9876525215804577, 1.9876525215804577, 1.9876525215804577, 1.9787683989852667, 1.9826968405395746, 1.9826968405395746, 1.9826968405395746, 1.9826968405395746, 1.9826968405395746, 1.9826968405395746, 1.9876525215804577, 1.979550739750266, 1.9876525215804577, 2.0058882869780064, 1.979550739750266, 1.9876525215804577, 1.9826968405395746, 1.9826968405395746, 1.9826968405395746, 1.9876525215804577, 1.9876525215804577, 1.9887300059199333, 2.0034421421587467, 2.0034421421587467, 1.985023507848382, 1.9819544684141874, 1.9897638466209173, 1.9897638466209173, 1.9897638466209173, 1.9897638466209173, 1.9897638466209173, 1.9947524704039097, 1.9947524704039097, 1.985023507848382, 1.9947524704039097, 1.9897638466209173, 1.9897638466209173, 1.9897638466209173, 1.9897638466209173, 1.9897638466209173, 1.9897638466209173, 1.9897638466209173, 1.985023507848382, 1.985023507848382, 1.9855172168463469, 1.9826968405395746, 1.985023507848382, 1.985023507848382, 1.9876525215804577, 1.9876525215804577, 2.021515369415283, 2.021515369415283, 2.0137355886399746, 2.0137355886399746, 2.0058882869780064, 2.0137355886399746, 2.0137355886399746, 2.0137355886399746, 2.015414860099554, 2.015414860099554, 2.0058882869780064, 2.0058882869780064, 2.0058882869780064, 2.0058882869780064, 2.0058882869780064, 2.0058882869780064, 2.015414860099554, 1.979550739750266, 2.015414860099554, 2.015414860099554, 2.0058882869780064, 2.0058882869780064, 2.0058882869780064, 2.0058882869780064, 2.0137355886399746, 2.0058882869780064, 2.015414860099554, 2.0058882869780064, 2.0058882869780064, 2.0058882869780064, 2.0137355886399746, 2.0137355886399746, 2.038963157683611, 2.0058882869780064, 2.0058882869780064, 2.0058882869780064, 2.0058882869780064, 2.0058882869780064, 1.979550739750266, 2.0058882869780064, 2.0058882869780064, 1.979550739750266, 1.9826968405395746, 1.9787683989852667, 2.0058882869780064, 2.0058882869780064, 2.0058882869780064, 2.0058882869780064, 1.979550739750266, 1.9876525215804577, 1.979550739750266, 1.979550739750266, 1.9787683989852667, 1.9787683989852667, 1.9787683989852667, 1.9787683989852667, 1.9876525215804577, 1.9787683989852667, 1.9876525215804577, 1.9787683989852667, 1.9826968405395746, 1.9826968405395746, 1.9826968405395746, 1.9826968405395746, 1.9826968405395746, 1.9826968405395746, 1.9826968405395746, 1.9826968405395746, 1.9826968405395746, 1.9826968405395746, 1.9826968405395746, 1.9826968405395746, 1.9826968405395746, 1.9819544684141874, 1.9876525215804577, 1.9887300059199333, 1.9876525215804577, 1.985023507848382, 1.9947524704039097, 1.9897638466209173, 1.9897638466209173, 1.9897638466209173, 1.9897638466209173, 1.9897638466209173, 1.9897638466209173, 1.9897638466209173, 1.9897638466209173, 1.9897638466209173, 1.9897638466209173, 1.9897638466209173, 1.969576571136713, 1.9897638466209173, 1.9947524704039097, 1.9897638466209173, 1.9947524704039097, 1.9947524704039097, 1.9947524704039097, 1.9947524704039097, 1.969576571136713, 1.969576571136713, 1.969576571136713, 1.9735962990671396, 1.9598587527871132, 1.9598587527871132, 1.9598587527871132, 1.9598587527871132, 1.9598587527871132, 1.9598587527871132, 1.9598587527871132, 2.04878718405962, 2.042994238436222, 2.0297519341111183, 2.0297519341111183, 2.0297519341111183, 2.0297519341111183, 2.0297519341111183, 2.038963157683611, 2.038963157683611, 2.0455527156591415, 2.065528705716133, 2.0686159953475, 2.0686159953475, 2.0455527156591415, 2.038963157683611, 2.038963157683611, 2.038963157683611, 2.0455527156591415, 2.0923248268663883, 2.0923248268663883, 2.0923248268663883, 2.0686159953475, 2.0455527156591415, 2.038963157683611, 2.0455527156591415, 2.0455527156591415, 2.038963157683611, 2.038963157683611, 2.0361831150949, 2.0361831150949, 2.0361831150949, 2.038963157683611, 2.038963157683611, 2.0455527156591415, 2.038963157683611, 2.0455527156591415, 2.0361831150949, 2.038963157683611, 2.038963157683611, 2.0455527156591415, 2.0965179838240147, 2.0686159953475, 2.0686159953475, 2.0946274511516094, 2.0965179838240147, 2.0946274511516094, 2.0946274511516094, 2.0946274511516094, 2.0946274511516094, 2.0946274511516094, 2.0946274511516094, 2.0946274511516094, 2.0946274511516094, 2.0946274511516094, 2.0946274511516094, 2.0946274511516094, 2.0946274511516094, 2.0946274511516094, 2.0946274511516094, 2.0946274511516094, 2.0946274511516094, 2.0946274511516094, 2.0946274511516094, 2.0946274511516094, 2.0946274511516094, 2.0946274511516094, 2.0946274511516094, 2.0946274511516094, 2.0946274511516094, 2.0946274511516094, 2.0946274511516094, 2.0946274511516094, 2.0923248268663883, 2.0923248268663883, 2.0923248268663883, 2.0923248268663883, 2.0923248268663883, 2.0946274511516094, 2.0923248268663883, 2.0923248268663883, 2.0923248268663883, 2.0923248268663883, 2.0923248268663883, 2.0923248268663883, 2.0923248268663883, 2.069478951394558, 2.069478951394558, 2.069478951394558, 2.07574175670743, 2.07574175670743, 2.07574175670743, 2.0777296870946884, 2.0777296870946884, 2.0777296870946884, 2.0777296870946884, 2.0777296870946884, 2.07574175670743, 2.07574175670743, 2.0946274511516094, 2.07574175670743, 2.069478951394558, 2.069478951394558, 2.0735077001154423, 2.069478951394558, 2.069478951394558, 2.0923248268663883, 2.0923248268663883, 2.0923248268663883, 2.0923248268663883, 2.0923248268663883, 2.0686159953475, 2.0923248268663883, 2.0946274511516094, 2.0946274511516094, 2.0946274511516094, 2.0912162251770496, 2.0946274511516094, 2.0912162251770496, 2.0912162251770496, 2.0912162251770496, 2.0912162251770496, 2.0912162251770496, 2.0912162251770496, 2.0912162251770496, 2.0946274511516094, 2.0946274511516094, 2.0946274511516094, 2.0946274511516094, 2.0946274511516094, 2.0912162251770496, 2.0912162251770496, 2.098162781447172, 2.0842647813260555, 2.088059216737747, 2.085799664258957, 2.085799664258957, 2.088059216737747, 2.088059216737747, 2.085799664258957, 2.085799664258957, 2.088059216737747, 2.085799664258957, 2.085799664258957, 2.088059216737747, 2.088059216737747, 2.088059216737747, 2.088059216737747, 2.088059216737747, 2.088059216737747, 2.088059216737747, 2.0897306948900223, 2.088059216737747, 2.088059216737747, 2.088059216737747, 2.088059216737747, 2.088059216737747, 2.088059216737747, 2.0897306948900223, 2.0897306948900223, 2.088059216737747, 2.088059216737747, 2.088059216737747, 2.0897306948900223, 2.0897306948900223, 2.0897306948900223, 2.0897306948900223, 2.0897306948900223, 2.0897306948900223, 2.0897306948900223, 2.0897306948900223, 2.0897306948900223, 2.0897306948900223, 2.0897306948900223, 2.088059216737747, 2.088059216737747, 2.088059216737747, 2.085799664258957, 2.085799664258957, 2.085799664258957, 2.085799664258957, 2.1016892418265343, 2.1016892418265343, 2.1016892418265343, 2.1016892418265343, 2.1016892418265343, 2.1016892418265343, 2.1016892418265343, 2.1016892418265343, 2.1016892418265343, 2.1016892418265343, 2.1196436882019043, 2.1016892418265343, 2.1016892418265343, 2.1016892418265343, 2.1016892418265343, 2.1016892418265343, 2.1016892418265343, 2.1016892418265343, 2.1016892418265343, 2.1016892418265343, 2.1016892418265343, 2.098162781447172, 2.098162781447172, 2.1026874631643295, 2.098162781447172, 2.098162781447172, 2.098162781447172, 2.098162781447172, 2.098162781447172, 2.098162781447172, 2.098162781447172, 2.1016892418265343, 2.1196436882019043, 2.1196436882019043, 2.1016892418265343, 2.1196436882019043, 2.1196436882019043, 2.1196436882019043, 2.1196436882019043, 2.1196436882019043, 2.1196436882019043, 2.1196436882019043, 2.1196436882019043, 2.1196436882019043, 2.1729828603565693, 2.1729828603565693, 2.1729828603565693, 2.1729828603565693, 2.1729828603565693, 2.1729828603565693, 2.1729828603565693, 2.1729828603565693, 2.1729828603565693, 2.1729828603565693, 2.1729828603565693, 2.1729828603565693, 2.1659925021231174, 2.1659925021231174, 2.1659925021231174, 2.1659925021231174, 2.1659925021231174, 2.1659925021231174, 2.1659925021231174, 2.1946791410446167, 2.1659925021231174, 2.1659925021231174, 2.1659925021231174, 2.1659925021231174, 2.1659925021231174, 2.168699238449335, 2.168699238449335, 2.168699238449335, 2.168699238449335, 2.168699238449335, 2.1729828603565693, 2.1729828603565693, 2.1729828603565693, 2.1729828603565693, 2.1659925021231174, 2.1729828603565693, 2.1659925021231174, 2.1659925021231174, 2.1729828603565693, 2.1659925021231174, 2.1729828603565693, 2.1729828603565693, 2.1729828603565693, 2.1729828603565693, 2.1729828603565693, 2.1729828603565693, 2.1729828603565693, 2.1729828603565693, 2.1729828603565693, 2.168699238449335, 2.168699238449335, 2.168699238449335, 2.168699238449335, 2.168699238449335, 2.168699238449335, 2.168699238449335, 2.168699238449335, 2.168699238449335, 2.1451898366212845, 2.1505900770425797, 2.1505900770425797, 2.1505900770425797, 2.1451898366212845, 2.1451898366212845, 2.1451898366212845, 2.1505900770425797, 2.1505900770425797, 2.1478449031710625, 2.1478449031710625, 2.1478449031710625, 2.1478449031710625, 2.1478449031710625, 2.1451898366212845, 2.1505900770425797, 2.1505900770425797, 2.1505900770425797, 2.1505900770425797, 2.1478449031710625, 2.1451898366212845, 2.1451898366212845, 2.1451898366212845, 2.1478449031710625, 2.1451898366212845, 2.1451898366212845, 2.1451898366212845, 2.1451898366212845, 2.1505900770425797, 2.1451898366212845, 2.155655488371849, 2.1505900770425797, 2.155655488371849, 2.159536048769951, 2.155655488371849, 2.154821328818798, 2.154821328818798, 2.153291128575802, 2.153291128575802, 2.154821328818798, 2.154821328818798, 2.155655488371849, 2.154821328818798, 2.154821328818798, 2.154821328818798, 2.154821328818798, 2.154821328818798, 2.154821328818798, 2.154821328818798, 2.153291128575802, 2.153291128575802, 2.153291128575802, 2.153291128575802, 2.153291128575802, 2.153291128575802, 2.153291128575802, 2.153291128575802, 2.153291128575802, 2.1568795666098595, 2.1568795666098595, 2.1568795666098595, 2.1568795666098595, 2.1568795666098595, 2.1568795666098595, 2.1568795666098595, 2.1568795666098595, 2.1568795666098595, 2.1568795666098595, 2.1568795666098595, 2.1624646335840225, 2.1624646335840225, 2.1624646335840225, 2.1624646335840225, 2.1624646335840225, 2.1624646335840225, 2.1624646335840225, 2.1624646335840225, 2.1624646335840225, 2.1624646335840225, 2.1624646335840225, 2.1624646335840225, 2.1624646335840225, 2.1624646335840225, 2.1624646335840225, 2.1624646335840225, 2.1624646335840225, 2.183297734707594, 2.183297734707594, 2.183297734707594, 2.1624646335840225, 2.183297734707594, 2.183297734707594, 2.183297734707594, 2.183297734707594, 2.1946791410446167, 2.183297734707594, 2.183297734707594, 2.183297734707594, 2.183297734707594, 2.1946791410446167, 2.1946791410446167, 2.1946791410446167, 2.1946791410446167, 2.1946791410446167, 2.1946791410446167, 2.1946791410446167, 2.1946791410446167, 2.18991631641984, 2.18991631641984, 2.18991631641984, 2.18991631641984, 2.18991631641984, 2.18991631641984, 2.18991631641984, 2.18991631641984, 2.18991631641984, 2.18991631641984, 2.18991631641984, 2.18991631641984, 2.201725095510483, 2.201725095510483, 2.201725095510483, 2.1946791410446167, 2.201725095510483, 2.1946791410446167, 2.18991631641984, 2.1946791410446167, 2.201725095510483, 2.201725095510483, 2.18991631641984, 2.201725095510483, 2.201725095510483, 2.2295845821499825, 2.201725095510483, 2.201725095510483, 2.201725095510483, 2.201725095510483, 2.201725095510483, 2.201725095510483, 2.201725095510483, 2.201725095510483, 2.201725095510483, 2.2295845821499825, 2.2295845821499825, 2.2295845821499825, 2.201725095510483, 2.201725095510483, 2.201725095510483, 2.2351584769785404, 2.2351584769785404, 2.2351584769785404, 2.2351584769785404, 2.2351584769785404, 2.2351584769785404, 2.2351584769785404, 2.2114302329719067, 2.201725095510483, 2.246465243399143, 2.246465243399143, 2.249635063111782, 2.249635063111782, 2.249635063111782, 2.246465243399143, 2.2114302329719067, 2.2295845821499825, 2.2295845821499825, 2.2295845821499825, 2.2295845821499825, 2.2295845821499825, 2.2218792475759983, 2.2218792475759983, 2.2218792475759983, 2.2295845821499825, 2.2295845821499825, 2.2295845821499825, 2.2295845821499825, 2.2295845821499825, 2.2295845821499825, 2.2295845821499825, 2.2295845821499825, 2.2295845821499825, 2.2295845821499825, 2.2295845821499825, 2.2295845821499825, 2.2218792475759983, 2.249635063111782, 2.249635063111782, 2.249635063111782, 2.246465243399143, 2.256894499063492, 2.2114302329719067, 2.246465243399143, 2.256894499063492, 2.246465243399143, 2.246465243399143, 2.249635063111782, 2.246465243399143, 2.246465243399143, 2.246465243399143, 2.2351584769785404, 2.2218792475759983, 2.256894499063492, 2.256894499063492, 2.2351584769785404, 2.246465243399143, 2.246465243399143, 2.256894499063492, 2.256894499063492, 2.256894499063492, 2.256894499063492, 2.256894499063492, 2.264440082013607, 2.256894499063492, 2.291638843715191, 2.2526670545339584, 2.271834395825863, 2.291638843715191, 2.291638843715191, 2.285147339105606, 2.285147339105606, 2.285147339105606, 2.291638843715191, 2.285147339105606, 2.2526670545339584, 2.2526670545339584, 2.2526670545339584, 2.2526670545339584, 2.2526670545339584, 2.2526670545339584, 2.2526670545339584, 2.2526670545339584, 2.2526670545339584, 2.2526670545339584, 2.2526670545339584, 2.2526670545339584, 2.2526670545339584, 2.2526670545339584, 2.2526670545339584, 2.285147339105606, 2.291638843715191, 2.291638843715191, 2.291638843715191, 2.2780935913324356, 2.275686487555504, 2.2780935913324356, 2.275686487555504, 2.275686487555504, 2.275686487555504, 2.275686487555504, 2.275686487555504, 2.275686487555504, 2.275686487555504, 2.275686487555504, 2.275686487555504, 2.275686487555504, 2.275686487555504, 2.275686487555504, 2.275686487555504, 2.275686487555504, 2.2816194221377373, 2.2816194221377373, 2.2816194221377373, 2.2816194221377373, 2.285147339105606, 2.285147339105606, 2.291638843715191, 2.291638843715191, 2.291638843715191, 2.291638843715191, 2.3093273788690567, 2.3093273788690567, 2.3093273788690567, 2.3093273788690567, 2.3093273788690567, 2.3093273788690567, 2.3572667315602303, 2.3337313160300255, 2.3337313160300255, 2.3337313160300255, 2.3337313160300255, 2.3337313160300255, 2.3337313160300255, 2.3337313160300255, 2.3572667315602303, 2.3337313160300255, 2.3337313160300255, 2.3337313160300255, 2.3572667315602303, 2.3572667315602303, 2.3932490795850754, 2.3932490795850754, 2.3932490795850754, 2.3932490795850754, 2.3932490795850754, 2.3932490795850754, 2.3932490795850754, 2.3932490795850754, 2.401694316416979, 2.3932490795850754, 2.3932490795850754, 2.3932490795850754, 2.3932490795850754, 2.4145108573138714, 2.3932490795850754, 2.4145108573138714, 2.4145108573138714, 2.401694316416979, 2.4145108573138714, 2.4360054694116116, 2.401694316416979, 2.4360054694116116, 2.4360054694116116, 2.4360054694116116, 2.4360054694116116, 2.4360054694116116, 2.4360054694116116, 2.4360054694116116, 2.4360054694116116, 2.4360054694116116, 2.4145108573138714, 2.4145108573138714, 2.407673340290785, 2.4145108573138714, 2.407673340290785, 2.4120060093700886, 2.4145108573138714, 2.4145108573138714, 2.407673340290785, 2.4145108573138714, 2.407673340290785, 2.407673340290785, 2.4120060093700886, 2.4120060093700886, 2.4120060093700886, 2.407673340290785, 2.407673340290785, 2.4145108573138714, 2.4120060093700886, 2.4120060093700886, 2.4120060093700886, 2.4120060093700886, 2.4120060093700886, 2.424214042723179, 2.3572667315602303, 2.3572667315602303, 2.3932490795850754, 2.3932490795850754, 2.3572667315602303, 2.3572667315602303, 2.3572667315602303, 2.3572667315602303, 2.3572667315602303, 2.4278573133051395, 2.4278573133051395, 2.4278573133051395, 2.4278573133051395, 2.4278573133051395, 2.4278573133051395, 2.4278573133051395, 2.4278573133051395, 2.3932490795850754, 2.3572667315602303, 2.3932490795850754, 2.3572667315602303, 2.479155346751213, 2.3337313160300255, 2.479155346751213, 2.4537572525441647, 2.479155346751213, 2.4537572525441647, 2.4932704865932465, 2.4537572525441647, 2.4537572525441647, 2.4537572525441647, 2.496788315474987, 2.504888240247965, 2.504888240247965, 2.504888240247965, 2.504888240247965, 2.4537572525441647, 2.4537572525441647, 2.4537572525441647, 2.4537572525441647, 2.4537572525441647, 2.4537572525441647, 2.504888240247965, 2.5139276273548603, 2.5139276273548603, 2.5139276273548603, 2.5139276273548603, 2.5139276273548603, 2.5139276273548603, 2.5139276273548603, 2.557989224791527, 2.616266965866089, 2.5139276273548603, 2.504888240247965, 2.528343003243208, 2.561856262385845, 2.5139276273548603, 2.5139276273548603, 2.5139276273548603, 2.557989224791527, 2.5139276273548603, 2.5139276273548603, 2.4360054694116116, 2.4360054694116116, 2.4120060093700886, 2.4360054694116116, 2.4360054694116116, 2.4360054694116116, 2.5139276273548603, 2.5139276273548603, 2.479155346751213, 2.4537572525441647, 2.504888240247965, 2.504888240247965, 2.504888240247965, 2.504888240247965, 2.504888240247965, 2.504888240247965, 2.504888240247965, 2.5139276273548603, 2.504888240247965, 2.5139276273548603, 2.5077084749937057, 2.5077084749937057, 2.5077084749937057, 2.5139276273548603, 2.5139276273548603, 2.5139276273548603, 2.5077084749937057, 2.5077084749937057, 2.5077084749937057, 2.5077084749937057, 2.5077084749937057, 2.5077084749937057, 2.557989224791527, 2.557989224791527, 2.5077084749937057, 2.557989224791527, 2.561856262385845, 2.561856262385845, 2.561856262385845, 2.557989224791527, 2.557989224791527, 2.561856262385845, 2.561856262385845, 2.6141475029289722, 2.6141475029289722, 2.561856262385845, 2.561856262385845, 2.561856262385845, 2.528343003243208, 2.528343003243208, 2.528343003243208, 2.528343003243208, 2.528343003243208, 2.557989224791527, 2.616266965866089, 2.616266965866089, 2.528343003243208, 2.528343003243208, 2.528343003243208, 2.4360054694116116, 2.5139276273548603, 2.5139276273548603, 2.616266965866089, 2.528343003243208, 2.528343003243208, 2.4360054694116116, 2.4360054694116116, 2.5139276273548603, 2.5139276273548603, 2.616266965866089, 2.642707098275423, 2.616266965866089, 2.616266965866089, 2.616266965866089, 2.616266965866089, 2.616266965866089, 2.616266965866089, 2.616266965866089, 2.642707098275423, 2.704897552728653, 2.704897552728653, 2.704897552728653, 2.704897552728653, 2.704897552728653, 2.704897552728653, 2.704897552728653, 2.747499044984579, 2.747499044984579, 2.747499044984579, 2.747499044984579, 2.747499044984579, 2.747499044984579, 2.766600836068392, 2.766600836068392, 2.766600836068392, 2.766600836068392, 2.766600836068392, 2.766600836068392, 2.766600836068392, 2.7263587079942226, 2.766600836068392, 2.766600836068392, 2.7263587079942226, 2.7263587079942226, 2.7263587079942226, 2.7263587079942226, 2.704897552728653, 2.7263587079942226, 2.7263587079942226, 2.7263587079942226, 2.7263587079942226, 2.7168335393071175, 2.766600836068392, 2.7168335393071175, 2.7168335393071175, 2.7168335393071175, 2.7168335393071175, 2.7168335393071175, 2.7263587079942226, 2.766600836068392, 2.7168335393071175, 2.7168335393071175, 2.7168335393071175, 2.7168335393071175, 2.7263587079942226, 2.7263587079942226, 2.704897552728653, 2.704897552728653, 2.704897552728653, 2.704897552728653, 2.677522122859955, 2.677522122859955, 2.677522122859955, 2.677522122859955, 2.677522122859955, 2.677522122859955, 2.677522122859955, 2.677522122859955, 2.677522122859955, 2.677522122859955, 2.677522122859955, 2.677522122859955, 2.677522122859955, 2.677522122859955, 2.677522122859955, 2.677522122859955, 2.677522122859955, 2.6564001590013504, 2.677522122859955, 2.677522122859955, 2.62678337469697, 2.6564001590013504, 2.6564001590013504, 2.677522122859955, 2.677522122859955, 2.677522122859955, 2.6564001590013504, 2.6564001590013504, 2.6564001590013504, 2.677522122859955, 2.677522122859955, 2.6705895885825157, 2.677522122859955, 2.6705895885825157, 2.62678337469697, 2.62678337469697, 2.6564001590013504, 2.6564001590013504, 2.677522122859955, 2.6705895885825157, 2.6705895885825157, 2.6705895885825157, 2.6564001590013504, 2.6564001590013504, 2.677522122859955, 2.6705895885825157, 2.6705895885825157, 2.6705895885825157, 2.6705895885825157, 2.6705895885825157, 2.660920564085245, 2.6564001590013504, 2.6564001590013504, 2.660920564085245, 2.660920564085245, 2.6375561468303204, 2.62678337469697, 2.62678337469697, 2.62678337469697, 2.6285239048302174, 2.6250337809324265, 2.6285239048302174, 2.6285239048302174, 2.6250337809324265, 2.6250337809324265, 2.6285239048302174, 2.6285239048302174, 2.6250337809324265, 2.6250337809324265, 2.6250337809324265, 2.6250337809324265, 2.6250337809324265, 2.610223412513733, 2.610223412513733, 2.610223412513733, 2.610223412513733, 2.5898988656699657, 2.5898988656699657, 2.5898988656699657, 2.596752390265465, 2.596752390265465, 2.596752390265465, 2.596752390265465, 2.6212420910596848, 2.6212420910596848, 2.6212420910596848, 2.6202025301754475, 2.6202025301754475, 2.6202025301754475, 2.6202025301754475, 2.647575680166483, 2.646289173513651, 2.646289173513651, 2.646289173513651, 2.646289173513651, 2.646289173513651, 2.646289173513651, 2.646289173513651, 2.646289173513651, 2.646289173513651, 2.646289173513651, 2.646289173513651, 2.647575680166483, 2.646289173513651, 2.647575680166483, 2.647575680166483, 2.6493023931980133, 2.6306533329188824, 2.6493023931980133, 2.6306533329188824, 2.6306533329188824, 2.6306533329188824, 2.6306533329188824, 2.617978848516941, 2.617978848516941, 2.6222021356225014, 2.6250337809324265, 2.617978848516941, 2.617978848516941, 2.6141475029289722, 2.617978848516941, 2.617978848516941, 2.617978848516941, 2.6250337809324265, 2.6141475029289722, 2.6074452064931393, 2.6141475029289722, 2.6141475029289722, 2.605809446424246, 2.594744771718979, 2.594744771718979, 2.605809446424246, 2.605809446424246, 2.594744771718979, 2.594744771718979, 2.57189517095685, 2.5898988656699657, 2.495783291757107, 2.5898988656699657, 2.494656339287758, 2.5898988656699657, 2.5898988656699657, 2.5898988656699657, 2.5898988656699657, 2.5898988656699657, 2.5898988656699657, 2.5898988656699657, 2.495783291757107, 2.495783291757107, 2.495783291757107, 2.494656339287758, 2.5898988656699657, 2.605809446424246, 2.536893520504236, 2.605809446424246, 2.605809446424246, 2.5674472711980343, 2.5674472711980343, 2.57189517095685, 2.5748379938304424, 2.536893520504236, 2.578857187181711, 2.594744771718979, 2.578857187181711, 2.5674472711980343, 2.5748379938304424, 2.5748379938304424, 2.578857187181711, 2.578857187181711, 2.578857187181711, 2.578857187181711, 2.578857187181711, 2.578857187181711, 2.5748379938304424, 2.5748379938304424, 2.5748379938304424, 2.5748379938304424, 2.5816227197647095, 2.5816227197647095, 2.596752390265465, 2.5415820851922035, 2.5415820851922035, 2.5415820851922035, 2.6202025301754475, 2.6202025301754475, 2.6202025301754475, 2.596752390265465, 2.6202025301754475, 2.6202025301754475, 2.6202025301754475, 2.596752390265465, 2.596752390265465, 2.6202025301754475, 2.5415820851922035, 2.596752390265465, 2.596752390265465, 2.596752390265465, 2.596752390265465, 2.5816227197647095, 2.596752390265465, 2.596752390265465, 2.5816227197647095, 2.5816227197647095, 2.5816227197647095, 2.5816227197647095, 2.5748379938304424, 2.5882331244647503, 2.5802567526698112, 2.5748379938304424, 2.578857187181711, 2.578857187181711, 2.5816227197647095, 2.5816227197647095, 2.5748379938304424, 2.5882331244647503, 2.5816227197647095, 2.5748379938304424, 2.5882331244647503, 2.578857187181711, 2.578857187181711, 2.5816227197647095, 2.5816227197647095, 2.5816227197647095, 2.578857187181711, 2.5816227197647095, 2.5816227197647095, 2.5816227197647095, 2.5816227197647095, 2.5898988656699657, 2.5898988656699657, 2.5898988656699657, 2.5898988656699657, 2.5816227197647095, 2.5768272913992405, 2.5768272913992405, 2.5768272913992405, 2.5768272913992405, 2.5768272913992405, 2.5768272913992405, 2.579691804945469, 2.578857187181711, 2.5768272913992405, 2.5768272913992405, 2.5865691490471363, 2.5865691490471363, 2.5865691490471363, 2.5865691490471363, 2.580871343612671, 2.580871343612671, 2.580871343612671, 2.5865691490471363, 2.5849779695272446, 2.5849779695272446, 2.5849779695272446, 2.5849779695272446, 2.5849779695272446, 2.5849779695272446, 2.5849779695272446, 2.5849779695272446, 2.5849779695272446, 2.5849779695272446, 2.5849779695272446, 2.5849779695272446, 2.5849779695272446, 2.5865691490471363, 2.5865691490471363, 2.5865691490471363, 2.5865691490471363, 2.5865691490471363, 2.5865691490471363, 2.5849779695272446, 2.5849779695272446, 2.5865691490471363, 2.5865691490471363, 2.580871343612671, 2.580871343612671, 2.583486147224903, 2.580871343612671, 2.580871343612671, 2.5849779695272446, 2.580871343612671, 2.5865691490471363, 2.5768272913992405, 2.580871343612671, 2.5849779695272446, 2.580871343612671, 2.580871343612671, 2.580871343612671, 2.580871343612671, 2.583486147224903, 2.5865691490471363, 2.5802567526698112, 2.5849779695272446, 2.580871343612671, 2.5865691490471363, 2.580871343612671, 2.580871343612671, 2.580871343612671, 2.580871343612671, 2.580871343612671, 2.4905388467013836, 2.4905388467013836, 2.4905388467013836, 2.4905388467013836, 2.4905388467013836, 2.4905388467013836, 2.4905388467013836, 2.4905388467013836, 2.4905388467013836, 2.4905388467013836, 2.5829042978584766, 2.4905388467013836, 2.4905388467013836, 2.4905388467013836, 2.4905388467013836, 2.4905388467013836, 2.495783291757107, 2.4890729673206806, 2.4890729673206806, 2.4890729673206806, 2.4866908490657806, 2.487699180841446, 2.4866908490657806, 2.487699180841446, 2.496788315474987, 2.471305549144745, 2.471305549144745, 2.4747138135135174, 2.471305549144745, 2.471305549144745, 2.471305549144745, 2.4537572525441647, 2.4866908490657806, 2.4866908490657806, 2.4537572525441647, 2.496788315474987, 2.496788315474987, 2.496788315474987, 2.496788315474987, 2.496788315474987, 2.487699180841446, 2.487699180841446, 2.496788315474987, 2.4866908490657806, 2.496788315474987, 2.496788315474987, 2.487699180841446, 2.4866908490657806, 2.471305549144745, 2.4866908490657806, 2.4866908490657806, 2.4866908490657806, 2.4866908490657806, 2.471305549144745, 2.471305549144745, 2.471305549144745, 2.471305549144745, 2.4775619506835938, 2.4775619506835938, 2.4775619506835938, 2.4775619506835938, 2.4775619506835938, 2.4775619506835938, 2.4775619506835938, 2.4775619506835938, 2.4775619506835938, 2.4775619506835938, 2.4747138135135174, 2.4775619506835938, 2.476247500628233, 2.4747138135135174, 2.4747138135135174, 2.476247500628233, 2.476247500628233, 2.468116670846939, 2.468116670846939, 2.476247500628233, 2.471305549144745, 2.4775619506835938, 2.4775619506835938, 2.4775619506835938, 2.4775619506835938, 2.4775619506835938, 2.468116670846939, 2.476247500628233, 2.4775619506835938, 2.476247500628233, 2.476247500628233, 2.476247500628233, 2.4866908490657806, 2.4866908490657806, 2.4866908490657806, 2.4866908490657806, 2.4866908490657806, 2.4775619506835938, 2.476247500628233, 2.476247500628233, 2.468116670846939, 2.468116670846939, 2.4866908490657806, 2.4866908490657806, 2.4866908490657806, 2.4866908490657806, 2.4866908490657806, 2.4866908490657806, 2.4866908490657806, 2.4866908490657806, 2.4866908490657806, 2.4866908490657806, 2.4890729673206806, 2.4890729673206806, 2.4905388467013836, 2.4905388467013836, 2.4905388467013836, 2.4905388467013836, 2.4905388467013836, 2.4905388467013836, 2.4905388467013836, 2.4905388467013836, 2.5829042978584766, 2.5829042978584766, 2.578857187181711, 2.495783291757107, 2.5898988656699657, 2.578857187181711, 2.5898988656699657, 2.5898988656699657, 2.5898988656699657, 2.5898988656699657, 2.5898988656699657, 2.5898988656699657, 2.5898988656699657, 2.495783291757107, 2.495783291757107, 2.5898988656699657, 2.495783291757107, 2.578857187181711, 2.5829042978584766, 2.5898988656699657, 2.536893520504236, 2.5445715934038162, 2.536893520504236, 2.536893520504236, 2.5445715934038162, 2.5415820851922035, 2.5415820851922035, 2.5415820851922035, 2.5415820851922035, 2.5415820851922035, 2.617978848516941, 2.5415820851922035, 2.5415820851922035, 2.5445715934038162, 2.5415820851922035, 2.5415820851922035, 2.5445715934038162, 2.5445715934038162, 2.5445715934038162, 2.536893520504236, 2.594744771718979, 2.617978848516941, 2.536893520504236, 2.536893520504236, 2.5445715934038162, 2.5445715934038162, 2.5445715934038162, 2.594744771718979, 2.605809446424246, 2.594744771718979, 2.5484486147761345, 2.536893520504236, 2.5674472711980343, 2.5748379938304424, 2.5674472711980343, 2.584266535937786, 2.5674472711980343, 2.594744771718979, 2.6074452064931393, 2.605809446424246, 2.6074452064931393, 2.617978848516941, 2.617978848516941, 2.6202025301754475, 2.6306533329188824, 2.617978848516941, 2.617978848516941, 2.617978848516941, 2.617978848516941, 2.617978848516941, 2.617978848516941, 2.617978848516941, 2.617978848516941, 2.617978848516941, 2.617978848516941, 2.617978848516941, 2.617978848516941, 2.617978848516941, 2.5415820851922035, 2.6306533329188824, 2.5415820851922035, 2.5415820851922035, 2.6306533329188824, 2.6306533329188824, 2.6306533329188824, 2.6306533329188824, 2.6306533329188824, 2.5415820851922035, 2.6306533329188824, 2.5415820851922035, 2.6306533329188824, 2.6306533329188824, 2.6306533329188824, 2.6306533329188824, 2.6306533329188824, 2.6306533329188824, 2.6306533329188824, 2.6306533329188824, 2.6306533329188824, 2.646289173513651, 2.6686170548200607, 2.747499044984579, 2.704897552728653, 2.6881242245435715, 2.704897552728653, 2.747499044984579, 2.704897552728653, 2.704897552728653, 2.747499044984579, 2.704897552728653, 2.704897552728653, 2.685486350208521, 2.704897552728653, 2.685486350208521, 2.6881242245435715, 2.6881242245435715, 2.6881242245435715, 2.6927962973713875, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.6881242245435715, 2.6881242245435715, 2.6881242245435715, 2.6881242245435715, 2.677522122859955, 2.685486350208521, 2.685486350208521, 2.6881242245435715, 2.6881242245435715, 2.6881242245435715, 2.6881242245435715, 2.683473389595747, 2.6744660399854183, 2.6881242245435715, 2.6881242245435715, 2.660920564085245, 2.6705895885825157, 2.6705895885825157, 2.651480495929718, 2.651480495929718, 2.6375561468303204, 2.6375561468303204, 2.6375561468303204, 2.6705895885825157, 2.6705895885825157, 2.6705895885825157, 2.6705895885825157, 2.6705895885825157, 2.6669421084225178, 2.651480495929718, 2.651480495929718, 2.651480495929718, 2.651480495929718, 2.651480495929718, 2.651480495929718, 2.651480495929718, 2.6705895885825157, 2.6705895885825157, 2.6705895885825157, 2.6705895885825157, 2.6705895885825157, 2.6625372394919395, 2.6669421084225178, 2.6669421084225178, 2.6653142161667347, 2.6669421084225178, 2.6705895885825157, 2.6669421084225178, 2.6653142161667347, 2.6705895885825157, 2.6705895885825157, 2.6669421084225178, 2.6653142161667347, 2.6653142161667347, 2.6653142161667347, 2.6653142161667347, 2.6641077771782875, 2.6653142161667347, 2.6653142161667347, 2.6653142161667347, 2.6653142161667347, 2.6653142161667347, 2.6653142161667347, 2.6653142161667347, 2.6653142161667347, 2.6653142161667347, 2.6653142161667347, 2.6653142161667347, 2.6641077771782875, 2.6641077771782875, 2.6212420910596848, 2.6641077771782875, 2.6375561468303204, 2.6375561468303204, 2.6375561468303204, 2.6285239048302174, 2.6285239048302174, 2.6285239048302174, 2.6285239048302174, 2.6285239048302174, 2.6285239048302174, 2.6285239048302174, 2.6285239048302174, 2.6285239048302174, 2.6285239048302174, 2.6285239048302174, 2.6285239048302174, 2.6285239048302174, 2.6285239048302174, 2.6285239048302174, 2.6285239048302174, 2.6285239048302174, 2.6285239048302174, 2.6285239048302174, 2.6285239048302174, 2.6285239048302174, 2.6285239048302174, 2.6250337809324265, 2.6250337809324265, 2.610223412513733, 2.6250337809324265, 2.610223412513733, 2.6285239048302174, 2.6212420910596848, 2.6250337809324265, 2.6250337809324265, 2.6250337809324265, 2.6250337809324265, 2.6493023931980133, 2.6493023931980133, 2.6493023931980133, 2.6375561468303204, 2.6493023931980133, 2.6493023931980133, 2.6493023931980133, 2.646289173513651, 2.646289173513651, 2.6375561468303204, 2.6375561468303204, 2.6375561468303204, 2.651480495929718, 2.6641077771782875, 2.651480495929718, 2.646289173513651, 2.646289173513651, 2.646289173513651, 2.646289173513651, 2.647575680166483, 2.6493023931980133, 2.6250337809324265, 2.610223412513733, 2.6250337809324265, 2.610223412513733, 2.610223412513733, 2.610223412513733, 2.610223412513733, 2.600518673658371, 2.600518673658371, 2.602745320647955, 2.600518673658371, 2.600518673658371, 2.600518673658371, 2.600518673658371, 2.600518673658371, 2.600518673658371, 2.600518673658371, 2.600518673658371, 2.600518673658371, 2.600518673658371, 2.600518673658371, 2.600518673658371, 2.600518673658371, 2.600518673658371, 2.600518673658371, 2.600518673658371, 2.600518673658371, 2.600518673658371, 2.600518673658371, 2.600518673658371, 2.600518673658371, 2.600518673658371, 2.4989612251520157, 2.502001892775297, 2.4989612251520157, 2.502001892775297]}, {\"line\": {\"color\": \"red\", \"width\": 1}, \"type\": \"scatter\", \"x\": [1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431], \"y\": [2.4890729673206806, 2.4905388467013836, 2.4905388467013836, 2.4890729673206806, 2.4989612251520157, 2.4989612251520157, 2.4989612251520157, 2.4989612251520157, 2.4989612251520157, 2.4905388467013836, 2.4905388467013836, 2.4905388467013836, 2.4905388467013836, 2.4890729673206806, 2.4890729673206806, 2.4890729673206806, 2.4890729673206806, 2.4890729673206806, 2.4890729673206806, 2.4890729673206806, 2.583486147224903, 2.583486147224903, 2.494656339287758, 2.494656339287758, 2.494656339287758, 2.4890729673206806, 2.600518673658371, 2.600518673658371, 2.600518673658371, 2.600518673658371, 2.496788315474987, 2.600518673658371, 2.600518673658371, 2.600518673658371, 2.496788315474987, 2.600518673658371, 2.496788315474987, 2.496788315474987, 2.496788315474987, 2.600518673658371, 2.602745320647955, 2.602745320647955, 2.602745320647955, 2.602745320647955, 2.602745320647955, 2.4989612251520157, 2.602745320647955, 2.4989612251520157, 2.4989612251520157, 2.4989612251520157, 2.4989612251520157, 2.602745320647955, 2.5542355328798294, 2.4989612251520157, 2.4989612251520157, 2.4989612251520157, 2.4989612251520157, 2.4989612251520157, 2.4989612251520157, 2.5542355328798294, 2.5542355328798294, 2.610223412513733, 2.602745320647955, 2.602745320647955, 2.602745320647955, 2.602745320647955, 2.602745320647955, 2.610223412513733, 2.610223412513733, 2.6074452064931393, 2.6141475029289722, 2.6074452064931393, 2.6074452064931393, 2.6250337809324265, 2.6141475029289722, 2.6250337809324265, 2.617978848516941, 2.617978848516941, 2.617978848516941, 2.617978848516941, 2.617978848516941, 2.6250337809324265, 2.6250337809324265, 2.6250337809324265, 2.6202025301754475, 2.6202025301754475, 2.6202025301754475, 2.6202025301754475, 2.6202025301754475, 2.6202025301754475, 2.6202025301754475, 2.6202025301754475, 2.6202025301754475, 2.6202025301754475, 2.6046337634325027, 2.6046337634325027, 2.596752390265465, 2.6046337634325027, 2.6046337634325027, 2.596752390265465, 2.596752390265465, 2.596752390265465, 2.596752390265465, 2.596752390265465, 2.596752390265465, 2.596752390265465, 2.596752390265465, 2.596752390265465, 2.596752390265465, 2.6212420910596848, 2.6202025301754475, 2.6202025301754475, 2.6202025301754475, 2.6202025301754475, 2.6202025301754475, 2.6250337809324265, 2.6222021356225014, 2.6250337809324265, 2.6306533329188824, 2.646289173513651, 2.6306533329188824, 2.6306533329188824, 2.6306533329188824, 2.6306533329188824, 2.646289173513651, 2.6306533329188824, 2.646289173513651, 2.646289173513651, 2.646289173513651, 2.646289173513651, 2.646289173513651, 2.6686170548200607, 2.646289173513651, 2.646289173513651, 2.646289173513651, 2.6306533329188824, 2.6306533329188824, 2.6306533329188824, 2.6306533329188824, 2.6306533329188824, 2.617978848516941, 2.616266965866089, 2.616266965866089, 2.616266965866089, 2.616266965866089, 2.561856262385845, 2.536893520504236, 2.536893520504236, 2.536893520504236, 2.536893520504236, 2.57189517095685, 2.5674472711980343, 2.5674472711980343, 2.57189517095685, 2.57189517095685, 2.57189517095685, 2.57189517095685, 2.57189517095685, 2.5195987038314342, 2.57189517095685, 2.557989224791527, 2.5986968651413918, 2.561856262385845, 2.602745320647955, 2.6074452064931393, 2.5986968651413918, 2.5986968651413918, 2.6074452064931393, 2.602745320647955, 2.6074452064931393, 2.6074452064931393, 2.6074452064931393, 2.6141475029289722, 2.6074452064931393, 2.561856262385845, 2.561856262385845, 2.6074452064931393, 2.6074452064931393, 2.5986968651413918, 2.5986968651413918, 2.5986968651413918, 2.5986968651413918, 2.6074452064931393, 2.594744771718979, 2.594744771718979, 2.594744771718979, 2.6074452064931393, 2.602745320647955, 2.6141475029289722, 2.6141475029289722, 2.6141475029289722, 2.6141475029289722, 2.6141475029289722, 2.561856262385845, 2.561856262385845, 2.557989224791527, 2.561856262385845, 2.557989224791527, 2.561856262385845, 2.561856262385845, 2.528343003243208, 2.528343003243208, 2.5854623690247536, 2.5854623690247536, 2.401694316416979, 2.5854623690247536, 2.5854623690247536, 2.5854623690247536, 2.3932490795850754, 2.3932490795850754, 2.5854623690247536, 2.5854623690247536, 2.401694316416979, 2.4360054694116116, 2.401694316416979, 2.5139276273548603, 2.5139276273548603, 2.528343003243208, 2.528343003243208, 2.561856262385845, 2.561856262385845, 2.561856262385845, 2.561856262385845, 2.561856262385845, 2.561856262385845, 2.6141475029289722, 2.6141475029289722, 2.633854866027832, 2.6141475029289722, 2.6141475029289722, 2.6141475029289722, 2.6141475029289722, 2.6141475029289722, 2.6141475029289722, 2.6141475029289722, 2.6141475029289722, 2.633854866027832, 2.642707098275423, 2.633854866027832, 2.642707098275423, 2.642707098275423, 2.642707098275423, 2.642707098275423, 2.6141475029289722, 2.642707098275423, 2.642707098275423, 2.642707098275423, 2.633854866027832, 2.642707098275423, 2.642707098275423, 2.642707098275423, 2.642707098275423, 2.642707098275423, 2.633854866027832, 2.642707098275423, 2.642707098275423, 2.6564001590013504, 2.6564001590013504, 2.642707098275423, 2.6564001590013504, 2.6564001590013504, 2.6564001590013504, 2.6564001590013504, 2.6564001590013504, 2.642707098275423, 2.642707098275423, 2.6564001590013504, 2.6564001590013504, 2.6564001590013504, 2.6564001590013504, 2.6564001590013504, 2.6564001590013504, 2.6812986359000206, 2.6812986359000206, 2.6812986359000206, 2.6812986359000206, 2.6812986359000206, 2.6881242245435715, 2.6881242245435715, 2.683473389595747, 2.683473389595747, 2.683473389595747, 2.683473389595747, 2.683473389595747, 2.685486350208521, 2.683473389595747, 2.685486350208521, 2.683473389595747, 2.683473389595747, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.704897552728653, 2.704897552728653, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.7263587079942226, 2.7263587079942226, 2.7263587079942226, 2.7263587079942226, 2.7263587079942226, 2.7263587079942226, 2.7263587079942226, 2.7263587079942226, 2.7263587079942226, 2.7263587079942226, 2.7263587079942226, 2.685486350208521, 2.6625372394919395, 2.6625372394919395, 2.6625372394919395, 2.6625372394919395, 2.6625372394919395, 2.766600836068392, 2.6625372394919395, 2.6625372394919395, 2.6625372394919395, 2.6625372394919395, 2.6625372394919395, 2.6625372394919395, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.704897552728653, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.7263587079942226, 2.7168335393071175, 2.7168335393071175, 2.7168335393071175, 2.7168335393071175, 2.7168335393071175, 2.7168335393071175, 2.7168335393071175, 2.685486350208521, 2.7263587079942226, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.685486350208521, 2.677522122859955, 2.685486350208521, 2.677522122859955, 2.677522122859955, 2.677522122859955, 2.677522122859955, 2.677522122859955, 2.677522122859955, 2.677522122859955, 2.677522122859955, 2.677522122859955, 2.677522122859955, 2.677522122859955, 2.677522122859955, 2.6564001590013504, 2.677522122859955, 2.677522122859955, 2.677522122859955, 2.677522122859955, 2.677522122859955, 2.677522122859955, 2.642707098275423, 2.642707098275423, 2.642707098275423, 2.642707098275423, 2.4537572525441647, 2.642707098275423, 2.642707098275423, 2.642707098275423, 2.642707098275423, 2.4537572525441647, 2.4537572525441647, 2.4537572525441647, 2.4537572525441647, 2.4537572525441647, 2.4537572525441647, 2.4537572525441647, 2.4537572525441647, 2.4537572525441647, 2.4537572525441647, 2.4537572525441647, 2.4537572525441647, 2.4537572525441647, 2.4537572525441647, 2.4537572525441647, 2.4537572525441647, 2.4537572525441647, 2.4537572525441647, 2.4537572525441647, 2.4537572525441647, 2.4537572525441647, 2.4537572525441647, 2.4537572525441647, 2.4537572525441647, 2.4537572525441647, 2.4537572525441647, 2.4537572525441647, 2.4537572525441647, 2.4537572525441647, 2.4537572525441647, 2.4537572525441647, 2.642707098275423, 2.642707098275423]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"rangeslider\": {\"visible\": true}}, \"yaxis\": {\"autorange\": true, \"fixedrange\": false}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('5e41caa9-78e7-4bf4-88cb-deb56f925a08');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8jvHwA0Ykis"
      },
      "source": [
        "**4. Prédictions multi-step**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYHqaGThYkit"
      },
      "source": [
        "predictions = []\n",
        "                                                        # x_val : (448,16,779)\n",
        "data_to_predict = x_val[0,:,:]                          # [[VAL#1_0,VAL#2_0, ... , VAL#779_0],[VAL#1_1,VAL#2_1, ... , VAL#779_1], ... , [VAL#1_15,VAL#2_15, ..., VAL#779_15]] : (16,779)\n",
        "data_to_predict = tf.expand_dims(data_to_predict,0)     # (1,16,779)\n",
        "\n",
        "prediction = model.predict(data_to_predict)             # [[[^VAL#1_1][^VAL#1_2]...[^VAL#1_16]]] : (1,16,1)\n",
        "predictions.append(prediction[0,taille_fenetre-1,0])    # [^VAL#1_16] : (1,)\n",
        "\n",
        "data_to_predict = x_val[1,0:taille_fenetre-1,:]         # [[VAL#1_1,VAL#2_1, ... , VAL#779_1],[VAL#1_2,VAL#2_2, ... , VAL#779_2], ... , [VAL#1_15,VAL#2_15, ..., VAL#779_15]] : (15,779)\n",
        "\n",
        "data_to_predict = np.insert(data_to_predict,taille_fenetre-1,     # [[VAL#1_1,VAL#2_1, ... , VAL#779_1],[VAL#1_2,VAL#2_2, ... , VAL#779_2], ... , [^VAL#1_16,VAL#2_16, ..., VAL#779_16]] : (16,779)\n",
        "                            np.concatenate((tf.expand_dims(np.array(predictions[0]),0),x_val[1,taille_fenetre-2,1:779]),axis=0),axis=0)\n",
        "\n",
        "for i in range(1,300):\n",
        "  data_to_predict = tf.expand_dims(data_to_predict,0)   # (16,779) => (1,16,779)\n",
        "  prediction = model.predict(data_to_predict)           # [[[^VAL#1_2][^VAL#1_3]...[^VAL#1_17]]] : (1,16,1)\n",
        "\n",
        "  predictions.append(prediction[0,taille_fenetre-1,0])  # [^VAL#1_17] : (1,)\n",
        "  data_to_predict = x_val[i+1,0:taille_fenetre-1,:]     # [[VAL#1_2,VAL#2_2, ... , VAL#779_2],[VAL#1_3,VAL#2_3, ... , VAL#779_3], ... , [VAL#1_17,VAL#2_17, ..., VAL#779_17]] : (15,779)\n",
        "\n",
        "  data_to_predict = np.insert(data_to_predict,taille_fenetre-1,     # [[VAL#1_1,VAL#2_1, ... , VAL#779_1],[VAL#1_2,VAL#2_2, ... , VAL#779_2], ... , [^VAL#1_16,VAL#2_16, ..., VAL#779_16]] : (16,779)\n",
        "                            np.concatenate((tf.expand_dims(np.array(predictions[i]),0),x_val[1,taille_fenetre-2,1:779]),axis=0),axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAcNpvbvYkit"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "n_max = len(predictions)\n",
        "\n",
        "# Courbes originales\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index,y=serie_entrainement_X_norm[0],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index[temps_separation:],y=serie_test_X_norm[0],line=dict(color='blue', width=1)))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index[taille_fenetre+horizon-1:],y=pred_ent[:,taille_fenetre-1,0],line=dict(color='green', width=1)))\n",
        "\n",
        "# Courbes des prédictions de validations\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index[temps_separation+taille_fenetre+horizon-1:temps_separation+taille_fenetre+horizon-1+n_max],y=predictions[0:n_max],line=dict(color='red', width=1)))\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5CeF8IyjQFT"
      },
      "source": [
        "# Création des datasets - Sans prix en X"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZ1MBty1xgoX"
      },
      "source": [
        "X_reduit_VIF.insert(0,'Price',df_data['Price'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoP-8584xjYv"
      },
      "source": [
        "X_reduit_VIF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_a3GiFgyxlbT"
      },
      "source": [
        "X_Avec_Prix = X_reduit_VIF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gy8wD4VLu5Qg"
      },
      "source": [
        "X_Avec_Prix = df_data\n",
        "X_Avec_Prix = X_Avec_Prix.drop(columns=['Dates', 'Price'])\n",
        "Xdrop = SimpleImputer(missing_values=np.nan,strategy='most_frequent').fit_transform(X_Avec_Prix)\n",
        "Xdrop = pd.DataFrame(Xdrop)\n",
        "Xdrop.columns = X_Avec_Prix.columns\n",
        "X_Avec_Prix = Xdrop\n",
        "X_Avec_Prix = X_Avec_Prix\n",
        "X_Avec_Prix = X_Avec_Prix.iloc[:,var_pred]\n",
        "X_Avec_Prix.insert(0,'Price',df_data['Price'])\n",
        "X_Avec_Prix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-44FsypRjQFe"
      },
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "\n",
        "def prepare_dataset_XY(series, taille_fenetre, horizon, batch_size):\n",
        "  serie_concat = tf.expand_dims(series[0],1)\n",
        "\n",
        "  for i in range(1,len(series)):\n",
        "    serie_ = tf.expand_dims(series[i],1)\n",
        "    serie_concat = tf.concat([serie_concat,serie_],1)\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie_concat)\n",
        "  dataset = dataset.window(taille_fenetre+horizon, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre + horizon))\n",
        "  dataset = dataset.map(lambda x: (x[0:taille_fenetre][:,1:],tf.expand_dims(x[-taille_fenetre:][:,0],1)))\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6t272rIjQFg"
      },
      "source": [
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "taille_fenetre = 64\n",
        "horizon = 1\n",
        "batch_size = 1\n",
        "\n",
        "# Création du dataset\n",
        "dataset = prepare_dataset_XY(serie_entrainement_X_norm,taille_fenetre,horizon,batch_size)\n",
        "dataset_val = prepare_dataset_XY(serie_test_X_norm,taille_fenetre,horizon,batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKO1WzQkjQFh"
      },
      "source": [
        "print(len(list(dataset.as_numpy_iterator())))\n",
        "for element in dataset.take(1):\n",
        "  print(element[0].shape)\n",
        "  print(element[1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqHV6Ac5jQFi"
      },
      "source": [
        "for element in dataset.take(1):\n",
        "  print(element)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvR1N4gojQFj"
      },
      "source": [
        "On extrait maintenant les deux tenseurs (X,Y) pour l'entrainement :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwJtYSVtjQFk"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset))              # #60x((32,5,779),(32,5,1)) => x = 60x(32,5,779) ; y = 60x(32,5,1)\n",
        "\n",
        "# Recombine les données\n",
        "x = np.asarray(x,dtype=np.float32)      # 60x(32,5,779) => (60,32,5,779)\n",
        "y = np.asarray(y,dtype=np.float32)      # 60x(32,5,1) => (60,32,5,1)\n",
        "\n",
        "x_train = np.asarray(tf.reshape(x,shape=(x.shape[0]*x.shape[1],taille_fenetre,x.shape[3])))     # (60,32,5,779) => (60*32,5,779)\n",
        "y_train = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],taille_fenetre,y.shape[3])))     # (60,32,5,1) => (60*32,5,1)\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Zny4A0jjQFk"
      },
      "source": [
        "Puis la même chose pour les données de validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhoA0UOxjQFl"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset_val))\n",
        "\n",
        "# Recombine les données\n",
        "x = np.asarray(x,dtype=np.float32)\n",
        "y = np.asarray(y,dtype=np.float32)\n",
        "\n",
        "x_val = np.asarray(tf.reshape(x,shape=(x.shape[0]*x.shape[1],taille_fenetre,x.shape[3])))\n",
        "y_val = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],taille_fenetre,y.shape[3])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6TE-8uonbYq"
      },
      "source": [
        "# Création du modèle type Wavenet Multivarié"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFUUNiNYnbZH"
      },
      "source": [
        "**1. Création du réseau**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfUkzVAwnbZH"
      },
      "source": [
        "from keras.layers import Conv1D\n",
        "from keras.layers import Conv1D\n",
        "from keras.utils.conv_utils import conv_output_length\n",
        "from keras import layers\n",
        "from keras.regularizers import l2\n",
        "import keras.backend as K\n",
        "from keras.engine import Input\n",
        "from keras.engine import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mDTWTKDnbZI"
      },
      "source": [
        "class CausalDilatedConv1D(Conv1D):\n",
        "    def __init__(self, nb_filter, filter_length, init='glorot_uniform', activation=None, weights=None,\n",
        "                 border_mode='valid', subsample_length=1, atrous_rate=1, W_regularizer=None, b_regularizer=None,\n",
        "                 activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True, causal=False, **kwargs):\n",
        "        super(CausalDilatedConv1D, self).__init__(nb_filter, filter_length, weights=weights, activation=activation, \n",
        "                padding=border_mode, strides=subsample_length, dilation_rate=atrous_rate, kernel_regularizer=W_regularizer, \n",
        "                bias_regularizer=b_regularizer, activity_regularizer=activity_regularizer, kernel_constraint=W_constraint, \n",
        "                bias_constraint=b_constraint, use_bias=bias, **kwargs)\n",
        "        self.causal = causal\n",
        "        self.nb_filter = nb_filter\n",
        "        self.atrous_rate = atrous_rate\n",
        "        self.filter_length = filter_length\n",
        "        self.subsample_length = subsample_length\n",
        "        self.border_mode = border_mode\n",
        "        if self.causal and border_mode != 'valid':\n",
        "            raise ValueError(\"Causal mode dictates border_mode=valid.\")\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        input_length = input_shape[1]\n",
        "        if self.causal:\n",
        "            input_length += self.atrous_rate * (self.filter_length - 1)\n",
        "        length = conv_output_length(input_length, self.filter_length, self.border_mode, self.strides[0], dilation=self.atrous_rate)\n",
        "        return (input_shape[0], length, self.nb_filter)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        if self.causal:\n",
        "            x = K.temporal_padding(x, padding=(self.atrous_rate * (self.filter_length - 1), 0))\n",
        "        # return super(CausalAtrousConvolution1D, self).call(x, mask)\n",
        "        return super(CausalDilatedConv1D, self).call(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVxqodltnbZJ"
      },
      "source": [
        "def _compute_receptive_field(dilation_depth, stacks):\n",
        "  return stacks * (2 ** dilation_depth * 2) - (stacks - 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKSxA3XVnbZK"
      },
      "source": [
        "def build_model_residual_block(x, i, s,nb_filters, dim_filters, use_bias,res_l2):\n",
        "        original_x = x\n",
        "        # TODO: initalization, regularization?\n",
        "        relu_out = CausalDilatedConv1D(nb_filters, dim_filters, atrous_rate=2 ** i, border_mode='valid', causal=True, bias=True, name='dilated_conv_%d_relu_s%d' % (2 ** i, s), activation='relu', W_regularizer=l2(res_l2))(x)\n",
        "        res_x = layers.Conv1D(1, 1, padding='same', use_bias=use_bias, kernel_regularizer=l2(res_l2))(relu_out)\n",
        "        skip_x = layers.Conv1D(1, 1, padding='same', use_bias=use_bias, kernel_regularizer=l2(res_l2))(relu_out)\n",
        "        res_x = layers.Add()([original_x, res_x])\n",
        "        return res_x, skip_x\n",
        "\n",
        "def build_model_couche_condition(x, output_bins, nb_filters, dim_filters, use_bias,res_l2):\n",
        "        original_x = x\n",
        "        skip_conditions = []\n",
        "\n",
        "        relu_out = CausalDilatedConv1D(nb_filters, dim_filters, atrous_rate=1, border_mode='valid', causal=True, bias=True, name='dilated_conv_%d_condition_%d_s%d' % (1,1, 0), activation='relu',\n",
        "                                       W_regularizer=l2(res_l2))(tf.expand_dims(x[:,:,0],2))\n",
        "\n",
        "        skip_ = layers.Conv1D(1, 1, padding='same', use_bias=use_bias, kernel_regularizer=l2(res_l2))(relu_out)\n",
        "        skip_conditions.append(skip_)\n",
        "\n",
        "        for i in range(1,output_bins-1):\n",
        "          relu_out = CausalDilatedConv1D(nb_filters, dim_filters, atrous_rate=1, border_mode='valid', causal=True, bias=True, name='dilated_conv_%d_condition_%d_s%d' % (1,i+1,0), activation='relu',\n",
        "                                                    W_regularizer=l2(res_l2))(tf.expand_dims(x[:,:,i],2))\n",
        "\n",
        "          skip_ = layers.Conv1D(1, 1, padding='same', use_bias=use_bias, kernel_regularizer=l2(res_l2))(relu_out)\n",
        "          skip_conditions.append(skip_)\n",
        "\n",
        "        if output_bins > 1:\n",
        "          out = layers.Add()(skip_conditions)\n",
        "        else:\n",
        "          out = skip_\n",
        "        return out\n",
        "\n",
        "\n",
        "def build_model(fragment_length, nb_filters, dim_filters, output_bins, dilation_depth, stacks, use_skip_connections, use_bias, res_l2, final_l2):\n",
        "        input_shape = Input(shape=(fragment_length, output_bins), name='input_part')\n",
        "        out = input_shape\n",
        "        skip_connections = []\n",
        "\n",
        "        for s in range(stacks):\n",
        "            # Couche conditionnée\n",
        "            out = build_model_couche_condition(out, output_bins, nb_filters, dim_filters, use_bias, res_l2)\n",
        "\n",
        "            # Couches intermédiaires\n",
        "            for i in range(1, dilation_depth + 1):\n",
        "                out, skip_out = build_model_residual_block(out, i, s, nb_filters, dim_filters, use_bias, res_l2)\n",
        "                skip_connections.append(skip_out)\n",
        "\n",
        "        if use_skip_connections:\n",
        "            out = layers.Add()(skip_connections)\n",
        "\n",
        "        # Couche de sortie\n",
        "        out = layers.Activation('linear', name=\"output_linear\")(out)\n",
        "        out = layers.Conv1D(1, 1, padding='same', kernel_regularizer=l2(final_l2))(out)\n",
        "        model = Model(input_shape, out)\n",
        "        return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBHv2GWNnbZL"
      },
      "source": [
        "**2. Construction du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNMIkaHYnbZL"
      },
      "source": [
        "def compute_receptive_field_(dilation_depth, nb_stacks):\n",
        "    receptive_field = nb_stacks * (2 ** dilation_depth * 2) - (nb_stacks - 1)\n",
        "    return receptive_field\n",
        "\n",
        "nb_filters = 5\n",
        "dim_filters = 4\n",
        "nb_output_bins = 7\n",
        "dilation_depth = 5\n",
        "nb_stacks = 1\n",
        "use_skip_connections = False\n",
        "use_bias = False\n",
        "res_l2 = 0.001\n",
        "final_l2 = 0.001\n",
        "\n",
        "fragment_length = compute_receptive_field_(dilation_depth, nb_stacks)\n",
        "fragment_length\n",
        "\n",
        "model = build_model(fragment_length, nb_filters, dim_filters, nb_output_bins, dilation_depth, nb_stacks, use_skip_connections, use_bias, res_l2, final_l2)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3uWCqzcnkB2"
      },
      "source": [
        "# Entrainement du modèle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kly6bhi8nkB3"
      },
      "source": [
        "**1. Optimisation de l'apprentissage**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB1gffpQnkB4"
      },
      "source": [
        "Pour accélérer le traitement des données, nous n'allons pas utiliser l'intégralité des données pendant la mise à jour du gradient, comme cela a été fait jusqu'à présent (en utilisant le dataset).  \n",
        "Cette fois-ci, nous allons forcer les mises à jour du gradient à se produire de manière moins fréquente en attribuant la valeur du batch_size à prendre en compte lors de la regression du modèle.  \n",
        "Pour cela, on utilise l'argument \"batch_size\" dans la méthode fit. En précisant un batch_size=1000, cela signifie que :\n",
        " - Sur notre total de 56000 échantillons, 56 seront utilisés pour les calculs du gradient\n",
        " - Il y aura également 56 itérations à chaque période.\n",
        "  \n",
        "    \n",
        "    \n",
        "Si nous avions pris le dataset comme entrée, nous aurions eu :\n",
        "- Un total de 56000 échantillons également\n",
        "- Chaque période aurait également pris 56 itérations pour se compléter\n",
        "- Mais 1000 échantillons auraient été utilisés pour le calcul du gradient, au lieu de 56 avec la méthode utilisée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJaE8myAnkB5"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "def  My_MSE(y_true,y_pred):\n",
        "  return(tf.keras.metrics.mse(y_true,y_pred)*std_X[0].numpy()+mean_X[0].numpy())\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam()\n",
        "#optimiseur=tf.keras.optimizers.SGD()\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"mse\", optimizer=optimiseur, metrics=[\"mse\",My_MSE])\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint],batch_size=batch_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mJxFSJNnkB7"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[30], taux[99], 0, 0.4])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2icNDS5nkB8"
      },
      "source": [
        "# Chargement des poids sauvegardés\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKErsgPwnkB9"
      },
      "source": [
        "**2. Entrainement du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJTgBXrZnkB9"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "max_periodes = 5000\n",
        "\n",
        "# Classe permettant d'arrêter l'entrainement si la variation\n",
        "# devient plus petite qu'une valeur à choisir sur un nombre\n",
        "# de périodes à choisir\n",
        "class StopTrain(keras.callbacks.Callback):\n",
        "    def __init__(self, delta=0.01,periodes=100, term=\"loss\", logs={}):\n",
        "      self.n_periodes = 0\n",
        "      self.periodes = periodes\n",
        "      self.loss_1 = 100\n",
        "      self.delta = delta\n",
        "      self.term = term\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "      diff_loss = abs(self.loss_1 - logs[self.term])\n",
        "      self.loss_1 = logs[self.term]\n",
        "      if (diff_loss < self.delta):\n",
        "        self.n_periodes = self.n_periodes + 1\n",
        "      else:\n",
        "        self.n_periodes = 0\n",
        "      if (self.n_periodes == self.periodes):\n",
        "        print(\"Arrêt de l'entrainement...\")\n",
        "        self.model.stop_training = True\n",
        "\n",
        "def  My_MSE(y_true,y_pred):\n",
        "  return(tf.keras.metrics.mse(y_true,y_pred)*std_X[0].numpy()+mean_X[0].numpy())\n",
        "  \n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.001,\n",
        "    decay_steps=20,\n",
        "    decay_rate=0.001)\n",
        "\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam(learning_rate=lr_schedule,amsgrad=True)\n",
        "#optimiseur=tf.keras.optimizers.SGD(learning_rate=lr_schedule,momentum=0.9)\n",
        "\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"logcosh\", optimizer=optimiseur, metrics=[\"mse\",My_MSE])\n",
        "\n",
        "# Entraine le modèle, avec une réduction des calculs du gradient\n",
        "historique = model.fit(x=x_train,y=y_train,validation_data=(x_val,y_val), epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=1e-2,periodes = 10, term=\"My_MSE\")],batch_size=16)\n",
        "\n",
        "# Entraine le modèle sans réduction de calculs\n",
        "#historique = model.fit(dataset,validation_data=dataset_val, epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=1e-3,periodes = 20, term=\"My_MSE\")])\n",
        "\n",
        "files.download('poids_train.hdf5')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcCcIoABnkB_"
      },
      "source": [
        "# Télécharge les résultats d'entrainement du modèle\n",
        "! wget --no-check-certificate --content-disposition \"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/Bitcoin/poids_train_BitWave_One_All.hdf5?raw=true\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwCBv230nkCA"
      },
      "source": [
        "model.load_weights(\"poids_train.hdf5\")\n",
        "#model.load_weights(\"poids_train_BitWave_One_All.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8zkTV7dnkCA"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMcgYWLMnkCB"
      },
      "source": [
        "# Evaluation du modèle\n",
        "\n",
        "model.evaluate(dataset)\n",
        "model.evaluate(dataset_val)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRxuxZCRnkCC"
      },
      "source": [
        "**3. Prédictions \"single step\"**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOnVsZVznkCD"
      },
      "source": [
        "# Création des instants d'entrainement et de validation\n",
        "y_train_timing = X_Avec_Prix.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_train)]\n",
        "y_val_timing = X_Avec_Prix.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_val)]\n",
        "\n",
        "# Calcul des prédictions\n",
        "pred_ent = model.predict(x_train, verbose=1)\n",
        "pred_val = model.predict(x_val, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBixEA6vnkCD"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbes originales\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index,y=serie_entrainement_X_norm[0],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index[temps_separation:],y=serie_test_X_norm[0],line=dict(color='blue', width=1)))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index[taille_fenetre+horizon-1:],y=pred_ent[:,taille_fenetre-1,0],line=dict(color='green', width=1)))\n",
        "\n",
        "# Courbes des prédictions de validations\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index[temps_separation+taille_fenetre+horizon-1:],y=pred_val[:,taille_fenetre-1,0],line=dict(color='red', width=1)))\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7in3IeG1nkCF"
      },
      "source": [
        "**4. Prédictions multi-step**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buIyA-dNnkCF"
      },
      "source": [
        "predictions = []\n",
        "                                                        # x_val : (448,16,779)\n",
        "data_to_predict = x_val[0,:,:]                          # [[VAL#1_0,VAL#2_0, ... , VAL#779_0],[VAL#1_1,VAL#2_1, ... , VAL#779_1], ... , [VAL#1_15,VAL#2_15, ..., VAL#779_15]] : (16,779)\n",
        "data_to_predict = tf.expand_dims(data_to_predict,0)     # (1,16,779)\n",
        "\n",
        "prediction = model.predict(data_to_predict)             # [[[^VAL#1_1][^VAL#1_2]...[^VAL#1_16]]] : (1,16,1)\n",
        "predictions.append(prediction[0,taille_fenetre-1,0])    # [^VAL#1_16] : (1,)\n",
        "\n",
        "data_to_predict = x_val[1,0:taille_fenetre-1,:]         # [[VAL#1_1,VAL#2_1, ... , VAL#779_1],[VAL#1_2,VAL#2_2, ... , VAL#779_2], ... , [VAL#1_15,VAL#2_15, ..., VAL#779_15]] : (15,779)\n",
        "\n",
        "data_to_predict = np.insert(data_to_predict,taille_fenetre-1,     # [[VAL#1_1,VAL#2_1, ... , VAL#779_1],[VAL#1_2,VAL#2_2, ... , VAL#779_2], ... , [^VAL#1_16,VAL#2_16, ..., VAL#779_16]] : (16,779)\n",
        "                            np.concatenate((tf.expand_dims(np.array(predictions[0]),0),x_val[1,taille_fenetre-2,1:779]),axis=0),axis=0)\n",
        "\n",
        "for i in range(1,300):\n",
        "  data_to_predict = tf.expand_dims(data_to_predict,0)   # (16,779) => (1,16,779)\n",
        "  prediction = model.predict(data_to_predict)           # [[[^VAL#1_2][^VAL#1_3]...[^VAL#1_17]]] : (1,16,1)\n",
        "\n",
        "  predictions.append(prediction[0,taille_fenetre-1,0])  # [^VAL#1_17] : (1,)\n",
        "  data_to_predict = x_val[i+1,0:taille_fenetre-1,:]     # [[VAL#1_2,VAL#2_2, ... , VAL#779_2],[VAL#1_3,VAL#2_3, ... , VAL#779_3], ... , [VAL#1_17,VAL#2_17, ..., VAL#779_17]] : (15,779)\n",
        "\n",
        "  data_to_predict = np.insert(data_to_predict,taille_fenetre-1,     # [[VAL#1_1,VAL#2_1, ... , VAL#779_1],[VAL#1_2,VAL#2_2, ... , VAL#779_2], ... , [^VAL#1_16,VAL#2_16, ..., VAL#779_16]] : (16,779)\n",
        "                            np.concatenate((tf.expand_dims(np.array(predictions[i]),0),x_val[1,taille_fenetre-2,1:779]),axis=0),axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "za6q5srpnkCH"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "n_max = len(predictions)\n",
        "\n",
        "# Courbes originales\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index,y=serie_entrainement_X_norm[0],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index[temps_separation:],y=serie_test_X_norm[0],line=dict(color='blue', width=1)))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index[taille_fenetre+horizon-1:],y=pred_ent[:,taille_fenetre-1,0],line=dict(color='green', width=1)))\n",
        "\n",
        "# Courbes des prédictions de validations\n",
        "fig.add_trace(go.Scatter(x=X_Avec_Prix.index[temps_separation+taille_fenetre+horizon-1:temps_separation+taille_fenetre+horizon-1+n_max],y=predictions[0:n_max],line=dict(color='red', width=1)))\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}