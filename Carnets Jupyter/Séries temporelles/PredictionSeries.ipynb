{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PredictionsSeries.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/PredictionSeries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVuCzMYKDp3z"
      },
      "source": [
        "# Chargement des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3nLyKtnliCW"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fln1XUm2bxPx",
        "outputId": "73c8d9fc-a55f-4e4f-f0bd-0cf9b43e7e49"
      },
      "source": [
        "!wget --no-check-certificate --content-disposition \"https://raw.githubusercontent.com/AlexandreBourrieau/ML/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/data/table-indicateurs-open-data-dep-serie.csv\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-23 15:52:58--  https://raw.githubusercontent.com/AlexandreBourrieau/ML/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/data/table-indicateurs-open-data-dep-serie.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4340764 (4.1M) [text/plain]\n",
            "Saving to: ‘table-indicateurs-open-data-dep-serie.csv’\n",
            "\n",
            "table-indicateurs-o 100%[===================>]   4.14M  23.5MB/s    in 0.2s    \n",
            "\n",
            "2021-04-23 15:52:58 (23.5 MB/s) - ‘table-indicateurs-open-data-dep-serie.csv’ saved [4340764/4340764]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPJ6nnrRsUBm"
      },
      "source": [
        "Charge la série sous Pandas et affiche les informations du fichier :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEB_JjihEYTP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        },
        "outputId": "b83877bb-a068-42ae-865a-583038537dc0"
      },
      "source": [
        "# Création de la série sous Pandas\n",
        "serie = pd.read_csv(\"table-indicateurs-open-data-dep-serie.csv\")\n",
        "serie"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>extract_date</th>\n",
              "      <th>departement</th>\n",
              "      <th>region</th>\n",
              "      <th>libelle_reg</th>\n",
              "      <th>libelle_dep</th>\n",
              "      <th>tx_incid</th>\n",
              "      <th>R</th>\n",
              "      <th>taux_occupation_sae</th>\n",
              "      <th>tx_pos</th>\n",
              "      <th>tx_incid_couleur</th>\n",
              "      <th>R_couleur</th>\n",
              "      <th>taux_occupation_sae_couleur</th>\n",
              "      <th>tx_pos_couleur</th>\n",
              "      <th>nb_orange</th>\n",
              "      <th>nb_rouge</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-03-20</td>\n",
              "      <td>01</td>\n",
              "      <td>84</td>\n",
              "      <td>Auvergne Rhône Alpes</td>\n",
              "      <td>Ain</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15.6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>vert</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-03-21</td>\n",
              "      <td>01</td>\n",
              "      <td>84</td>\n",
              "      <td>Auvergne Rhône Alpes</td>\n",
              "      <td>Ain</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15.7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>vert</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-03-19</td>\n",
              "      <td>01</td>\n",
              "      <td>84</td>\n",
              "      <td>Auvergne Rhône Alpes</td>\n",
              "      <td>Ain</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>14.1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>vert</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-03-18</td>\n",
              "      <td>01</td>\n",
              "      <td>84</td>\n",
              "      <td>Auvergne Rhône Alpes</td>\n",
              "      <td>Ain</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>vert</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-04-28</td>\n",
              "      <td>01</td>\n",
              "      <td>84</td>\n",
              "      <td>Auvergne Rhône Alpes</td>\n",
              "      <td>Ain</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>72.6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>rouge</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40496</th>\n",
              "      <td>2021-04-15</td>\n",
              "      <td>84</td>\n",
              "      <td>93</td>\n",
              "      <td>Provence Alpes Côte d'Azur</td>\n",
              "      <td>Vaucluse</td>\n",
              "      <td>403.39</td>\n",
              "      <td>0.92</td>\n",
              "      <td>124.6</td>\n",
              "      <td>12.075129</td>\n",
              "      <td>rouge</td>\n",
              "      <td>vert</td>\n",
              "      <td>rouge</td>\n",
              "      <td>rouge</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40497</th>\n",
              "      <td>2021-04-16</td>\n",
              "      <td>84</td>\n",
              "      <td>93</td>\n",
              "      <td>Provence Alpes Côte d'Azur</td>\n",
              "      <td>Vaucluse</td>\n",
              "      <td>408.38</td>\n",
              "      <td>NaN</td>\n",
              "      <td>127.4</td>\n",
              "      <td>12.472100</td>\n",
              "      <td>rouge</td>\n",
              "      <td>NaN</td>\n",
              "      <td>rouge</td>\n",
              "      <td>rouge</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40498</th>\n",
              "      <td>2021-04-13</td>\n",
              "      <td>84</td>\n",
              "      <td>93</td>\n",
              "      <td>Provence Alpes Côte d'Azur</td>\n",
              "      <td>Vaucluse</td>\n",
              "      <td>434.58</td>\n",
              "      <td>NaN</td>\n",
              "      <td>124.3</td>\n",
              "      <td>12.066320</td>\n",
              "      <td>rouge</td>\n",
              "      <td>NaN</td>\n",
              "      <td>rouge</td>\n",
              "      <td>rouge</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40499</th>\n",
              "      <td>2021-04-18</td>\n",
              "      <td>84</td>\n",
              "      <td>93</td>\n",
              "      <td>Provence Alpes Côte d'Azur</td>\n",
              "      <td>Vaucluse</td>\n",
              "      <td>396.26</td>\n",
              "      <td>NaN</td>\n",
              "      <td>124.3</td>\n",
              "      <td>12.487361</td>\n",
              "      <td>rouge</td>\n",
              "      <td>NaN</td>\n",
              "      <td>rouge</td>\n",
              "      <td>rouge</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40500</th>\n",
              "      <td>2021-04-17</td>\n",
              "      <td>84</td>\n",
              "      <td>93</td>\n",
              "      <td>Provence Alpes Côte d'Azur</td>\n",
              "      <td>Vaucluse</td>\n",
              "      <td>403.75</td>\n",
              "      <td>NaN</td>\n",
              "      <td>124.3</td>\n",
              "      <td>12.593128</td>\n",
              "      <td>rouge</td>\n",
              "      <td>NaN</td>\n",
              "      <td>rouge</td>\n",
              "      <td>rouge</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>40501 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      extract_date departement  region  ... tx_pos_couleur nb_orange  nb_rouge\n",
              "0       2020-03-20          01      84  ...            NaN         0         0\n",
              "1       2020-03-21          01      84  ...            NaN         0         0\n",
              "2       2020-03-19          01      84  ...            NaN         0         0\n",
              "3       2020-03-18          01      84  ...            NaN         0         0\n",
              "4       2020-04-28          01      84  ...            NaN         0         1\n",
              "...            ...         ...     ...  ...            ...       ...       ...\n",
              "40496   2021-04-15          84      93  ...          rouge         0         3\n",
              "40497   2021-04-16          84      93  ...          rouge         0         3\n",
              "40498   2021-04-13          84      93  ...          rouge         0         3\n",
              "40499   2021-04-18          84      93  ...          rouge         0         3\n",
              "40500   2021-04-17          84      93  ...          rouge         0         3\n",
              "\n",
              "[40501 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "id": "cbYvPt63d-GS",
        "outputId": "4a3d17ee-e2e0-4508-bb65-c9a58207d6f8"
      },
      "source": [
        "serie.groupby(by=\"region\").agg(['count'])"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>extract_date</th>\n",
              "      <th>departement</th>\n",
              "      <th>libelle_reg</th>\n",
              "      <th>libelle_dep</th>\n",
              "      <th>tx_incid</th>\n",
              "      <th>R</th>\n",
              "      <th>taux_occupation_sae</th>\n",
              "      <th>tx_pos</th>\n",
              "      <th>tx_incid_couleur</th>\n",
              "      <th>R_couleur</th>\n",
              "      <th>taux_occupation_sae_couleur</th>\n",
              "      <th>tx_pos_couleur</th>\n",
              "      <th>nb_orange</th>\n",
              "      <th>nb_rouge</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>count</th>\n",
              "      <th>count</th>\n",
              "      <th>count</th>\n",
              "      <th>count</th>\n",
              "      <th>count</th>\n",
              "      <th>count</th>\n",
              "      <th>count</th>\n",
              "      <th>count</th>\n",
              "      <th>count</th>\n",
              "      <th>count</th>\n",
              "      <th>count</th>\n",
              "      <th>count</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>region</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "      <td>342</td>\n",
              "      <td>54</td>\n",
              "      <td>401</td>\n",
              "      <td>336</td>\n",
              "      <td>342</td>\n",
              "      <td>54</td>\n",
              "      <td>401</td>\n",
              "      <td>336</td>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "      <td>342</td>\n",
              "      <td>56</td>\n",
              "      <td>401</td>\n",
              "      <td>336</td>\n",
              "      <td>342</td>\n",
              "      <td>56</td>\n",
              "      <td>401</td>\n",
              "      <td>336</td>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "      <td>342</td>\n",
              "      <td>68</td>\n",
              "      <td>401</td>\n",
              "      <td>336</td>\n",
              "      <td>342</td>\n",
              "      <td>68</td>\n",
              "      <td>401</td>\n",
              "      <td>336</td>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "      <td>342</td>\n",
              "      <td>55</td>\n",
              "      <td>401</td>\n",
              "      <td>336</td>\n",
              "      <td>342</td>\n",
              "      <td>55</td>\n",
              "      <td>401</td>\n",
              "      <td>336</td>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "      <td>342</td>\n",
              "      <td>35</td>\n",
              "      <td>401</td>\n",
              "      <td>336</td>\n",
              "      <td>342</td>\n",
              "      <td>35</td>\n",
              "      <td>401</td>\n",
              "      <td>336</td>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>3208</td>\n",
              "      <td>3208</td>\n",
              "      <td>3208</td>\n",
              "      <td>3208</td>\n",
              "      <td>2736</td>\n",
              "      <td>544</td>\n",
              "      <td>3208</td>\n",
              "      <td>2688</td>\n",
              "      <td>2736</td>\n",
              "      <td>544</td>\n",
              "      <td>3208</td>\n",
              "      <td>2688</td>\n",
              "      <td>3208</td>\n",
              "      <td>3208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>2406</td>\n",
              "      <td>2406</td>\n",
              "      <td>2406</td>\n",
              "      <td>2406</td>\n",
              "      <td>2052</td>\n",
              "      <td>408</td>\n",
              "      <td>2406</td>\n",
              "      <td>2016</td>\n",
              "      <td>2052</td>\n",
              "      <td>408</td>\n",
              "      <td>2406</td>\n",
              "      <td>2016</td>\n",
              "      <td>2406</td>\n",
              "      <td>2406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>3208</td>\n",
              "      <td>3208</td>\n",
              "      <td>3208</td>\n",
              "      <td>3208</td>\n",
              "      <td>2736</td>\n",
              "      <td>544</td>\n",
              "      <td>3208</td>\n",
              "      <td>2688</td>\n",
              "      <td>2736</td>\n",
              "      <td>544</td>\n",
              "      <td>3208</td>\n",
              "      <td>2688</td>\n",
              "      <td>3208</td>\n",
              "      <td>3208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>2005</td>\n",
              "      <td>2005</td>\n",
              "      <td>2005</td>\n",
              "      <td>2005</td>\n",
              "      <td>1710</td>\n",
              "      <td>340</td>\n",
              "      <td>2005</td>\n",
              "      <td>1680</td>\n",
              "      <td>1710</td>\n",
              "      <td>340</td>\n",
              "      <td>2005</td>\n",
              "      <td>1680</td>\n",
              "      <td>2005</td>\n",
              "      <td>2005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>2005</td>\n",
              "      <td>2005</td>\n",
              "      <td>2005</td>\n",
              "      <td>2005</td>\n",
              "      <td>1710</td>\n",
              "      <td>340</td>\n",
              "      <td>2005</td>\n",
              "      <td>1680</td>\n",
              "      <td>1710</td>\n",
              "      <td>340</td>\n",
              "      <td>2005</td>\n",
              "      <td>1680</td>\n",
              "      <td>2005</td>\n",
              "      <td>2005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>4010</td>\n",
              "      <td>4010</td>\n",
              "      <td>4010</td>\n",
              "      <td>4010</td>\n",
              "      <td>3420</td>\n",
              "      <td>680</td>\n",
              "      <td>4010</td>\n",
              "      <td>3360</td>\n",
              "      <td>3420</td>\n",
              "      <td>680</td>\n",
              "      <td>4010</td>\n",
              "      <td>3360</td>\n",
              "      <td>4010</td>\n",
              "      <td>4010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>2005</td>\n",
              "      <td>2005</td>\n",
              "      <td>2005</td>\n",
              "      <td>2005</td>\n",
              "      <td>1710</td>\n",
              "      <td>340</td>\n",
              "      <td>2005</td>\n",
              "      <td>1680</td>\n",
              "      <td>1710</td>\n",
              "      <td>340</td>\n",
              "      <td>2005</td>\n",
              "      <td>1680</td>\n",
              "      <td>2005</td>\n",
              "      <td>2005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>1604</td>\n",
              "      <td>1604</td>\n",
              "      <td>1604</td>\n",
              "      <td>1604</td>\n",
              "      <td>1368</td>\n",
              "      <td>272</td>\n",
              "      <td>1604</td>\n",
              "      <td>1344</td>\n",
              "      <td>1368</td>\n",
              "      <td>272</td>\n",
              "      <td>1604</td>\n",
              "      <td>1344</td>\n",
              "      <td>1604</td>\n",
              "      <td>1604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>4812</td>\n",
              "      <td>4812</td>\n",
              "      <td>4812</td>\n",
              "      <td>4812</td>\n",
              "      <td>4104</td>\n",
              "      <td>816</td>\n",
              "      <td>4812</td>\n",
              "      <td>4032</td>\n",
              "      <td>4104</td>\n",
              "      <td>816</td>\n",
              "      <td>4812</td>\n",
              "      <td>4032</td>\n",
              "      <td>4812</td>\n",
              "      <td>4812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>5213</td>\n",
              "      <td>5213</td>\n",
              "      <td>5213</td>\n",
              "      <td>5213</td>\n",
              "      <td>4446</td>\n",
              "      <td>884</td>\n",
              "      <td>5213</td>\n",
              "      <td>4368</td>\n",
              "      <td>4446</td>\n",
              "      <td>884</td>\n",
              "      <td>5213</td>\n",
              "      <td>4368</td>\n",
              "      <td>5213</td>\n",
              "      <td>5213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>4812</td>\n",
              "      <td>4812</td>\n",
              "      <td>4812</td>\n",
              "      <td>4812</td>\n",
              "      <td>4104</td>\n",
              "      <td>816</td>\n",
              "      <td>4812</td>\n",
              "      <td>4032</td>\n",
              "      <td>4104</td>\n",
              "      <td>816</td>\n",
              "      <td>4812</td>\n",
              "      <td>4032</td>\n",
              "      <td>4812</td>\n",
              "      <td>4812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>2406</td>\n",
              "      <td>2406</td>\n",
              "      <td>2406</td>\n",
              "      <td>2406</td>\n",
              "      <td>2052</td>\n",
              "      <td>408</td>\n",
              "      <td>2406</td>\n",
              "      <td>2016</td>\n",
              "      <td>2052</td>\n",
              "      <td>408</td>\n",
              "      <td>2406</td>\n",
              "      <td>2016</td>\n",
              "      <td>2406</td>\n",
              "      <td>2406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>802</td>\n",
              "      <td>802</td>\n",
              "      <td>802</td>\n",
              "      <td>802</td>\n",
              "      <td>684</td>\n",
              "      <td>104</td>\n",
              "      <td>802</td>\n",
              "      <td>672</td>\n",
              "      <td>684</td>\n",
              "      <td>104</td>\n",
              "      <td>802</td>\n",
              "      <td>672</td>\n",
              "      <td>802</td>\n",
              "      <td>802</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       extract_date departement libelle_reg  ... tx_pos_couleur nb_orange nb_rouge\n",
              "              count       count       count  ...          count     count    count\n",
              "region                                       ...                                  \n",
              "1               401         401         401  ...            336       401      401\n",
              "2               401         401         401  ...            336       401      401\n",
              "3               401         401         401  ...            336       401      401\n",
              "4               401         401         401  ...            336       401      401\n",
              "6               401         401         401  ...            336       401      401\n",
              "11             3208        3208        3208  ...           2688      3208     3208\n",
              "24             2406        2406        2406  ...           2016      2406     2406\n",
              "27             3208        3208        3208  ...           2688      3208     3208\n",
              "28             2005        2005        2005  ...           1680      2005     2005\n",
              "32             2005        2005        2005  ...           1680      2005     2005\n",
              "44             4010        4010        4010  ...           3360      4010     4010\n",
              "52             2005        2005        2005  ...           1680      2005     2005\n",
              "53             1604        1604        1604  ...           1344      1604     1604\n",
              "75             4812        4812        4812  ...           4032      4812     4812\n",
              "76             5213        5213        5213  ...           4368      5213     5213\n",
              "84             4812        4812        4812  ...           4032      4812     4812\n",
              "93             2406        2406        2406  ...           2016      2406     2406\n",
              "94              802         802         802  ...            672       802      802\n",
              "\n",
              "[18 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJLcuFiNevyy"
      },
      "source": [
        "Regardons l'évolution du taux d'incidence sur paris :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "Q17wIXPLe2i_",
        "outputId": "5d288d63-125d-4822-b665-cb4e4de26c38"
      },
      "source": [
        "serie_paris = serie.loc[serie['region']==84]\n",
        "serie_paris = serie_paris[['extract_date','tx_incid']]\n",
        "\n",
        "serie_paris"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>extract_date</th>\n",
              "      <th>tx_incid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-03-20</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-03-21</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-03-19</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-03-18</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-04-28</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4807</th>\n",
              "      <td>2021-04-16</td>\n",
              "      <td>291.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4808</th>\n",
              "      <td>2021-04-13</td>\n",
              "      <td>330.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4809</th>\n",
              "      <td>2021-04-18</td>\n",
              "      <td>281.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4810</th>\n",
              "      <td>2021-04-19</td>\n",
              "      <td>264.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4811</th>\n",
              "      <td>2021-04-17</td>\n",
              "      <td>280.78</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4812 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     extract_date  tx_incid\n",
              "0      2020-03-20       NaN\n",
              "1      2020-03-21       NaN\n",
              "2      2020-03-19       NaN\n",
              "3      2020-03-18       NaN\n",
              "4      2020-04-28       NaN\n",
              "...           ...       ...\n",
              "4807   2021-04-16    291.40\n",
              "4808   2021-04-13    330.15\n",
              "4809   2021-04-18    281.02\n",
              "4810   2021-04-19    264.48\n",
              "4811   2021-04-17    280.78\n",
              "\n",
              "[4812 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "cTSg8XwLknp9",
        "outputId": "80329476-f829-4e0e-c148-9441cb252f48"
      },
      "source": [
        "df_paris = pd.DataFrame(data={'taux' : serie_paris['tx_incid'].values},index=serie_paris['extract_date'])\n",
        "df_paris.index = pd.to_datetime(df_paris.index)\n",
        "df_paris = df_paris[~df_paris.index.duplicated(keep='first')]\n",
        "df_paris"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>taux</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>extract_date</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2020-03-20</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-21</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-19</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-18</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-04-28</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-27</th>\n",
              "      <td>80.22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-28</th>\n",
              "      <td>75.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-29</th>\n",
              "      <td>75.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-30</th>\n",
              "      <td>74.59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-10-01</th>\n",
              "      <td>77.78</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>401 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               taux\n",
              "extract_date       \n",
              "2020-03-20      NaN\n",
              "2020-03-21      NaN\n",
              "2020-03-19      NaN\n",
              "2020-03-18      NaN\n",
              "2020-04-28      NaN\n",
              "...             ...\n",
              "2020-09-27    80.22\n",
              "2020-09-28    75.20\n",
              "2020-09-29    75.50\n",
              "2020-09-30    74.59\n",
              "2020-10-01    77.78\n",
              "\n",
              "[401 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "m23cEXjcny_q",
        "outputId": "c5f8199d-5528-41e2-a578-25be959accfd"
      },
      "source": [
        "df_paris.index = df_paris.index.sort_values()\n",
        "df_paris"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>taux</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>extract_date</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2020-03-18</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-19</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-20</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-21</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-22</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-18</th>\n",
              "      <td>80.22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-19</th>\n",
              "      <td>75.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-20</th>\n",
              "      <td>75.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-21</th>\n",
              "      <td>74.59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-22</th>\n",
              "      <td>77.78</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>401 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               taux\n",
              "extract_date       \n",
              "2020-03-18      NaN\n",
              "2020-03-19      NaN\n",
              "2020-03-20      NaN\n",
              "2020-03-21      NaN\n",
              "2020-03-22      NaN\n",
              "...             ...\n",
              "2021-04-18    80.22\n",
              "2021-04-19    75.20\n",
              "2021-04-20    75.50\n",
              "2021-04-21    74.59\n",
              "2021-04-22    77.78\n",
              "\n",
              "[401 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "qYpzzcbGi5FE",
        "outputId": "feec558c-a501-4984-eb36-5cf1ed0eeb00"
      },
      "source": [
        "plt.plot(df_paris)"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f78f8035490>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 211
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF62FumBty9H"
      },
      "source": [
        "# Pré-traitement des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUzjd-qN2s-T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef25d71f-3bae-4289-d916-f49cc2bd3840"
      },
      "source": [
        "# Affichage du nombre total de données manquantes\n",
        "\n",
        "data_manquantes = sum(np.isnan(df_paris['taux']))\n",
        "print (\"Nombre de données manquantes : %s\" %data_manquantes)"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nombre de données manquantes : 59\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tkbi7uuBnUD3"
      },
      "source": [
        "**3. Correction des données**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IQuSqAknduG"
      },
      "source": [
        "Pour corriger les données, on va tout simplement utiliser la fonction [fillna](https://pandas.pydata.org/docs/reference/api/pandas.Series.fillna.html) de Pandas avec la fonctionnalité de type `backfill` :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2Oav6gin5aP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "908d0e15-a3bf-418f-82ca-05d88df0aac2"
      },
      "source": [
        "# Applique la fonction de remplissage automatique des données non numérique avec l'option backfill\n",
        "df_paris = df_paris.interpolate(method=\"slinear\")\n",
        "data_manquantes = sum(np.isnan(df_paris['taux']))\n",
        "print (\"Nombre de données manquantes : %s\" %data_manquantes)"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nombre de données manquantes : 44\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n84L5raJo72w",
        "outputId": "07129a7b-bd71-4826-d9c2-77c0039578bd"
      },
      "source": [
        "df_paris = df_paris.fillna(method=\"backfill\")\n",
        "data_manquantes = sum(np.isnan(df_paris['taux']))\n",
        "print (\"Nombre de données manquantes : %s\" %data_manquantes)"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nombre de données manquantes : 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v05rWWccJI26"
      },
      "source": [
        "**4. Affichage des données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1QKMBThNQni",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "1e903dbb-d5e5-4e6c-f582-bf7c768a23fc"
      },
      "source": [
        "# Affiche la série\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(df_paris)\n",
        "plt.title(\"Evolution du prix du BTC\")"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Evolution du prix du BTC')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU7u1mA1E6jk"
      },
      "source": [
        "# Préparation des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwOeFLtLSPnv"
      },
      "source": [
        "**2. Détection des anomalies dans la série \"horaire\"**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1joYv2Kd7Js"
      },
      "source": [
        "Les anomalies sont fréquentes dans les séries temporelles, et la performance des prédictions est souvent améliorée lorsque ces anomalies sont traitées.  \n",
        "Pour avoir un apperçu de ces éventuelles anomalies, nous allons utiliser la méthode [\"Isolation Forest\"](https://scikit-learn.org/stable/modules/outlier_detection.html#isolation-forest) disponnible dans Scikit-learn.  \n",
        "\n",
        "Les paramètres utilisés sont les suivants :\n",
        " - **n_estimators** : C'est le nombre de sous-groupes d'échantillons à utiliser. Une valeur de 128 ou 256 est préconnisée dans le document de recherche.\n",
        " - **max_samples** : C'est le nombre d'échantillons maximum à utiliser. Nous utiliserons l'ensemble des échantillons.\n",
        " - **max_features** :  C'est le nombre de motifs aléatoirement choisis sur chaque noeud de l'arbre. Nous choisirons un seul motif.\n",
        " - **contamination** : C'est le pourcentage estimé d'anomalies dans les données. Ce paramètre permet de régler la sensibilité de l'algorithme. On va commencer avec 5% et affiner si nécessaire par la suite."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHag65S4dH7x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d692ae8a-1629-4ef5-a8df-7724f30963d7"
      },
      "source": [
        "# Initialise le modèle\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "clf = IsolationForest(n_estimators=256,max_samples=df_paris['taux'].size, contamination=0.01,max_features=1, verbose=1)\n",
        "clf.fit(df_paris['taux'].values.reshape(-1,1))"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "IsolationForest(behaviour='deprecated', bootstrap=False, contamination=0.01,\n",
              "                max_features=1, max_samples=401, n_estimators=256, n_jobs=None,\n",
              "                random_state=None, verbose=1, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAPFfAaffb4h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f58e3100-2138-4c20-d770-6f295f3b6037"
      },
      "source": [
        "# Réalise les prédictions\n",
        "pred = clf.predict(df_paris['taux'].values.reshape(-1,1))\n",
        "pred"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1, -1,  1,\n",
              "        1,  1,  1,  1, -1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 222
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU0TN1UEBqR2"
      },
      "source": [
        "On ajoute maintenant ces informations dans la série journalière et on affiche les informations :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWg0uUb9G5Ws",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "outputId": "d153748e-0a25-4baf-bb60-3c38c01bd28c"
      },
      "source": [
        "# Ajoute une colonne \"Anomalie\" dans la série\n",
        "df_paris['Anomalies']=pred\n",
        "df_paris['Anomalies'] = df_paris['Anomalies'].apply(lambda x: 1 if (x==-1) else 0)\n",
        "df_paris"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>taux</th>\n",
              "      <th>Anomalies</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>extract_date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2020-03-18</th>\n",
              "      <td>108.53</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-19</th>\n",
              "      <td>108.53</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-20</th>\n",
              "      <td>108.53</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-21</th>\n",
              "      <td>108.53</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-22</th>\n",
              "      <td>108.53</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-18</th>\n",
              "      <td>80.22</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-19</th>\n",
              "      <td>75.20</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-20</th>\n",
              "      <td>75.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-21</th>\n",
              "      <td>74.59</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-22</th>\n",
              "      <td>77.78</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>401 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                taux  Anomalies\n",
              "extract_date                   \n",
              "2020-03-18    108.53          0\n",
              "2020-03-19    108.53          0\n",
              "2020-03-20    108.53          0\n",
              "2020-03-21    108.53          0\n",
              "2020-03-22    108.53          0\n",
              "...              ...        ...\n",
              "2021-04-18     80.22          0\n",
              "2021-04-19     75.20          0\n",
              "2021-04-20     75.50          0\n",
              "2021-04-21     74.59          0\n",
              "2021-04-22     77.78          0\n",
              "\n",
              "[401 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "105qNoy1EwWd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "601ff73b-c009-4c42-bc82-13b085516c0e"
      },
      "source": [
        "# Affiche les informations sur les anomalies\n",
        "print(df_paris['Anomalies'].value_counts())"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    397\n",
            "1      4\n",
            "Name: Anomalies, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaV_MfJyFXkF"
      },
      "source": [
        "**3. Affichage des anomalies sur le graphique**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2idYKYImFh8v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "08e15cca-91bf-4f40-cabb-19d12301e390"
      },
      "source": [
        "# Affiche la série\n",
        "\n",
        "fig = px.line(x=df_paris.index,y=df_paris['taux'],title=\"Evolution du prix du BTC\")\n",
        "fig.add_trace(px.scatter(x=df_paris.index,y=df_paris['Anomalies']*df_paris['taux'],color=df_paris['Anomalies'].astype(np.bool)).data[0])\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"d761acf0-dc52-4b7e-9227-37df53b1147a\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"d761acf0-dc52-4b7e-9227-37df53b1147a\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'd761acf0-dc52-4b7e-9227-37df53b1147a',\n",
              "                        [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"x=%{x}<br>y=%{y}\", \"legendgroup\": \"\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [\"2020-03-18T00:00:00\", \"2020-03-19T00:00:00\", \"2020-03-20T00:00:00\", \"2020-03-21T00:00:00\", \"2020-03-22T00:00:00\", \"2020-03-23T00:00:00\", \"2020-03-24T00:00:00\", \"2020-03-25T00:00:00\", \"2020-03-26T00:00:00\", \"2020-03-27T00:00:00\", \"2020-03-28T00:00:00\", \"2020-03-29T00:00:00\", \"2020-03-30T00:00:00\", \"2020-03-31T00:00:00\", \"2020-04-01T00:00:00\", \"2020-04-02T00:00:00\", \"2020-04-03T00:00:00\", \"2020-04-04T00:00:00\", \"2020-04-05T00:00:00\", \"2020-04-06T00:00:00\", \"2020-04-07T00:00:00\", \"2020-04-08T00:00:00\", \"2020-04-09T00:00:00\", \"2020-04-10T00:00:00\", \"2020-04-11T00:00:00\", \"2020-04-12T00:00:00\", \"2020-04-13T00:00:00\", \"2020-04-14T00:00:00\", \"2020-04-15T00:00:00\", \"2020-04-16T00:00:00\", \"2020-04-17T00:00:00\", \"2020-04-18T00:00:00\", \"2020-04-19T00:00:00\", \"2020-04-20T00:00:00\", \"2020-04-21T00:00:00\", \"2020-04-22T00:00:00\", \"2020-04-23T00:00:00\", \"2020-04-24T00:00:00\", \"2020-04-25T00:00:00\", \"2020-04-26T00:00:00\", \"2020-04-27T00:00:00\", \"2020-04-28T00:00:00\", \"2020-04-29T00:00:00\", \"2020-04-30T00:00:00\", \"2020-05-01T00:00:00\", \"2020-05-02T00:00:00\", \"2020-05-03T00:00:00\", \"2020-05-04T00:00:00\", \"2020-05-05T00:00:00\", \"2020-05-06T00:00:00\", \"2020-05-07T00:00:00\", \"2020-05-08T00:00:00\", \"2020-05-09T00:00:00\", \"2020-05-10T00:00:00\", \"2020-05-11T00:00:00\", \"2020-05-12T00:00:00\", \"2020-05-13T00:00:00\", \"2020-05-14T00:00:00\", \"2020-05-15T00:00:00\", \"2020-05-16T00:00:00\", \"2020-05-17T00:00:00\", \"2020-05-18T00:00:00\", \"2020-05-19T00:00:00\", \"2020-05-20T00:00:00\", \"2020-05-21T00:00:00\", \"2020-05-22T00:00:00\", \"2020-05-23T00:00:00\", \"2020-05-24T00:00:00\", \"2020-05-25T00:00:00\", \"2020-05-26T00:00:00\", \"2020-05-27T00:00:00\", \"2020-05-28T00:00:00\", \"2020-05-29T00:00:00\", \"2020-05-30T00:00:00\", \"2020-05-31T00:00:00\", \"2020-06-01T00:00:00\", \"2020-06-02T00:00:00\", \"2020-06-03T00:00:00\", \"2020-06-04T00:00:00\", \"2020-06-05T00:00:00\", \"2020-06-06T00:00:00\", \"2020-06-07T00:00:00\", \"2020-06-08T00:00:00\", \"2020-06-09T00:00:00\", \"2020-06-10T00:00:00\", \"2020-06-11T00:00:00\", \"2020-06-12T00:00:00\", \"2020-06-13T00:00:00\", \"2020-06-14T00:00:00\", \"2020-06-15T00:00:00\", \"2020-06-16T00:00:00\", \"2020-06-17T00:00:00\", \"2020-06-18T00:00:00\", \"2020-06-19T00:00:00\", \"2020-06-20T00:00:00\", \"2020-06-21T00:00:00\", \"2020-06-22T00:00:00\", \"2020-06-23T00:00:00\", \"2020-06-24T00:00:00\", \"2020-06-25T00:00:00\", \"2020-06-26T00:00:00\", \"2020-06-27T00:00:00\", \"2020-06-28T00:00:00\", \"2020-06-29T00:00:00\", \"2020-06-30T00:00:00\", \"2020-07-01T00:00:00\", \"2020-07-02T00:00:00\", \"2020-07-03T00:00:00\", \"2020-07-04T00:00:00\", \"2020-07-05T00:00:00\", \"2020-07-06T00:00:00\", \"2020-07-07T00:00:00\", \"2020-07-08T00:00:00\", \"2020-07-09T00:00:00\", \"2020-07-10T00:00:00\", \"2020-07-11T00:00:00\", \"2020-07-12T00:00:00\", \"2020-07-13T00:00:00\", \"2020-07-14T00:00:00\", \"2020-07-15T00:00:00\", \"2020-07-16T00:00:00\", \"2020-07-17T00:00:00\", \"2020-07-18T00:00:00\", \"2020-07-19T00:00:00\", \"2020-07-20T00:00:00\", \"2020-07-21T00:00:00\", \"2020-07-22T00:00:00\", \"2020-07-23T00:00:00\", \"2020-07-24T00:00:00\", \"2020-07-25T00:00:00\", \"2020-07-26T00:00:00\", \"2020-07-27T00:00:00\", \"2020-07-28T00:00:00\", \"2020-07-29T00:00:00\", \"2020-07-30T00:00:00\", \"2020-07-31T00:00:00\", \"2020-08-01T00:00:00\", \"2020-08-02T00:00:00\", \"2020-08-03T00:00:00\", \"2020-08-04T00:00:00\", \"2020-08-05T00:00:00\", \"2020-08-06T00:00:00\", \"2020-08-07T00:00:00\", \"2020-08-08T00:00:00\", \"2020-08-09T00:00:00\", \"2020-08-10T00:00:00\", \"2020-08-11T00:00:00\", \"2020-08-12T00:00:00\", \"2020-08-13T00:00:00\", \"2020-08-14T00:00:00\", \"2020-08-15T00:00:00\", \"2020-08-16T00:00:00\", \"2020-08-17T00:00:00\", \"2020-08-18T00:00:00\", \"2020-08-19T00:00:00\", \"2020-08-20T00:00:00\", \"2020-08-21T00:00:00\", \"2020-08-22T00:00:00\", \"2020-08-23T00:00:00\", \"2020-08-24T00:00:00\", \"2020-08-25T00:00:00\", \"2020-08-26T00:00:00\", \"2020-08-27T00:00:00\", \"2020-08-28T00:00:00\", \"2020-08-29T00:00:00\", \"2020-08-30T00:00:00\", \"2020-08-31T00:00:00\", \"2020-09-01T00:00:00\", \"2020-09-02T00:00:00\", \"2020-09-03T00:00:00\", \"2020-09-04T00:00:00\", \"2020-09-05T00:00:00\", \"2020-09-06T00:00:00\", \"2020-09-07T00:00:00\", \"2020-09-08T00:00:00\", \"2020-09-09T00:00:00\", \"2020-09-10T00:00:00\", \"2020-09-11T00:00:00\", \"2020-09-12T00:00:00\", \"2020-09-13T00:00:00\", \"2020-09-14T00:00:00\", \"2020-09-15T00:00:00\", \"2020-09-16T00:00:00\", \"2020-09-17T00:00:00\", \"2020-09-18T00:00:00\", \"2020-09-19T00:00:00\", \"2020-09-20T00:00:00\", \"2020-09-21T00:00:00\", \"2020-09-22T00:00:00\", \"2020-09-23T00:00:00\", \"2020-09-24T00:00:00\", \"2020-09-25T00:00:00\", \"2020-09-26T00:00:00\", \"2020-09-27T00:00:00\", \"2020-09-28T00:00:00\", \"2020-09-29T00:00:00\", \"2020-09-30T00:00:00\", \"2020-10-01T00:00:00\", \"2020-10-02T00:00:00\", \"2020-10-03T00:00:00\", \"2020-10-04T00:00:00\", \"2020-10-05T00:00:00\", \"2020-10-06T00:00:00\", \"2020-10-07T00:00:00\", \"2020-10-08T00:00:00\", \"2020-10-09T00:00:00\", \"2020-10-10T00:00:00\", \"2020-10-11T00:00:00\", \"2020-10-12T00:00:00\", \"2020-10-13T00:00:00\", \"2020-10-14T00:00:00\", \"2020-10-15T00:00:00\", \"2020-10-16T00:00:00\", \"2020-10-17T00:00:00\", \"2020-10-18T00:00:00\", \"2020-10-19T00:00:00\", \"2020-10-20T00:00:00\", \"2020-10-21T00:00:00\", \"2020-10-22T00:00:00\", \"2020-10-23T00:00:00\", \"2020-10-24T00:00:00\", \"2020-10-25T00:00:00\", \"2020-10-26T00:00:00\", \"2020-10-27T00:00:00\", \"2020-10-28T00:00:00\", \"2020-10-29T00:00:00\", \"2020-10-30T00:00:00\", \"2020-10-31T00:00:00\", \"2020-11-01T00:00:00\", \"2020-11-02T00:00:00\", \"2020-11-03T00:00:00\", \"2020-11-04T00:00:00\", \"2020-11-05T00:00:00\", \"2020-11-06T00:00:00\", \"2020-11-07T00:00:00\", \"2020-11-08T00:00:00\", \"2020-11-09T00:00:00\", \"2020-11-10T00:00:00\", \"2020-11-11T00:00:00\", \"2020-11-12T00:00:00\", \"2020-11-13T00:00:00\", \"2020-11-14T00:00:00\", \"2020-11-15T00:00:00\", \"2020-11-16T00:00:00\", \"2020-11-17T00:00:00\", \"2020-11-18T00:00:00\", \"2020-11-19T00:00:00\", \"2020-11-20T00:00:00\", \"2020-11-21T00:00:00\", \"2020-11-22T00:00:00\", \"2020-11-23T00:00:00\", \"2020-11-24T00:00:00\", \"2020-11-25T00:00:00\", \"2020-11-26T00:00:00\", \"2020-11-27T00:00:00\", \"2020-11-28T00:00:00\", \"2020-11-29T00:00:00\", \"2020-11-30T00:00:00\", \"2020-12-01T00:00:00\", \"2020-12-02T00:00:00\", \"2020-12-03T00:00:00\", \"2020-12-04T00:00:00\", \"2020-12-05T00:00:00\", \"2020-12-06T00:00:00\", \"2020-12-07T00:00:00\", \"2020-12-08T00:00:00\", \"2020-12-09T00:00:00\", \"2020-12-10T00:00:00\", \"2020-12-11T00:00:00\", \"2020-12-12T00:00:00\", \"2020-12-13T00:00:00\", \"2020-12-14T00:00:00\", \"2020-12-15T00:00:00\", \"2020-12-16T00:00:00\", \"2020-12-17T00:00:00\", \"2020-12-18T00:00:00\", \"2020-12-19T00:00:00\", \"2020-12-20T00:00:00\", \"2020-12-21T00:00:00\", \"2020-12-22T00:00:00\", \"2020-12-23T00:00:00\", \"2020-12-24T00:00:00\", \"2020-12-25T00:00:00\", \"2020-12-26T00:00:00\", \"2020-12-27T00:00:00\", \"2020-12-28T00:00:00\", \"2020-12-29T00:00:00\", \"2020-12-30T00:00:00\", \"2020-12-31T00:00:00\", \"2021-01-01T00:00:00\", \"2021-01-02T00:00:00\", \"2021-01-03T00:00:00\", \"2021-01-04T00:00:00\", \"2021-01-05T00:00:00\", \"2021-01-06T00:00:00\", \"2021-01-07T00:00:00\", \"2021-01-08T00:00:00\", \"2021-01-09T00:00:00\", \"2021-01-10T00:00:00\", \"2021-01-11T00:00:00\", \"2021-01-12T00:00:00\", \"2021-01-13T00:00:00\", \"2021-01-14T00:00:00\", \"2021-01-15T00:00:00\", \"2021-01-16T00:00:00\", \"2021-01-17T00:00:00\", \"2021-01-18T00:00:00\", \"2021-01-19T00:00:00\", \"2021-01-20T00:00:00\", \"2021-01-21T00:00:00\", \"2021-01-22T00:00:00\", \"2021-01-23T00:00:00\", \"2021-01-24T00:00:00\", \"2021-01-25T00:00:00\", \"2021-01-26T00:00:00\", \"2021-01-27T00:00:00\", \"2021-01-28T00:00:00\", \"2021-01-29T00:00:00\", \"2021-01-30T00:00:00\", \"2021-01-31T00:00:00\", \"2021-02-01T00:00:00\", \"2021-02-02T00:00:00\", \"2021-02-03T00:00:00\", \"2021-02-04T00:00:00\", \"2021-02-05T00:00:00\", \"2021-02-06T00:00:00\", \"2021-02-07T00:00:00\", \"2021-02-08T00:00:00\", \"2021-02-09T00:00:00\", \"2021-02-10T00:00:00\", \"2021-02-11T00:00:00\", \"2021-02-12T00:00:00\", \"2021-02-13T00:00:00\", \"2021-02-14T00:00:00\", \"2021-02-15T00:00:00\", \"2021-02-16T00:00:00\", \"2021-02-17T00:00:00\", \"2021-02-18T00:00:00\", \"2021-02-19T00:00:00\", \"2021-02-20T00:00:00\", \"2021-02-21T00:00:00\", \"2021-02-22T00:00:00\", \"2021-02-23T00:00:00\", \"2021-02-24T00:00:00\", \"2021-02-25T00:00:00\", \"2021-02-26T00:00:00\", \"2021-02-27T00:00:00\", \"2021-02-28T00:00:00\", \"2021-03-01T00:00:00\", \"2021-03-02T00:00:00\", \"2021-03-03T00:00:00\", \"2021-03-04T00:00:00\", \"2021-03-05T00:00:00\", \"2021-03-06T00:00:00\", \"2021-03-07T00:00:00\", \"2021-03-08T00:00:00\", \"2021-03-09T00:00:00\", \"2021-03-10T00:00:00\", \"2021-03-11T00:00:00\", \"2021-03-12T00:00:00\", \"2021-03-13T00:00:00\", \"2021-03-14T00:00:00\", \"2021-03-15T00:00:00\", \"2021-03-16T00:00:00\", \"2021-03-17T00:00:00\", \"2021-03-18T00:00:00\", \"2021-03-19T00:00:00\", \"2021-03-20T00:00:00\", \"2021-03-21T00:00:00\", \"2021-03-22T00:00:00\", \"2021-03-23T00:00:00\", \"2021-03-24T00:00:00\", \"2021-03-25T00:00:00\", \"2021-03-26T00:00:00\", \"2021-03-27T00:00:00\", \"2021-03-28T00:00:00\", \"2021-03-29T00:00:00\", \"2021-03-30T00:00:00\", \"2021-03-31T00:00:00\", \"2021-04-01T00:00:00\", \"2021-04-02T00:00:00\", \"2021-04-03T00:00:00\", \"2021-04-04T00:00:00\", \"2021-04-05T00:00:00\", \"2021-04-06T00:00:00\", \"2021-04-07T00:00:00\", \"2021-04-08T00:00:00\", \"2021-04-09T00:00:00\", \"2021-04-10T00:00:00\", \"2021-04-11T00:00:00\", \"2021-04-12T00:00:00\", \"2021-04-13T00:00:00\", \"2021-04-14T00:00:00\", \"2021-04-15T00:00:00\", \"2021-04-16T00:00:00\", \"2021-04-17T00:00:00\", \"2021-04-18T00:00:00\", \"2021-04-19T00:00:00\", \"2021-04-20T00:00:00\", \"2021-04-21T00:00:00\", \"2021-04-22T00:00:00\"], \"xaxis\": \"x\", \"y\": [108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 121.47, 141.71, 165.16, 189.97, 196.36, 196.82, 237.61, 264.25, 290.58, 333.2, 369.58, 384.96, 387.09, 428.8, 494.1, 554.07, 608.41, 681.63, 705.83, 712.99, 779.51, 838.11, 889.25, 926.24, 933.25, 939.03, 939.79, 961.71, 972.36, 943.29, 916.35, 879.51, 859.57, 854.09, 783.62, 694.57, 558.94, 504.75, 459.09, 443.71, 442.04, 397.29, 349.64, 386.78, 338.53, 299.87, 286.17, 284.04, 246.29, 221.17, 203.36, 197.12, 186.31, 188.9, 188.29, 172.92, 166.07, 167.59, 152.67, 148.26, 145.06, 145.06, 149.93, 156.78, 153.28, 156.33, 157.09, 158.91, 159.22, 158.31, 161.35, 161.96, 167.74, 182.97, 198.95, 215.69, 213.41, 220.87, 223.3, 212.04, 173.22, 159.83, 144.3, 147.96, 142.17, 148.87, 156.48, 155.41, 157.39, 158.61, 170.79, 178.25, 174.75, 180.99, 218.89, 215.39, 218.13, 209.91, 202.3, 206.71, 206.1, 203.36, 205.49, 200.17, 208.84, 214.32, 214.78, 218.58, 218.28, 223.46, 225.59, 231.98, 238.98, 238.98, 238.37, 244.77, 245.53, 244.61, 246.29, 241.87, 249.94, 253.9, 249.79, 243.55, 242.63, 224.06, 224.06, 215.69, 206.71, 200.01, 199.86, 200.01, 202.14, 193.77, 188.14, 191.34, 190.88, 191.95, 193.32, 200.17, 202.45, 209.91, 206.1, 209.15, 208.84, 206.71, 196.06, 189.05, 179.01, 173.68, 169.42, 169.11, 169.57, 170.64, 178.09, 179.62, 187.68, 197.27, 200.93, 202.6, 213.87, 228.48, 237.0, 247.2, 254.2, 259.84, 260.44, 272.32, 274.9, 293.63, 307.94, 327.42, 335.33, 335.64, 354.21, 371.26, 381.0, 398.35, 401.85, 403.99, 405.05, 329.86, 350.4, 345.84, 324.68, 314.79, 313.26, 313.57, 379.33, 340.21, 334.42, 331.68, 318.29, 314.18, 312.35, 305.04, 235.66750000000002, 166.29500000000002, 96.92250000000001, 27.55, 31.36, 33.64, 38.05, 46.43, 48.1, 49.17, 45.49307692307692, 41.81615384615384, 38.139230769230764, 34.46230769230769, 30.78538461538461, 27.108461538461537, 23.43153846153846, 19.75461538461538, 16.077692307692306, 12.40076923076923, 8.723846153846154, 5.046923076923076, 1.37, 2.74, 3.5, 3.5, 3.5, 5.18, 6.55, 6.85, 5.63, 5.94, 6.24, 6.24, 5.02, 3.96, 2.89, 2.89, 2.74, 2.74, 2.89, 3.04, 3.2, 2.89, 2.74, 2.59, 2.74, 2.74, 2.13, 1.83, 1.67, 2.89, 2.74, 2.28, 2.28, 2.28, 2.59, 3.2, 2.59, 2.44, 3.04, 2.89, 3.65, 4.72, 4.26, 4.41, 5.18, 5.48, 5.48, 5.63, 5.33, 5.48, 4.72, 3.96, 3.2, 3.2, 2.59, 2.13, 2.13, 2.59, 2.59, 2.44, 2.59, 2.28, 1.52, 1.52, 1.07, 1.52, 2.13, 1.98, 2.44, 3.96, 4.26, 5.18, 7.46, 7.15, 7.46, 8.98, 8.98, 9.29, 10.96, 10.35, 10.81, 10.81, 10.2, 10.81, 11.26, 10.96, 9.74, 10.2, 10.05, 12.03, 11.57, 12.33, 12.48, 14.92, 13.85, 13.7, 14.77, 16.59, 18.42, 21.01, 24.81, 27.25, 84.79, 90.26, 91.48, 72.0, 71.85, 72.0, 75.35, 49.17, 56.02, 61.34, 63.17, 65.3, 67.89, 70.32, 70.48, 69.11, 102.14, 94.53, 86.61, 84.48, 75.8, 76.11, 80.37, 89.81, 102.9, 102.29, 102.75, 100.62, 100.46, 81.28, 80.68, 80.22, 75.2, 75.5, 74.59, 77.78], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"x=%{x}<br>y=%{y}<br>color=%{marker.color}\", \"legendgroup\": \"\", \"marker\": {\"color\": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], \"coloraxis\": \"coloraxis\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [\"2020-03-18T00:00:00\", \"2020-03-19T00:00:00\", \"2020-03-20T00:00:00\", \"2020-03-21T00:00:00\", \"2020-03-22T00:00:00\", \"2020-03-23T00:00:00\", \"2020-03-24T00:00:00\", \"2020-03-25T00:00:00\", \"2020-03-26T00:00:00\", \"2020-03-27T00:00:00\", \"2020-03-28T00:00:00\", \"2020-03-29T00:00:00\", \"2020-03-30T00:00:00\", \"2020-03-31T00:00:00\", \"2020-04-01T00:00:00\", \"2020-04-02T00:00:00\", \"2020-04-03T00:00:00\", \"2020-04-04T00:00:00\", \"2020-04-05T00:00:00\", \"2020-04-06T00:00:00\", \"2020-04-07T00:00:00\", \"2020-04-08T00:00:00\", \"2020-04-09T00:00:00\", \"2020-04-10T00:00:00\", \"2020-04-11T00:00:00\", \"2020-04-12T00:00:00\", \"2020-04-13T00:00:00\", \"2020-04-14T00:00:00\", \"2020-04-15T00:00:00\", \"2020-04-16T00:00:00\", \"2020-04-17T00:00:00\", \"2020-04-18T00:00:00\", \"2020-04-19T00:00:00\", \"2020-04-20T00:00:00\", \"2020-04-21T00:00:00\", \"2020-04-22T00:00:00\", \"2020-04-23T00:00:00\", \"2020-04-24T00:00:00\", \"2020-04-25T00:00:00\", \"2020-04-26T00:00:00\", \"2020-04-27T00:00:00\", \"2020-04-28T00:00:00\", \"2020-04-29T00:00:00\", \"2020-04-30T00:00:00\", \"2020-05-01T00:00:00\", \"2020-05-02T00:00:00\", \"2020-05-03T00:00:00\", \"2020-05-04T00:00:00\", \"2020-05-05T00:00:00\", \"2020-05-06T00:00:00\", \"2020-05-07T00:00:00\", \"2020-05-08T00:00:00\", \"2020-05-09T00:00:00\", \"2020-05-10T00:00:00\", \"2020-05-11T00:00:00\", \"2020-05-12T00:00:00\", \"2020-05-13T00:00:00\", \"2020-05-14T00:00:00\", \"2020-05-15T00:00:00\", \"2020-05-16T00:00:00\", \"2020-05-17T00:00:00\", \"2020-05-18T00:00:00\", \"2020-05-19T00:00:00\", \"2020-05-20T00:00:00\", \"2020-05-21T00:00:00\", \"2020-05-22T00:00:00\", \"2020-05-23T00:00:00\", \"2020-05-24T00:00:00\", \"2020-05-25T00:00:00\", \"2020-05-26T00:00:00\", \"2020-05-27T00:00:00\", \"2020-05-28T00:00:00\", \"2020-05-29T00:00:00\", \"2020-05-30T00:00:00\", \"2020-05-31T00:00:00\", \"2020-06-01T00:00:00\", \"2020-06-02T00:00:00\", \"2020-06-03T00:00:00\", \"2020-06-04T00:00:00\", \"2020-06-05T00:00:00\", \"2020-06-06T00:00:00\", \"2020-06-07T00:00:00\", \"2020-06-08T00:00:00\", \"2020-06-09T00:00:00\", \"2020-06-10T00:00:00\", \"2020-06-11T00:00:00\", \"2020-06-12T00:00:00\", \"2020-06-13T00:00:00\", \"2020-06-14T00:00:00\", \"2020-06-15T00:00:00\", \"2020-06-16T00:00:00\", \"2020-06-17T00:00:00\", \"2020-06-18T00:00:00\", \"2020-06-19T00:00:00\", \"2020-06-20T00:00:00\", \"2020-06-21T00:00:00\", \"2020-06-22T00:00:00\", \"2020-06-23T00:00:00\", \"2020-06-24T00:00:00\", \"2020-06-25T00:00:00\", \"2020-06-26T00:00:00\", \"2020-06-27T00:00:00\", \"2020-06-28T00:00:00\", \"2020-06-29T00:00:00\", \"2020-06-30T00:00:00\", \"2020-07-01T00:00:00\", \"2020-07-02T00:00:00\", \"2020-07-03T00:00:00\", \"2020-07-04T00:00:00\", \"2020-07-05T00:00:00\", \"2020-07-06T00:00:00\", \"2020-07-07T00:00:00\", \"2020-07-08T00:00:00\", \"2020-07-09T00:00:00\", \"2020-07-10T00:00:00\", \"2020-07-11T00:00:00\", \"2020-07-12T00:00:00\", \"2020-07-13T00:00:00\", \"2020-07-14T00:00:00\", \"2020-07-15T00:00:00\", \"2020-07-16T00:00:00\", \"2020-07-17T00:00:00\", \"2020-07-18T00:00:00\", \"2020-07-19T00:00:00\", \"2020-07-20T00:00:00\", \"2020-07-21T00:00:00\", \"2020-07-22T00:00:00\", \"2020-07-23T00:00:00\", \"2020-07-24T00:00:00\", \"2020-07-25T00:00:00\", \"2020-07-26T00:00:00\", \"2020-07-27T00:00:00\", \"2020-07-28T00:00:00\", \"2020-07-29T00:00:00\", \"2020-07-30T00:00:00\", \"2020-07-31T00:00:00\", \"2020-08-01T00:00:00\", \"2020-08-02T00:00:00\", \"2020-08-03T00:00:00\", \"2020-08-04T00:00:00\", \"2020-08-05T00:00:00\", \"2020-08-06T00:00:00\", \"2020-08-07T00:00:00\", \"2020-08-08T00:00:00\", \"2020-08-09T00:00:00\", \"2020-08-10T00:00:00\", \"2020-08-11T00:00:00\", \"2020-08-12T00:00:00\", \"2020-08-13T00:00:00\", \"2020-08-14T00:00:00\", \"2020-08-15T00:00:00\", \"2020-08-16T00:00:00\", \"2020-08-17T00:00:00\", \"2020-08-18T00:00:00\", \"2020-08-19T00:00:00\", \"2020-08-20T00:00:00\", \"2020-08-21T00:00:00\", \"2020-08-22T00:00:00\", \"2020-08-23T00:00:00\", \"2020-08-24T00:00:00\", \"2020-08-25T00:00:00\", \"2020-08-26T00:00:00\", \"2020-08-27T00:00:00\", \"2020-08-28T00:00:00\", \"2020-08-29T00:00:00\", \"2020-08-30T00:00:00\", \"2020-08-31T00:00:00\", \"2020-09-01T00:00:00\", \"2020-09-02T00:00:00\", \"2020-09-03T00:00:00\", \"2020-09-04T00:00:00\", \"2020-09-05T00:00:00\", \"2020-09-06T00:00:00\", \"2020-09-07T00:00:00\", \"2020-09-08T00:00:00\", \"2020-09-09T00:00:00\", \"2020-09-10T00:00:00\", \"2020-09-11T00:00:00\", \"2020-09-12T00:00:00\", \"2020-09-13T00:00:00\", \"2020-09-14T00:00:00\", \"2020-09-15T00:00:00\", \"2020-09-16T00:00:00\", \"2020-09-17T00:00:00\", \"2020-09-18T00:00:00\", \"2020-09-19T00:00:00\", \"2020-09-20T00:00:00\", \"2020-09-21T00:00:00\", \"2020-09-22T00:00:00\", \"2020-09-23T00:00:00\", \"2020-09-24T00:00:00\", \"2020-09-25T00:00:00\", \"2020-09-26T00:00:00\", \"2020-09-27T00:00:00\", \"2020-09-28T00:00:00\", \"2020-09-29T00:00:00\", \"2020-09-30T00:00:00\", \"2020-10-01T00:00:00\", \"2020-10-02T00:00:00\", \"2020-10-03T00:00:00\", \"2020-10-04T00:00:00\", \"2020-10-05T00:00:00\", \"2020-10-06T00:00:00\", \"2020-10-07T00:00:00\", \"2020-10-08T00:00:00\", \"2020-10-09T00:00:00\", \"2020-10-10T00:00:00\", \"2020-10-11T00:00:00\", \"2020-10-12T00:00:00\", \"2020-10-13T00:00:00\", \"2020-10-14T00:00:00\", \"2020-10-15T00:00:00\", \"2020-10-16T00:00:00\", \"2020-10-17T00:00:00\", \"2020-10-18T00:00:00\", \"2020-10-19T00:00:00\", \"2020-10-20T00:00:00\", \"2020-10-21T00:00:00\", \"2020-10-22T00:00:00\", \"2020-10-23T00:00:00\", \"2020-10-24T00:00:00\", \"2020-10-25T00:00:00\", \"2020-10-26T00:00:00\", \"2020-10-27T00:00:00\", \"2020-10-28T00:00:00\", \"2020-10-29T00:00:00\", \"2020-10-30T00:00:00\", \"2020-10-31T00:00:00\", \"2020-11-01T00:00:00\", \"2020-11-02T00:00:00\", \"2020-11-03T00:00:00\", \"2020-11-04T00:00:00\", \"2020-11-05T00:00:00\", \"2020-11-06T00:00:00\", \"2020-11-07T00:00:00\", \"2020-11-08T00:00:00\", \"2020-11-09T00:00:00\", \"2020-11-10T00:00:00\", \"2020-11-11T00:00:00\", \"2020-11-12T00:00:00\", \"2020-11-13T00:00:00\", \"2020-11-14T00:00:00\", \"2020-11-15T00:00:00\", \"2020-11-16T00:00:00\", \"2020-11-17T00:00:00\", \"2020-11-18T00:00:00\", \"2020-11-19T00:00:00\", \"2020-11-20T00:00:00\", \"2020-11-21T00:00:00\", \"2020-11-22T00:00:00\", \"2020-11-23T00:00:00\", \"2020-11-24T00:00:00\", \"2020-11-25T00:00:00\", \"2020-11-26T00:00:00\", \"2020-11-27T00:00:00\", \"2020-11-28T00:00:00\", \"2020-11-29T00:00:00\", \"2020-11-30T00:00:00\", \"2020-12-01T00:00:00\", \"2020-12-02T00:00:00\", \"2020-12-03T00:00:00\", \"2020-12-04T00:00:00\", \"2020-12-05T00:00:00\", \"2020-12-06T00:00:00\", \"2020-12-07T00:00:00\", \"2020-12-08T00:00:00\", \"2020-12-09T00:00:00\", \"2020-12-10T00:00:00\", \"2020-12-11T00:00:00\", \"2020-12-12T00:00:00\", \"2020-12-13T00:00:00\", \"2020-12-14T00:00:00\", \"2020-12-15T00:00:00\", \"2020-12-16T00:00:00\", \"2020-12-17T00:00:00\", \"2020-12-18T00:00:00\", \"2020-12-19T00:00:00\", \"2020-12-20T00:00:00\", \"2020-12-21T00:00:00\", \"2020-12-22T00:00:00\", \"2020-12-23T00:00:00\", \"2020-12-24T00:00:00\", \"2020-12-25T00:00:00\", \"2020-12-26T00:00:00\", \"2020-12-27T00:00:00\", \"2020-12-28T00:00:00\", \"2020-12-29T00:00:00\", \"2020-12-30T00:00:00\", \"2020-12-31T00:00:00\", \"2021-01-01T00:00:00\", \"2021-01-02T00:00:00\", \"2021-01-03T00:00:00\", \"2021-01-04T00:00:00\", \"2021-01-05T00:00:00\", \"2021-01-06T00:00:00\", \"2021-01-07T00:00:00\", \"2021-01-08T00:00:00\", \"2021-01-09T00:00:00\", \"2021-01-10T00:00:00\", \"2021-01-11T00:00:00\", \"2021-01-12T00:00:00\", \"2021-01-13T00:00:00\", \"2021-01-14T00:00:00\", \"2021-01-15T00:00:00\", \"2021-01-16T00:00:00\", \"2021-01-17T00:00:00\", \"2021-01-18T00:00:00\", \"2021-01-19T00:00:00\", \"2021-01-20T00:00:00\", \"2021-01-21T00:00:00\", \"2021-01-22T00:00:00\", \"2021-01-23T00:00:00\", \"2021-01-24T00:00:00\", \"2021-01-25T00:00:00\", \"2021-01-26T00:00:00\", \"2021-01-27T00:00:00\", \"2021-01-28T00:00:00\", \"2021-01-29T00:00:00\", \"2021-01-30T00:00:00\", \"2021-01-31T00:00:00\", \"2021-02-01T00:00:00\", \"2021-02-02T00:00:00\", \"2021-02-03T00:00:00\", \"2021-02-04T00:00:00\", \"2021-02-05T00:00:00\", \"2021-02-06T00:00:00\", \"2021-02-07T00:00:00\", \"2021-02-08T00:00:00\", \"2021-02-09T00:00:00\", \"2021-02-10T00:00:00\", \"2021-02-11T00:00:00\", \"2021-02-12T00:00:00\", \"2021-02-13T00:00:00\", \"2021-02-14T00:00:00\", \"2021-02-15T00:00:00\", \"2021-02-16T00:00:00\", \"2021-02-17T00:00:00\", \"2021-02-18T00:00:00\", \"2021-02-19T00:00:00\", \"2021-02-20T00:00:00\", \"2021-02-21T00:00:00\", \"2021-02-22T00:00:00\", \"2021-02-23T00:00:00\", \"2021-02-24T00:00:00\", \"2021-02-25T00:00:00\", \"2021-02-26T00:00:00\", \"2021-02-27T00:00:00\", \"2021-02-28T00:00:00\", \"2021-03-01T00:00:00\", \"2021-03-02T00:00:00\", \"2021-03-03T00:00:00\", \"2021-03-04T00:00:00\", \"2021-03-05T00:00:00\", \"2021-03-06T00:00:00\", \"2021-03-07T00:00:00\", \"2021-03-08T00:00:00\", \"2021-03-09T00:00:00\", \"2021-03-10T00:00:00\", \"2021-03-11T00:00:00\", \"2021-03-12T00:00:00\", \"2021-03-13T00:00:00\", \"2021-03-14T00:00:00\", \"2021-03-15T00:00:00\", \"2021-03-16T00:00:00\", \"2021-03-17T00:00:00\", \"2021-03-18T00:00:00\", \"2021-03-19T00:00:00\", \"2021-03-20T00:00:00\", \"2021-03-21T00:00:00\", \"2021-03-22T00:00:00\", \"2021-03-23T00:00:00\", \"2021-03-24T00:00:00\", \"2021-03-25T00:00:00\", \"2021-03-26T00:00:00\", \"2021-03-27T00:00:00\", \"2021-03-28T00:00:00\", \"2021-03-29T00:00:00\", \"2021-03-30T00:00:00\", \"2021-03-31T00:00:00\", \"2021-04-01T00:00:00\", \"2021-04-02T00:00:00\", \"2021-04-03T00:00:00\", \"2021-04-04T00:00:00\", \"2021-04-05T00:00:00\", \"2021-04-06T00:00:00\", \"2021-04-07T00:00:00\", \"2021-04-08T00:00:00\", \"2021-04-09T00:00:00\", \"2021-04-10T00:00:00\", \"2021-04-11T00:00:00\", \"2021-04-12T00:00:00\", \"2021-04-13T00:00:00\", \"2021-04-14T00:00:00\", \"2021-04-15T00:00:00\", \"2021-04-16T00:00:00\", \"2021-04-17T00:00:00\", \"2021-04-18T00:00:00\", \"2021-04-19T00:00:00\", \"2021-04-20T00:00:00\", \"2021-04-21T00:00:00\", \"2021-04-22T00:00:00\"], \"xaxis\": \"x\", \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 608.41, 0.0, 0.0, 0.0, 0.0, 838.11, 0.0, 0.0, 0.0, 0.0, 0.0, 961.71, 972.36, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], \"yaxis\": \"y\"}],\n",
              "                        {\"legend\": {\"tracegroupgap\": 0}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Evolution du prix du BTC\"}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"rangeslider\": {\"visible\": true}, \"title\": {\"text\": \"x\"}}, \"yaxis\": {\"anchor\": \"x\", \"autorange\": true, \"domain\": [0.0, 1.0], \"fixedrange\": false, \"title\": {\"text\": \"y\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('d761acf0-dc52-4b7e-9227-37df53b1147a');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVAWVQioe-kS"
      },
      "source": [
        "Comme les anomalies détectées ne sembles pas cohérentes, nous n'allons pas les traiter..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vDYEK-cxE0C"
      },
      "source": [
        "# Analyse de la série"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGY4fCB3xdUx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "outputId": "89d1ab50-bfa2-402c-be37-c9da63377843"
      },
      "source": [
        "df_paris"
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>taux</th>\n",
              "      <th>Anomalies</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>extract_date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2020-03-18</th>\n",
              "      <td>108.53</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-19</th>\n",
              "      <td>108.53</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-20</th>\n",
              "      <td>108.53</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-21</th>\n",
              "      <td>108.53</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-22</th>\n",
              "      <td>108.53</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-18</th>\n",
              "      <td>80.22</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-19</th>\n",
              "      <td>75.20</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-20</th>\n",
              "      <td>75.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-21</th>\n",
              "      <td>74.59</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-22</th>\n",
              "      <td>77.78</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>401 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                taux  Anomalies\n",
              "extract_date                   \n",
              "2020-03-18    108.53          0\n",
              "2020-03-19    108.53          0\n",
              "2020-03-20    108.53          0\n",
              "2020-03-21    108.53          0\n",
              "2020-03-22    108.53          0\n",
              "...              ...        ...\n",
              "2021-04-18     80.22          0\n",
              "2021-04-19     75.20          0\n",
              "2021-04-20     75.50          0\n",
              "2021-04-21     74.59          0\n",
              "2021-04-22     77.78          0\n",
              "\n",
              "[401 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 226
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBCbjPSNxWXy"
      },
      "source": [
        "**1. ACF & PACF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CleN4htOxYqZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "6e2a84ae-228d-48d0-887c-082f729e315c"
      },
      "source": [
        "# ACF & PACF du bruit blanc\n",
        "\n",
        "serie = df_paris['taux']\n",
        "\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "f1, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "f1.subplots_adjust(hspace=0.3,wspace=0.2)\n",
        "\n",
        "plot_acf(serie, ax=ax1, lags = range(0,40))\n",
        "ax1.set_title(\"Autocorrélation du bruit blanc\")\n",
        "\n",
        "plot_pacf(serie, ax=ax2, lags = range(0, 40))\n",
        "ax2.set_title(\"Autocorrélation partielle du bruit blanc\")"
      ],
      "execution_count": 393,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/regression/linear_model.py:1358: RuntimeWarning:\n",
            "\n",
            "invalid value encountered in sqrt\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Autocorrélation partielle du bruit blanc')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 393
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o66MqHZTyHX5"
      },
      "source": [
        "**2. Test de Dickey-Fuller**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm5VXMhkyLSN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d66f1f85-9ef4-498a-dd7c-eedaffaf1458"
      },
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "serie_test = serie\n",
        "\n",
        "adf, p, usedlag, nobs, cvs, aic = sm.tsa.stattools.adfuller(serie_test)\n",
        "\n",
        "adf_results_string = 'ADF: {}\\np-value: {},\\nN: {}, \\ncritical values: {}'\n",
        "print(adf_results_string.format(adf, p, nobs, cvs))"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ADF: -3.2152287548025362\n",
            "p-value: 0.019119929430470778,\n",
            "N: 390, \n",
            "critical values: {'1%': -3.4472291365835566, '5%': -2.8689795375849223, '10%': -2.5707330834976987}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moHyQgGfyTi4"
      },
      "source": [
        "**3. Suppression de la tendance non linéaire et test de sationnarité**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43AcGds6y0pI"
      },
      "source": [
        "from scipy.stats import boxcox\n",
        "\n",
        "serie_log, lam = boxcox(serie)\n",
        "\n",
        "f1, (ax1,ax2) = plt.subplots(2, 1, figsize=(15, 5))\n",
        "ax1.plot(serie_etude.index,serie_log)\n",
        "ax2.plot(serie_etude.index,serie)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fRPXCkO0DUh"
      },
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "serie_test = serie_log\n",
        "\n",
        "adf, p, usedlag, nobs, cvs, aic = sm.tsa.stattools.adfuller(serie_test)\n",
        "\n",
        "adf_results_string = 'ADF: {}\\np-value: {},\\nN: {}, \\ncritical values: {}'\n",
        "print(adf_results_string.format(adf, p, nobs, cvs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI8sp_Rlz6GT"
      },
      "source": [
        "***4. Suppression de la tendance linéaire et test de stationnarité***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iHrAWJH0TdT"
      },
      "source": [
        "f1, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 5))\n",
        "\n",
        "# Calcul des coefficients\n",
        "x = np.linspace(0,len(serie_log),len(serie_log))\n",
        "coefs = np.polyfit(x,serie_log,1)\n",
        "\n",
        "# Calcul de la tendance non linéaire\n",
        "trend = coefs[0]*np.power(x,1) + coefs[1]\n",
        "\n",
        "# Calcul de la série sans tendance\n",
        "serie_log_detrend = serie_log - trend\n",
        "\n",
        "# Affiche les résultats\n",
        "ax1.plot(trend)\n",
        "ax1.plot(serie_log)\n",
        "ax1.set_title(\"Série originale et tendance non linéaire\")\n",
        "\n",
        "ax2.plot(serie_log_detrend)\n",
        "ax2.set_title(\"Série avec tendance non linéaire supprimée\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNXs9Fm--Kcl"
      },
      "source": [
        "# ACF & PACF du bruit blanc\n",
        "\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "f1, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "f1.subplots_adjust(hspace=0.3,wspace=0.2)\n",
        "\n",
        "plot_acf(serie_log_detrend, ax=ax1, lags = range(0,50))\n",
        "ax1.set_title(\"Autocorrélation du bruit blanc\")\n",
        "\n",
        "plot_pacf(serie_log_detrend, ax=ax2, lags = range(0, 50))\n",
        "ax2.set_title(\"Autocorrélation partielle du bruit blanc\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_r01cDgq0xaJ"
      },
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "serie_test = serie_log_detrend\n",
        "\n",
        "adf, p, usedlag, nobs, cvs, aic = sm.tsa.stattools.adfuller(serie_test)\n",
        "\n",
        "adf_results_string = 'ADF: {}\\np-value: {},\\nN: {}, \\ncritical values: {}'\n",
        "print(adf_results_string.format(adf, p, nobs, cvs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ychdf1RxMPDD"
      },
      "source": [
        "**5. Différentiation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qaHkgtQMOqJ"
      },
      "source": [
        "# Différenciation d'odre 1 et saisonnale à l'odre 1 et de période 12\n",
        "\n",
        "from statsmodels.tsa.statespace.tools import diff\n",
        "\n",
        "serie_log_detrend_diff1 = diff(serie_log_detrend,1)       # diff=1 ; diff_saison=1 ; periode = 12\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(serie_log_detrend_diff1)\n",
        "plt.title(\"Signal différencié d'ordre 1 + saisonalité\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFLlzv0JMks5"
      },
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "serie_test = serie_log_detrend_diff1\n",
        "\n",
        "adf, p, usedlag, nobs, cvs, aic = sm.tsa.stattools.adfuller(serie_test)\n",
        "\n",
        "adf_results_string = 'ADF: {}\\np-value: {},\\nN: {}, \\ncritical values: {}'\n",
        "print(adf_results_string.format(adf, p, nobs, cvs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0Oqd_7XMqaZ"
      },
      "source": [
        "# ACF & PACF du bruit blanc\n",
        "\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "f1, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "f1.subplots_adjust(hspace=0.3,wspace=0.2)\n",
        "\n",
        "plot_acf(serie_log_detrend_diff1, ax=ax1, lags = range(0,50))\n",
        "ax1.set_title(\"Autocorrélation du bruit blanc\")\n",
        "\n",
        "plot_pacf(serie_log_detrend_diff1, ax=ax2, lags = range(0, 50))\n",
        "ax2.set_title(\"Autocorrélation partielle du bruit blanc\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5oIY2Yl5Tlt"
      },
      "source": [
        "**5. Enregistrement des données dans le dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjFSWhdeM4KM"
      },
      "source": [
        "serie_log_detrend_diff1 = np.insert(serie_log_detrend_diff1,0,0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ele3kFOp5TTW"
      },
      "source": [
        "serie_etude['diff'] = serie_log_detrend_diff1\n",
        "serie_etude['diff'][0] = \"Nan\"\n",
        "serie_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG5Fyx5O5oe5"
      },
      "source": [
        "# Prépartion des datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7cGUeWb5oe7"
      },
      "source": [
        "**1. Séparation des données en données pour l'entrainement et la validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob-EAw_j5oe8"
      },
      "source": [
        "On réserve 20% des données pour l'entrainement et le reste pour la validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5AWeK_Z5oe8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dad9e924-b273-4c22-9d3d-88982554584f"
      },
      "source": [
        "# Sépare les données en entrainement et tests\n",
        "pourcentage = 0.95\n",
        "temps_separation = int(len(df_paris['taux']) * pourcentage)\n",
        "date_separation = df_paris.index[temps_separation]\n",
        "\n",
        "serie_entrainement = df_paris['taux'].iloc[:temps_separation]\n",
        "serie_test = df_paris['taux'].iloc[temps_separation:]\n",
        "\n",
        "print(\"Taille de l'entrainement : %d\" %len(serie_entrainement))\n",
        "print(\"Taille de la validation : %d\" %len(serie_test))"
      ],
      "execution_count": 503,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Taille de l'entrainement : 380\n",
            "Taille de la validation : 21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZUMMMro5oe9"
      },
      "source": [
        "On normalise les données :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu_YxoSI5oe9"
      },
      "source": [
        "# Calcul de la moyenne et de l'écart type de la série\n",
        "mean = tf.math.reduce_mean(np.asarray(serie_entrainement))\n",
        "std = tf.math.reduce_std(np.asarray((serie_entrainement)))\n",
        "\n",
        "# Normalisation des données\n",
        "serie_entrainement = (serie_entrainement-mean)/std\n",
        "serie_test = (serie_test-mean)/std"
      ],
      "execution_count": 504,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_4OZJ-p5oe9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "f5447b0f-2c62-4292-ad7f-661663c06e06"
      },
      "source": [
        "# Affiche la série\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "ax.plot(serie_entrainement, label=\"Entrainement\")\n",
        "ax.plot(serie_test,label=\"Validation\")\n",
        "\n",
        "ax.set_title(\"Evolution du prix du BTC\")\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": 396,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-bANnT35oe-"
      },
      "source": [
        "**2. Création des datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_EweLDJ5oe-"
      },
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "\n",
        "def prepare_dataset_XY(serie, taille_fenetre, horizon, batch_size):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie)\n",
        "  dataset = dataset.window(taille_fenetre+horizon, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre + horizon))\n",
        "  dataset = dataset.map(lambda x: (tf.expand_dims(x[0:taille_fenetre],axis=1),x[-1:]))\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "  return dataset"
      ],
      "execution_count": 557,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Jh1RZYo5oe_"
      },
      "source": [
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "taille_fenetre = 5\n",
        "horizon = 5\n",
        "batch_size = 8\n",
        "\n",
        "# Création du dataset\n",
        "dataset = prepare_dataset_XY(serie_entrainement,taille_fenetre,horizon,batch_size)\n",
        "dataset_val = prepare_dataset_XY(serie_test,taille_fenetre,horizon,batch_size)"
      ],
      "execution_count": 558,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr2pbMox5oe_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fbb73fd-00de-4dcd-91d6-7706c45c03f9"
      },
      "source": [
        "print(len(list(dataset.as_numpy_iterator())))\n",
        "for element in dataset.take(1):\n",
        "  print(element[0].shape)\n",
        "  print(element[1].shape)"
      ],
      "execution_count": 559,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "46\n",
            "(8, 5, 1)\n",
            "(8, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itll4lU9q67C",
        "outputId": "868cfce8-e85e-4baf-f118-b57c0f482b71"
      },
      "source": [
        "print(len(list(dataset_val.as_numpy_iterator())))\n",
        "for element in dataset_val.take(1):\n",
        "  print(element[0].shape)\n",
        "  print(element[1].shape)"
      ],
      "execution_count": 560,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "(8, 5, 1)\n",
            "(8, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHCcYn6i5oe_"
      },
      "source": [
        "On extrait maintenant les deux tenseurs (X,Y) pour l'entrainement :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3ZhLIK15ofA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bb98d11-b634-48a6-f549-9b6cd3479565"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "#56x((1000,4,1),(1000,1)) => (56*1000,4,1) ; (56*1000,1)\n",
        "\n",
        "x,y = tuple(zip(*dataset))\n",
        "\n",
        "# Recombine les données\n",
        "# (56,1000,4,1) => (56*128,4,1)\n",
        "# (56,1000,1) => (56*128,1)\n",
        "x_train = np.asarray(tf.reshape(np.asarray(x,dtype=np.float32),shape=(np.asarray(x).shape[0]*np.asarray(x).shape[1],taille_fenetre,1)))\n",
        "y_train = np.asarray(tf.reshape(np.asarray(y,dtype=np.float32),shape=(np.asarray(y).shape[0]*np.asarray(y).shape[1])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": 561,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(368, 5, 1)\n",
            "(368,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llnKyLvl5ofA"
      },
      "source": [
        "Puis la même chose pour les données de validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hadrKVrZ5ofB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ca89747-9292-44cc-a4e7-369939d342b4"
      },
      "source": [
        "# Extrait les X,Y du dataset_val\n",
        "\n",
        "x,y = tuple(zip(*dataset_val))\n",
        "\n",
        "# Recombine les données\n",
        "\n",
        "x_val = np.asarray(tf.reshape(np.asarray(x,dtype=np.float32),shape=(np.asarray(x).shape[0]*np.asarray(x).shape[1],taille_fenetre,1)))\n",
        "y_val = np.asarray(tf.reshape(np.asarray(y,dtype=np.float32),shape=(np.asarray(y).shape[0]*np.asarray(y).shape[1])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": 562,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8, 5, 1)\n",
            "(8,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "083QISTMM3AM"
      },
      "source": [
        "# Optimisation des hyperparamètres"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VzGM7ODMf8e"
      },
      "source": [
        "**1. Création de la série horaire pour l'optimisation des hyperparamètres**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUpvJmTQiNk-"
      },
      "source": [
        "**3. Définition du modèle**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuAyb8pciUle"
      },
      "source": [
        "Dans le modèle, les paramètres dim_LSTM, l1_reg, l2_reg seront optimisés :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRY7JTnlrTMf"
      },
      "source": [
        "def ModelLSTM(dim_LSTM = 10, l1_reg=0, l2_reg=0):\n",
        "  # Définition de l'entrée du modèle\n",
        "  entrees = tf.keras.layers.Input(shape=(taille_fenetre,1))\n",
        "\n",
        "  # Encodeur\n",
        "  s_encodeur = tf.keras.layers.LSTM(dim_LSTM, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg,l2=l2_reg))(entrees)\n",
        "\n",
        "  # Décodeur\n",
        "  s_decodeur = tf.keras.layers.Dense(dim_LSTM,activation=\"tanh\",kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg,l2=l2_reg))(s_encodeur)\n",
        "  s_decodeur = tf.keras.layers.Concatenate()([s_decodeur,s_encodeur])\n",
        "\n",
        "  # Générateur\n",
        "  sortie = tf.keras.layers.Dense(1,kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg,l2=l2_reg))(s_decodeur)\n",
        "\n",
        "  # Construction du modèle\n",
        "  model = tf.keras.Model(entrees,sortie)\n",
        "  optimiseur=tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "  model.compile(loss=\"mse\", optimizer=optimiseur,metrics=[\"mse\"])\n",
        "  return(model)"
      ],
      "execution_count": 563,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYTuqrcYii9w"
      },
      "source": [
        "**4. Cross-validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_g-t1CbmsOkn",
        "outputId": "4a06d263-9c0d-40aa-fe20-072280b9b588"
      },
      "source": [
        "batch_size"
      ],
      "execution_count": 474,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 474
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVCNBdgcihuv"
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import KFold, TimeSeriesSplit, GridSearchCV\n",
        "\n",
        "# Définitions des paramètres\n",
        "dim_LSTM = [5,10,20,40,60]\n",
        "l1_reg = [0,0.001,0.01]\n",
        "l2_reg = [0,0.001,0.01]\n",
        "\n",
        "param_grid = {'dim_LSTM': dim_LSTM, 'l1_reg': l1_reg, 'l2_reg': l2_reg}\n",
        "\n",
        "max_periodes = 5\n",
        "\n",
        "# Surveillance de l'entrainement\n",
        "es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=50, min_delta=1e-7, restore_best_weights=True)\n",
        "\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits = 5)\n",
        "model = KerasRegressor(build_fn=ModelLSTM, epochs=max_periodes, verbose=3,batch_size=batch_size)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=tscv, n_jobs=1, verbose=3)\n",
        "\n",
        "grid_result = grid.fit(x_train, y_train,callbacks=[es])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWTWBh8DjJpP"
      },
      "source": [
        "# Affiche les résultats\n",
        "print(\"Meilleur résultat : %f avec %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params_ = grid_result.cv_results_['params']\n",
        "for mean, stdev, param_ in zip(means, stds, params_):\n",
        "  print(\"%f (%f) with %r\" % (mean, stdev, param_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHfiXLFZhrPs"
      },
      "source": [
        "# Création du modèle LSTM de type encodeur-décodeur"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd9AzWFtjnjs"
      },
      "source": [
        "**1. Création du réseau**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9OCzL7UjAhL"
      },
      "source": [
        "Par défaut, la dimension des vecteurs cachés est de 10 et aucune régularisation n'est utilisée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnFw_FPPhxiE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d432010f-daf7-4efc-ef49-d27940c78f38"
      },
      "source": [
        "dim_LSTM = 50\n",
        "l1_reg = 0.0\n",
        "l2_reg = 0.0\n",
        "\n",
        "# Définition de l'entrée du modèle\n",
        "entrees = tf.keras.layers.Input(shape=(taille_fenetre,1))\n",
        "\n",
        "# Encodeur\n",
        "s_encodeur = tf.keras.layers.LSTM(dim_LSTM, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg,l2=l2_reg),)(entrees)\n",
        "\n",
        "# Décodeur\n",
        "s_decodeur = tf.keras.layers.Dense(dim_LSTM,activation=\"tanh\",kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg,l2=l2_reg))(s_encodeur)\n",
        "s_decodeur = tf.keras.layers.Concatenate()([s_decodeur,s_encodeur])\n",
        "\n",
        "# Générateur\n",
        "sortie = tf.keras.layers.Dense(1,kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg,l2=l2_reg))(s_decodeur)\n",
        "\n",
        "# Construction du modèle\n",
        "model = tf.keras.Model(entrees,sortie)\n",
        "model.summary()"
      ],
      "execution_count": 568,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_846\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_861 (InputLayer)          [(None, 5, 1)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_860 (LSTM)                 (None, 50)           10400       input_861[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_1694 (Dense)              (None, 50)           2550        lstm_860[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_847 (Concatenate)   (None, 100)          0           dense_1694[0][0]                 \n",
            "                                                                 lstm_860[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_1695 (Dense)              (None, 1)            101         concatenate_847[0][0]            \n",
            "==================================================================================================\n",
            "Total params: 13,051\n",
            "Trainable params: 13,051\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_azfJaeUo2nU"
      },
      "source": [
        "**2. Optimisation de l'apprentissage**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf3lwaQBnjxL"
      },
      "source": [
        "Pour accélérer le traitement des données, nous n'allons pas utiliser l'intégralité des données pendant la mise à jour du gradient, comme cela a été fait jusqu'à présent (en utilisant le dataset).  \n",
        "Cette fois-ci, nous allons forcer les mises à jour du gradient à se produire de manière moins fréquente en attribuant la valeur du batch_size à prendre en compte lors de la regression du modèle.  \n",
        "Pour cela, on utilise l'argument \"batch_size\" dans la méthode fit. En précisant un batch_size=1000, cela signifie que :\n",
        " - Sur notre total de 56000 échantillons, 56 seront utilisés pour les calculs du gradient\n",
        " - Il y aura également 56 itérations à chaque période.\n",
        "  \n",
        "    \n",
        "    \n",
        "Si nous avions pris le dataset comme entrée, nous aurions eu :\n",
        "- Un total de 56000 échantillons également\n",
        "- Chaque période aurait également pris 56 itérations pour se compléter\n",
        "- Mais 1000 échantillons auraient été utilisés pour le calcul du gradient, au lieu de 56 avec la méthode utilisée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6Z35rNWj5SA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "779263b7-0665-47bc-9d44-825592b8de15"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam()\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"mse\", optimizer=optimiseur, metrics=\"mse\")\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint],batch_size=batch_size)"
      ],
      "execution_count": 497,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "46/46 [==============================] - 1s 3ms/step - loss: 1.3662 - mse: 1.3662\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.05747, saving model to poids.hdf5\n",
            "Epoch 2/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.3662 - mse: 1.3662\n",
            "\n",
            "Epoch 00002: loss improved from 1.05747 to 1.05746, saving model to poids.hdf5\n",
            "Epoch 3/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.3662 - mse: 1.3662\n",
            "\n",
            "Epoch 00003: loss improved from 1.05746 to 1.05746, saving model to poids.hdf5\n",
            "Epoch 4/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.3662 - mse: 1.3662\n",
            "\n",
            "Epoch 00004: loss improved from 1.05746 to 1.05745, saving model to poids.hdf5\n",
            "Epoch 5/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.3662 - mse: 1.3662\n",
            "\n",
            "Epoch 00005: loss improved from 1.05745 to 1.05743, saving model to poids.hdf5\n",
            "Epoch 6/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.3662 - mse: 1.3662\n",
            "\n",
            "Epoch 00006: loss improved from 1.05743 to 1.05741, saving model to poids.hdf5\n",
            "Epoch 7/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.3662 - mse: 1.3662\n",
            "\n",
            "Epoch 00007: loss improved from 1.05741 to 1.05739, saving model to poids.hdf5\n",
            "Epoch 8/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.3661 - mse: 1.3661\n",
            "\n",
            "Epoch 00008: loss improved from 1.05739 to 1.05735, saving model to poids.hdf5\n",
            "Epoch 9/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.3661 - mse: 1.3661\n",
            "\n",
            "Epoch 00009: loss improved from 1.05735 to 1.05731, saving model to poids.hdf5\n",
            "Epoch 10/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.3660 - mse: 1.3660\n",
            "\n",
            "Epoch 00010: loss improved from 1.05731 to 1.05726, saving model to poids.hdf5\n",
            "Epoch 11/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.3659 - mse: 1.3659\n",
            "\n",
            "Epoch 00011: loss improved from 1.05726 to 1.05719, saving model to poids.hdf5\n",
            "Epoch 12/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.3658 - mse: 1.3658\n",
            "\n",
            "Epoch 00012: loss improved from 1.05719 to 1.05710, saving model to poids.hdf5\n",
            "Epoch 13/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.3657 - mse: 1.3657\n",
            "\n",
            "Epoch 00013: loss improved from 1.05710 to 1.05700, saving model to poids.hdf5\n",
            "Epoch 14/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.3656 - mse: 1.3656\n",
            "\n",
            "Epoch 00014: loss improved from 1.05700 to 1.05686, saving model to poids.hdf5\n",
            "Epoch 15/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.3654 - mse: 1.3654\n",
            "\n",
            "Epoch 00015: loss improved from 1.05686 to 1.05669, saving model to poids.hdf5\n",
            "Epoch 16/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.3651 - mse: 1.3651\n",
            "\n",
            "Epoch 00016: loss improved from 1.05669 to 1.05647, saving model to poids.hdf5\n",
            "Epoch 17/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.3648 - mse: 1.3648\n",
            "\n",
            "Epoch 00017: loss improved from 1.05647 to 1.05620, saving model to poids.hdf5\n",
            "Epoch 18/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.3645 - mse: 1.3645\n",
            "\n",
            "Epoch 00018: loss improved from 1.05620 to 1.05586, saving model to poids.hdf5\n",
            "Epoch 19/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.3640 - mse: 1.3640\n",
            "\n",
            "Epoch 00019: loss improved from 1.05586 to 1.05542, saving model to poids.hdf5\n",
            "Epoch 20/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.3634 - mse: 1.3634\n",
            "\n",
            "Epoch 00020: loss improved from 1.05542 to 1.05488, saving model to poids.hdf5\n",
            "Epoch 21/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.3626 - mse: 1.3626\n",
            "\n",
            "Epoch 00021: loss improved from 1.05488 to 1.05420, saving model to poids.hdf5\n",
            "Epoch 22/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.3617 - mse: 1.3617\n",
            "\n",
            "Epoch 00022: loss improved from 1.05420 to 1.05334, saving model to poids.hdf5\n",
            "Epoch 23/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.3605 - mse: 1.3605\n",
            "\n",
            "Epoch 00023: loss improved from 1.05334 to 1.05226, saving model to poids.hdf5\n",
            "Epoch 24/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.3590 - mse: 1.3590\n",
            "\n",
            "Epoch 00024: loss improved from 1.05226 to 1.05090, saving model to poids.hdf5\n",
            "Epoch 25/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.3571 - mse: 1.3571\n",
            "\n",
            "Epoch 00025: loss improved from 1.05090 to 1.04918, saving model to poids.hdf5\n",
            "Epoch 26/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.3547 - mse: 1.3547\n",
            "\n",
            "Epoch 00026: loss improved from 1.04918 to 1.04703, saving model to poids.hdf5\n",
            "Epoch 27/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.3517 - mse: 1.3517\n",
            "\n",
            "Epoch 00027: loss improved from 1.04703 to 1.04433, saving model to poids.hdf5\n",
            "Epoch 28/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.3479 - mse: 1.3479\n",
            "\n",
            "Epoch 00028: loss improved from 1.04433 to 1.04094, saving model to poids.hdf5\n",
            "Epoch 29/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.3432 - mse: 1.3432\n",
            "\n",
            "Epoch 00029: loss improved from 1.04094 to 1.03669, saving model to poids.hdf5\n",
            "Epoch 30/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.3373 - mse: 1.3373\n",
            "\n",
            "Epoch 00030: loss improved from 1.03669 to 1.03136, saving model to poids.hdf5\n",
            "Epoch 31/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.3299 - mse: 1.3299\n",
            "\n",
            "Epoch 00031: loss improved from 1.03136 to 1.02468, saving model to poids.hdf5\n",
            "Epoch 32/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.3206 - mse: 1.3206\n",
            "\n",
            "Epoch 00032: loss improved from 1.02468 to 1.01635, saving model to poids.hdf5\n",
            "Epoch 33/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.3090 - mse: 1.3090\n",
            "\n",
            "Epoch 00033: loss improved from 1.01635 to 1.00594, saving model to poids.hdf5\n",
            "Epoch 34/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.2945 - mse: 1.2945\n",
            "\n",
            "Epoch 00034: loss improved from 1.00594 to 0.99298, saving model to poids.hdf5\n",
            "Epoch 35/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.2765 - mse: 1.2765\n",
            "\n",
            "Epoch 00035: loss improved from 0.99298 to 0.97689, saving model to poids.hdf5\n",
            "Epoch 36/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.2542 - mse: 1.2542\n",
            "\n",
            "Epoch 00036: loss improved from 0.97689 to 0.95695, saving model to poids.hdf5\n",
            "Epoch 37/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.2265 - mse: 1.2265\n",
            "\n",
            "Epoch 00037: loss improved from 0.95695 to 0.93232, saving model to poids.hdf5\n",
            "Epoch 38/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.1922 - mse: 1.1922\n",
            "\n",
            "Epoch 00038: loss improved from 0.93232 to 0.90197, saving model to poids.hdf5\n",
            "Epoch 39/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.1500 - mse: 1.1500\n",
            "\n",
            "Epoch 00039: loss improved from 0.90197 to 0.86468, saving model to poids.hdf5\n",
            "Epoch 40/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.0981 - mse: 1.0981\n",
            "\n",
            "Epoch 00040: loss improved from 0.86468 to 0.81893, saving model to poids.hdf5\n",
            "Epoch 41/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.0343 - mse: 1.0343\n",
            "\n",
            "Epoch 00041: loss improved from 0.81893 to 0.76288, saving model to poids.hdf5\n",
            "Epoch 42/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.9558 - mse: 0.9558\n",
            "\n",
            "Epoch 00042: loss improved from 0.76288 to 0.69430, saving model to poids.hdf5\n",
            "Epoch 43/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.8597 - mse: 0.8597\n",
            "\n",
            "Epoch 00043: loss improved from 0.69430 to 0.61101, saving model to poids.hdf5\n",
            "Epoch 44/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7433 - mse: 0.7433\n",
            "\n",
            "Epoch 00044: loss improved from 0.61101 to 0.51211, saving model to poids.hdf5\n",
            "Epoch 45/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.6073 - mse: 0.6073\n",
            "\n",
            "Epoch 00045: loss improved from 0.51211 to 0.40114, saving model to poids.hdf5\n",
            "Epoch 46/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4607 - mse: 0.4607\n",
            "\n",
            "Epoch 00046: loss improved from 0.40114 to 0.29048, saving model to poids.hdf5\n",
            "Epoch 47/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3253 - mse: 0.3253\n",
            "\n",
            "Epoch 00047: loss improved from 0.29048 to 0.20058, saving model to poids.hdf5\n",
            "Epoch 48/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2266 - mse: 0.2266\n",
            "\n",
            "Epoch 00048: loss improved from 0.20058 to 0.14471, saving model to poids.hdf5\n",
            "Epoch 49/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.1705 - mse: 0.1705\n",
            "\n",
            "Epoch 00049: loss improved from 0.14471 to 0.11531, saving model to poids.hdf5\n",
            "Epoch 50/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.1410 - mse: 0.1410\n",
            "\n",
            "Epoch 00050: loss improved from 0.11531 to 0.09802, saving model to poids.hdf5\n",
            "Epoch 51/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.1233 - mse: 0.1233\n",
            "\n",
            "Epoch 00051: loss improved from 0.09802 to 0.08635, saving model to poids.hdf5\n",
            "Epoch 52/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.1114 - mse: 0.1114\n",
            "\n",
            "Epoch 00052: loss improved from 0.08635 to 0.07844, saving model to poids.hdf5\n",
            "Epoch 53/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.1042 - mse: 0.1042\n",
            "\n",
            "Epoch 00053: loss improved from 0.07844 to 0.07372, saving model to poids.hdf5\n",
            "Epoch 54/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.1005 - mse: 0.1005\n",
            "\n",
            "Epoch 00054: loss improved from 0.07372 to 0.07120, saving model to poids.hdf5\n",
            "Epoch 55/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0984 - mse: 0.0984\n",
            "\n",
            "Epoch 00055: loss improved from 0.07120 to 0.06946, saving model to poids.hdf5\n",
            "Epoch 56/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0967 - mse: 0.0967\n",
            "\n",
            "Epoch 00056: loss improved from 0.06946 to 0.06762, saving model to poids.hdf5\n",
            "Epoch 57/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0947 - mse: 0.0947\n",
            "\n",
            "Epoch 00057: loss improved from 0.06762 to 0.06544, saving model to poids.hdf5\n",
            "Epoch 58/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0917 - mse: 0.0917\n",
            "\n",
            "Epoch 00058: loss improved from 0.06544 to 0.06286, saving model to poids.hdf5\n",
            "Epoch 59/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0874 - mse: 0.0874\n",
            "\n",
            "Epoch 00059: loss improved from 0.06286 to 0.05980, saving model to poids.hdf5\n",
            "Epoch 60/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0812 - mse: 0.0812\n",
            "\n",
            "Epoch 00060: loss improved from 0.05980 to 0.05621, saving model to poids.hdf5\n",
            "Epoch 61/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0728 - mse: 0.0728\n",
            "\n",
            "Epoch 00061: loss improved from 0.05621 to 0.05208, saving model to poids.hdf5\n",
            "Epoch 62/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0603 - mse: 0.0603\n",
            "\n",
            "Epoch 00062: loss improved from 0.05208 to 0.04645, saving model to poids.hdf5\n",
            "Epoch 63/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0426 - mse: 0.0426\n",
            "\n",
            "Epoch 00063: loss improved from 0.04645 to 0.03564, saving model to poids.hdf5\n",
            "Epoch 64/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0253 - mse: 0.0253\n",
            "\n",
            "Epoch 00064: loss improved from 0.03564 to 0.02360, saving model to poids.hdf5\n",
            "Epoch 65/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0235 - mse: 0.0235\n",
            "\n",
            "Epoch 00065: loss improved from 0.02360 to 0.01891, saving model to poids.hdf5\n",
            "Epoch 66/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0410 - mse: 0.0410\n",
            "\n",
            "Epoch 00066: loss did not improve from 0.01891\n",
            "Epoch 67/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0703 - mse: 0.0703\n",
            "\n",
            "Epoch 00067: loss did not improve from 0.01891\n",
            "Epoch 68/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3025 - mse: 0.3025\n",
            "\n",
            "Epoch 00068: loss did not improve from 0.01891\n",
            "Epoch 69/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7158 - mse: 0.7158\n",
            "\n",
            "Epoch 00069: loss did not improve from 0.01891\n",
            "Epoch 70/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.2433 - mse: 1.2433\n",
            "\n",
            "Epoch 00070: loss did not improve from 0.01891\n",
            "Epoch 71/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.9682 - mse: 1.9682\n",
            "\n",
            "Epoch 00071: loss did not improve from 0.01891\n",
            "Epoch 72/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7345 - mse: 0.7345\n",
            "\n",
            "Epoch 00072: loss did not improve from 0.01891\n",
            "Epoch 73/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 2.5599 - mse: 2.5599\n",
            "\n",
            "Epoch 00073: loss did not improve from 0.01891\n",
            "Epoch 74/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.2815 - mse: 1.2815\n",
            "\n",
            "Epoch 00074: loss did not improve from 0.01891\n",
            "Epoch 75/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 2.9675 - mse: 2.9675\n",
            "\n",
            "Epoch 00075: loss did not improve from 0.01891\n",
            "Epoch 76/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 3.3131 - mse: 3.3131\n",
            "\n",
            "Epoch 00076: loss did not improve from 0.01891\n",
            "Epoch 77/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 2.2634 - mse: 2.2634\n",
            "\n",
            "Epoch 00077: loss did not improve from 0.01891\n",
            "Epoch 78/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 2.0725 - mse: 2.0725\n",
            "\n",
            "Epoch 00078: loss did not improve from 0.01891\n",
            "Epoch 79/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 2.1072 - mse: 2.1072\n",
            "\n",
            "Epoch 00079: loss did not improve from 0.01891\n",
            "Epoch 80/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.9396 - mse: 0.9396\n",
            "\n",
            "Epoch 00080: loss did not improve from 0.01891\n",
            "Epoch 81/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.0407 - mse: 1.0407\n",
            "\n",
            "Epoch 00081: loss did not improve from 0.01891\n",
            "Epoch 82/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.8724 - mse: 0.8724\n",
            "\n",
            "Epoch 00082: loss did not improve from 0.01891\n",
            "Epoch 83/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.6029 - mse: 1.6029\n",
            "\n",
            "Epoch 00083: loss did not improve from 0.01891\n",
            "Epoch 84/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.9123 - mse: 0.9123\n",
            "\n",
            "Epoch 00084: loss did not improve from 0.01891\n",
            "Epoch 85/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 230.2243 - mse: 230.2243\n",
            "\n",
            "Epoch 00085: loss did not improve from 0.01891\n",
            "Epoch 86/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 2.0482 - mse: 2.0482\n",
            "\n",
            "Epoch 00086: loss did not improve from 0.01891\n",
            "Epoch 87/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7376 - mse: 0.7376\n",
            "\n",
            "Epoch 00087: loss did not improve from 0.01891\n",
            "Epoch 88/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 244.9545 - mse: 244.9545\n",
            "\n",
            "Epoch 00088: loss did not improve from 0.01891\n",
            "Epoch 89/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 14.5492 - mse: 14.5492\n",
            "\n",
            "Epoch 00089: loss did not improve from 0.01891\n",
            "Epoch 90/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1023.0399 - mse: 1023.0399\n",
            "\n",
            "Epoch 00090: loss did not improve from 0.01891\n",
            "Epoch 91/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 64.2960 - mse: 64.2960\n",
            "\n",
            "Epoch 00091: loss did not improve from 0.01891\n",
            "Epoch 92/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 2.2669 - mse: 2.2669\n",
            "\n",
            "Epoch 00092: loss did not improve from 0.01891\n",
            "Epoch 93/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 6974.6828 - mse: 6974.6828\n",
            "\n",
            "Epoch 00093: loss did not improve from 0.01891\n",
            "Epoch 94/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 204.8889 - mse: 204.8889\n",
            "\n",
            "Epoch 00094: loss did not improve from 0.01891\n",
            "Epoch 95/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 6.1114 - mse: 6.1114\n",
            "\n",
            "Epoch 00095: loss did not improve from 0.01891\n",
            "Epoch 96/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 3928.3572 - mse: 3928.3572\n",
            "\n",
            "Epoch 00096: loss did not improve from 0.01891\n",
            "Epoch 97/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1237.7685 - mse: 1237.7685\n",
            "\n",
            "Epoch 00097: loss did not improve from 0.01891\n",
            "Epoch 98/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 53897.7721 - mse: 53897.7721\n",
            "\n",
            "Epoch 00098: loss did not improve from 0.01891\n",
            "Epoch 99/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 29514.3352 - mse: 29514.3352\n",
            "\n",
            "Epoch 00099: loss did not improve from 0.01891\n",
            "Epoch 100/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 20359.4468 - mse: 20359.4468\n",
            "\n",
            "Epoch 00100: loss did not improve from 0.01891\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2aP9J3TkNGG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "65ebc3d9-1f5a-47f2-d01b-248cdbe08588"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 1])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": 498,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, \"Evolution de l'erreur en fonction du taux d'apprentissage\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 498
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAF5CAYAAAC7nq8lAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZxbZ3n3/88laRbP6tieGcceJ7YnXmJnA0wWSAIPkBACJJQ9tOwQeJ6m5Sltf1BoIUApUCi0bD8Ie6EkBGhpoIFA2EICCXEgCbFjB2+J7cSe8YztWTzSaLmfP86RLI9nkWaOdLR836+XX56RNNItnTOjS9d93ddtzjlEREREZG4iYQ9AREREpJopmBIRERGZBwVTIiIiIvOgYEpERERkHhRMiYiIiMyDgikRERGReVAwJaEyM2dmZ8zxZy8xs+1Bj2max9pjZs+Zw88908z2lWJM1cbMnm5mfzSzUTN7URkf93Nm9g9leJyaONa18jwmM7M/NbMfhz0OqU0KpqQgfjAx7r8RZv99usxjOCHwcs79yjm3rpxjmC//dVwZ9jhC8n7g0865Nufc90rxAGb2OjO7M/8y59xbnXMfKMXjBWWqcVeKajxnzWyl//cilr3MOfcfzrnLwxyX1K7Y7DcRyXmhc+72sAdRj8ws5pxLzXbZPO7fAHPOZYK4v2mcDmwp4f1LjQjy3BYpB2WmZF7MrMnMjpjZWXmXdflZrG7/+zeb2Q4zGzKzW8xs2TT39Qsze1Pe97lP62Z2h3/xA35W7BWTpyPM7Ez/Po6Y2RYzuyrvuq+a2WfM7H/MbMTM7jGzvhme16vN7FEzGzSzd0+6LmJm7zSznf71N5vZoiJfuuxr9zEze8zMDvrTUQv8655pZvvM7B1mdgD4ipldb2bfMbNvmNkw8Doz6zSzL5nZE2a238z+0cyi/n1cb2bfyHu8Ez6t+6/VB83sLuAYsHqKMS4zs++a2YCZ7Tazv8y77nr/uf+7/5puMbNN0zzXnf79f98/fk3+fd/inxc7zOzNhd63ma0ws//0xzVoZp82szOBzwEX+Y9xxL/tV83sH/N+dtrz0X993mredOQR/5yxaZ7TAv++D5vZVuCpk64/IZM6eRx5l0837ueb2e/NbNjM9prZ9Xk/c9JUnOVNRZvZrWb2L3nX3WRmX57L85h025nGlD2/rjWzx/1z8m/yrs+ev9/yj+nvzOzcSeN/h5k9CIyZWczMLjSzX/vH4gEze2be7X9hZh8ws7v8+/uxmS3xr87+vTjiv6YX2Yl/T8zMPmFm/f5z+YP5f8PM7Eoz2+rf5/7sczCzU8zsB/45d9j/ujdvPKvM7A7/5273z538379pn4vUAOec/unfrP+APcBzprnuy8AH877/c+BH/tfPAg4BTwaagE8Bd+Td1gFn+F//AnhT3nWvA+6c6rb+988E9vlfNwA7gHcBjf7jjgDr/Ou/CgwC5+NlZP8DuGma57MBGAUu9cf8cSCVff7A24C7gV7/+s8DN05zX7kxTnHdJ4BbgEVAO/B94EN5P5cCPuI/xgLgeiAJvAjvg9AC4L/8x28FuoHfAm/x7+N64Bt5j7fSfw1jea/3Y8BG/zVpmDS+CHAf8B7/NV0N7AKem3f/ceBKIAp8CLi70HMI7w3vs0AzcB4wADxrtvv2v3/Af/1a/Z+/eKpzJu/Y/2MR5+MPgIXAaf6Yrpjm+XwY+JV//FYAD+Ufa04+X3PjmOK+phr3M4Gz/eNwDnAQeNF051X+6wssBfr95/un/nFrn8vzKGJMK/3nfKN/XM72X7/smK7HO39fivf7+jfAbvzzzh///f4YFgDL8X5nr/Qf7zL/+66883cnsNa//S+AD091rk9+jYHn4p3bCwEDzgRO9a97ArjE//oU4Mn+14uBlwAteL+v3wa+l3f/vwE+hve7cjEwjP/7N9tz0b/q/xf6APSvOv75f+hGgSN5/97sX/ccYGfebe8CXuN//SXgn/Oua/P/oK70vw8qmLoEOABE8q6/Ebje//qrwBfzrrsS2DbNc30PeYEW3hvDBMffFB4Gnp13/an+c4pNcV+5MU663IAxoC/vsouA3Xk/NwE0511/PSe+8fcACWBB3mXXAD/Pu/1swdT7ZzjmFwCPTbrs74Cv5N3/7XnXbQDGZzmHsq/hCiBN3hs8XsD01dnu23+dBqZ5vU84Z/KOfTaYKuR8vDjv+puBd07zfHaRF2gB1xJgMDXFbf4V+MR05xUnB6svAfbiBY8Xz3C/Mz6PIsaUPb/W513/z8CX8o7p3XnXRTgxcNkDvCHv+ncAX5/0eLcBr807f/8+77r/w/EPcdmxTBdMPQt4BLiQvL8Z/nWPAW8BOmZ57ucBh/2vT8P78NOSd/03OB5Mzfhc9K/6/2maT4rxIufcwrx/X/Av/znQYmYXmFeoeh5exgRgGfBo9g6cc6N4n8iWBzy2ZcBed2LNz6OTHudA3tfH8N5Ip72v7DfOuTG8MWedDvyXn64/ghdcpfGCm0J14X3CvS/vfn7kX5414JyLT/q5vXlfn473Cf+JvPv4PF6GqlB7Z7judGBZ9r79+38XJz7Pya9ps+UV/c5gGTDknBvJu2y245W97xXAo25uNTWFnI9zOk/y7zcI/u/Tz/1ppaPAW4Els/1cnu/jZfG2O+dmKm4v+HkUOKbJ97Vsquv839V9012Pd/69bNL5dzHeh5esQo/VCZxzPwM+DXwG6DezG8ysw7/6JXgfth41s1+a2UX+c28xs8+bN/0/jJdZXWjetHr2fD42j+ciVUzBlMybcy6N9wn+Gv/fD/LeJB/H+0MCgJm14qXL909xV2N4AUbW0iKG8Tiwwszyz+nTpnmc2TyB94YNeH9E8cactRd43qTAstk5V8xjHQLGgY1599HpnMt/M3BT/Fz+ZXvxMlNL8u6jwzm30b++kNdzqsfIv//dk55nu3Puylmf3eweBxaZWXveZYUer73AadMEbTM9n+zjFno+zuaE8wRv/PmOUfj5PNW4v4k3DbzCOdeJV1eVrd864dj6b+hdk37+g3iB/qlmds0Mjz3b8yh0TFmT7+vxqa7zf1d7J10/+fz++qTzr9U59+EZxjfV/Ux9A+c+6Zx7Cl7Wcy3wt/7l9zrnrsb7UPI9vL9tAH8NrAMucM514JUBgPf8n8A7n/OPd/7rMJ/nIlVAwZQE5ZvAK/DqM76Zd/mNwOvN7DwzawL+CbjHObdnivu4H3ix/wnwDOCNk64/yBRF0r578N68/j8za/CLO18I3DSH5/Id4AVmdrGZNeIt6c//Xfkc8EEzOx1yBfdXF/MA/qfyLwCfsOOF+svN7LlF3McTwI+BfzGzDvMK4/vM7Bn+Te4HLjWz08ysE2+Krhi/BUb8ouAFZhY1s7PMbNoC5SLGvhf4NfAhM2s2s3Pwjvc3Zv7J3LieAD5sZq3+zz/dv+4g0Osft6kUcz7O5mbg7/zC5F7gLyZdfz/wKv91uwJ4xkn3cNxU427Hy3bEzex84FV51z2Cl6l7vpk1AH+PVwMGgJldCrweeA3wWuBTZjZdNni255FvpjFl/YP/O7zRH8O38q57ipm92A+E/y/eh4G7p3msbwAvNLPn+q9hs3mF973T3D7fAJBhmr8XZvZUP8vWgBeYxoGMmTWa14+q0zmXxKt7yma72/E+AB0xb8HJe7P355x7FNgMXO/fx0V4f3+CeC5SBRRMSTGyK7Gy/7JTeTjn7sH7o7QM+GHe5bcD/wB8F+8NsA945TT3/wm8OqGDwNfwisTzXQ98zU+Tvzz/CufcBN4fr+fhZX0+i1e3ta3YJ+mc24JXRP9Nf8yH8aYjsv4N79P5j81sBO/N4IJiHwevjmIHcLc/bXA73iffYrwGr+B1qz/O7+BPHTjnfoL3RvYgXrHtD4q5Yz/j+AK8advdeK/rF4HOIsc4nWvwalsex5sWfq8roPWGP64XAmfg1bfswwvkAX6G137hgJkdmuJnizkfZ/M+vGms3XhB7dcnXf82f5xH8D5kzNRba6px/x/g/f459h6OZ0hwzh31r/8iXlZtDP8c9aer/h24zjm33zn3K7xasa+YTbkycbbnkW/aMeX5Jd55/VPgY865/EaZ/413rA4DrwZe7ActJ/ED7qvxppYH8LI7f0sB71v+dNsHgbv8vxcXTrpJB96HmcN4z30Q+Kh/3auBPf7v5Fvxjh149WEL8H4P7sabls/3p3j1fIPAP+L97iXm+1ykOphzs2ZDRUREZmRevWR2dd5J9WzmtVE4wzn3Z+UdWTjM7Ft4i1zeO+uNpeopKhYREZknf+qwz59uvwIvE1WSTv9SeWYNpszsy+Y1NntomuvNzD5pXhO8B83sycEPU0REpKItxWvXMAp8EvjfzrnfhzoiKZtZp/n8QsZR4N+dc2dNcf2VeAWLV+LVjfybc24u9SMiIiIiVaeQQr47gKEZbnI1XqDlnHN34/XdUO8MERERqQtB1Ewt58TmZPsIviGjiIiISEUqpFNxYMzsWrytCmhtbX3K+vXry/nwIiV3eGyC/UfGiUaMFYtaaGsq66+YSFUaiafYMzhGX1cbLY3RKW/zh/1H6W5voqejGYCHnximY0EDyxcuKOdQi7ajf5RY1Fi5uLXgn0mmHdsODLN84QIWtU7XMg2G/L83rY0xVnfNfv/7j4xzdDzJhlM7Trh8y+PDnNLawLLOyn4tw3bfffcdcs5Nbo4LBBNM7efETq+9TNNN2Dl3A3ADwKZNm9zmzZsDeHiRyrLtwDB//h+/Y/ehMV5/+Tr+zzP7mLq9j4gA/OihJ3jrN37Ht//yYjYum7qN2Zp338qbLlnNO67wPoQ/6f0/5gXnLOMDLzqplLeiXP2Zu1i4oIGvveH8gn9m/5Fxnv7hn/Hhl5zDy5+6Ytrb3fTbx3jnf/6B81cu4ua3XjTr/b7/+1u5efNeNr/vxN7AZ7/3Nl62aQXveeGGgsdYj8xs2q2WgpjmuwV4jb+q70LgqN+ZWaQurV/awS3XXcwLzlnGR2/bzvu+v5VMRv3cRKaTSHlNxpsbps5KAcQiEVLp41tvpjOOaKTyP6TEIka6yN//dNq7/WzPLxb13sJTmcyMt8tqa4oymkjp71EJzJqZMrMb8XYoX2Jm+/Ba6DcAOOc+B9yKt5JvB952Hq8v1WBFqkVrU4x/e+V5dLc38cU7dzM8nuQjLz2Hhqhau4lMFk+mAWiKTf/70RA1kunjQUC1BFPRiJFMFxbsZGWDo1h0lmDKf/6pAoOjtmbvLf9YMq0ShIDN+mo652baIBPn9Vb488BGJFIjzIx3P/9MFrY08LEfP8JwPMWnX/WkGT99i9SjQjJTDdHICUFJ2lVHMNUQNRLJ4oKpbCZr9syUH0ylCwumWv0AaiyRUjAVMH1MFikhM+O6Z63hA1dv5KfbDvK6r/yWkfiUW5GJ1K1sZmrGab6onRA0ZDKzBxuVIBqJFJw5ysrePjZbMJXLTBU6zecFUKOJk3b7kXlSMCVSBq++aCX/+orz2LznMH/6xXsYGpsIe0giFSPuZ25mmuaLRSIk84KGVCZDtAoWdsypZiqXmZr5LToWydZMFTjNlw2m4gqmgqZgSqRMrj5vOTe85ilsPzDCG792b+7TuEi9S6TSRCM2Y01hYyySy0w558g4iFRFZmouNVOFZaai85jmk2ApmBIpo2et7+HfXnke9+89wl99636tqhHBy0w1z5CVAi+wyAYl2V+b2YKNStAQLT4zlV21ONs0ZoOfmSr0/jXNVzoKpkTK7IqzTuXdV57JDx86wEd+tC3s4YiELpFK0zTLwoxYNJJbzZetEaqWmqmig6lCM1P+9YVmvhRMlY7K+UVC8MaLV/HY0DE+f8cuehe18OoLTw97SCKhKSQz1RC1XBCVLZ2KVEnNVLEF6NngKzZLK5WGaHGtETTNVzoKpkRCYGa85wUb2H94nPf+90P0LlzA/1rfHfawREIRT86emWqIHq+ZSrvCMjeVIBqxE5qNFiJVcGsEvwC96MyU6jWDpmk+kZDEohE+ec2T2LCsgz//5u94aP/RsIckEopEKjPjSj7wAqcJP2jIdgivhgJ0L6NWbGbKb9pZcGuEwu6/uSFCxGA0ofYsQVMwJRKi1qYYX37tU1m4oIE3fPVeDo0mwh6SSNnFk+lZm9l6mSk/mPIzU7M0CK8I0Tm0RkgVvJ1MccGUmdHWFGNMmanAKZgSCVl3RzNfet1TOTKe5J3ffRDntMJP6ktBmam8DE+uD1MVbM8Um0PTzuM1UwVmpoqYRmxriqkAvQQq/0wUqQNnntrBO65Yz+0P93Pjb/eGPRyRskoUmJnKrubLBVNVUIA+n5qp2af5vLfwYmK11qaYmnaWgIIpkQrx+qet5OIzlvCBH2xl18Bo2MMRKZt4MkNzw+wr15KTp/mq4B0sNqeaqcI6oM+lNURbc4yxCQVTQauCU1GkPkQixsdedi6NsQh/dfMDRXdNFqlWiVSaptgsfaYix2umMgUGG5VgLtvJZH/3Z8tMzdQxfjqa5iuNyj8TRerI0s5m/ulPzuaBvUf41M92hD0ckbIoJDMVi1pe087qyUxlNzouphYyXWBrhLlkplobNc1XClVwKorUl+efcyovfvJyPvPzHdz36OGwhyNScvHU7DVTjdFIrmlnNtiolqadUPiWL5BXMzVLAXrDHJYztjXH1LSzBBRMiVSg9121kVM7m3n7zfcrJS81L55MF7SaL5uZyuSadlb+W1ix7QsgbzVfKWqmNM1XEpV/JorUofbmBj7+8vPYO3SMf7r14bCHI1IyzjkSqcysmalYJJKrJTreh6nkw5u3+WSmZt3oeA4vQGtTlLGJtFqwBKwKTkWR+nT+qkW8/umruPG3j6k7utSsiXQG5yigNYLlgqhsZqoapvmyRfLFZaaK64BejLamBtIZRzypBS5BUjAlUsH+8tlrOKWlkQ/8YKs+SUpNyr6pzzbN1zBFzdRsNUWVILcZcRGrcwvNTM1tms8LWjXVFywFUyIVrHNBA2+/bC337B7iti0Hwh6OSOASKW9rk9k2Oo75TTudc7lgozoyU8VP82X3Hpwt82RzeP6t/mbHKkIPloIpkQr3yqeuYG1PG/9067bcG49IrUj4manm2TJTeZv6ZlxhmZtKUOxmxPm3LcXza/ODKWWmgqVgSqTCxaIR/uEFG3hs6BhfvWtP2MMRCVQ8WXhmCrzi80L7MFWCXM1UuphgKkM0YnPKPM1GwVRpKJgSqQKXrOni2eu7+dTPdjAwkgh7OCKBSaQKzEz5tUfJTKaq9ubL1UxliquZKlWgqGm+0lAwJVIl3vX8M4kn03z8J4+EPRSRwGQzU4VsdAyQTGWqLDM1fc3U/iPjPHF0/KTL02k3p5V6hWhrVmaqFBRMiVSJvq42Xn3R6Xzr3sd4+InhsIcjEohCV/PlN79M10jN1Nu/dT9/8+0HTro8lSlhMKVpvpJQMCVSRd727DV0LGjgH/9HrRKkNmQXVcyamfJrj5LpTG61WzUEUzPVTO0dOsbDT4ycdHk643I1YkHTNF9pKJgSqSILWxr5q+es5a4dg/x8e3/YwxGZt2xmatZgKpbt13Q8M1UNrRFi09RMZTKO/pEEQ2MTDI1NnHBdsTVTG07tKPi2Lf7rrM2Og6VgSqTKvOqC01i+cAGf/fnOsIciMm+51XyzTfPlZaYyVdS0c7rtZAbHJnJTfzv6R0+4Lp3JFDzNt+0DV/Df1z294PFEIubvz6c2K0FSMCVSZRqiEd58ySo2P3qYzXuGwh6OyLzkVvMVsJ0MQDJ9vGlnNazmi05TM3VwOJ77enIwVUxmqrkhWvQefa1NUU3zBUzBlEgVevlTV3BKSwOf++WusIciMi/HV/MVlplKZTLH9+argpqp2DQ1U/0jx4OpnQOTM1OlK0AHr25KBejBUjAlUoVaGmO85qKV3P7wQf548OQCVpFqEc9uJxObrWYqO813vGlnKQOOoExXM3Vw2OsXt7i18eTMVLp0faYA2hVMBU7BlEiVeu3TVtLcEOGGO5SdkuqVKHSj40h2mi9TVXvzTVczdeBoHDO4YPWiKab5MrmMVim0NsU0zRcwBVMiVWpRayOv2LSC792/f8rGfyLVIJ5K0xiLzDpll7+dTKYKm3ZOrpnqH4mzuLWJdT0d7D8yzrGJ48FNuoQd0GHqab4qiEsrmoIpkSr2pktWk3Hw5Tt3hz0UkTlJJDOzZqXg+HRZMpPJtUaoimm+aWqmDg4n6Olo4ozuNgB2DYzlrktlXK7gvhQ0zRc8BVMiVWzFohaef/apfPOexzg6ngx7OCJFS6TSs67kA2icYqPjqihAn7ZmKk5PR3MumMovQi9HZkrTfMFSMCVS5d7yjNWMTaT5xt2Phj0UkaLFk5lZV/JBXmYqXV0bHU9XM+VlpppZuaSFiJ3YHiGVdmWomTreZ0p7KcyfgimRKrdxWSeXru3iK3ftyS0zF6kW8WR61pV8cGLTzlwwVQVNO6eqmUqmMwyOedN8TbEopy9uPSGYKnVmqr05xkQ6k9vKR+ZPwZRIDXjrpas5NJrgP3+3P+yhiBQlkSosM5WtIcqf5quOzNTJNVMDIwmcg56OZgD6uk4MplKZTEm7u7c2esHrmLqgB0bBlEgNuKhvMef0dnLDHTtzK51EqkE8maa5gMxUtst3Kq8AvRpW82WDonRezVS2+3lPRxMAfd1t7BkcI5XO+Lctfc0UaLPjICmYEqkBZsYbL17FnsFj3L17MOzhiBQsnkzTVETN1ESVtUaITTHNl23Y2d3uZabO6GojmXY8NnQM8BqTlnKlYnuzF0yNaLPjwCiYEqkRz924lPbmGN/ZvC/soYgULJHKFJaZyk2XZapyb778AvTsVjJLO/1gyl/Rl53qK1tmakLBVFAUTInUiOaGKC88dxm3PvQEI3G1SZDqEE8W1hohllczlami1gjZ6clkXs3UgaNxYhFjUUsj4E3zAezw2yOUowM6oF5TAVIwJVJDXvaUXuLJDLf+4YmwhyJSkHiBTTtzQYlfM1UNDTshPzOVXzOVoLu9KRcMdjQ30NPRdEJmqpQF6O3ZYErTfIFRMCVSQ85bsZC+rla+rak+qRKJVIamAjJTuWAq5UhlXFVkpeD4VGRq0jRft7+SL6uvq42d/aO526oAvboomBKpIWbGyzatYPOjh9k1MDr7D4iELJFMF9QaIRoxzLwpsEzGVUW9FHhTkRE7sTWC1/286YTbndHdxs6BMZzzWj+UMvOmab7gKZgSqTEvftJyIgbf/Z2yU1L54qnCmnaCV4SeTDvSmerYly8rFo2ckJk6cDTO0kmZqTO62xhNpDg4nPAzUyWsmfL7TCmYCo6CKZEa093RzDPWdvHd+/aftIWFSCVJZxzJtCsoMwVe485UOkM6k6maaT7wAr9szdT4RJrheOqkab4zuo6v6Ct1ZioWjbCgIappvgApmBKpQS/btIIDw3Hu3HEo7KGITCu7nUkhq/nACwKSaa8AvRp6TGVFI5bLTGXbIvRMkZkC2NE/QjKdKfnza22KMaoO6IFRMCVSg559ZjcLWxr4zn2a6pPKFU962ZpCVvOBl5lKZrxpvmoKpmIRy9VMZRt2Tq6Z6mpvor05xo6B0memANqaoprmC5CCKZEa1BSLcvW5y7htywGOHlPPKalMxWamGqKR3DRftRSgw4k1U8e3kjkxM2Vm/oq+Ma9mqsSbOLc1xzTNFyAFUyI16mWbVjCRynDLg4+HPRSRKWUzU4XWTMWi5m90XH2ZqWzN1HTBFHhTfeXKTLU2xpSZCpCCKZEatXFZB+uXtmuqTypWPOlnpopYzTeRzpCp4pqpg8NxmhsidPj74+U7o7uNgZGEv51Mad+e25qUmQqSgimRGpXtOfXA3iP88eBI2MMROUki5ddMFZmZKnVTy6BNrpnq6WjGppimzK7oA2godc1UszJTQVIwJVLDXnTeMmIRU3ZKKlLRmaloJNe0s4piKWLRSK5NycHhOD3tJ0/xwfEVfUDJa6ZalZkKlIIpkRq2uK2Ji9cs4YcPHcA59ZySypINpgrZTgayrRGyHcKr5+0rFjFSeTVT3ZNW8mWtWNRCo79tTulX8ykzFaTqORtFZE4u29DDY0PHeOSgtpeRypKb5iu0NULESKYzVbU3H3g1U+mMwznHweHESd3P82+3akmr/3Xpa6biyQypdGb2G8usCjpaZnaFmW03sx1m9s4prj/NzH5uZr83swfN7Mrghyoic/GcM3sA+MnWAyGPROREuWm+gjNTXu2RV4BeypEFKxYxkmnHSCLFeDI95Uq+rOxUX8lX8+U2O1bjziDMejqaWRT4DPA8YANwjZltmHSzvwduds49CXgl8NmgByoic9PT0cy5Kxbyk60Hwx6KyAkSRbZGaIhGSGYyZVntFqRszVS/3xZhumk+gD4/mCp1gX1bk78/34Sm+oJQyNl4PrDDObfLOTcB3ARcPek2Dujwv+4E1NhGpIJcvqGHB/YdzfW4EakE2aadBW90HI34faYcJa7PDlTUr5k63v08/MxUW1MDAKNxBVNBKCSYWg7szft+n39ZvuuBPzOzfcCtwF8EMjoRCcRlG7JTfcpOSeUoummnXzOVrsLWCOmM48DR6Rt2Zq3xg6nGAuvI5qo1m5lSEXoggjpa1wBfdc71AlcCXzezk+7bzK41s81mtnlgYCCghxaR2azpbuP0xS0KpqSiFFsz1VClGx1nVyEezG1yPP003/ql7Xz85efmPgCVSluuZkrBVBAKCab2Ayvyvu/1L8v3RuBmAOfcb4BmYMnkO3LO3eCc2+Sc29TV1TW3EYtI0cyMy87s4Tc7B/VJVCpGIpUhYoVPaTVEvU7i1ZqZ6h9O0N4co6Xx5O7nWWbGi5/cS3tzQ0nH1OZ3YNffg2AUEkzdC6wxs1Vm1ohXYH7LpNs8BjwbwMzOxAumlHoSqSCXbehhIp3hl9v1qymVIZ5M09wQnbIb+FRieTVTkSra6Di7nczB4fiMU3zl1NqoYCpIswZTzrkUcB1wG/Aw3qq9LWb2fjO7yr/ZXwNvNrMHgBuB1zl1CBSpKE85/RROaWng9oc11SeVIZ5KFzzFB15mKrs3X6kLtIOU3ejYC6amn058lSgAACAASURBVOIrJ03zBWv6XGMe59yteIXl+Ze9J+/rrcDTgx2aiAQpFo3wrPU93P7wQZLpDA3V1KhHalIimSm4YSdALBIhlc6QSlfZNJ+fUTs4nOCCVYvCHg5wvM+UVvMFQ39NRerIZRt6ODqe5N49Q2EPRYR4KlNkZiqSa9pZTdN8sYiXUesfidPTWRnTfI2xCI2xiPpMBUTBlEgduXTtEppiEa3qk4oQT6aLykxlp/nSGUesihpNRSPGodEEybSjp70ypvnAm+rTNF8wFEyJ1JGWxhgXn7GEn2w9qI2PJXSJVKbgTY7B304mU30F6LGI5XpqVUoBOni9prSdTDAUTInUmcs29LDv8DjbDoyEPRSpc/FkmuYia6bSGUeq2loj5GXRuisomGpramDEr5mqnlezMimYEqkzzzqzGzN1Q5fwJZLFrebLdgVPpNLVFUzl7SNYKav5wNufT9N8wVAwJVJnutubOU8bH0sFSKSKXc3nBVDxZIZoFU3z5Qd+3e2Vk5lqbYoxpgL0QCiYEqlDl23o4Q/7j/L4kfGwhyJ1LF5kZioWrdbMlDfWxa2NJd9zrxhtTTG1RghI5RxVESmb55zp7fv1qz+qG7qEJ57MFLzJMXir+bI/V1XBlD/uSqqXAj+Y0jRfIBRMidShNd1tLGlr5O5d6jcl4Umk0jTFiuszlVVNwVTUr5mqpHop8Kf5FEwFQsGUSB0yMy5YvZjf7BxUiwQJTbGZqfwtZKqtNQJATwXVS4HfZ2oiTUZ/A+ZNwZRInbpw9WIODMd5dPBY2EOROuScm8PefMffsqppb75sFq1Sup9nZffnOzahXlPzpWBKpE5dtHoxAL/ZNRjySKQeTaQzOEdxq/ny+jVV0zRfttarEqf5JBgKpkTqVF9XK13tTdytYEpCkEh5HcHnmpmKVFEwlauZqrRpvmYFU0FRMCVSp8yMC1U3JSGJJ72ppWK2k2nIy0xV0zRfrmaq4lbzFf7ay8wUTInUsQtXL6J/JMHuQ2NhD0XqTMLfq664pp15makqKkDvam+iKRah95QFYQ/lBK2NykwFRcGUSB1T3ZSEJZHyMlPFNe2szpqpKzYu5a53PotTWhvDHsoJNM0XHAVTInVs1ZJWutub1G9Kyi7uZ6aK2ei4sUr7TEUixpK2yio+h+Or+WT+FEyJ1DEz46I+1U1J+c2lZipWpcFUpdJqvuAomBKpcxeuXsyh0QQ7B1Q3JeWTW803h42Ogara6LhSKTMVHAVTInVOdVMShmxmaq6tEZSZmr+mWKSqVkVWMgVTInXu9MUtLO1oVr8pKatszVTTHDY6BgVTQTAzTfUFRMGUSJ3L1k3ds0t1U1I+udV8c9zouJqadlYyTfUFQ8GUiHDh6kUcGp1gR/9o2EOROpFbzTfH1giangqGgqlgKJgSES5avQRQ3ZSUT2413xybdqoAPRit6oIeCAVTIsKKRQtY1qm6KSmfuezN16hpvsCpZioYCqZEJLdP3927hshkVDclpTenzJSm+QLXri7ogVAwJSIAXNi3mKGxCf6ouikpg3gqTWM0UlSGKT+YUmYqGNqfLxgKpkQEyOs3tfNQyCORepBIZopqiwDQoJqpwGmaLxgKpkQEgBWLWli+cIH26ZOySKTSRdVLgZeNyvaXUp+pYGiaLxgKpkQk58LVi/ntniH1m5KSiyczRdVLZcUUTAVKmalgKJgSkZwnnbaQobEJ9h0eD3soUuPmkpmC4407o3r3CoSCqWDodBSRnHN7FwJw/94jIY9Eal08maG5yJopOF6EHo3o7SsI7QqmAqGzUURy1i1tpzEa4cF9CqaktOLJNE1FbCWTlctMqQA9EMpMBUPBlIjkNMYinLmsgwf2HQ17KFLjEqm5ZaYa/FopJaaCoQ7owdDpKCInOLe3k4f2HyWt5p1SQvFkuqhNjrNifmYqpmgqEO1NDWEPoSbobBSRE5zTu5BjE2l2Dqh5p5ROPJkuus8U5NdMBT2i+qTMVDB0OorICc5b0QnAAypClxJKpDJzykxl9+eLqGYqEG2qmQqEgikROcHqJW20NcV4UHVTUkLxZIamObRGyGamNM0XjDY17QyEzkYROUEkYpy1vEMr+qSkEsn0HJt2+pkpvXsFYkFDFPU/nT+djiJyknN7F/LwEyMkUumwhyI1ylvNN/dpPnVAD4aZsWFZB6cvbgl7KFVN+T0ROck5vQuZSGfY9sQI565YGPZwpMakM46J9PyadsYUTAXmB39xSdhDqHrKTInISc7p9YrQNdUnpZDNeM6laWdMBehSgRRMichJek9ZwKLWRjXvlJJIJDMA82raqWk+qSQKpkTkJGbGub2dykxJScT9zNT8NjpWMCWVQ8GUiEzpnN6F7OgfZSyRCnsoUmPifmZqTqv5ospMSeVRMCUiUzp3RScZBw/t11SfBCsRRGZKNVNSQRRMiciUzun1VvGpeacELT6PmqmYaqakAimYEpEpLWlrYvnCBTyguikJWDw599V8DTHVTEnlUTAlItM6p7dTmSkJXCI1/9V8EQVTUkEUTInItM7pXchjQ8c4PDYR9lCkhswnM5XtM6WmnVJJFEyJyLTOXeE179RUnwQpG0zNpwO6mnZKJVEwJSLTOnt5J2YqQpdgZaf55pKZ0t58UokUTInItNqbG1i9pFXNOyVQieTcWyOctqiFUzubNc0nFUUbHYvIjM7tXcivdhzCOYdpakUCkGvaOYdpvpc+pZeXbVoR9JBE5kWZKRGZ0Tm9nQyMJDgwHA97KFIjck075zDNp4BeKpGCKRGZ0TkrvOadD+xV3ZQEI57MEDFoiCowktqgYEpEZrTh1A5iEVPdlAQmnkzTFIsqyyQ1o6BgysyuMLPtZrbDzN45zW1ebmZbzWyLmX0z2GGKSFiaG6L0dbWx/cBI2EORGpFIZebUFkGkUs1agG5mUeAzwGXAPuBeM7vFObc17zZrgL8Dnu6cO2xm3aUasIiU37ql7dz36OGwhyE1Ip5Mz2kln0ilKuSjwfnADufcLufcBHATcPWk27wZ+Ixz7jCAc64/2GGKSJjWLW1n/5FxRuLJsIciNSCeytAUU2ZKakchZ/NyYG/e9/v8y/KtBdaa2V1mdreZXTHVHZnZtWa22cw2DwwMzG3EIlJ263raAXjkoKb6ZP4SykxJjQnqo0EMWAM8E7gG+IKZLZx8I+fcDc65Tc65TV1dXQE9tIiU2rqlXjC1/cBoyCORWhBPZWhSMCU1pJBgaj+Q3yGt178s3z7gFudc0jm3G3gEL7gSkRrQe8oCWhujbD8wHPZQpAZ4q/k0zSe1o5Cz+V5gjZmtMrNG4JXALZNu8z28rBRmtgRv2m9XgOMUkRCZGWuXtrNNK/okAN5qPmWmpHbMGkw551LAdcBtwMPAzc65LWb2fjO7yr/ZbcCgmW0Ffg78rXNusFSDFpHyW7+0ne0HR3DOhT0UqXKJZJpmZaakhhS0N59z7lbg1kmXvSfvawe83f8nIjVoXU87N/52LwMjCbo7msMejlSxeDKtmimpKfpoICIFWbe0A0BTfTJviVRGmSmpKTqbRaQgx1f0KZiS+VHTTqk1CqZEpCCLWhvpam9SZkrmLZ5U006pLTqbRaRg65e2q3GnzItzjkRKmSmpLQqmRKRg63q8YCqd0Yo+mZtk2pFxaKNjqSk6m0WkYGuXtpNIZXh0cCzsoUiViqfSADTFlJmS2qFgSkQKtl5F6DJPiWQGUGZKaovOZhEp2Jrudsxgu+qmZI7iST8zpZopqSEKpkSkYAsao5y+qEWZKZmzRG6aT28/Ujt0NotIUdYtbVcwJXMWz03zKTMltUPBlIgUZd3SDvYMjuWma0SKkc1MKZiSWqJgSkSKsn5pOxkHO/pHwx6KVKFsZkrTfFJLdDaLSFHW9ngr+tQJXeYim9FUZkpqiYIpESnKysUtNMYibD8wHPZQpAolUmqNILVHZ7OIFCUWjbCmu43tBzXNJ8XLtUZQ006pIQqmRKRo3oo+ZaakeHE17ZQapLNZRIq2rqedg8MJjhybCHsoUmVyq/mUmZIaomBKRIq2bqmK0GVucqv5lJmSGqKzWUSKtn5pBwCPaFsZKVJuNZ8yU1JDFEyJSNF6OproaI4pMyVFS6QyNEYjRCIW9lBEAqNgSkSKZmasX9qhbWWkaPFkWlN8UnN0RovInKxb2s4jB0ZwzoU9FKkiiVRabRGk5iiYEpE5Wbe0nZFEiv1HxsMeilSRRDKjtghSc3RGi8icZLeV0R59Uox4Kq2tZKTmKJgSkTlZ3dUKwK6BsZBHItUknsxok2OpOTqjRWROFrc20rmggZ0DykxJ4RLKTEkNUjAlInNiZvR1tSqYkqLEVTMlNUhntIjMWV9XGzs1zSdFiCe1mk9qj4IpEZmzvu42BkYSHB1Phj0UqRKJlDJTUnt0RovInPV1tQGwS1N9UqB4Mq2tZKTmKJgSkTnr81f0aapPChVPZtQBXWqOzmgRmbMVi1qIRUyZKSmYOqBLLVIwJSJz1hCNcPriFq3ok4J5HdAVTEltUTAlIvOiFX1SqHTGMZFW006pPTqjRWRe+rrbeHRwjGQ6E/ZQpMJNpLxzRJkpqTUKpkRkXvq62kimHXuHjoU9FKlw8WQaQK0RpObojBaRedGKPilUPOUFUypAl1qjYEpE5mW132tKRegym0QyO82ntx6pLTqjRWReOhc0sKStiZ39CqZkZtnMlGqmpNYomBKReevramXXIU3zyczifmZKq/mk1uiMFpF56+tuY0f/KM65sIciFSyRVGZKapOCKRGZt76uNo6OJxkamwh7KFLB4inVTElt0hktIvOmFX1SiGxrBK3mk1qjYEpE5q1PK/qkAAllpqRG6YwWkXlbvnABTbGIVvTJjJSZklqlYEpE5i0SMVYtaVVmSmZ0LJECYEGjgimpLQqmRCQQfd1tao8gMxoam8AMTmlpDHsoIoFSMCUigejramPv0LHcVI7IZINjEyxc0EA0YmEPRSRQCqZEJBB9Xa1kHDw6qA2PZWpDYxMsalVWSmqPgikRCYRW9MlsBscmWNzaFPYwRAKnYEpEArE622tKK/pkGkNjEyxuU2ZKao+CKREJREtjjGWdzcpMybQ0zSe1SsGUiASmr7tNXdBlSumM4/CxCRYrmJIapGBKRALT19XGrgFteCwnO3xsAudQZkpqkoIpEQlMX1crYxNpDg4nwh6KVJjsJtiL2lSALrVHwZSIBEYr+mQ6g6NeMKVpPqlFBQVTZnaFmW03sx1m9s4ZbvcSM3Nmtim4IYpItejrVjAlU8tlphRMSQ2aNZgysyjwGeB5wAbgGjPbMMXt2oG3AfcEPUgRqQ7d7U20NkbVHkFOMjTmTf2qNYLUokIyU+cDO5xzu5xzE8BNwNVT3O4DwEeAeIDjE5EqYmZa0SdTGvQzU9qXT2pRIcHUcmBv3vf7/MtyzOzJwArn3P8EODYRqUJ9XW3sUGZKJhkcnaBzQQMNUZXqSu2Z91ltZhHg48BfF3Dba81ss5ltHhgYmO9Di0gF6utq5cBwnNFEKuyhSAUZGlOPKaldhQRT+4EVed/3+pdltQNnAb8wsz3AhcAtUxWhO+ducM5tcs5t6urqmvuoRaRiZVf07dZUn+QZHEuo+FxqViHB1L3AGjNbZWaNwCuBW7JXOueOOueWOOdWOudWAncDVznnNpdkxCJS0bSiT6airWSkls0aTDnnUsB1wG3Aw8DNzrktZvZ+M7uq1AMUkepy+uIWIga7FExJHm+TYzXslNoUK+RGzrlbgVsnXfaeaW77zPkPS0SqVVMsymmLWrSiT3IyGcfhY0nVTEnN0rIKEQlcX1ebpvkk5+h4knTGaZpPapaCKREJXF93G7sOjZHOaMNj8YrPQQ07pXYpmBKRwPV1tTKRyrD/8HjYQ5EKkN2XT5kpqVUKpkQkcNrwWPJpXz6pdQqmRCRwCqYkX3YrmSVazSc1SsGUiATulNZGFrU2KpgS4HhmSvvySa1SMCUiJdHX1crOfrVHEC+Yam+O0RjTW47UJp3ZIlISao8gWYPal09qnIIpESmJvq42BscmOOxP8Uj9GhzVvnxS2xRMiUhJ9HW3ArDrkLJT9c7bl0/F51K7FEyJSEnkVvSpbqruDY5NsEQNO6WGKZgSkZLoPaWFxmhEdVN1zjnH4bEJTfNJTVMwJSIlEY0Yq5a0Kpiqc8PjKVLal09qnIIpESmZvu5Wdg5omq+eaV8+qQcKpkSkZPq62nhs6BiJVDrsoUhIjm8lowJ0qV0KpkSkZPq62khnHI8NHgt7KBKSQ/4mx+ozJbVMwZSIlIz26JNsZkrTfFLLFEyJSMms7vJ6Taluqn4N+TVTKkCXWqZgSkRKprUpxqmdzezsV2aqXg2OTdDWFKMpFg17KCIlo2BKREpKe/TVtyH1mJI6oGBKREqqr8trj+CcC3soEgIFU1IPFEyJSEn1dbcxmkjRP5IIeygSgsHRCa3kk5qnYEpESur4Hn2a6qtHg2MJZaak5imYEpGSUnuE+uWcY2hsgsVtatgptU3BlIiUVE9HE62NUbVHqEMjiRTJtNM0n9Q8BVMiUlJmRl+3VvTVo6HR7FYyCqaktimYEpGS6+tqY5cyU3VnMLsvn7qfS41TMCUiJdfX1cr+I+Mcm0iFPRQpo9xWMspMSY1TMCUiJZctQld2qr5oKxmpFwqmRKTk+rq1oq8eHRrNZqa0mk9qm4IpESm50xe3EIsYjxwcCXsoUkZDYxO0NEZZ0Kh9+aS2KZgSkZJrikU5o7uNLY8Phz0UKSNtJSP1QsGUiJTFxmWdPLRfwVQ9GRzTVjJSHxRMiUhZbFzWwaHRBP3D8bCHImUypK1kpE4omBKRsjhreSeApvrqyNDoBItUfC51QMGUiJTFmae2A7Dl8aMhj0TKwTnHobEJlqhhp9QBBVMiUhbtzQ2sXNyiuqk6MTaRZiKV0TSf1AUFUyJSNhuXdbLlCWWm6oH25ZN6omBKRMpmw7IO9g6Nc3Q8GfZQpMQG/e7nizXNJ3VAwZSIlE22CH2ritBrXnZfPhWgSz1QMCUiZbNxWQegIvR6MKhNjqWOKJgSkbJZ0tZET0eT2iPUgWxmStN8Ug8UTIlIWW1c1qnMVB0YHE3Q3BChpTEW9lBESk7BlIiU1cZlHewcGCOeTIc9FCkhbysZ1UtJfVAwJSJltXFZJ+mMY9uBkbCHIiWkTY6lniiYEpGyyhahP7RfU321TMGU1BMFUyJSVr2nLKBzQYOK0Gvc4OiEVvJJ3VAwJSJlZWZsOLWDrSpCr2lDYxNaySd1Q8GUiJTdxmUdPHxghGQ6E/ZQpASOTaQYT6bVsFPqhoIpESm7jcs7mEhl2DkwGvZQpAT6h72tZLraFUxJfVAwJSJld9Yyb1uZLftVN1WL+ke8YKpbwZTUCQVTIlJ2q7vaaG6IqAi9RvWPxAHo7lAwJfVBwZSIlF00Yqxf2qFO6DUqO83X3d4c8khEykPBlIiEYuOyDrY+Pkwm48IeigSsfyRBQ9Q4paUh7KGIlIWCKREJxcZlnYwkUuw9fCzsoUjA+kfidLU1YWZhD0WkLBRMiUgozlrudUJX3VTtGRhJ0NWhKT6pHwUFU2Z2hZltN7MdZvbOKa5/u5ltNbMHzeynZnZ68EMVkVqytqedaMRUN1WD+ocTWskndWXWYMrMosBngOcBG4BrzGzDpJv9HtjknDsH+A7wz0EPVERqS3NDlDXdbTyk9gg1Z2BUwZTUl0IyU+cDO5xzu5xzE8BNwNX5N3DO/dw5ly18uBvoDXaYIlKLNizr0DRfjZlIZRgam9BKPqkrhQRTy4G9ed/v8y+bzhuBH85nUCJSH85a1smh0QT7VIReMw6N+m0R1GNK6kigBehm9mfAJuCj01x/rZltNrPNAwMDQT60iFShS9d2AfCzbf0hj0SCou7nUo8KCab2Ayvyvu/1LzuBmT0HeDdwlXMuMdUdOeducM5tcs5t6urqmst4RaSGnNHdxuquVn685WDYQ5GA9A/73c81zSd1pJBg6l5gjZmtMrNG4JXALfk3MLMnAZ/HC6T0EVNECnbZhh7u3jXI0fFk2EORAOQyU5rmkzoyazDlnEsB1wG3AQ8DNzvntpjZ+83sKv9mHwXagG+b2f1mdss0dycicoLLN/SQyjh+sV2fw2pB/0gCM1jc2hj2UETKJlbIjZxztwK3TrrsPXlfPyfgcYlInThvxSksaWvkJ1sPcvV5M61tkWowMBJncWsTsah6Qkv90NkuIqGKRoxnr+/hl9sHmEhlwh6OzJMadko9UjAlIqG7bEMPI4kUd+8aDHsoMk/9IwnVS0ndUTAlIqG7eM0SFjRE+fHWA2EPReapfySuzJTUHQVTIhK65oYol6xZwu1b+3HOhT0cmaN0xnFoVN3Ppf4omBKRinD5xqUcGI7zh/3a+LhaDY1NkM44TfNJ3VEwJSIV4Vnru4kY/GSrGnhWq/6RbMNOBVNSXxRMiUhFWNTayKaVixRMVbFsw84uTfNJnVEwJSIV4/INPWw7MMJjg9r4uBoNDGtfPqlPCqZEpGJctqEHQKv6qlR2mq9LwZTUGQVTIlIxTl/cytqeNk31Van+kQSdCxpoboiGPRSRslIwJSIV5bINPdy7Z4jDYxNhD0WKpO7nUq8UTIlIRbl8w1IyDn62TRsfV5v+kbjaIkhdUjAlIhXl7OWd9HQ08f0HHw97KFKk/pGEGnZKXVIwJSIVJRIxXn3h6fxi+wB3/vFQ2MORAjnn/GBKmSmpPwqmRKTivOmS1Zy2qIXrv7+FZDoT9nCkAMPjKSZSGa3kk7qkYEpEKk5zQ5R/eMEGdvSP8rVf7wl7OFKAXPfzDk3zSf1RMCUiFek5Z3Zz6dou/u32PzLgd9aWypXtfq5pPqlHCqZEpCKZGe994QbiqTQfvW1b2MORWWhfPqlnCqZEpGL1dbXxhqev4ubN+7h/75GwhyMz6M9uJaNpPqlDCqZEpKJd96wz6Gpv4r23bCGTcWEPR6bRP5KgpTFKW1Ms7KGIlJ2CKRGpaO3NDfzd89bzwN4jfOd3+8IejkxDbRGknimYEpGK96LzlvPk0xbyzz/axtHxZNjDkSn0D8fVsFPqloIpEal4kYjxvqvOYmhsgrd8fTPHJlJhD0kmGRhJ0KWtZKROKZgSkapwdm8nn3jFefx29xBv+Oq9CqgqjKb5pJ4pmBKRqnH1ecsVUFWgYxMpRhMpTfNJ3VIwJSJVRQFV5cm1RVBmSuqUgikRqToKqCpLrvu5aqakTimYEpGqlB9Qve7L93JwOB72kOrW8e7nmuaT+qRgSkSq1tXnLedfX/kkHtx/hMs+/kv+6/f7cE6NPctN03xS7xRMiUhVu+rcZfzwbZeypqedv/rWA1z79fu0MXKZ9Y8kaIxGWNjSEPZQREKhYEpEqt6qJa3c/JaLePeVZ/LLRwa4/BO/5AcPPh72sOpG/0icrvYmzCzsoYiEQsGUiNSEaMR486WrufUvL+a0RS1c983f8+ov3cOvdx7S1F+JDYwk6NIUn9QxBVMiUlPO6G7nu//7abzryvU8/MQwr/rCPVz16bv4/gOPk0pnwh5eTeofVsNOqW8KpkSk5sSiEa69tI873/Es/ulPzmYskeIvbvw9z/zYL/jKXbu18i9g/SNxtUWQuhYLewAiIqXS3BDlVRecxiufuoKfPHyQG+7Yxfu+v5X3fX8r63rauXjNEi5Zs4QLVi1mQWM07OFWpYlUhsPHkmqLIHVNwZSI1LxIxHjuxqU8d+NSth0Y5o5HBrjjkUN8/e5H+dKdu2mMRjint5O+rjZWLmlllf/v9MUtNDcoyJrJwKjaIogomBKRurJ+aQfrl3Zw7aV9jE+k+e2eIe54ZIAH9h7hp9sOcmh0IndbM1jc2kR3exM9HU30dDTT3d5Ed0czSzuaWdrp/VvU0kgkUp8r2fr9KVNN80k9UzAlInVrQWOUZ6zt4hlru3KXDceTPHroGLsOjbLn0DEODI9zcDhB/0ichx4f5tBogsmLAxuiRnd7M8sWNrPilBZ6T1lAb97/yxY2E4vWZolqbisZTfNJHVMwJSKSp6O5gbN7Ozm7t3PK61PpDIdGJzgwHOfA0TgHjo5zYDjBgaPjPH40zj27h/je/eNk8gKuhqhx2qIWVi1po6/Lm0Ls625jbU87nQuqu9Hl8WBKmSmpXwqmRESKEItGctN7rJj6Nsl0hieOxNl3+Bh7Dx9j96Fj7D40yu5DY9zxyAATeS0alnU2s25pO+uWdnDmqe1sOLWD1V1tRKtk2nBgOE7EYHGbgimpXwqmREQC1hCNcNriFk5b3HLSdemM4/Ej4+zoH2XbgRG2Hxhm24ER7txxiGTaS2e1NkY5a3kn565YyDm9nZzbu5DeUxZUZIfx/pEEi9uaqib4EykFBVMiImUUjRgrFrWwYlEL/2t9d+7yiVSGXYdGeWj/MA/uO8ID+47y1bv25LJY3e1NPHXVIs5fuYinrlzE+qXtoRe9Hz2W5Kfb+lnT3RbqOETCpmBKRKQCNMYiuZWGL31KL+AFWNsPjHD/3sPcu+cw9+4Z4n8efAKA9uYYF6xazHM39nDZhh4WtjSWfcwf+uHDDI1N8K4rzyz7Y4tUEgVTIiIVqjEWyRXDv/qilTjn2Hd4nHv3DHHvniHueOQQtz98kFjEuKhvMVeefSqXb+gpS/3Sr3ce4qZ79/KWZ6zmrOVTF+uL1AsLawPQTZs2uc2bN4fy2CIitcA5x4P7jvLDhw7ww4ee4NHBY0QMXnDOMv7++WfS3VGadgXxZJrn/usdAPzobZeqe7zUBTO7zzm3aarrlJkSEalSZsa5KxZy7oqFvOOKdTz8xAj/ff9+vvLrPfx8ez/vuGI9rzr/tMBrq/719j/y6OAxvvnmCxRIiaCNjkVEaoKZsWFZB3935Zn86G2XcPbyTv7+ew/x0s/9mm0HhgN7nIf2fLEc7wAABhxJREFUH+ULv9rFKzat4Gl9SwK7X5FqpmBKRKTGrO5q4z/edAH/8rJz2X1ojBd88k4+8qNtxJPped1vKp3hHd99kEWtjSo6F8mjYEpEpAaZGS95Si8//etn8idPWs7//4udvPBTd/LQ/qNzvs8v3rmbLY8P8/6rNtLZUt2d20WCpGBKRKSGLWpt5KMvO5evveF8jo4n+ZPP3sVnf7GDdKa4xUe/3T3EJ37yCJdv6OGKs5aWaLQi1UnBlIhIHXjG2i5u+7+XcvmGpfzzj7bzis//hr1Dx2b9ufGJNB/4wVZeccNv6O5o4gMvOqsiO7GLhEmtEURE6ohzju/dv5/3fG8LGed4++XruOKspSxfuOCk29736GH+9tsPsOvQGK++8HTe+bz1tDZpEbjUp5laIyiYEhGpQ/uPjPM3Nz/Ab3YNArC6q5VL13Rx8RlLeNJpC/n8Hbv44q92cWrnAj760nN42hlauSf1TcGUiIicxDnHH/tH+dUfD/GrPw5wz64hxvNW/L3qgtN415Vn0qZslIiadoqIyMnMjLU97aztaeeNF68ikUpz36OHuXf3YZ668hRlo0QKpGBKREQAaIpFeVrfEjXjFClSQav5zOwKM9tuZjvM7J1TXN9kZt/yr7/HzFYGPVARERGRSjRrMGVmUeAzwPOADcA1ZrZh0s3eCBx2zp0BfAL4SNADFREREalEhWSmzgd2OOd2OecmgJuAqyfd5mrga/7X3wGebWpEIiIiInWgkGBqObA37/t9/mVT3sY5lwKOAouDGKCIiIhIJStrAbqZXQtc63+bMLOHyvn4ErglwKGwByHzomNY3XT8qp+OYfU4fborCgmm9gMr8r7v9S+b6jb7zCwGdAKDk+/IOXcDcAOAmW2erl+DVAcdw+qnY1jddPyqn45hbShkmu9eYI2ZrTKzRuCVwC2TbnML8Fr/65cCP3NhdQMVERERKaNZM1POuZSZXQfcBkSBLzvntpjZ+4HNzrlbgC8BXzezHcAQXsAlIiIiUvMKqplyzt0K3DrpsvfkfR0HXlbkY99Q5O2l8ugYVj8dw+qm41f9dAxrQGh784mIiIjUgoI6oIuIiIjI1BRMiYiIiMyDgikRERGReajIYMrMTjOz75nZl6faWFkqm5lFzOyDZvYpM3vt7D8hlcjMWs1ss5m9IOyxSPHM7EVm9gV/E/rLwx6PFMb/vfuaf+z+NOzxSGECD6b8AKh/cndzM7vCzLab2Y4CAqSzge84594APCnoMcr0Ajp+V+M1d03ibT8kZRTQMQR4B3BzaUYpMwniGDrnvuecezPwVuAVpRyvzKzI4/livPe/NwNXlX2wMieBr+Yzs0uBUeDfnXNn+ZdFgUeAy/DeXO8FrsHrW/WhSXfxBiCNt2GyA77unPtKoIOUaQV0/N4AHHbOfd7MvuOce2m5xi+BHcNz8fbXbAYOOed+UJ7RCwRzDJ1z/f7P/QvwH86535Vp+DJJkcfzauCHzrn7zeybzrlXhTRsKULge/M55+4ws5WTLj4f2OGc2wVgZjcBVzvnPgScNIVgZn8DvNe/r+8ACqbKJKDjtw+Y8L9Nl260MpWAjuEzgVZgAzBuZrc65zKlHLccF9AxNODDeG/MCqRCVMzxxAuseoH7qdBSHDlZuTY6Xg7szft+H3DBDLf/EXC9mb0K2FPCcUlhij1+/wl8yswuAe4o5cCkYEUdQ+fcuwHM7HV4mSkFUuEr9vfwL4DnAJ1mdoZz7nOlHJwUbbrj+Ung02b2fOD7YQxMileuYKoozrmH8Pb4kyrknDsGvDHsccj8Oee+GvYYZG6cc5/Ee2OWKuKcGwNeH/Y4pDjlSiHuB1bkfd/rXybVQcev+ukYVj8dw9qi41lDyhVM3QusMbNVZtaItxHyLWV6bJk/Hb/qp2NY/XQMa4uOZw0pRWuEG4HfAOvMbJ+ZvdE5lwKuA24DHgZuds5tCfqxZf50/KqfjmH10zGsLTqetU8bHYuIiIjMg5ZdioiIiMyDgikRERGReVAwJSIiIjIPCqZERERE5kHBlIiIiMg8KJgSERERmQcFUyIiIiLzoGBKREREZB4UTImIiIjMw/8DkGeoPABZUXoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZxCgpuYkQ2Q"
      },
      "source": [
        "# Chargement des poids sauvegardés\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": 499,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmdbo23qkTKE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46a8425a-d72f-4b5b-e647-0bff9a7c2134"
      },
      "source": [
        "max_periodes = 10000\n",
        "\n",
        "# Classe permettant d'arrêter l'entrainement si la variation\n",
        "# devient plus petite qu'une valeur à choisir sur un nombre\n",
        "# de périodes à choisir\n",
        "class StopTrain(keras.callbacks.Callback):\n",
        "    def __init__(self, delta=0.01,periodes=100, term=\"loss\", logs={}):\n",
        "      self.n_periodes = 0\n",
        "      self.periodes = periodes\n",
        "      self.loss_1 = 100\n",
        "      self.delta = delta\n",
        "      self.term = term\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "      diff_loss = abs(self.loss_1 - logs[self.term])\n",
        "      self.loss_1 = logs[self.term]\n",
        "      if (diff_loss < self.delta):\n",
        "        self.n_periodes = self.n_periodes + 1\n",
        "      else:\n",
        "        self.n_periodes = 0\n",
        "      if (self.n_periodes == self.periodes):\n",
        "        print(\"Arrêt de l'entrainement...\")\n",
        "        self.model.stop_training = True\n",
        "\n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.015,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0.01)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"mse\", optimizer=optimiseur, metrics=[\"mse\"])\n",
        "\n",
        "# Entraine le modèle, avec une réduction des calculs du gradient\n",
        "#historique = model.fit(x=x_train,y=y_train,validation_data=(x_val,y_val), epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=1e-7,periodes = 10, term=\"My_MSE\")],batch_size=batch_size)\n",
        "\n",
        "# Entraine le modèle sans réduction de calculs\n",
        "historique = model.fit(dataset,validation_data=dataset_val, epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=1e-8,periodes = 10, term=\"loss\")])\n",
        "#historique = model.fit(dataset, epochs=max_periodes)\n"
      ],
      "execution_count": 569,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mLe flux de sortie a été tronqué et ne contient que les 5000 dernières lignes.\u001b[0m\n",
            "Epoch 8751/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0760 - val_mse: 0.0760\n",
            "\n",
            "Epoch 08751: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8752/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0760 - val_mse: 0.0760\n",
            "\n",
            "Epoch 08752: loss did not improve from 0.00372\n",
            "Epoch 8753/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0761 - val_mse: 0.0761\n",
            "\n",
            "Epoch 08753: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8754/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0760 - val_mse: 0.0760\n",
            "\n",
            "Epoch 08754: loss did not improve from 0.00372\n",
            "Epoch 8755/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0761 - val_mse: 0.0761\n",
            "\n",
            "Epoch 08755: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8756/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0760 - val_mse: 0.0760\n",
            "\n",
            "Epoch 08756: loss did not improve from 0.00372\n",
            "Epoch 8757/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0761 - val_mse: 0.0761\n",
            "\n",
            "Epoch 08757: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8758/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0760 - val_mse: 0.0760\n",
            "\n",
            "Epoch 08758: loss did not improve from 0.00372\n",
            "Epoch 8759/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0761 - val_mse: 0.0761\n",
            "\n",
            "Epoch 08759: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8760/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0760 - val_mse: 0.0760\n",
            "\n",
            "Epoch 08760: loss did not improve from 0.00372\n",
            "Epoch 8761/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0761 - val_mse: 0.0761\n",
            "\n",
            "Epoch 08761: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8762/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0760 - val_mse: 0.0760\n",
            "\n",
            "Epoch 08762: loss did not improve from 0.00372\n",
            "Epoch 8763/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0761 - val_mse: 0.0761\n",
            "\n",
            "Epoch 08763: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8764/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0760 - val_mse: 0.0760\n",
            "\n",
            "Epoch 08764: loss did not improve from 0.00372\n",
            "Epoch 8765/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0761 - val_mse: 0.0761\n",
            "\n",
            "Epoch 08765: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8766/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0760 - val_mse: 0.0760\n",
            "\n",
            "Epoch 08766: loss did not improve from 0.00372\n",
            "Epoch 8767/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0761 - val_mse: 0.0761\n",
            "\n",
            "Epoch 08767: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8768/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0760 - val_mse: 0.0760\n",
            "\n",
            "Epoch 08768: loss did not improve from 0.00372\n",
            "Epoch 8769/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0762 - val_mse: 0.0762\n",
            "\n",
            "Epoch 08769: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8770/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0761 - val_mse: 0.0761\n",
            "\n",
            "Epoch 08770: loss did not improve from 0.00372\n",
            "Epoch 8771/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0762 - val_mse: 0.0762\n",
            "\n",
            "Epoch 08771: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8772/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0761 - val_mse: 0.0761\n",
            "\n",
            "Epoch 08772: loss did not improve from 0.00372\n",
            "Epoch 8773/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0762 - val_mse: 0.0762\n",
            "\n",
            "Epoch 08773: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8774/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0761 - val_mse: 0.0761\n",
            "\n",
            "Epoch 08774: loss did not improve from 0.00372\n",
            "Epoch 8775/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0762 - val_mse: 0.0762\n",
            "\n",
            "Epoch 08775: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8776/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0761 - val_mse: 0.0761\n",
            "\n",
            "Epoch 08776: loss did not improve from 0.00372\n",
            "Epoch 8777/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0762 - val_mse: 0.0762\n",
            "\n",
            "Epoch 08777: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8778/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0761 - val_mse: 0.0761\n",
            "\n",
            "Epoch 08778: loss did not improve from 0.00372\n",
            "Epoch 8779/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0762 - val_mse: 0.0762\n",
            "\n",
            "Epoch 08779: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8780/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0761 - val_mse: 0.0761\n",
            "\n",
            "Epoch 08780: loss did not improve from 0.00372\n",
            "Epoch 8781/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0762 - val_mse: 0.0762\n",
            "\n",
            "Epoch 08781: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8782/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0761 - val_mse: 0.0761\n",
            "\n",
            "Epoch 08782: loss did not improve from 0.00372\n",
            "Epoch 8783/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0762 - val_mse: 0.0762\n",
            "\n",
            "Epoch 08783: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8784/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0761 - val_mse: 0.0761\n",
            "\n",
            "Epoch 08784: loss did not improve from 0.00372\n",
            "Epoch 8785/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0762 - val_mse: 0.0762\n",
            "\n",
            "Epoch 08785: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8786/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0762 - val_mse: 0.0762\n",
            "\n",
            "Epoch 08786: loss did not improve from 0.00372\n",
            "Epoch 8787/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0763 - val_mse: 0.0763\n",
            "\n",
            "Epoch 08787: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8788/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0762 - val_mse: 0.0762\n",
            "\n",
            "Epoch 08788: loss did not improve from 0.00372\n",
            "Epoch 8789/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0763 - val_mse: 0.0763\n",
            "\n",
            "Epoch 08789: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8790/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0762 - val_mse: 0.0762\n",
            "\n",
            "Epoch 08790: loss did not improve from 0.00372\n",
            "Epoch 8791/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0763 - val_mse: 0.0763\n",
            "\n",
            "Epoch 08791: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8792/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0762 - val_mse: 0.0762\n",
            "\n",
            "Epoch 08792: loss did not improve from 0.00372\n",
            "Epoch 8793/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0763 - val_mse: 0.0763\n",
            "\n",
            "Epoch 08793: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8794/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0762 - val_mse: 0.0762\n",
            "\n",
            "Epoch 08794: loss did not improve from 0.00372\n",
            "Epoch 8795/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0763 - val_mse: 0.0763\n",
            "\n",
            "Epoch 08795: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8796/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0762 - val_mse: 0.0762\n",
            "\n",
            "Epoch 08796: loss did not improve from 0.00372\n",
            "Epoch 8797/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0763 - val_mse: 0.0763\n",
            "\n",
            "Epoch 08797: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8798/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0762 - val_mse: 0.0762\n",
            "\n",
            "Epoch 08798: loss did not improve from 0.00372\n",
            "Epoch 8799/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0763 - val_mse: 0.0763\n",
            "\n",
            "Epoch 08799: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8800/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0762 - val_mse: 0.0762\n",
            "\n",
            "Epoch 08800: loss did not improve from 0.00372\n",
            "Epoch 8801/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0763 - val_mse: 0.0763\n",
            "\n",
            "Epoch 08801: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8802/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0763 - val_mse: 0.0763\n",
            "\n",
            "Epoch 08802: loss did not improve from 0.00372\n",
            "Epoch 8803/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0764 - val_mse: 0.0764\n",
            "\n",
            "Epoch 08803: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8804/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0763 - val_mse: 0.0763\n",
            "\n",
            "Epoch 08804: loss did not improve from 0.00372\n",
            "Epoch 8805/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0764 - val_mse: 0.0764\n",
            "\n",
            "Epoch 08805: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8806/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0763 - val_mse: 0.0763\n",
            "\n",
            "Epoch 08806: loss did not improve from 0.00372\n",
            "Epoch 8807/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0764 - val_mse: 0.0764\n",
            "\n",
            "Epoch 08807: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8808/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0763 - val_mse: 0.0763\n",
            "\n",
            "Epoch 08808: loss did not improve from 0.00372\n",
            "Epoch 8809/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0764 - val_mse: 0.0764\n",
            "\n",
            "Epoch 08809: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8810/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0763 - val_mse: 0.0763\n",
            "\n",
            "Epoch 08810: loss did not improve from 0.00372\n",
            "Epoch 8811/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0764 - val_mse: 0.0764\n",
            "\n",
            "Epoch 08811: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8812/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0763 - val_mse: 0.0763\n",
            "\n",
            "Epoch 08812: loss did not improve from 0.00372\n",
            "Epoch 8813/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0764 - val_mse: 0.0764\n",
            "\n",
            "Epoch 08813: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8814/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0763 - val_mse: 0.0763\n",
            "\n",
            "Epoch 08814: loss did not improve from 0.00372\n",
            "Epoch 8815/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0764 - val_mse: 0.0764\n",
            "\n",
            "Epoch 08815: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8816/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0763 - val_mse: 0.0763\n",
            "\n",
            "Epoch 08816: loss did not improve from 0.00372\n",
            "Epoch 8817/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0764 - val_mse: 0.0764\n",
            "\n",
            "Epoch 08817: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8818/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0764 - val_mse: 0.0764\n",
            "\n",
            "Epoch 08818: loss did not improve from 0.00372\n",
            "Epoch 8819/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0765 - val_mse: 0.0765\n",
            "\n",
            "Epoch 08819: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8820/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0764 - val_mse: 0.0764\n",
            "\n",
            "Epoch 08820: loss did not improve from 0.00372\n",
            "Epoch 8821/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0765 - val_mse: 0.0765\n",
            "\n",
            "Epoch 08821: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8822/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0764 - val_mse: 0.0764\n",
            "\n",
            "Epoch 08822: loss did not improve from 0.00372\n",
            "Epoch 8823/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0765 - val_mse: 0.0765\n",
            "\n",
            "Epoch 08823: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8824/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0764 - val_mse: 0.0764\n",
            "\n",
            "Epoch 08824: loss did not improve from 0.00372\n",
            "Epoch 8825/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0765 - val_mse: 0.0765\n",
            "\n",
            "Epoch 08825: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8826/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0764 - val_mse: 0.0764\n",
            "\n",
            "Epoch 08826: loss did not improve from 0.00372\n",
            "Epoch 8827/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0765 - val_mse: 0.0765\n",
            "\n",
            "Epoch 08827: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8828/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0764 - val_mse: 0.0764\n",
            "\n",
            "Epoch 08828: loss did not improve from 0.00372\n",
            "Epoch 8829/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0765 - val_mse: 0.0765\n",
            "\n",
            "Epoch 08829: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8830/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0764 - val_mse: 0.0764\n",
            "\n",
            "Epoch 08830: loss did not improve from 0.00372\n",
            "Epoch 8831/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0765 - val_mse: 0.0765\n",
            "\n",
            "Epoch 08831: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8832/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0764 - val_mse: 0.0764\n",
            "\n",
            "Epoch 08832: loss did not improve from 0.00372\n",
            "Epoch 8833/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0765 - val_mse: 0.0765\n",
            "\n",
            "Epoch 08833: loss improved from 0.00372 to 0.00372, saving model to poids_train.hdf5\n",
            "Epoch 8834/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0764 - val_mse: 0.0764\n",
            "\n",
            "Epoch 08834: loss did not improve from 0.00372\n",
            "Epoch 8835/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0765 - val_mse: 0.0765\n",
            "\n",
            "Epoch 08835: loss improved from 0.00372 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8836/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0765 - val_mse: 0.0765\n",
            "\n",
            "Epoch 08836: loss did not improve from 0.00371\n",
            "Epoch 8837/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0766 - val_mse: 0.0766\n",
            "\n",
            "Epoch 08837: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8838/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0765 - val_mse: 0.0765\n",
            "\n",
            "Epoch 08838: loss did not improve from 0.00371\n",
            "Epoch 8839/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0766 - val_mse: 0.0766\n",
            "\n",
            "Epoch 08839: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8840/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0765 - val_mse: 0.0765\n",
            "\n",
            "Epoch 08840: loss did not improve from 0.00371\n",
            "Epoch 8841/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0766 - val_mse: 0.0766\n",
            "\n",
            "Epoch 08841: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8842/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0765 - val_mse: 0.0765\n",
            "\n",
            "Epoch 08842: loss did not improve from 0.00371\n",
            "Epoch 8843/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0766 - val_mse: 0.0766\n",
            "\n",
            "Epoch 08843: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8844/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0765 - val_mse: 0.0765\n",
            "\n",
            "Epoch 08844: loss did not improve from 0.00371\n",
            "Epoch 8845/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0766 - val_mse: 0.0766\n",
            "\n",
            "Epoch 08845: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8846/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0765 - val_mse: 0.0765\n",
            "\n",
            "Epoch 08846: loss did not improve from 0.00371\n",
            "Epoch 8847/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0766 - val_mse: 0.0766\n",
            "\n",
            "Epoch 08847: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8848/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0765 - val_mse: 0.0765\n",
            "\n",
            "Epoch 08848: loss did not improve from 0.00371\n",
            "Epoch 8849/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0766 - val_mse: 0.0766\n",
            "\n",
            "Epoch 08849: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8850/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0765 - val_mse: 0.0765\n",
            "\n",
            "Epoch 08850: loss did not improve from 0.00371\n",
            "Epoch 8851/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0766 - val_mse: 0.0766\n",
            "\n",
            "Epoch 08851: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8852/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0766 - val_mse: 0.0766\n",
            "\n",
            "Epoch 08852: loss did not improve from 0.00371\n",
            "Epoch 8853/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0767 - val_mse: 0.0767\n",
            "\n",
            "Epoch 08853: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8854/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0766 - val_mse: 0.0766\n",
            "\n",
            "Epoch 08854: loss did not improve from 0.00371\n",
            "Epoch 8855/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0767 - val_mse: 0.0767\n",
            "\n",
            "Epoch 08855: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8856/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0766 - val_mse: 0.0766\n",
            "\n",
            "Epoch 08856: loss did not improve from 0.00371\n",
            "Epoch 8857/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0767 - val_mse: 0.0767\n",
            "\n",
            "Epoch 08857: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8858/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0766 - val_mse: 0.0766\n",
            "\n",
            "Epoch 08858: loss did not improve from 0.00371\n",
            "Epoch 8859/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0767 - val_mse: 0.0767\n",
            "\n",
            "Epoch 08859: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8860/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0766 - val_mse: 0.0766\n",
            "\n",
            "Epoch 08860: loss did not improve from 0.00371\n",
            "Epoch 8861/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0767 - val_mse: 0.0767\n",
            "\n",
            "Epoch 08861: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8862/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0766 - val_mse: 0.0766\n",
            "\n",
            "Epoch 08862: loss did not improve from 0.00371\n",
            "Epoch 8863/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0767 - val_mse: 0.0767\n",
            "\n",
            "Epoch 08863: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8864/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0766 - val_mse: 0.0766\n",
            "\n",
            "Epoch 08864: loss did not improve from 0.00371\n",
            "Epoch 8865/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0767 - val_mse: 0.0767\n",
            "\n",
            "Epoch 08865: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8866/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0766 - val_mse: 0.0766\n",
            "\n",
            "Epoch 08866: loss did not improve from 0.00371\n",
            "Epoch 8867/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0767 - val_mse: 0.0767\n",
            "\n",
            "Epoch 08867: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8868/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0767 - val_mse: 0.0767\n",
            "\n",
            "Epoch 08868: loss did not improve from 0.00371\n",
            "Epoch 8869/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0768 - val_mse: 0.0768\n",
            "\n",
            "Epoch 08869: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8870/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0767 - val_mse: 0.0767\n",
            "\n",
            "Epoch 08870: loss did not improve from 0.00371\n",
            "Epoch 8871/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0768 - val_mse: 0.0768\n",
            "\n",
            "Epoch 08871: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8872/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0767 - val_mse: 0.0767\n",
            "\n",
            "Epoch 08872: loss did not improve from 0.00371\n",
            "Epoch 8873/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0768 - val_mse: 0.0768\n",
            "\n",
            "Epoch 08873: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8874/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0767 - val_mse: 0.0767\n",
            "\n",
            "Epoch 08874: loss did not improve from 0.00371\n",
            "Epoch 8875/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0768 - val_mse: 0.0768\n",
            "\n",
            "Epoch 08875: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8876/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0767 - val_mse: 0.0767\n",
            "\n",
            "Epoch 08876: loss did not improve from 0.00371\n",
            "Epoch 8877/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0768 - val_mse: 0.0768\n",
            "\n",
            "Epoch 08877: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8878/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0767 - val_mse: 0.0767\n",
            "\n",
            "Epoch 08878: loss did not improve from 0.00371\n",
            "Epoch 8879/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0768 - val_mse: 0.0768\n",
            "\n",
            "Epoch 08879: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8880/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0767 - val_mse: 0.0767\n",
            "\n",
            "Epoch 08880: loss did not improve from 0.00371\n",
            "Epoch 8881/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0768 - val_mse: 0.0768\n",
            "\n",
            "Epoch 08881: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8882/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0767 - val_mse: 0.0767\n",
            "\n",
            "Epoch 08882: loss did not improve from 0.00371\n",
            "Epoch 8883/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0768 - val_mse: 0.0768\n",
            "\n",
            "Epoch 08883: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8884/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0767 - val_mse: 0.0767\n",
            "\n",
            "Epoch 08884: loss did not improve from 0.00371\n",
            "Epoch 8885/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0768 - val_mse: 0.0768\n",
            "\n",
            "Epoch 08885: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8886/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0768 - val_mse: 0.0768\n",
            "\n",
            "Epoch 08886: loss did not improve from 0.00371\n",
            "Epoch 8887/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0769 - val_mse: 0.0769\n",
            "\n",
            "Epoch 08887: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8888/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0768 - val_mse: 0.0768\n",
            "\n",
            "Epoch 08888: loss did not improve from 0.00371\n",
            "Epoch 8889/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0769 - val_mse: 0.0769\n",
            "\n",
            "Epoch 08889: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8890/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0768 - val_mse: 0.0768\n",
            "\n",
            "Epoch 08890: loss did not improve from 0.00371\n",
            "Epoch 8891/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0769 - val_mse: 0.0769\n",
            "\n",
            "Epoch 08891: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8892/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0768 - val_mse: 0.0768\n",
            "\n",
            "Epoch 08892: loss did not improve from 0.00371\n",
            "Epoch 8893/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0769 - val_mse: 0.0769\n",
            "\n",
            "Epoch 08893: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8894/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0768 - val_mse: 0.0768\n",
            "\n",
            "Epoch 08894: loss did not improve from 0.00371\n",
            "Epoch 8895/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0769 - val_mse: 0.0769\n",
            "\n",
            "Epoch 08895: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8896/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0768 - val_mse: 0.0768\n",
            "\n",
            "Epoch 08896: loss did not improve from 0.00371\n",
            "Epoch 8897/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0769 - val_mse: 0.0769\n",
            "\n",
            "Epoch 08897: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8898/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0768 - val_mse: 0.0768\n",
            "\n",
            "Epoch 08898: loss did not improve from 0.00371\n",
            "Epoch 8899/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0769 - val_mse: 0.0769\n",
            "\n",
            "Epoch 08899: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8900/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0768 - val_mse: 0.0768\n",
            "\n",
            "Epoch 08900: loss did not improve from 0.00371\n",
            "Epoch 8901/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0769 - val_mse: 0.0769\n",
            "\n",
            "Epoch 08901: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8902/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0769 - val_mse: 0.0769\n",
            "\n",
            "Epoch 08902: loss did not improve from 0.00371\n",
            "Epoch 8903/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0770 - val_mse: 0.0770\n",
            "\n",
            "Epoch 08903: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8904/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0769 - val_mse: 0.0769\n",
            "\n",
            "Epoch 08904: loss did not improve from 0.00371\n",
            "Epoch 8905/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0770 - val_mse: 0.0770\n",
            "\n",
            "Epoch 08905: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8906/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0769 - val_mse: 0.0769\n",
            "\n",
            "Epoch 08906: loss did not improve from 0.00371\n",
            "Epoch 8907/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0770 - val_mse: 0.0770\n",
            "\n",
            "Epoch 08907: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8908/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0769 - val_mse: 0.0769\n",
            "\n",
            "Epoch 08908: loss did not improve from 0.00371\n",
            "Epoch 8909/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0770 - val_mse: 0.0770\n",
            "\n",
            "Epoch 08909: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8910/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0769 - val_mse: 0.0769\n",
            "\n",
            "Epoch 08910: loss did not improve from 0.00371\n",
            "Epoch 8911/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0770 - val_mse: 0.0770\n",
            "\n",
            "Epoch 08911: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8912/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0769 - val_mse: 0.0769\n",
            "\n",
            "Epoch 08912: loss did not improve from 0.00371\n",
            "Epoch 8913/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0770 - val_mse: 0.0770\n",
            "\n",
            "Epoch 08913: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8914/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0769 - val_mse: 0.0769\n",
            "\n",
            "Epoch 08914: loss did not improve from 0.00371\n",
            "Epoch 8915/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0770 - val_mse: 0.0770\n",
            "\n",
            "Epoch 08915: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8916/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0769 - val_mse: 0.0769\n",
            "\n",
            "Epoch 08916: loss did not improve from 0.00371\n",
            "Epoch 8917/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0770 - val_mse: 0.0770\n",
            "\n",
            "Epoch 08917: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8918/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0770 - val_mse: 0.0770\n",
            "\n",
            "Epoch 08918: loss did not improve from 0.00371\n",
            "Epoch 8919/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0771 - val_mse: 0.0771\n",
            "\n",
            "Epoch 08919: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8920/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0770 - val_mse: 0.0770\n",
            "\n",
            "Epoch 08920: loss did not improve from 0.00371\n",
            "Epoch 8921/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0771 - val_mse: 0.0771\n",
            "\n",
            "Epoch 08921: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8922/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0770 - val_mse: 0.0770\n",
            "\n",
            "Epoch 08922: loss did not improve from 0.00371\n",
            "Epoch 8923/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0771 - val_mse: 0.0771\n",
            "\n",
            "Epoch 08923: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8924/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0770 - val_mse: 0.0770\n",
            "\n",
            "Epoch 08924: loss did not improve from 0.00371\n",
            "Epoch 8925/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0771 - val_mse: 0.0771\n",
            "\n",
            "Epoch 08925: loss improved from 0.00371 to 0.00371, saving model to poids_train.hdf5\n",
            "Epoch 8926/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0770 - val_mse: 0.0770\n",
            "\n",
            "Epoch 08926: loss did not improve from 0.00371\n",
            "Epoch 8927/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0771 - val_mse: 0.0771\n",
            "\n",
            "Epoch 08927: loss improved from 0.00371 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 8928/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0770 - val_mse: 0.0770\n",
            "\n",
            "Epoch 08928: loss did not improve from 0.00370\n",
            "Epoch 8929/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0771 - val_mse: 0.0771\n",
            "\n",
            "Epoch 08929: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 8930/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0770 - val_mse: 0.0770\n",
            "\n",
            "Epoch 08930: loss did not improve from 0.00370\n",
            "Epoch 8931/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0771 - val_mse: 0.0771\n",
            "\n",
            "Epoch 08931: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 8932/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0770 - val_mse: 0.0770\n",
            "\n",
            "Epoch 08932: loss did not improve from 0.00370\n",
            "Epoch 8933/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0771 - val_mse: 0.0771\n",
            "\n",
            "Epoch 08933: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 8934/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0770 - val_mse: 0.0770\n",
            "\n",
            "Epoch 08934: loss did not improve from 0.00370\n",
            "Epoch 8935/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0771 - val_mse: 0.0771\n",
            "\n",
            "Epoch 08935: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 8936/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0771 - val_mse: 0.0771\n",
            "\n",
            "Epoch 08936: loss did not improve from 0.00370\n",
            "Epoch 8937/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0772 - val_mse: 0.0772\n",
            "\n",
            "Epoch 08937: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 8938/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0771 - val_mse: 0.0771\n",
            "\n",
            "Epoch 08938: loss did not improve from 0.00370\n",
            "Epoch 8939/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0772 - val_mse: 0.0772\n",
            "\n",
            "Epoch 08939: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 8940/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0771 - val_mse: 0.0771\n",
            "\n",
            "Epoch 08940: loss did not improve from 0.00370\n",
            "Epoch 8941/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0772 - val_mse: 0.0772\n",
            "\n",
            "Epoch 08941: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 8942/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0771 - val_mse: 0.0771\n",
            "\n",
            "Epoch 08942: loss did not improve from 0.00370\n",
            "Epoch 8943/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0772 - val_mse: 0.0772\n",
            "\n",
            "Epoch 08943: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 8944/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0771 - val_mse: 0.0771\n",
            "\n",
            "Epoch 08944: loss did not improve from 0.00370\n",
            "Epoch 8945/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0772 - val_mse: 0.0772\n",
            "\n",
            "Epoch 08945: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 8946/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0771 - val_mse: 0.0771\n",
            "\n",
            "Epoch 08946: loss did not improve from 0.00370\n",
            "Epoch 8947/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0772 - val_mse: 0.0772\n",
            "\n",
            "Epoch 08947: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 8948/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0771 - val_mse: 0.0771\n",
            "\n",
            "Epoch 08948: loss did not improve from 0.00370\n",
            "Epoch 8949/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0772 - val_mse: 0.0772\n",
            "\n",
            "Epoch 08949: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 8950/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0771 - val_mse: 0.0771\n",
            "\n",
            "Epoch 08950: loss did not improve from 0.00370\n",
            "Epoch 8951/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0772 - val_mse: 0.0772\n",
            "\n",
            "Epoch 08951: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 8952/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0772 - val_mse: 0.0772\n",
            "\n",
            "Epoch 08952: loss did not improve from 0.00370\n",
            "Epoch 8953/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0773 - val_mse: 0.0773\n",
            "\n",
            "Epoch 08953: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 8954/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0772 - val_mse: 0.0772\n",
            "\n",
            "Epoch 08954: loss did not improve from 0.00370\n",
            "Epoch 8955/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0773 - val_mse: 0.0773\n",
            "\n",
            "Epoch 08955: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 8956/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0772 - val_mse: 0.0772\n",
            "\n",
            "Epoch 08956: loss did not improve from 0.00370\n",
            "Epoch 8957/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0773 - val_mse: 0.0773\n",
            "\n",
            "Epoch 08957: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 8958/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0772 - val_mse: 0.0772\n",
            "\n",
            "Epoch 08958: loss did not improve from 0.00370\n",
            "Epoch 8959/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0773 - val_mse: 0.0773\n",
            "\n",
            "Epoch 08959: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 8960/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0772 - val_mse: 0.0772\n",
            "\n",
            "Epoch 08960: loss did not improve from 0.00370\n",
            "Epoch 8961/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0773 - val_mse: 0.0773\n",
            "\n",
            "Epoch 08961: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 8962/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0772 - val_mse: 0.0772\n",
            "\n",
            "Epoch 08962: loss did not improve from 0.00370\n",
            "Epoch 8963/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0773 - val_mse: 0.0773\n",
            "\n",
            "Epoch 08963: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 8964/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0772 - val_mse: 0.0772\n",
            "\n",
            "Epoch 08964: loss did not improve from 0.00370\n",
            "Epoch 8965/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0773 - val_mse: 0.0773\n",
            "\n",
            "Epoch 08965: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 8966/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0772 - val_mse: 0.0772\n",
            "\n",
            "Epoch 08966: loss did not improve from 0.00370\n",
            "Epoch 8967/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0773 - val_mse: 0.0773\n",
            "\n",
            "Epoch 08967: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 8968/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0772 - val_mse: 0.0772\n",
            "\n",
            "Epoch 08968: loss did not improve from 0.00370\n",
            "Epoch 8969/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0773 - val_mse: 0.0773\n",
            "\n",
            "Epoch 08969: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 8970/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0773 - val_mse: 0.0773\n",
            "\n",
            "Epoch 08970: loss did not improve from 0.00370\n",
            "Epoch 8971/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0774 - val_mse: 0.0774\n",
            "\n",
            "Epoch 08971: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 8972/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0773 - val_mse: 0.0773\n",
            "\n",
            "Epoch 08972: loss did not improve from 0.00370\n",
            "Epoch 8973/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0774 - val_mse: 0.0774\n",
            "\n",
            "Epoch 08973: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 8974/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0773 - val_mse: 0.0773\n",
            "\n",
            "Epoch 08974: loss did not improve from 0.00370\n",
            "Epoch 8975/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0774 - val_mse: 0.0774\n",
            "\n",
            "Epoch 08975: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 8976/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0773 - val_mse: 0.0773\n",
            "\n",
            "Epoch 08976: loss did not improve from 0.00370\n",
            "Epoch 8977/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0774 - val_mse: 0.0774\n",
            "\n",
            "Epoch 08977: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 8978/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0773 - val_mse: 0.0773\n",
            "\n",
            "Epoch 08978: loss did not improve from 0.00370\n",
            "Epoch 8979/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0774 - val_mse: 0.0774\n",
            "\n",
            "Epoch 08979: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 8980/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0773 - val_mse: 0.0773\n",
            "\n",
            "Epoch 08980: loss did not improve from 0.00370\n",
            "Epoch 8981/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0774 - val_mse: 0.0774\n",
            "\n",
            "Epoch 08981: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 8982/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0773 - val_mse: 0.0773\n",
            "\n",
            "Epoch 08982: loss did not improve from 0.00370\n",
            "Epoch 8983/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0774 - val_mse: 0.0774\n",
            "\n",
            "Epoch 08983: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 8984/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0773 - val_mse: 0.0773\n",
            "\n",
            "Epoch 08984: loss did not improve from 0.00370\n",
            "Epoch 8985/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0774 - val_mse: 0.0774\n",
            "\n",
            "Epoch 08985: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 8986/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0774 - val_mse: 0.0774\n",
            "\n",
            "Epoch 08986: loss did not improve from 0.00370\n",
            "Epoch 8987/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0775 - val_mse: 0.0775\n",
            "\n",
            "Epoch 08987: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 8988/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0774 - val_mse: 0.0774\n",
            "\n",
            "Epoch 08988: loss did not improve from 0.00370\n",
            "Epoch 8989/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0775 - val_mse: 0.0775\n",
            "\n",
            "Epoch 08989: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 8990/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0774 - val_mse: 0.0774\n",
            "\n",
            "Epoch 08990: loss did not improve from 0.00370\n",
            "Epoch 8991/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0775 - val_mse: 0.0775\n",
            "\n",
            "Epoch 08991: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 8992/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0774 - val_mse: 0.0774\n",
            "\n",
            "Epoch 08992: loss did not improve from 0.00370\n",
            "Epoch 8993/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0775 - val_mse: 0.0775\n",
            "\n",
            "Epoch 08993: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 8994/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0774 - val_mse: 0.0774\n",
            "\n",
            "Epoch 08994: loss did not improve from 0.00370\n",
            "Epoch 8995/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0775 - val_mse: 0.0775\n",
            "\n",
            "Epoch 08995: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 8996/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0774 - val_mse: 0.0774\n",
            "\n",
            "Epoch 08996: loss did not improve from 0.00370\n",
            "Epoch 8997/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0775 - val_mse: 0.0775\n",
            "\n",
            "Epoch 08997: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 8998/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0774 - val_mse: 0.0774\n",
            "\n",
            "Epoch 08998: loss did not improve from 0.00370\n",
            "Epoch 8999/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0775 - val_mse: 0.0775\n",
            "\n",
            "Epoch 08999: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 9000/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0774 - val_mse: 0.0774\n",
            "\n",
            "Epoch 09000: loss did not improve from 0.00370\n",
            "Epoch 9001/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0775 - val_mse: 0.0775\n",
            "\n",
            "Epoch 09001: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 9002/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0774 - val_mse: 0.0774\n",
            "\n",
            "Epoch 09002: loss did not improve from 0.00370\n",
            "Epoch 9003/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0775 - val_mse: 0.0775\n",
            "\n",
            "Epoch 09003: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 9004/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0775 - val_mse: 0.0775\n",
            "\n",
            "Epoch 09004: loss did not improve from 0.00370\n",
            "Epoch 9005/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0776 - val_mse: 0.0776\n",
            "\n",
            "Epoch 09005: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 9006/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0775 - val_mse: 0.0775\n",
            "\n",
            "Epoch 09006: loss did not improve from 0.00370\n",
            "Epoch 9007/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0776 - val_mse: 0.0776\n",
            "\n",
            "Epoch 09007: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 9008/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0775 - val_mse: 0.0775\n",
            "\n",
            "Epoch 09008: loss did not improve from 0.00370\n",
            "Epoch 9009/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0776 - val_mse: 0.0776\n",
            "\n",
            "Epoch 09009: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 9010/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0775 - val_mse: 0.0775\n",
            "\n",
            "Epoch 09010: loss did not improve from 0.00370\n",
            "Epoch 9011/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0776 - val_mse: 0.0776\n",
            "\n",
            "Epoch 09011: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 9012/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0775 - val_mse: 0.0775\n",
            "\n",
            "Epoch 09012: loss did not improve from 0.00370\n",
            "Epoch 9013/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0776 - val_mse: 0.0776\n",
            "\n",
            "Epoch 09013: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 9014/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0775 - val_mse: 0.0775\n",
            "\n",
            "Epoch 09014: loss did not improve from 0.00370\n",
            "Epoch 9015/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0776 - val_mse: 0.0776\n",
            "\n",
            "Epoch 09015: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 9016/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0775 - val_mse: 0.0775\n",
            "\n",
            "Epoch 09016: loss did not improve from 0.00370\n",
            "Epoch 9017/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0776 - val_mse: 0.0776\n",
            "\n",
            "Epoch 09017: loss improved from 0.00370 to 0.00370, saving model to poids_train.hdf5\n",
            "Epoch 9018/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0775 - val_mse: 0.0775\n",
            "\n",
            "Epoch 09018: loss did not improve from 0.00370\n",
            "Epoch 9019/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0776 - val_mse: 0.0776\n",
            "\n",
            "Epoch 09019: loss improved from 0.00370 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9020/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0776 - val_mse: 0.0776\n",
            "\n",
            "Epoch 09020: loss did not improve from 0.00369\n",
            "Epoch 9021/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0777 - val_mse: 0.0777\n",
            "\n",
            "Epoch 09021: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9022/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0776 - val_mse: 0.0776\n",
            "\n",
            "Epoch 09022: loss did not improve from 0.00369\n",
            "Epoch 9023/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0777 - val_mse: 0.0777\n",
            "\n",
            "Epoch 09023: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9024/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0776 - val_mse: 0.0776\n",
            "\n",
            "Epoch 09024: loss did not improve from 0.00369\n",
            "Epoch 9025/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0777 - val_mse: 0.0777\n",
            "\n",
            "Epoch 09025: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9026/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0776 - val_mse: 0.0776\n",
            "\n",
            "Epoch 09026: loss did not improve from 0.00369\n",
            "Epoch 9027/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0777 - val_mse: 0.0777\n",
            "\n",
            "Epoch 09027: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9028/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0776 - val_mse: 0.0776\n",
            "\n",
            "Epoch 09028: loss did not improve from 0.00369\n",
            "Epoch 9029/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0777 - val_mse: 0.0777\n",
            "\n",
            "Epoch 09029: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9030/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0776 - val_mse: 0.0776\n",
            "\n",
            "Epoch 09030: loss did not improve from 0.00369\n",
            "Epoch 9031/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0777 - val_mse: 0.0777\n",
            "\n",
            "Epoch 09031: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9032/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0776 - val_mse: 0.0776\n",
            "\n",
            "Epoch 09032: loss did not improve from 0.00369\n",
            "Epoch 9033/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0777 - val_mse: 0.0777\n",
            "\n",
            "Epoch 09033: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9034/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0776 - val_mse: 0.0776\n",
            "\n",
            "Epoch 09034: loss did not improve from 0.00369\n",
            "Epoch 9035/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0777 - val_mse: 0.0777\n",
            "\n",
            "Epoch 09035: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9036/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0776 - val_mse: 0.0776\n",
            "\n",
            "Epoch 09036: loss did not improve from 0.00369\n",
            "Epoch 9037/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0777 - val_mse: 0.0777\n",
            "\n",
            "Epoch 09037: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9038/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0777 - val_mse: 0.0777\n",
            "\n",
            "Epoch 09038: loss did not improve from 0.00369\n",
            "Epoch 9039/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0778 - val_mse: 0.0778\n",
            "\n",
            "Epoch 09039: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9040/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0777 - val_mse: 0.0777\n",
            "\n",
            "Epoch 09040: loss did not improve from 0.00369\n",
            "Epoch 9041/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0778 - val_mse: 0.0778\n",
            "\n",
            "Epoch 09041: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9042/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0777 - val_mse: 0.0777\n",
            "\n",
            "Epoch 09042: loss did not improve from 0.00369\n",
            "Epoch 9043/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0778 - val_mse: 0.0778\n",
            "\n",
            "Epoch 09043: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9044/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0777 - val_mse: 0.0777\n",
            "\n",
            "Epoch 09044: loss did not improve from 0.00369\n",
            "Epoch 9045/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0778 - val_mse: 0.0778\n",
            "\n",
            "Epoch 09045: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9046/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0777 - val_mse: 0.0777\n",
            "\n",
            "Epoch 09046: loss did not improve from 0.00369\n",
            "Epoch 9047/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0778 - val_mse: 0.0778\n",
            "\n",
            "Epoch 09047: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9048/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0777 - val_mse: 0.0777\n",
            "\n",
            "Epoch 09048: loss did not improve from 0.00369\n",
            "Epoch 9049/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0778 - val_mse: 0.0778\n",
            "\n",
            "Epoch 09049: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9050/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0777 - val_mse: 0.0777\n",
            "\n",
            "Epoch 09050: loss did not improve from 0.00369\n",
            "Epoch 9051/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0778 - val_mse: 0.0778\n",
            "\n",
            "Epoch 09051: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9052/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0777 - val_mse: 0.0777\n",
            "\n",
            "Epoch 09052: loss did not improve from 0.00369\n",
            "Epoch 9053/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0778 - val_mse: 0.0778\n",
            "\n",
            "Epoch 09053: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9054/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0778 - val_mse: 0.0778\n",
            "\n",
            "Epoch 09054: loss did not improve from 0.00369\n",
            "Epoch 9055/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0779 - val_mse: 0.0779\n",
            "\n",
            "Epoch 09055: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9056/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0778 - val_mse: 0.0778\n",
            "\n",
            "Epoch 09056: loss did not improve from 0.00369\n",
            "Epoch 9057/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0779 - val_mse: 0.0779\n",
            "\n",
            "Epoch 09057: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9058/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0778 - val_mse: 0.0778\n",
            "\n",
            "Epoch 09058: loss did not improve from 0.00369\n",
            "Epoch 9059/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0779 - val_mse: 0.0779\n",
            "\n",
            "Epoch 09059: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9060/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0778 - val_mse: 0.0778\n",
            "\n",
            "Epoch 09060: loss did not improve from 0.00369\n",
            "Epoch 9061/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0779 - val_mse: 0.0779\n",
            "\n",
            "Epoch 09061: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9062/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0778 - val_mse: 0.0778\n",
            "\n",
            "Epoch 09062: loss did not improve from 0.00369\n",
            "Epoch 9063/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0779 - val_mse: 0.0779\n",
            "\n",
            "Epoch 09063: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9064/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0778 - val_mse: 0.0778\n",
            "\n",
            "Epoch 09064: loss did not improve from 0.00369\n",
            "Epoch 9065/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0779 - val_mse: 0.0779\n",
            "\n",
            "Epoch 09065: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9066/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0778 - val_mse: 0.0778\n",
            "\n",
            "Epoch 09066: loss did not improve from 0.00369\n",
            "Epoch 9067/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0779 - val_mse: 0.0779\n",
            "\n",
            "Epoch 09067: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9068/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0778 - val_mse: 0.0778\n",
            "\n",
            "Epoch 09068: loss did not improve from 0.00369\n",
            "Epoch 9069/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0779 - val_mse: 0.0779\n",
            "\n",
            "Epoch 09069: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9070/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0778 - val_mse: 0.0778\n",
            "\n",
            "Epoch 09070: loss did not improve from 0.00369\n",
            "Epoch 9071/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0779 - val_mse: 0.0779\n",
            "\n",
            "Epoch 09071: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9072/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0779 - val_mse: 0.0779\n",
            "\n",
            "Epoch 09072: loss did not improve from 0.00369\n",
            "Epoch 9073/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0780 - val_mse: 0.0780\n",
            "\n",
            "Epoch 09073: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9074/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0779 - val_mse: 0.0779\n",
            "\n",
            "Epoch 09074: loss did not improve from 0.00369\n",
            "Epoch 9075/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0780 - val_mse: 0.0780\n",
            "\n",
            "Epoch 09075: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9076/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0779 - val_mse: 0.0779\n",
            "\n",
            "Epoch 09076: loss did not improve from 0.00369\n",
            "Epoch 9077/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0780 - val_mse: 0.0780\n",
            "\n",
            "Epoch 09077: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9078/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0779 - val_mse: 0.0779\n",
            "\n",
            "Epoch 09078: loss did not improve from 0.00369\n",
            "Epoch 9079/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0780 - val_mse: 0.0780\n",
            "\n",
            "Epoch 09079: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9080/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0779 - val_mse: 0.0779\n",
            "\n",
            "Epoch 09080: loss did not improve from 0.00369\n",
            "Epoch 9081/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0780 - val_mse: 0.0780\n",
            "\n",
            "Epoch 09081: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9082/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0779 - val_mse: 0.0779\n",
            "\n",
            "Epoch 09082: loss did not improve from 0.00369\n",
            "Epoch 9083/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0780 - val_mse: 0.0780\n",
            "\n",
            "Epoch 09083: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9084/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0779 - val_mse: 0.0779\n",
            "\n",
            "Epoch 09084: loss did not improve from 0.00369\n",
            "Epoch 9085/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0780 - val_mse: 0.0780\n",
            "\n",
            "Epoch 09085: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9086/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0779 - val_mse: 0.0779\n",
            "\n",
            "Epoch 09086: loss did not improve from 0.00369\n",
            "Epoch 9087/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0780 - val_mse: 0.0780\n",
            "\n",
            "Epoch 09087: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9088/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0780 - val_mse: 0.0780\n",
            "\n",
            "Epoch 09088: loss did not improve from 0.00369\n",
            "Epoch 9089/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0780 - val_mse: 0.0780\n",
            "\n",
            "Epoch 09089: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9090/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0780 - val_mse: 0.0780\n",
            "\n",
            "Epoch 09090: loss did not improve from 0.00369\n",
            "Epoch 9091/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0781 - val_mse: 0.0781\n",
            "\n",
            "Epoch 09091: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9092/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0780 - val_mse: 0.0780\n",
            "\n",
            "Epoch 09092: loss did not improve from 0.00369\n",
            "Epoch 9093/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0781 - val_mse: 0.0781\n",
            "\n",
            "Epoch 09093: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9094/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0780 - val_mse: 0.0780\n",
            "\n",
            "Epoch 09094: loss did not improve from 0.00369\n",
            "Epoch 9095/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0781 - val_mse: 0.0781\n",
            "\n",
            "Epoch 09095: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9096/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0780 - val_mse: 0.0780\n",
            "\n",
            "Epoch 09096: loss did not improve from 0.00369\n",
            "Epoch 9097/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0781 - val_mse: 0.0781\n",
            "\n",
            "Epoch 09097: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9098/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0780 - val_mse: 0.0780\n",
            "\n",
            "Epoch 09098: loss did not improve from 0.00369\n",
            "Epoch 9099/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0781 - val_mse: 0.0781\n",
            "\n",
            "Epoch 09099: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9100/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0780 - val_mse: 0.0780\n",
            "\n",
            "Epoch 09100: loss did not improve from 0.00369\n",
            "Epoch 9101/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0781 - val_mse: 0.0781\n",
            "\n",
            "Epoch 09101: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9102/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0780 - val_mse: 0.0780\n",
            "\n",
            "Epoch 09102: loss did not improve from 0.00369\n",
            "Epoch 9103/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0781 - val_mse: 0.0781\n",
            "\n",
            "Epoch 09103: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9104/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0780 - val_mse: 0.0780\n",
            "\n",
            "Epoch 09104: loss did not improve from 0.00369\n",
            "Epoch 9105/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0781 - val_mse: 0.0781\n",
            "\n",
            "Epoch 09105: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9106/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0781 - val_mse: 0.0781\n",
            "\n",
            "Epoch 09106: loss did not improve from 0.00369\n",
            "Epoch 9107/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0782 - val_mse: 0.0782\n",
            "\n",
            "Epoch 09107: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9108/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0781 - val_mse: 0.0781\n",
            "\n",
            "Epoch 09108: loss did not improve from 0.00369\n",
            "Epoch 9109/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0782 - val_mse: 0.0782\n",
            "\n",
            "Epoch 09109: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9110/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0781 - val_mse: 0.0781\n",
            "\n",
            "Epoch 09110: loss did not improve from 0.00369\n",
            "Epoch 9111/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0782 - val_mse: 0.0782\n",
            "\n",
            "Epoch 09111: loss improved from 0.00369 to 0.00369, saving model to poids_train.hdf5\n",
            "Epoch 9112/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0781 - val_mse: 0.0781\n",
            "\n",
            "Epoch 09112: loss did not improve from 0.00369\n",
            "Epoch 9113/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0782 - val_mse: 0.0782\n",
            "\n",
            "Epoch 09113: loss improved from 0.00369 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9114/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0781 - val_mse: 0.0781\n",
            "\n",
            "Epoch 09114: loss did not improve from 0.00368\n",
            "Epoch 9115/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0782 - val_mse: 0.0782\n",
            "\n",
            "Epoch 09115: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9116/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0781 - val_mse: 0.0781\n",
            "\n",
            "Epoch 09116: loss did not improve from 0.00368\n",
            "Epoch 9117/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0782 - val_mse: 0.0782\n",
            "\n",
            "Epoch 09117: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9118/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0781 - val_mse: 0.0781\n",
            "\n",
            "Epoch 09118: loss did not improve from 0.00368\n",
            "Epoch 9119/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0782 - val_mse: 0.0782\n",
            "\n",
            "Epoch 09119: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9120/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0781 - val_mse: 0.0781\n",
            "\n",
            "Epoch 09120: loss did not improve from 0.00368\n",
            "Epoch 9121/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0782 - val_mse: 0.0782\n",
            "\n",
            "Epoch 09121: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9122/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0781 - val_mse: 0.0781\n",
            "\n",
            "Epoch 09122: loss did not improve from 0.00368\n",
            "Epoch 9123/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0782 - val_mse: 0.0782\n",
            "\n",
            "Epoch 09123: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9124/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0782 - val_mse: 0.0782\n",
            "\n",
            "Epoch 09124: loss did not improve from 0.00368\n",
            "Epoch 9125/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0783 - val_mse: 0.0783\n",
            "\n",
            "Epoch 09125: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9126/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0782 - val_mse: 0.0782\n",
            "\n",
            "Epoch 09126: loss did not improve from 0.00368\n",
            "Epoch 9127/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0783 - val_mse: 0.0783\n",
            "\n",
            "Epoch 09127: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9128/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0782 - val_mse: 0.0782\n",
            "\n",
            "Epoch 09128: loss did not improve from 0.00368\n",
            "Epoch 9129/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0783 - val_mse: 0.0783\n",
            "\n",
            "Epoch 09129: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9130/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0782 - val_mse: 0.0782\n",
            "\n",
            "Epoch 09130: loss did not improve from 0.00368\n",
            "Epoch 9131/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0783 - val_mse: 0.0783\n",
            "\n",
            "Epoch 09131: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9132/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0782 - val_mse: 0.0782\n",
            "\n",
            "Epoch 09132: loss did not improve from 0.00368\n",
            "Epoch 9133/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0783 - val_mse: 0.0783\n",
            "\n",
            "Epoch 09133: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9134/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0782 - val_mse: 0.0782\n",
            "\n",
            "Epoch 09134: loss did not improve from 0.00368\n",
            "Epoch 9135/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0783 - val_mse: 0.0783\n",
            "\n",
            "Epoch 09135: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9136/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0782 - val_mse: 0.0782\n",
            "\n",
            "Epoch 09136: loss did not improve from 0.00368\n",
            "Epoch 9137/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0783 - val_mse: 0.0783\n",
            "\n",
            "Epoch 09137: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9138/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0782 - val_mse: 0.0782\n",
            "\n",
            "Epoch 09138: loss did not improve from 0.00368\n",
            "Epoch 9139/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0783 - val_mse: 0.0783\n",
            "\n",
            "Epoch 09139: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9140/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0783 - val_mse: 0.0783\n",
            "\n",
            "Epoch 09140: loss did not improve from 0.00368\n",
            "Epoch 9141/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0783 - val_mse: 0.0783\n",
            "\n",
            "Epoch 09141: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9142/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0783 - val_mse: 0.0783\n",
            "\n",
            "Epoch 09142: loss did not improve from 0.00368\n",
            "Epoch 9143/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0784 - val_mse: 0.0784\n",
            "\n",
            "Epoch 09143: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9144/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0783 - val_mse: 0.0783\n",
            "\n",
            "Epoch 09144: loss did not improve from 0.00368\n",
            "Epoch 9145/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0784 - val_mse: 0.0784\n",
            "\n",
            "Epoch 09145: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9146/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0783 - val_mse: 0.0783\n",
            "\n",
            "Epoch 09146: loss did not improve from 0.00368\n",
            "Epoch 9147/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0784 - val_mse: 0.0784\n",
            "\n",
            "Epoch 09147: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9148/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0783 - val_mse: 0.0783\n",
            "\n",
            "Epoch 09148: loss did not improve from 0.00368\n",
            "Epoch 9149/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0784 - val_mse: 0.0784\n",
            "\n",
            "Epoch 09149: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9150/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0783 - val_mse: 0.0783\n",
            "\n",
            "Epoch 09150: loss did not improve from 0.00368\n",
            "Epoch 9151/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0784 - val_mse: 0.0784\n",
            "\n",
            "Epoch 09151: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9152/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0783 - val_mse: 0.0783\n",
            "\n",
            "Epoch 09152: loss did not improve from 0.00368\n",
            "Epoch 9153/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0784 - val_mse: 0.0784\n",
            "\n",
            "Epoch 09153: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9154/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0783 - val_mse: 0.0783\n",
            "\n",
            "Epoch 09154: loss did not improve from 0.00368\n",
            "Epoch 9155/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0784 - val_mse: 0.0784\n",
            "\n",
            "Epoch 09155: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9156/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0783 - val_mse: 0.0783\n",
            "\n",
            "Epoch 09156: loss did not improve from 0.00368\n",
            "Epoch 9157/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0784 - val_mse: 0.0784\n",
            "\n",
            "Epoch 09157: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9158/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0784 - val_mse: 0.0784\n",
            "\n",
            "Epoch 09158: loss did not improve from 0.00368\n",
            "Epoch 9159/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0784 - val_mse: 0.0784\n",
            "\n",
            "Epoch 09159: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9160/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0784 - val_mse: 0.0784\n",
            "\n",
            "Epoch 09160: loss did not improve from 0.00368\n",
            "Epoch 9161/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0785 - val_mse: 0.0785\n",
            "\n",
            "Epoch 09161: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9162/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0784 - val_mse: 0.0784\n",
            "\n",
            "Epoch 09162: loss did not improve from 0.00368\n",
            "Epoch 9163/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0785 - val_mse: 0.0785\n",
            "\n",
            "Epoch 09163: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9164/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0784 - val_mse: 0.0784\n",
            "\n",
            "Epoch 09164: loss did not improve from 0.00368\n",
            "Epoch 9165/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0785 - val_mse: 0.0785\n",
            "\n",
            "Epoch 09165: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9166/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0784 - val_mse: 0.0784\n",
            "\n",
            "Epoch 09166: loss did not improve from 0.00368\n",
            "Epoch 9167/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0785 - val_mse: 0.0785\n",
            "\n",
            "Epoch 09167: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9168/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0784 - val_mse: 0.0784\n",
            "\n",
            "Epoch 09168: loss did not improve from 0.00368\n",
            "Epoch 9169/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0785 - val_mse: 0.0785\n",
            "\n",
            "Epoch 09169: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9170/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0784 - val_mse: 0.0784\n",
            "\n",
            "Epoch 09170: loss did not improve from 0.00368\n",
            "Epoch 9171/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0785 - val_mse: 0.0785\n",
            "\n",
            "Epoch 09171: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9172/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0784 - val_mse: 0.0784\n",
            "\n",
            "Epoch 09172: loss did not improve from 0.00368\n",
            "Epoch 9173/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0785 - val_mse: 0.0785\n",
            "\n",
            "Epoch 09173: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9174/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0784 - val_mse: 0.0784\n",
            "\n",
            "Epoch 09174: loss did not improve from 0.00368\n",
            "Epoch 9175/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0785 - val_mse: 0.0785\n",
            "\n",
            "Epoch 09175: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9176/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0785 - val_mse: 0.0785\n",
            "\n",
            "Epoch 09176: loss did not improve from 0.00368\n",
            "Epoch 9177/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0786 - val_mse: 0.0786\n",
            "\n",
            "Epoch 09177: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9178/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0785 - val_mse: 0.0785\n",
            "\n",
            "Epoch 09178: loss did not improve from 0.00368\n",
            "Epoch 9179/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0786 - val_mse: 0.0786\n",
            "\n",
            "Epoch 09179: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9180/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0785 - val_mse: 0.0785\n",
            "\n",
            "Epoch 09180: loss did not improve from 0.00368\n",
            "Epoch 9181/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0786 - val_mse: 0.0786\n",
            "\n",
            "Epoch 09181: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9182/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0785 - val_mse: 0.0785\n",
            "\n",
            "Epoch 09182: loss did not improve from 0.00368\n",
            "Epoch 9183/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0786 - val_mse: 0.0786\n",
            "\n",
            "Epoch 09183: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9184/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0785 - val_mse: 0.0785\n",
            "\n",
            "Epoch 09184: loss did not improve from 0.00368\n",
            "Epoch 9185/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0786 - val_mse: 0.0786\n",
            "\n",
            "Epoch 09185: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9186/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0785 - val_mse: 0.0785\n",
            "\n",
            "Epoch 09186: loss did not improve from 0.00368\n",
            "Epoch 9187/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0786 - val_mse: 0.0786\n",
            "\n",
            "Epoch 09187: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9188/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0785 - val_mse: 0.0785\n",
            "\n",
            "Epoch 09188: loss did not improve from 0.00368\n",
            "Epoch 9189/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0786 - val_mse: 0.0786\n",
            "\n",
            "Epoch 09189: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9190/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0785 - val_mse: 0.0785\n",
            "\n",
            "Epoch 09190: loss did not improve from 0.00368\n",
            "Epoch 9191/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0786 - val_mse: 0.0786\n",
            "\n",
            "Epoch 09191: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9192/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0785 - val_mse: 0.0785\n",
            "\n",
            "Epoch 09192: loss did not improve from 0.00368\n",
            "Epoch 9193/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0786 - val_mse: 0.0786\n",
            "\n",
            "Epoch 09193: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9194/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0786 - val_mse: 0.0786\n",
            "\n",
            "Epoch 09194: loss did not improve from 0.00368\n",
            "Epoch 9195/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0787 - val_mse: 0.0787\n",
            "\n",
            "Epoch 09195: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9196/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0786 - val_mse: 0.0786\n",
            "\n",
            "Epoch 09196: loss did not improve from 0.00368\n",
            "Epoch 9197/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0787 - val_mse: 0.0787\n",
            "\n",
            "Epoch 09197: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9198/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0786 - val_mse: 0.0786\n",
            "\n",
            "Epoch 09198: loss did not improve from 0.00368\n",
            "Epoch 9199/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0787 - val_mse: 0.0787\n",
            "\n",
            "Epoch 09199: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9200/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0786 - val_mse: 0.0786\n",
            "\n",
            "Epoch 09200: loss did not improve from 0.00368\n",
            "Epoch 9201/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0787 - val_mse: 0.0787\n",
            "\n",
            "Epoch 09201: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9202/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0786 - val_mse: 0.0786\n",
            "\n",
            "Epoch 09202: loss did not improve from 0.00368\n",
            "Epoch 9203/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0787 - val_mse: 0.0787\n",
            "\n",
            "Epoch 09203: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9204/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0786 - val_mse: 0.0786\n",
            "\n",
            "Epoch 09204: loss did not improve from 0.00368\n",
            "Epoch 9205/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0787 - val_mse: 0.0787\n",
            "\n",
            "Epoch 09205: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9206/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0786 - val_mse: 0.0786\n",
            "\n",
            "Epoch 09206: loss did not improve from 0.00368\n",
            "Epoch 9207/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0787 - val_mse: 0.0787\n",
            "\n",
            "Epoch 09207: loss improved from 0.00368 to 0.00368, saving model to poids_train.hdf5\n",
            "Epoch 9208/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0786 - val_mse: 0.0786\n",
            "\n",
            "Epoch 09208: loss did not improve from 0.00368\n",
            "Epoch 9209/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0787 - val_mse: 0.0787\n",
            "\n",
            "Epoch 09209: loss improved from 0.00368 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9210/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0787 - val_mse: 0.0787\n",
            "\n",
            "Epoch 09210: loss did not improve from 0.00367\n",
            "Epoch 9211/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0787 - val_mse: 0.0787\n",
            "\n",
            "Epoch 09211: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9212/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0787 - val_mse: 0.0787\n",
            "\n",
            "Epoch 09212: loss did not improve from 0.00367\n",
            "Epoch 9213/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0788 - val_mse: 0.0788\n",
            "\n",
            "Epoch 09213: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9214/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0787 - val_mse: 0.0787\n",
            "\n",
            "Epoch 09214: loss did not improve from 0.00367\n",
            "Epoch 9215/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0788 - val_mse: 0.0788\n",
            "\n",
            "Epoch 09215: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9216/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0787 - val_mse: 0.0787\n",
            "\n",
            "Epoch 09216: loss did not improve from 0.00367\n",
            "Epoch 9217/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0788 - val_mse: 0.0788\n",
            "\n",
            "Epoch 09217: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9218/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0787 - val_mse: 0.0787\n",
            "\n",
            "Epoch 09218: loss did not improve from 0.00367\n",
            "Epoch 9219/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0788 - val_mse: 0.0788\n",
            "\n",
            "Epoch 09219: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9220/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0787 - val_mse: 0.0787\n",
            "\n",
            "Epoch 09220: loss did not improve from 0.00367\n",
            "Epoch 9221/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0788 - val_mse: 0.0788\n",
            "\n",
            "Epoch 09221: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9222/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0787 - val_mse: 0.0787\n",
            "\n",
            "Epoch 09222: loss did not improve from 0.00367\n",
            "Epoch 9223/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0788 - val_mse: 0.0788\n",
            "\n",
            "Epoch 09223: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9224/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0787 - val_mse: 0.0787\n",
            "\n",
            "Epoch 09224: loss did not improve from 0.00367\n",
            "Epoch 9225/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0788 - val_mse: 0.0788\n",
            "\n",
            "Epoch 09225: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9226/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0787 - val_mse: 0.0787\n",
            "\n",
            "Epoch 09226: loss did not improve from 0.00367\n",
            "Epoch 9227/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0788 - val_mse: 0.0788\n",
            "\n",
            "Epoch 09227: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9228/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0788 - val_mse: 0.0788\n",
            "\n",
            "Epoch 09228: loss did not improve from 0.00367\n",
            "Epoch 9229/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0788 - val_mse: 0.0788\n",
            "\n",
            "Epoch 09229: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9230/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0788 - val_mse: 0.0788\n",
            "\n",
            "Epoch 09230: loss did not improve from 0.00367\n",
            "Epoch 9231/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0789 - val_mse: 0.0789\n",
            "\n",
            "Epoch 09231: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9232/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0788 - val_mse: 0.0788\n",
            "\n",
            "Epoch 09232: loss did not improve from 0.00367\n",
            "Epoch 9233/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0789 - val_mse: 0.0789\n",
            "\n",
            "Epoch 09233: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9234/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0788 - val_mse: 0.0788\n",
            "\n",
            "Epoch 09234: loss did not improve from 0.00367\n",
            "Epoch 9235/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0789 - val_mse: 0.0789\n",
            "\n",
            "Epoch 09235: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9236/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0788 - val_mse: 0.0788\n",
            "\n",
            "Epoch 09236: loss did not improve from 0.00367\n",
            "Epoch 9237/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0789 - val_mse: 0.0789\n",
            "\n",
            "Epoch 09237: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9238/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0788 - val_mse: 0.0788\n",
            "\n",
            "Epoch 09238: loss did not improve from 0.00367\n",
            "Epoch 9239/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0789 - val_mse: 0.0789\n",
            "\n",
            "Epoch 09239: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9240/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0788 - val_mse: 0.0788\n",
            "\n",
            "Epoch 09240: loss did not improve from 0.00367\n",
            "Epoch 9241/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0789 - val_mse: 0.0789\n",
            "\n",
            "Epoch 09241: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9242/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0788 - val_mse: 0.0788\n",
            "\n",
            "Epoch 09242: loss did not improve from 0.00367\n",
            "Epoch 9243/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0789 - val_mse: 0.0789\n",
            "\n",
            "Epoch 09243: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9244/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0788 - val_mse: 0.0788\n",
            "\n",
            "Epoch 09244: loss did not improve from 0.00367\n",
            "Epoch 9245/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0789 - val_mse: 0.0789\n",
            "\n",
            "Epoch 09245: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9246/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0789 - val_mse: 0.0789\n",
            "\n",
            "Epoch 09246: loss did not improve from 0.00367\n",
            "Epoch 9247/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0789 - val_mse: 0.0789\n",
            "\n",
            "Epoch 09247: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9248/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0789 - val_mse: 0.0789\n",
            "\n",
            "Epoch 09248: loss did not improve from 0.00367\n",
            "Epoch 9249/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0790 - val_mse: 0.0790\n",
            "\n",
            "Epoch 09249: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9250/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0789 - val_mse: 0.0789\n",
            "\n",
            "Epoch 09250: loss did not improve from 0.00367\n",
            "Epoch 9251/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0790 - val_mse: 0.0790\n",
            "\n",
            "Epoch 09251: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9252/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0789 - val_mse: 0.0789\n",
            "\n",
            "Epoch 09252: loss did not improve from 0.00367\n",
            "Epoch 9253/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0790 - val_mse: 0.0790\n",
            "\n",
            "Epoch 09253: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9254/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0789 - val_mse: 0.0789\n",
            "\n",
            "Epoch 09254: loss did not improve from 0.00367\n",
            "Epoch 9255/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0790 - val_mse: 0.0790\n",
            "\n",
            "Epoch 09255: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9256/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0789 - val_mse: 0.0789\n",
            "\n",
            "Epoch 09256: loss did not improve from 0.00367\n",
            "Epoch 9257/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0790 - val_mse: 0.0790\n",
            "\n",
            "Epoch 09257: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9258/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0789 - val_mse: 0.0789\n",
            "\n",
            "Epoch 09258: loss did not improve from 0.00367\n",
            "Epoch 9259/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0790 - val_mse: 0.0790\n",
            "\n",
            "Epoch 09259: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9260/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0789 - val_mse: 0.0789\n",
            "\n",
            "Epoch 09260: loss did not improve from 0.00367\n",
            "Epoch 9261/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0790 - val_mse: 0.0790\n",
            "\n",
            "Epoch 09261: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9262/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0789 - val_mse: 0.0789\n",
            "\n",
            "Epoch 09262: loss did not improve from 0.00367\n",
            "Epoch 9263/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0790 - val_mse: 0.0790\n",
            "\n",
            "Epoch 09263: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9264/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0790 - val_mse: 0.0790\n",
            "\n",
            "Epoch 09264: loss did not improve from 0.00367\n",
            "Epoch 9265/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0790 - val_mse: 0.0790\n",
            "\n",
            "Epoch 09265: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9266/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0790 - val_mse: 0.0790\n",
            "\n",
            "Epoch 09266: loss did not improve from 0.00367\n",
            "Epoch 9267/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0791 - val_mse: 0.0791\n",
            "\n",
            "Epoch 09267: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9268/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0790 - val_mse: 0.0790\n",
            "\n",
            "Epoch 09268: loss did not improve from 0.00367\n",
            "Epoch 9269/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0791 - val_mse: 0.0791\n",
            "\n",
            "Epoch 09269: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9270/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0790 - val_mse: 0.0790\n",
            "\n",
            "Epoch 09270: loss did not improve from 0.00367\n",
            "Epoch 9271/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0791 - val_mse: 0.0791\n",
            "\n",
            "Epoch 09271: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9272/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0790 - val_mse: 0.0790\n",
            "\n",
            "Epoch 09272: loss did not improve from 0.00367\n",
            "Epoch 9273/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0791 - val_mse: 0.0791\n",
            "\n",
            "Epoch 09273: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9274/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0790 - val_mse: 0.0790\n",
            "\n",
            "Epoch 09274: loss did not improve from 0.00367\n",
            "Epoch 9275/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0791 - val_mse: 0.0791\n",
            "\n",
            "Epoch 09275: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9276/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0790 - val_mse: 0.0790\n",
            "\n",
            "Epoch 09276: loss did not improve from 0.00367\n",
            "Epoch 9277/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0791 - val_mse: 0.0791\n",
            "\n",
            "Epoch 09277: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9278/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0790 - val_mse: 0.0790\n",
            "\n",
            "Epoch 09278: loss did not improve from 0.00367\n",
            "Epoch 9279/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0791 - val_mse: 0.0791\n",
            "\n",
            "Epoch 09279: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9280/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0790 - val_mse: 0.0790\n",
            "\n",
            "Epoch 09280: loss did not improve from 0.00367\n",
            "Epoch 9281/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0791 - val_mse: 0.0791\n",
            "\n",
            "Epoch 09281: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9282/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0791 - val_mse: 0.0791\n",
            "\n",
            "Epoch 09282: loss did not improve from 0.00367\n",
            "Epoch 9283/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0792 - val_mse: 0.0792\n",
            "\n",
            "Epoch 09283: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9284/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0791 - val_mse: 0.0791\n",
            "\n",
            "Epoch 09284: loss did not improve from 0.00367\n",
            "Epoch 9285/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0792 - val_mse: 0.0792\n",
            "\n",
            "Epoch 09285: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9286/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0791 - val_mse: 0.0791\n",
            "\n",
            "Epoch 09286: loss did not improve from 0.00367\n",
            "Epoch 9287/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0792 - val_mse: 0.0792\n",
            "\n",
            "Epoch 09287: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9288/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0791 - val_mse: 0.0791\n",
            "\n",
            "Epoch 09288: loss did not improve from 0.00367\n",
            "Epoch 9289/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0792 - val_mse: 0.0792\n",
            "\n",
            "Epoch 09289: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9290/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0791 - val_mse: 0.0791\n",
            "\n",
            "Epoch 09290: loss did not improve from 0.00367\n",
            "Epoch 9291/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0792 - val_mse: 0.0792\n",
            "\n",
            "Epoch 09291: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9292/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0791 - val_mse: 0.0791\n",
            "\n",
            "Epoch 09292: loss did not improve from 0.00367\n",
            "Epoch 9293/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0792 - val_mse: 0.0792\n",
            "\n",
            "Epoch 09293: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9294/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0791 - val_mse: 0.0791\n",
            "\n",
            "Epoch 09294: loss did not improve from 0.00367\n",
            "Epoch 9295/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0792 - val_mse: 0.0792\n",
            "\n",
            "Epoch 09295: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9296/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0791 - val_mse: 0.0791\n",
            "\n",
            "Epoch 09296: loss did not improve from 0.00367\n",
            "Epoch 9297/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0792 - val_mse: 0.0792\n",
            "\n",
            "Epoch 09297: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9298/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0791 - val_mse: 0.0791\n",
            "\n",
            "Epoch 09298: loss did not improve from 0.00367\n",
            "Epoch 9299/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0792 - val_mse: 0.0792\n",
            "\n",
            "Epoch 09299: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9300/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0792 - val_mse: 0.0792\n",
            "\n",
            "Epoch 09300: loss did not improve from 0.00367\n",
            "Epoch 9301/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0793 - val_mse: 0.0793\n",
            "\n",
            "Epoch 09301: loss improved from 0.00367 to 0.00367, saving model to poids_train.hdf5\n",
            "Epoch 9302/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0792 - val_mse: 0.0792\n",
            "\n",
            "Epoch 09302: loss did not improve from 0.00367\n",
            "Epoch 9303/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0793 - val_mse: 0.0793\n",
            "\n",
            "Epoch 09303: loss improved from 0.00367 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9304/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0792 - val_mse: 0.0792\n",
            "\n",
            "Epoch 09304: loss did not improve from 0.00366\n",
            "Epoch 9305/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0793 - val_mse: 0.0793\n",
            "\n",
            "Epoch 09305: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9306/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0792 - val_mse: 0.0792\n",
            "\n",
            "Epoch 09306: loss did not improve from 0.00366\n",
            "Epoch 9307/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0793 - val_mse: 0.0793\n",
            "\n",
            "Epoch 09307: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9308/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0792 - val_mse: 0.0792\n",
            "\n",
            "Epoch 09308: loss did not improve from 0.00366\n",
            "Epoch 9309/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0793 - val_mse: 0.0793\n",
            "\n",
            "Epoch 09309: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9310/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0792 - val_mse: 0.0792\n",
            "\n",
            "Epoch 09310: loss did not improve from 0.00366\n",
            "Epoch 9311/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0793 - val_mse: 0.0793\n",
            "\n",
            "Epoch 09311: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9312/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0792 - val_mse: 0.0792\n",
            "\n",
            "Epoch 09312: loss did not improve from 0.00366\n",
            "Epoch 9313/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0793 - val_mse: 0.0793\n",
            "\n",
            "Epoch 09313: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9314/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0792 - val_mse: 0.0792\n",
            "\n",
            "Epoch 09314: loss did not improve from 0.00366\n",
            "Epoch 9315/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0793 - val_mse: 0.0793\n",
            "\n",
            "Epoch 09315: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9316/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0792 - val_mse: 0.0792\n",
            "\n",
            "Epoch 09316: loss did not improve from 0.00366\n",
            "Epoch 9317/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0793 - val_mse: 0.0793\n",
            "\n",
            "Epoch 09317: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9318/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0793 - val_mse: 0.0793\n",
            "\n",
            "Epoch 09318: loss did not improve from 0.00366\n",
            "Epoch 9319/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0794 - val_mse: 0.0794\n",
            "\n",
            "Epoch 09319: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9320/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0793 - val_mse: 0.0793\n",
            "\n",
            "Epoch 09320: loss did not improve from 0.00366\n",
            "Epoch 9321/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0794 - val_mse: 0.0794\n",
            "\n",
            "Epoch 09321: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9322/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0793 - val_mse: 0.0793\n",
            "\n",
            "Epoch 09322: loss did not improve from 0.00366\n",
            "Epoch 9323/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0794 - val_mse: 0.0794\n",
            "\n",
            "Epoch 09323: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9324/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0793 - val_mse: 0.0793\n",
            "\n",
            "Epoch 09324: loss did not improve from 0.00366\n",
            "Epoch 9325/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0794 - val_mse: 0.0794\n",
            "\n",
            "Epoch 09325: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9326/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0793 - val_mse: 0.0793\n",
            "\n",
            "Epoch 09326: loss did not improve from 0.00366\n",
            "Epoch 9327/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0794 - val_mse: 0.0794\n",
            "\n",
            "Epoch 09327: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9328/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0793 - val_mse: 0.0793\n",
            "\n",
            "Epoch 09328: loss did not improve from 0.00366\n",
            "Epoch 9329/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0794 - val_mse: 0.0794\n",
            "\n",
            "Epoch 09329: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9330/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0793 - val_mse: 0.0793\n",
            "\n",
            "Epoch 09330: loss did not improve from 0.00366\n",
            "Epoch 9331/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0794 - val_mse: 0.0794\n",
            "\n",
            "Epoch 09331: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9332/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0793 - val_mse: 0.0793\n",
            "\n",
            "Epoch 09332: loss did not improve from 0.00366\n",
            "Epoch 9333/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0794 - val_mse: 0.0794\n",
            "\n",
            "Epoch 09333: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9334/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0793 - val_mse: 0.0793\n",
            "\n",
            "Epoch 09334: loss did not improve from 0.00366\n",
            "Epoch 9335/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0794 - val_mse: 0.0794\n",
            "\n",
            "Epoch 09335: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9336/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0794 - val_mse: 0.0794\n",
            "\n",
            "Epoch 09336: loss did not improve from 0.00366\n",
            "Epoch 9337/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0795 - val_mse: 0.0795\n",
            "\n",
            "Epoch 09337: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9338/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0794 - val_mse: 0.0794\n",
            "\n",
            "Epoch 09338: loss did not improve from 0.00366\n",
            "Epoch 9339/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0795 - val_mse: 0.0795\n",
            "\n",
            "Epoch 09339: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9340/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0794 - val_mse: 0.0794\n",
            "\n",
            "Epoch 09340: loss did not improve from 0.00366\n",
            "Epoch 9341/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0795 - val_mse: 0.0795\n",
            "\n",
            "Epoch 09341: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9342/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0794 - val_mse: 0.0794\n",
            "\n",
            "Epoch 09342: loss did not improve from 0.00366\n",
            "Epoch 9343/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0795 - val_mse: 0.0795\n",
            "\n",
            "Epoch 09343: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9344/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0794 - val_mse: 0.0794\n",
            "\n",
            "Epoch 09344: loss did not improve from 0.00366\n",
            "Epoch 9345/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0795 - val_mse: 0.0795\n",
            "\n",
            "Epoch 09345: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9346/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0794 - val_mse: 0.0794\n",
            "\n",
            "Epoch 09346: loss did not improve from 0.00366\n",
            "Epoch 9347/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0795 - val_mse: 0.0795\n",
            "\n",
            "Epoch 09347: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9348/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0794 - val_mse: 0.0794\n",
            "\n",
            "Epoch 09348: loss did not improve from 0.00366\n",
            "Epoch 9349/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0795 - val_mse: 0.0795\n",
            "\n",
            "Epoch 09349: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9350/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0794 - val_mse: 0.0794\n",
            "\n",
            "Epoch 09350: loss did not improve from 0.00366\n",
            "Epoch 9351/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0795 - val_mse: 0.0795\n",
            "\n",
            "Epoch 09351: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9352/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0794 - val_mse: 0.0794\n",
            "\n",
            "Epoch 09352: loss did not improve from 0.00366\n",
            "Epoch 9353/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0795 - val_mse: 0.0795\n",
            "\n",
            "Epoch 09353: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9354/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0795 - val_mse: 0.0795\n",
            "\n",
            "Epoch 09354: loss did not improve from 0.00366\n",
            "Epoch 9355/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0796 - val_mse: 0.0796\n",
            "\n",
            "Epoch 09355: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9356/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0795 - val_mse: 0.0795\n",
            "\n",
            "Epoch 09356: loss did not improve from 0.00366\n",
            "Epoch 9357/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0796 - val_mse: 0.0796\n",
            "\n",
            "Epoch 09357: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9358/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0795 - val_mse: 0.0795\n",
            "\n",
            "Epoch 09358: loss did not improve from 0.00366\n",
            "Epoch 9359/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0796 - val_mse: 0.0796\n",
            "\n",
            "Epoch 09359: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9360/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0795 - val_mse: 0.0795\n",
            "\n",
            "Epoch 09360: loss did not improve from 0.00366\n",
            "Epoch 9361/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0796 - val_mse: 0.0796\n",
            "\n",
            "Epoch 09361: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9362/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0795 - val_mse: 0.0795\n",
            "\n",
            "Epoch 09362: loss did not improve from 0.00366\n",
            "Epoch 9363/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0796 - val_mse: 0.0796\n",
            "\n",
            "Epoch 09363: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9364/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0795 - val_mse: 0.0795\n",
            "\n",
            "Epoch 09364: loss did not improve from 0.00366\n",
            "Epoch 9365/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0796 - val_mse: 0.0796\n",
            "\n",
            "Epoch 09365: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9366/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0795 - val_mse: 0.0795\n",
            "\n",
            "Epoch 09366: loss did not improve from 0.00366\n",
            "Epoch 9367/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0796 - val_mse: 0.0796\n",
            "\n",
            "Epoch 09367: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9368/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0795 - val_mse: 0.0795\n",
            "\n",
            "Epoch 09368: loss did not improve from 0.00366\n",
            "Epoch 9369/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0796 - val_mse: 0.0796\n",
            "\n",
            "Epoch 09369: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9370/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0795 - val_mse: 0.0795\n",
            "\n",
            "Epoch 09370: loss did not improve from 0.00366\n",
            "Epoch 9371/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0796 - val_mse: 0.0796\n",
            "\n",
            "Epoch 09371: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9372/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0796 - val_mse: 0.0796\n",
            "\n",
            "Epoch 09372: loss did not improve from 0.00366\n",
            "Epoch 9373/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0797 - val_mse: 0.0797\n",
            "\n",
            "Epoch 09373: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9374/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0796 - val_mse: 0.0796\n",
            "\n",
            "Epoch 09374: loss did not improve from 0.00366\n",
            "Epoch 9375/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0797 - val_mse: 0.0797\n",
            "\n",
            "Epoch 09375: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9376/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0796 - val_mse: 0.0796\n",
            "\n",
            "Epoch 09376: loss did not improve from 0.00366\n",
            "Epoch 9377/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0797 - val_mse: 0.0797\n",
            "\n",
            "Epoch 09377: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9378/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0796 - val_mse: 0.0796\n",
            "\n",
            "Epoch 09378: loss did not improve from 0.00366\n",
            "Epoch 9379/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0797 - val_mse: 0.0797\n",
            "\n",
            "Epoch 09379: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9380/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0796 - val_mse: 0.0796\n",
            "\n",
            "Epoch 09380: loss did not improve from 0.00366\n",
            "Epoch 9381/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0797 - val_mse: 0.0797\n",
            "\n",
            "Epoch 09381: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9382/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0796 - val_mse: 0.0796\n",
            "\n",
            "Epoch 09382: loss did not improve from 0.00366\n",
            "Epoch 9383/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0797 - val_mse: 0.0797\n",
            "\n",
            "Epoch 09383: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9384/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0796 - val_mse: 0.0796\n",
            "\n",
            "Epoch 09384: loss did not improve from 0.00366\n",
            "Epoch 9385/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0797 - val_mse: 0.0797\n",
            "\n",
            "Epoch 09385: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9386/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0796 - val_mse: 0.0796\n",
            "\n",
            "Epoch 09386: loss did not improve from 0.00366\n",
            "Epoch 9387/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0797 - val_mse: 0.0797\n",
            "\n",
            "Epoch 09387: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9388/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0796 - val_mse: 0.0796\n",
            "\n",
            "Epoch 09388: loss did not improve from 0.00366\n",
            "Epoch 9389/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0797 - val_mse: 0.0797\n",
            "\n",
            "Epoch 09389: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9390/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0797 - val_mse: 0.0797\n",
            "\n",
            "Epoch 09390: loss did not improve from 0.00366\n",
            "Epoch 9391/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0798 - val_mse: 0.0798\n",
            "\n",
            "Epoch 09391: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9392/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0797 - val_mse: 0.0797\n",
            "\n",
            "Epoch 09392: loss did not improve from 0.00366\n",
            "Epoch 9393/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0798 - val_mse: 0.0798\n",
            "\n",
            "Epoch 09393: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9394/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0797 - val_mse: 0.0797\n",
            "\n",
            "Epoch 09394: loss did not improve from 0.00366\n",
            "Epoch 9395/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0798 - val_mse: 0.0798\n",
            "\n",
            "Epoch 09395: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9396/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0797 - val_mse: 0.0797\n",
            "\n",
            "Epoch 09396: loss did not improve from 0.00366\n",
            "Epoch 9397/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0798 - val_mse: 0.0798\n",
            "\n",
            "Epoch 09397: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9398/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0797 - val_mse: 0.0797\n",
            "\n",
            "Epoch 09398: loss did not improve from 0.00366\n",
            "Epoch 9399/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0798 - val_mse: 0.0798\n",
            "\n",
            "Epoch 09399: loss improved from 0.00366 to 0.00366, saving model to poids_train.hdf5\n",
            "Epoch 9400/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0797 - val_mse: 0.0797\n",
            "\n",
            "Epoch 09400: loss did not improve from 0.00366\n",
            "Epoch 9401/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0798 - val_mse: 0.0798\n",
            "\n",
            "Epoch 09401: loss improved from 0.00366 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9402/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0797 - val_mse: 0.0797\n",
            "\n",
            "Epoch 09402: loss did not improve from 0.00365\n",
            "Epoch 9403/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0798 - val_mse: 0.0798\n",
            "\n",
            "Epoch 09403: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9404/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0797 - val_mse: 0.0797\n",
            "\n",
            "Epoch 09404: loss did not improve from 0.00365\n",
            "Epoch 9405/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0798 - val_mse: 0.0798\n",
            "\n",
            "Epoch 09405: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9406/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0797 - val_mse: 0.0797\n",
            "\n",
            "Epoch 09406: loss did not improve from 0.00365\n",
            "Epoch 9407/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0798 - val_mse: 0.0798\n",
            "\n",
            "Epoch 09407: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9408/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0798 - val_mse: 0.0798\n",
            "\n",
            "Epoch 09408: loss did not improve from 0.00365\n",
            "Epoch 9409/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0799 - val_mse: 0.0799\n",
            "\n",
            "Epoch 09409: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9410/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0798 - val_mse: 0.0798\n",
            "\n",
            "Epoch 09410: loss did not improve from 0.00365\n",
            "Epoch 9411/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0799 - val_mse: 0.0799\n",
            "\n",
            "Epoch 09411: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9412/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0798 - val_mse: 0.0798\n",
            "\n",
            "Epoch 09412: loss did not improve from 0.00365\n",
            "Epoch 9413/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0799 - val_mse: 0.0799\n",
            "\n",
            "Epoch 09413: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9414/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0798 - val_mse: 0.0798\n",
            "\n",
            "Epoch 09414: loss did not improve from 0.00365\n",
            "Epoch 9415/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0799 - val_mse: 0.0799\n",
            "\n",
            "Epoch 09415: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9416/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0798 - val_mse: 0.0798\n",
            "\n",
            "Epoch 09416: loss did not improve from 0.00365\n",
            "Epoch 9417/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0799 - val_mse: 0.0799\n",
            "\n",
            "Epoch 09417: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9418/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0798 - val_mse: 0.0798\n",
            "\n",
            "Epoch 09418: loss did not improve from 0.00365\n",
            "Epoch 9419/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0799 - val_mse: 0.0799\n",
            "\n",
            "Epoch 09419: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9420/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0798 - val_mse: 0.0798\n",
            "\n",
            "Epoch 09420: loss did not improve from 0.00365\n",
            "Epoch 9421/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0799 - val_mse: 0.0799\n",
            "\n",
            "Epoch 09421: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9422/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0798 - val_mse: 0.0798\n",
            "\n",
            "Epoch 09422: loss did not improve from 0.00365\n",
            "Epoch 9423/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0799 - val_mse: 0.0799\n",
            "\n",
            "Epoch 09423: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9424/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0798 - val_mse: 0.0798\n",
            "\n",
            "Epoch 09424: loss did not improve from 0.00365\n",
            "Epoch 9425/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0799 - val_mse: 0.0799\n",
            "\n",
            "Epoch 09425: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9426/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0799 - val_mse: 0.0799\n",
            "\n",
            "Epoch 09426: loss did not improve from 0.00365\n",
            "Epoch 9427/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0799 - val_mse: 0.0799\n",
            "\n",
            "Epoch 09427: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9428/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0799 - val_mse: 0.0799\n",
            "\n",
            "Epoch 09428: loss did not improve from 0.00365\n",
            "Epoch 9429/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0800 - val_mse: 0.0800\n",
            "\n",
            "Epoch 09429: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9430/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0799 - val_mse: 0.0799\n",
            "\n",
            "Epoch 09430: loss did not improve from 0.00365\n",
            "Epoch 9431/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0800 - val_mse: 0.0800\n",
            "\n",
            "Epoch 09431: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9432/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0799 - val_mse: 0.0799\n",
            "\n",
            "Epoch 09432: loss did not improve from 0.00365\n",
            "Epoch 9433/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0800 - val_mse: 0.0800\n",
            "\n",
            "Epoch 09433: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9434/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0799 - val_mse: 0.0799\n",
            "\n",
            "Epoch 09434: loss did not improve from 0.00365\n",
            "Epoch 9435/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0800 - val_mse: 0.0800\n",
            "\n",
            "Epoch 09435: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9436/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0799 - val_mse: 0.0799\n",
            "\n",
            "Epoch 09436: loss did not improve from 0.00365\n",
            "Epoch 9437/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0800 - val_mse: 0.0800\n",
            "\n",
            "Epoch 09437: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9438/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0799 - val_mse: 0.0799\n",
            "\n",
            "Epoch 09438: loss did not improve from 0.00365\n",
            "Epoch 9439/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0800 - val_mse: 0.0800\n",
            "\n",
            "Epoch 09439: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9440/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0799 - val_mse: 0.0799\n",
            "\n",
            "Epoch 09440: loss did not improve from 0.00365\n",
            "Epoch 9441/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0800 - val_mse: 0.0800\n",
            "\n",
            "Epoch 09441: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9442/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0799 - val_mse: 0.0799\n",
            "\n",
            "Epoch 09442: loss did not improve from 0.00365\n",
            "Epoch 9443/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0800 - val_mse: 0.0800\n",
            "\n",
            "Epoch 09443: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9444/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0800 - val_mse: 0.0800\n",
            "\n",
            "Epoch 09444: loss did not improve from 0.00365\n",
            "Epoch 9445/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0800 - val_mse: 0.0800\n",
            "\n",
            "Epoch 09445: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9446/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0800 - val_mse: 0.0800\n",
            "\n",
            "Epoch 09446: loss did not improve from 0.00365\n",
            "Epoch 9447/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0801 - val_mse: 0.0801\n",
            "\n",
            "Epoch 09447: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9448/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0800 - val_mse: 0.0800\n",
            "\n",
            "Epoch 09448: loss did not improve from 0.00365\n",
            "Epoch 9449/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0801 - val_mse: 0.0801\n",
            "\n",
            "Epoch 09449: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9450/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0800 - val_mse: 0.0800\n",
            "\n",
            "Epoch 09450: loss did not improve from 0.00365\n",
            "Epoch 9451/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0801 - val_mse: 0.0801\n",
            "\n",
            "Epoch 09451: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9452/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0800 - val_mse: 0.0800\n",
            "\n",
            "Epoch 09452: loss did not improve from 0.00365\n",
            "Epoch 9453/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0801 - val_mse: 0.0801\n",
            "\n",
            "Epoch 09453: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9454/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0800 - val_mse: 0.0800\n",
            "\n",
            "Epoch 09454: loss did not improve from 0.00365\n",
            "Epoch 9455/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0801 - val_mse: 0.0801\n",
            "\n",
            "Epoch 09455: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9456/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0800 - val_mse: 0.0800\n",
            "\n",
            "Epoch 09456: loss did not improve from 0.00365\n",
            "Epoch 9457/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0801 - val_mse: 0.0801\n",
            "\n",
            "Epoch 09457: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9458/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0800 - val_mse: 0.0800\n",
            "\n",
            "Epoch 09458: loss did not improve from 0.00365\n",
            "Epoch 9459/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0801 - val_mse: 0.0801\n",
            "\n",
            "Epoch 09459: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9460/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0800 - val_mse: 0.0800\n",
            "\n",
            "Epoch 09460: loss did not improve from 0.00365\n",
            "Epoch 9461/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0801 - val_mse: 0.0801\n",
            "\n",
            "Epoch 09461: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9462/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0801 - val_mse: 0.0801\n",
            "\n",
            "Epoch 09462: loss did not improve from 0.00365\n",
            "Epoch 9463/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0801 - val_mse: 0.0801\n",
            "\n",
            "Epoch 09463: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9464/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0801 - val_mse: 0.0801\n",
            "\n",
            "Epoch 09464: loss did not improve from 0.00365\n",
            "Epoch 9465/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0802 - val_mse: 0.0802\n",
            "\n",
            "Epoch 09465: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9466/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0801 - val_mse: 0.0801\n",
            "\n",
            "Epoch 09466: loss did not improve from 0.00365\n",
            "Epoch 9467/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0802 - val_mse: 0.0802\n",
            "\n",
            "Epoch 09467: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9468/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0801 - val_mse: 0.0801\n",
            "\n",
            "Epoch 09468: loss did not improve from 0.00365\n",
            "Epoch 9469/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0802 - val_mse: 0.0802\n",
            "\n",
            "Epoch 09469: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9470/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0801 - val_mse: 0.0801\n",
            "\n",
            "Epoch 09470: loss did not improve from 0.00365\n",
            "Epoch 9471/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0802 - val_mse: 0.0802\n",
            "\n",
            "Epoch 09471: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9472/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0801 - val_mse: 0.0801\n",
            "\n",
            "Epoch 09472: loss did not improve from 0.00365\n",
            "Epoch 9473/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0802 - val_mse: 0.0802\n",
            "\n",
            "Epoch 09473: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9474/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0801 - val_mse: 0.0801\n",
            "\n",
            "Epoch 09474: loss did not improve from 0.00365\n",
            "Epoch 9475/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0802 - val_mse: 0.0802\n",
            "\n",
            "Epoch 09475: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9476/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0801 - val_mse: 0.0801\n",
            "\n",
            "Epoch 09476: loss did not improve from 0.00365\n",
            "Epoch 9477/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0802 - val_mse: 0.0802\n",
            "\n",
            "Epoch 09477: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9478/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0801 - val_mse: 0.0801\n",
            "\n",
            "Epoch 09478: loss did not improve from 0.00365\n",
            "Epoch 9479/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0802 - val_mse: 0.0802\n",
            "\n",
            "Epoch 09479: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9480/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0802 - val_mse: 0.0802\n",
            "\n",
            "Epoch 09480: loss did not improve from 0.00365\n",
            "Epoch 9481/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0802 - val_mse: 0.0802\n",
            "\n",
            "Epoch 09481: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9482/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0802 - val_mse: 0.0802\n",
            "\n",
            "Epoch 09482: loss did not improve from 0.00365\n",
            "Epoch 9483/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0803 - val_mse: 0.0803\n",
            "\n",
            "Epoch 09483: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9484/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0802 - val_mse: 0.0802\n",
            "\n",
            "Epoch 09484: loss did not improve from 0.00365\n",
            "Epoch 9485/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0803 - val_mse: 0.0803\n",
            "\n",
            "Epoch 09485: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9486/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0802 - val_mse: 0.0802\n",
            "\n",
            "Epoch 09486: loss did not improve from 0.00365\n",
            "Epoch 9487/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0803 - val_mse: 0.0803\n",
            "\n",
            "Epoch 09487: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9488/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0802 - val_mse: 0.0802\n",
            "\n",
            "Epoch 09488: loss did not improve from 0.00365\n",
            "Epoch 9489/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0803 - val_mse: 0.0803\n",
            "\n",
            "Epoch 09489: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9490/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0802 - val_mse: 0.0802\n",
            "\n",
            "Epoch 09490: loss did not improve from 0.00365\n",
            "Epoch 9491/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0803 - val_mse: 0.0803\n",
            "\n",
            "Epoch 09491: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9492/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0802 - val_mse: 0.0802\n",
            "\n",
            "Epoch 09492: loss did not improve from 0.00365\n",
            "Epoch 9493/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0803 - val_mse: 0.0803\n",
            "\n",
            "Epoch 09493: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9494/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0802 - val_mse: 0.0802\n",
            "\n",
            "Epoch 09494: loss did not improve from 0.00365\n",
            "Epoch 9495/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0803 - val_mse: 0.0803\n",
            "\n",
            "Epoch 09495: loss improved from 0.00365 to 0.00365, saving model to poids_train.hdf5\n",
            "Epoch 9496/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0802 - val_mse: 0.0802\n",
            "\n",
            "Epoch 09496: loss did not improve from 0.00365\n",
            "Epoch 9497/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0803 - val_mse: 0.0803\n",
            "\n",
            "Epoch 09497: loss improved from 0.00365 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9498/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0803 - val_mse: 0.0803\n",
            "\n",
            "Epoch 09498: loss did not improve from 0.00364\n",
            "Epoch 9499/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0803 - val_mse: 0.0803\n",
            "\n",
            "Epoch 09499: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9500/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0803 - val_mse: 0.0803\n",
            "\n",
            "Epoch 09500: loss did not improve from 0.00364\n",
            "Epoch 9501/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0804 - val_mse: 0.0804\n",
            "\n",
            "Epoch 09501: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9502/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0803 - val_mse: 0.0803\n",
            "\n",
            "Epoch 09502: loss did not improve from 0.00364\n",
            "Epoch 9503/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0804 - val_mse: 0.0804\n",
            "\n",
            "Epoch 09503: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9504/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0803 - val_mse: 0.0803\n",
            "\n",
            "Epoch 09504: loss did not improve from 0.00364\n",
            "Epoch 9505/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0804 - val_mse: 0.0804\n",
            "\n",
            "Epoch 09505: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9506/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0803 - val_mse: 0.0803\n",
            "\n",
            "Epoch 09506: loss did not improve from 0.00364\n",
            "Epoch 9507/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0804 - val_mse: 0.0804\n",
            "\n",
            "Epoch 09507: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9508/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0803 - val_mse: 0.0803\n",
            "\n",
            "Epoch 09508: loss did not improve from 0.00364\n",
            "Epoch 9509/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0804 - val_mse: 0.0804\n",
            "\n",
            "Epoch 09509: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9510/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0803 - val_mse: 0.0803\n",
            "\n",
            "Epoch 09510: loss did not improve from 0.00364\n",
            "Epoch 9511/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0804 - val_mse: 0.0804\n",
            "\n",
            "Epoch 09511: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9512/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0803 - val_mse: 0.0803\n",
            "\n",
            "Epoch 09512: loss did not improve from 0.00364\n",
            "Epoch 9513/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0804 - val_mse: 0.0804\n",
            "\n",
            "Epoch 09513: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9514/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0803 - val_mse: 0.0803\n",
            "\n",
            "Epoch 09514: loss did not improve from 0.00364\n",
            "Epoch 9515/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0804 - val_mse: 0.0804\n",
            "\n",
            "Epoch 09515: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9516/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0804 - val_mse: 0.0804\n",
            "\n",
            "Epoch 09516: loss did not improve from 0.00364\n",
            "Epoch 9517/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0804 - val_mse: 0.0804\n",
            "\n",
            "Epoch 09517: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9518/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0804 - val_mse: 0.0804\n",
            "\n",
            "Epoch 09518: loss did not improve from 0.00364\n",
            "Epoch 9519/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0805 - val_mse: 0.0805\n",
            "\n",
            "Epoch 09519: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9520/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0804 - val_mse: 0.0804\n",
            "\n",
            "Epoch 09520: loss did not improve from 0.00364\n",
            "Epoch 9521/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0805 - val_mse: 0.0805\n",
            "\n",
            "Epoch 09521: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9522/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0804 - val_mse: 0.0804\n",
            "\n",
            "Epoch 09522: loss did not improve from 0.00364\n",
            "Epoch 9523/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0805 - val_mse: 0.0805\n",
            "\n",
            "Epoch 09523: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9524/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0804 - val_mse: 0.0804\n",
            "\n",
            "Epoch 09524: loss did not improve from 0.00364\n",
            "Epoch 9525/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0805 - val_mse: 0.0805\n",
            "\n",
            "Epoch 09525: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9526/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0804 - val_mse: 0.0804\n",
            "\n",
            "Epoch 09526: loss did not improve from 0.00364\n",
            "Epoch 9527/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0805 - val_mse: 0.0805\n",
            "\n",
            "Epoch 09527: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9528/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0804 - val_mse: 0.0804\n",
            "\n",
            "Epoch 09528: loss did not improve from 0.00364\n",
            "Epoch 9529/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0805 - val_mse: 0.0805\n",
            "\n",
            "Epoch 09529: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9530/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0804 - val_mse: 0.0804\n",
            "\n",
            "Epoch 09530: loss did not improve from 0.00364\n",
            "Epoch 9531/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0805 - val_mse: 0.0805\n",
            "\n",
            "Epoch 09531: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9532/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0804 - val_mse: 0.0804\n",
            "\n",
            "Epoch 09532: loss did not improve from 0.00364\n",
            "Epoch 9533/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0805 - val_mse: 0.0805\n",
            "\n",
            "Epoch 09533: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9534/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0805 - val_mse: 0.0805\n",
            "\n",
            "Epoch 09534: loss did not improve from 0.00364\n",
            "Epoch 9535/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0805 - val_mse: 0.0805\n",
            "\n",
            "Epoch 09535: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9536/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0805 - val_mse: 0.0805\n",
            "\n",
            "Epoch 09536: loss did not improve from 0.00364\n",
            "Epoch 9537/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0806 - val_mse: 0.0806\n",
            "\n",
            "Epoch 09537: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9538/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0805 - val_mse: 0.0805\n",
            "\n",
            "Epoch 09538: loss did not improve from 0.00364\n",
            "Epoch 9539/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0806 - val_mse: 0.0806\n",
            "\n",
            "Epoch 09539: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9540/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0805 - val_mse: 0.0805\n",
            "\n",
            "Epoch 09540: loss did not improve from 0.00364\n",
            "Epoch 9541/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0806 - val_mse: 0.0806\n",
            "\n",
            "Epoch 09541: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9542/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0805 - val_mse: 0.0805\n",
            "\n",
            "Epoch 09542: loss did not improve from 0.00364\n",
            "Epoch 9543/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0806 - val_mse: 0.0806\n",
            "\n",
            "Epoch 09543: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9544/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0805 - val_mse: 0.0805\n",
            "\n",
            "Epoch 09544: loss did not improve from 0.00364\n",
            "Epoch 9545/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0806 - val_mse: 0.0806\n",
            "\n",
            "Epoch 09545: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9546/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0805 - val_mse: 0.0805\n",
            "\n",
            "Epoch 09546: loss did not improve from 0.00364\n",
            "Epoch 9547/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0806 - val_mse: 0.0806\n",
            "\n",
            "Epoch 09547: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9548/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0805 - val_mse: 0.0805\n",
            "\n",
            "Epoch 09548: loss did not improve from 0.00364\n",
            "Epoch 9549/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0806 - val_mse: 0.0806\n",
            "\n",
            "Epoch 09549: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9550/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0805 - val_mse: 0.0805\n",
            "\n",
            "Epoch 09550: loss did not improve from 0.00364\n",
            "Epoch 9551/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0806 - val_mse: 0.0806\n",
            "\n",
            "Epoch 09551: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9552/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0805 - val_mse: 0.0805\n",
            "\n",
            "Epoch 09552: loss did not improve from 0.00364\n",
            "Epoch 9553/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0806 - val_mse: 0.0806\n",
            "\n",
            "Epoch 09553: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9554/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0806 - val_mse: 0.0806\n",
            "\n",
            "Epoch 09554: loss did not improve from 0.00364\n",
            "Epoch 9555/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0806 - val_mse: 0.0806\n",
            "\n",
            "Epoch 09555: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9556/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0806 - val_mse: 0.0806\n",
            "\n",
            "Epoch 09556: loss did not improve from 0.00364\n",
            "Epoch 9557/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0807 - val_mse: 0.0807\n",
            "\n",
            "Epoch 09557: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9558/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0806 - val_mse: 0.0806\n",
            "\n",
            "Epoch 09558: loss did not improve from 0.00364\n",
            "Epoch 9559/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0807 - val_mse: 0.0807\n",
            "\n",
            "Epoch 09559: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9560/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0806 - val_mse: 0.0806\n",
            "\n",
            "Epoch 09560: loss did not improve from 0.00364\n",
            "Epoch 9561/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0807 - val_mse: 0.0807\n",
            "\n",
            "Epoch 09561: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9562/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0806 - val_mse: 0.0806\n",
            "\n",
            "Epoch 09562: loss did not improve from 0.00364\n",
            "Epoch 9563/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0807 - val_mse: 0.0807\n",
            "\n",
            "Epoch 09563: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9564/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0806 - val_mse: 0.0806\n",
            "\n",
            "Epoch 09564: loss did not improve from 0.00364\n",
            "Epoch 9565/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0807 - val_mse: 0.0807\n",
            "\n",
            "Epoch 09565: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9566/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0806 - val_mse: 0.0806\n",
            "\n",
            "Epoch 09566: loss did not improve from 0.00364\n",
            "Epoch 9567/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0807 - val_mse: 0.0807\n",
            "\n",
            "Epoch 09567: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9568/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0806 - val_mse: 0.0806\n",
            "\n",
            "Epoch 09568: loss did not improve from 0.00364\n",
            "Epoch 9569/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0807 - val_mse: 0.0807\n",
            "\n",
            "Epoch 09569: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9570/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0806 - val_mse: 0.0806\n",
            "\n",
            "Epoch 09570: loss did not improve from 0.00364\n",
            "Epoch 9571/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0807 - val_mse: 0.0807\n",
            "\n",
            "Epoch 09571: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9572/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0807 - val_mse: 0.0807\n",
            "\n",
            "Epoch 09572: loss did not improve from 0.00364\n",
            "Epoch 9573/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0807 - val_mse: 0.0807\n",
            "\n",
            "Epoch 09573: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9574/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0807 - val_mse: 0.0807\n",
            "\n",
            "Epoch 09574: loss did not improve from 0.00364\n",
            "Epoch 9575/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0808 - val_mse: 0.0808\n",
            "\n",
            "Epoch 09575: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9576/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0807 - val_mse: 0.0807\n",
            "\n",
            "Epoch 09576: loss did not improve from 0.00364\n",
            "Epoch 9577/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0808 - val_mse: 0.0808\n",
            "\n",
            "Epoch 09577: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9578/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0807 - val_mse: 0.0807\n",
            "\n",
            "Epoch 09578: loss did not improve from 0.00364\n",
            "Epoch 9579/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0808 - val_mse: 0.0808\n",
            "\n",
            "Epoch 09579: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9580/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0807 - val_mse: 0.0807\n",
            "\n",
            "Epoch 09580: loss did not improve from 0.00364\n",
            "Epoch 9581/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0808 - val_mse: 0.0808\n",
            "\n",
            "Epoch 09581: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9582/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0807 - val_mse: 0.0807\n",
            "\n",
            "Epoch 09582: loss did not improve from 0.00364\n",
            "Epoch 9583/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0808 - val_mse: 0.0808\n",
            "\n",
            "Epoch 09583: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9584/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0807 - val_mse: 0.0807\n",
            "\n",
            "Epoch 09584: loss did not improve from 0.00364\n",
            "Epoch 9585/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0808 - val_mse: 0.0808\n",
            "\n",
            "Epoch 09585: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9586/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0807 - val_mse: 0.0807\n",
            "\n",
            "Epoch 09586: loss did not improve from 0.00364\n",
            "Epoch 9587/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0808 - val_mse: 0.0808\n",
            "\n",
            "Epoch 09587: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9588/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0807 - val_mse: 0.0807\n",
            "\n",
            "Epoch 09588: loss did not improve from 0.00364\n",
            "Epoch 9589/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0808 - val_mse: 0.0808\n",
            "\n",
            "Epoch 09589: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9590/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0808 - val_mse: 0.0808\n",
            "\n",
            "Epoch 09590: loss did not improve from 0.00364\n",
            "Epoch 9591/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0808 - val_mse: 0.0808\n",
            "\n",
            "Epoch 09591: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9592/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0808 - val_mse: 0.0808\n",
            "\n",
            "Epoch 09592: loss did not improve from 0.00364\n",
            "Epoch 9593/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0809 - val_mse: 0.0809\n",
            "\n",
            "Epoch 09593: loss improved from 0.00364 to 0.00364, saving model to poids_train.hdf5\n",
            "Epoch 9594/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0808 - val_mse: 0.0808\n",
            "\n",
            "Epoch 09594: loss did not improve from 0.00364\n",
            "Epoch 9595/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0809 - val_mse: 0.0809\n",
            "\n",
            "Epoch 09595: loss improved from 0.00364 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9596/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0808 - val_mse: 0.0808\n",
            "\n",
            "Epoch 09596: loss did not improve from 0.00363\n",
            "Epoch 9597/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0809 - val_mse: 0.0809\n",
            "\n",
            "Epoch 09597: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9598/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0808 - val_mse: 0.0808\n",
            "\n",
            "Epoch 09598: loss did not improve from 0.00363\n",
            "Epoch 9599/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0809 - val_mse: 0.0809\n",
            "\n",
            "Epoch 09599: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9600/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0808 - val_mse: 0.0808\n",
            "\n",
            "Epoch 09600: loss did not improve from 0.00363\n",
            "Epoch 9601/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0809 - val_mse: 0.0809\n",
            "\n",
            "Epoch 09601: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9602/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0808 - val_mse: 0.0808\n",
            "\n",
            "Epoch 09602: loss did not improve from 0.00363\n",
            "Epoch 9603/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0809 - val_mse: 0.0809\n",
            "\n",
            "Epoch 09603: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9604/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0808 - val_mse: 0.0808\n",
            "\n",
            "Epoch 09604: loss did not improve from 0.00363\n",
            "Epoch 9605/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0809 - val_mse: 0.0809\n",
            "\n",
            "Epoch 09605: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9606/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0808 - val_mse: 0.0808\n",
            "\n",
            "Epoch 09606: loss did not improve from 0.00363\n",
            "Epoch 9607/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0809 - val_mse: 0.0809\n",
            "\n",
            "Epoch 09607: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9608/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0809 - val_mse: 0.0809\n",
            "\n",
            "Epoch 09608: loss did not improve from 0.00363\n",
            "Epoch 9609/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0809 - val_mse: 0.0809\n",
            "\n",
            "Epoch 09609: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9610/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0809 - val_mse: 0.0809\n",
            "\n",
            "Epoch 09610: loss did not improve from 0.00363\n",
            "Epoch 9611/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0809 - val_mse: 0.0809\n",
            "\n",
            "Epoch 09611: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9612/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0809 - val_mse: 0.0809\n",
            "\n",
            "Epoch 09612: loss did not improve from 0.00363\n",
            "Epoch 9613/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0810 - val_mse: 0.0810\n",
            "\n",
            "Epoch 09613: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9614/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0809 - val_mse: 0.0809\n",
            "\n",
            "Epoch 09614: loss did not improve from 0.00363\n",
            "Epoch 9615/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0810 - val_mse: 0.0810\n",
            "\n",
            "Epoch 09615: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9616/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0809 - val_mse: 0.0809\n",
            "\n",
            "Epoch 09616: loss did not improve from 0.00363\n",
            "Epoch 9617/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0810 - val_mse: 0.0810\n",
            "\n",
            "Epoch 09617: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9618/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0809 - val_mse: 0.0809\n",
            "\n",
            "Epoch 09618: loss did not improve from 0.00363\n",
            "Epoch 9619/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0810 - val_mse: 0.0810\n",
            "\n",
            "Epoch 09619: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9620/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0809 - val_mse: 0.0809\n",
            "\n",
            "Epoch 09620: loss did not improve from 0.00363\n",
            "Epoch 9621/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0810 - val_mse: 0.0810\n",
            "\n",
            "Epoch 09621: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9622/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0809 - val_mse: 0.0809\n",
            "\n",
            "Epoch 09622: loss did not improve from 0.00363\n",
            "Epoch 9623/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0810 - val_mse: 0.0810\n",
            "\n",
            "Epoch 09623: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9624/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0809 - val_mse: 0.0809\n",
            "\n",
            "Epoch 09624: loss did not improve from 0.00363\n",
            "Epoch 9625/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0810 - val_mse: 0.0810\n",
            "\n",
            "Epoch 09625: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9626/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0809 - val_mse: 0.0809\n",
            "\n",
            "Epoch 09626: loss did not improve from 0.00363\n",
            "Epoch 9627/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0810 - val_mse: 0.0810\n",
            "\n",
            "Epoch 09627: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9628/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0810 - val_mse: 0.0810\n",
            "\n",
            "Epoch 09628: loss did not improve from 0.00363\n",
            "Epoch 9629/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0810 - val_mse: 0.0810\n",
            "\n",
            "Epoch 09629: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9630/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0810 - val_mse: 0.0810\n",
            "\n",
            "Epoch 09630: loss did not improve from 0.00363\n",
            "Epoch 9631/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0811 - val_mse: 0.0811\n",
            "\n",
            "Epoch 09631: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9632/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0810 - val_mse: 0.0810\n",
            "\n",
            "Epoch 09632: loss did not improve from 0.00363\n",
            "Epoch 9633/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0811 - val_mse: 0.0811\n",
            "\n",
            "Epoch 09633: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9634/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0810 - val_mse: 0.0810\n",
            "\n",
            "Epoch 09634: loss did not improve from 0.00363\n",
            "Epoch 9635/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0811 - val_mse: 0.0811\n",
            "\n",
            "Epoch 09635: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9636/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0810 - val_mse: 0.0810\n",
            "\n",
            "Epoch 09636: loss did not improve from 0.00363\n",
            "Epoch 9637/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0811 - val_mse: 0.0811\n",
            "\n",
            "Epoch 09637: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9638/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0810 - val_mse: 0.0810\n",
            "\n",
            "Epoch 09638: loss did not improve from 0.00363\n",
            "Epoch 9639/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0811 - val_mse: 0.0811\n",
            "\n",
            "Epoch 09639: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9640/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0810 - val_mse: 0.0810\n",
            "\n",
            "Epoch 09640: loss did not improve from 0.00363\n",
            "Epoch 9641/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0811 - val_mse: 0.0811\n",
            "\n",
            "Epoch 09641: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9642/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0810 - val_mse: 0.0810\n",
            "\n",
            "Epoch 09642: loss did not improve from 0.00363\n",
            "Epoch 9643/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0811 - val_mse: 0.0811\n",
            "\n",
            "Epoch 09643: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9644/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0810 - val_mse: 0.0810\n",
            "\n",
            "Epoch 09644: loss did not improve from 0.00363\n",
            "Epoch 9645/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0811 - val_mse: 0.0811\n",
            "\n",
            "Epoch 09645: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9646/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0811 - val_mse: 0.0811\n",
            "\n",
            "Epoch 09646: loss did not improve from 0.00363\n",
            "Epoch 9647/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0811 - val_mse: 0.0811\n",
            "\n",
            "Epoch 09647: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9648/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0811 - val_mse: 0.0811\n",
            "\n",
            "Epoch 09648: loss did not improve from 0.00363\n",
            "Epoch 9649/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0812 - val_mse: 0.0812\n",
            "\n",
            "Epoch 09649: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9650/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0811 - val_mse: 0.0811\n",
            "\n",
            "Epoch 09650: loss did not improve from 0.00363\n",
            "Epoch 9651/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0812 - val_mse: 0.0812\n",
            "\n",
            "Epoch 09651: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9652/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0811 - val_mse: 0.0811\n",
            "\n",
            "Epoch 09652: loss did not improve from 0.00363\n",
            "Epoch 9653/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0812 - val_mse: 0.0812\n",
            "\n",
            "Epoch 09653: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9654/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0811 - val_mse: 0.0811\n",
            "\n",
            "Epoch 09654: loss did not improve from 0.00363\n",
            "Epoch 9655/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0812 - val_mse: 0.0812\n",
            "\n",
            "Epoch 09655: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9656/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0811 - val_mse: 0.0811\n",
            "\n",
            "Epoch 09656: loss did not improve from 0.00363\n",
            "Epoch 9657/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0812 - val_mse: 0.0812\n",
            "\n",
            "Epoch 09657: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9658/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0811 - val_mse: 0.0811\n",
            "\n",
            "Epoch 09658: loss did not improve from 0.00363\n",
            "Epoch 9659/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0812 - val_mse: 0.0812\n",
            "\n",
            "Epoch 09659: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9660/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0811 - val_mse: 0.0811\n",
            "\n",
            "Epoch 09660: loss did not improve from 0.00363\n",
            "Epoch 9661/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0812 - val_mse: 0.0812\n",
            "\n",
            "Epoch 09661: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9662/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0811 - val_mse: 0.0811\n",
            "\n",
            "Epoch 09662: loss did not improve from 0.00363\n",
            "Epoch 9663/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0812 - val_mse: 0.0812\n",
            "\n",
            "Epoch 09663: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9664/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0812 - val_mse: 0.0812\n",
            "\n",
            "Epoch 09664: loss did not improve from 0.00363\n",
            "Epoch 9665/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0812 - val_mse: 0.0812\n",
            "\n",
            "Epoch 09665: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9666/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0812 - val_mse: 0.0812\n",
            "\n",
            "Epoch 09666: loss did not improve from 0.00363\n",
            "Epoch 9667/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0812 - val_mse: 0.0812\n",
            "\n",
            "Epoch 09667: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9668/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0812 - val_mse: 0.0812\n",
            "\n",
            "Epoch 09668: loss did not improve from 0.00363\n",
            "Epoch 9669/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0813 - val_mse: 0.0813\n",
            "\n",
            "Epoch 09669: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9670/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0812 - val_mse: 0.0812\n",
            "\n",
            "Epoch 09670: loss did not improve from 0.00363\n",
            "Epoch 9671/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0813 - val_mse: 0.0813\n",
            "\n",
            "Epoch 09671: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9672/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0812 - val_mse: 0.0812\n",
            "\n",
            "Epoch 09672: loss did not improve from 0.00363\n",
            "Epoch 9673/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0813 - val_mse: 0.0813\n",
            "\n",
            "Epoch 09673: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9674/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0812 - val_mse: 0.0812\n",
            "\n",
            "Epoch 09674: loss did not improve from 0.00363\n",
            "Epoch 9675/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0813 - val_mse: 0.0813\n",
            "\n",
            "Epoch 09675: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9676/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0812 - val_mse: 0.0812\n",
            "\n",
            "Epoch 09676: loss did not improve from 0.00363\n",
            "Epoch 9677/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0813 - val_mse: 0.0813\n",
            "\n",
            "Epoch 09677: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9678/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0812 - val_mse: 0.0812\n",
            "\n",
            "Epoch 09678: loss did not improve from 0.00363\n",
            "Epoch 9679/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0813 - val_mse: 0.0813\n",
            "\n",
            "Epoch 09679: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9680/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0812 - val_mse: 0.0812\n",
            "\n",
            "Epoch 09680: loss did not improve from 0.00363\n",
            "Epoch 9681/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0813 - val_mse: 0.0813\n",
            "\n",
            "Epoch 09681: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9682/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0812 - val_mse: 0.0812\n",
            "\n",
            "Epoch 09682: loss did not improve from 0.00363\n",
            "Epoch 9683/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0813 - val_mse: 0.0813\n",
            "\n",
            "Epoch 09683: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9684/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0813 - val_mse: 0.0813\n",
            "\n",
            "Epoch 09684: loss did not improve from 0.00363\n",
            "Epoch 9685/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0813 - val_mse: 0.0813\n",
            "\n",
            "Epoch 09685: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9686/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0813 - val_mse: 0.0813\n",
            "\n",
            "Epoch 09686: loss did not improve from 0.00363\n",
            "Epoch 9687/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0814 - val_mse: 0.0814\n",
            "\n",
            "Epoch 09687: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9688/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0813 - val_mse: 0.0813\n",
            "\n",
            "Epoch 09688: loss did not improve from 0.00363\n",
            "Epoch 9689/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0814 - val_mse: 0.0814\n",
            "\n",
            "Epoch 09689: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9690/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0813 - val_mse: 0.0813\n",
            "\n",
            "Epoch 09690: loss did not improve from 0.00363\n",
            "Epoch 9691/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0814 - val_mse: 0.0814\n",
            "\n",
            "Epoch 09691: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9692/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0813 - val_mse: 0.0813\n",
            "\n",
            "Epoch 09692: loss did not improve from 0.00363\n",
            "Epoch 9693/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0814 - val_mse: 0.0814\n",
            "\n",
            "Epoch 09693: loss improved from 0.00363 to 0.00363, saving model to poids_train.hdf5\n",
            "Epoch 9694/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0813 - val_mse: 0.0813\n",
            "\n",
            "Epoch 09694: loss did not improve from 0.00363\n",
            "Epoch 9695/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0814 - val_mse: 0.0814\n",
            "\n",
            "Epoch 09695: loss improved from 0.00363 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9696/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0813 - val_mse: 0.0813\n",
            "\n",
            "Epoch 09696: loss did not improve from 0.00362\n",
            "Epoch 9697/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0814 - val_mse: 0.0814\n",
            "\n",
            "Epoch 09697: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9698/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0813 - val_mse: 0.0813\n",
            "\n",
            "Epoch 09698: loss did not improve from 0.00362\n",
            "Epoch 9699/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0814 - val_mse: 0.0814\n",
            "\n",
            "Epoch 09699: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9700/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0813 - val_mse: 0.0813\n",
            "\n",
            "Epoch 09700: loss did not improve from 0.00362\n",
            "Epoch 9701/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0814 - val_mse: 0.0814\n",
            "\n",
            "Epoch 09701: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9702/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0814 - val_mse: 0.0814\n",
            "\n",
            "Epoch 09702: loss did not improve from 0.00362\n",
            "Epoch 9703/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0814 - val_mse: 0.0814\n",
            "\n",
            "Epoch 09703: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9704/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0814 - val_mse: 0.0814\n",
            "\n",
            "Epoch 09704: loss did not improve from 0.00362\n",
            "Epoch 9705/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0814 - val_mse: 0.0814\n",
            "\n",
            "Epoch 09705: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9706/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0814 - val_mse: 0.0814\n",
            "\n",
            "Epoch 09706: loss did not improve from 0.00362\n",
            "Epoch 9707/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0815 - val_mse: 0.0815\n",
            "\n",
            "Epoch 09707: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9708/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0814 - val_mse: 0.0814\n",
            "\n",
            "Epoch 09708: loss did not improve from 0.00362\n",
            "Epoch 9709/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0815 - val_mse: 0.0815\n",
            "\n",
            "Epoch 09709: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9710/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0814 - val_mse: 0.0814\n",
            "\n",
            "Epoch 09710: loss did not improve from 0.00362\n",
            "Epoch 9711/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0815 - val_mse: 0.0815\n",
            "\n",
            "Epoch 09711: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9712/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0814 - val_mse: 0.0814\n",
            "\n",
            "Epoch 09712: loss did not improve from 0.00362\n",
            "Epoch 9713/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0815 - val_mse: 0.0815\n",
            "\n",
            "Epoch 09713: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9714/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0814 - val_mse: 0.0814\n",
            "\n",
            "Epoch 09714: loss did not improve from 0.00362\n",
            "Epoch 9715/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0815 - val_mse: 0.0815\n",
            "\n",
            "Epoch 09715: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9716/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0814 - val_mse: 0.0814\n",
            "\n",
            "Epoch 09716: loss did not improve from 0.00362\n",
            "Epoch 9717/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0815 - val_mse: 0.0815\n",
            "\n",
            "Epoch 09717: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9718/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0814 - val_mse: 0.0814\n",
            "\n",
            "Epoch 09718: loss did not improve from 0.00362\n",
            "Epoch 9719/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0815 - val_mse: 0.0815\n",
            "\n",
            "Epoch 09719: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9720/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0814 - val_mse: 0.0814\n",
            "\n",
            "Epoch 09720: loss did not improve from 0.00362\n",
            "Epoch 9721/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0815 - val_mse: 0.0815\n",
            "\n",
            "Epoch 09721: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9722/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0815 - val_mse: 0.0815\n",
            "\n",
            "Epoch 09722: loss did not improve from 0.00362\n",
            "Epoch 9723/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0815 - val_mse: 0.0815\n",
            "\n",
            "Epoch 09723: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9724/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0815 - val_mse: 0.0815\n",
            "\n",
            "Epoch 09724: loss did not improve from 0.00362\n",
            "Epoch 9725/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0816 - val_mse: 0.0816\n",
            "\n",
            "Epoch 09725: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9726/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0815 - val_mse: 0.0815\n",
            "\n",
            "Epoch 09726: loss did not improve from 0.00362\n",
            "Epoch 9727/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0816 - val_mse: 0.0816\n",
            "\n",
            "Epoch 09727: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9728/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0815 - val_mse: 0.0815\n",
            "\n",
            "Epoch 09728: loss did not improve from 0.00362\n",
            "Epoch 9729/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0816 - val_mse: 0.0816\n",
            "\n",
            "Epoch 09729: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9730/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0815 - val_mse: 0.0815\n",
            "\n",
            "Epoch 09730: loss did not improve from 0.00362\n",
            "Epoch 9731/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0816 - val_mse: 0.0816\n",
            "\n",
            "Epoch 09731: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9732/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0815 - val_mse: 0.0815\n",
            "\n",
            "Epoch 09732: loss did not improve from 0.00362\n",
            "Epoch 9733/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0816 - val_mse: 0.0816\n",
            "\n",
            "Epoch 09733: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9734/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0815 - val_mse: 0.0815\n",
            "\n",
            "Epoch 09734: loss did not improve from 0.00362\n",
            "Epoch 9735/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0816 - val_mse: 0.0816\n",
            "\n",
            "Epoch 09735: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9736/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0815 - val_mse: 0.0815\n",
            "\n",
            "Epoch 09736: loss did not improve from 0.00362\n",
            "Epoch 9737/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0816 - val_mse: 0.0816\n",
            "\n",
            "Epoch 09737: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9738/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0815 - val_mse: 0.0815\n",
            "\n",
            "Epoch 09738: loss did not improve from 0.00362\n",
            "Epoch 9739/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0816 - val_mse: 0.0816\n",
            "\n",
            "Epoch 09739: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9740/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0816 - val_mse: 0.0816\n",
            "\n",
            "Epoch 09740: loss did not improve from 0.00362\n",
            "Epoch 9741/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0816 - val_mse: 0.0816\n",
            "\n",
            "Epoch 09741: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9742/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0816 - val_mse: 0.0816\n",
            "\n",
            "Epoch 09742: loss did not improve from 0.00362\n",
            "Epoch 9743/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0816 - val_mse: 0.0816\n",
            "\n",
            "Epoch 09743: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9744/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0816 - val_mse: 0.0816\n",
            "\n",
            "Epoch 09744: loss did not improve from 0.00362\n",
            "Epoch 9745/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0817 - val_mse: 0.0817\n",
            "\n",
            "Epoch 09745: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9746/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0816 - val_mse: 0.0816\n",
            "\n",
            "Epoch 09746: loss did not improve from 0.00362\n",
            "Epoch 9747/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0817 - val_mse: 0.0817\n",
            "\n",
            "Epoch 09747: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9748/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0816 - val_mse: 0.0816\n",
            "\n",
            "Epoch 09748: loss did not improve from 0.00362\n",
            "Epoch 9749/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0817 - val_mse: 0.0817\n",
            "\n",
            "Epoch 09749: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9750/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0816 - val_mse: 0.0816\n",
            "\n",
            "Epoch 09750: loss did not improve from 0.00362\n",
            "Epoch 9751/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0817 - val_mse: 0.0817\n",
            "\n",
            "Epoch 09751: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9752/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0816 - val_mse: 0.0816\n",
            "\n",
            "Epoch 09752: loss did not improve from 0.00362\n",
            "Epoch 9753/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0817 - val_mse: 0.0817\n",
            "\n",
            "Epoch 09753: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9754/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0816 - val_mse: 0.0816\n",
            "\n",
            "Epoch 09754: loss did not improve from 0.00362\n",
            "Epoch 9755/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0817 - val_mse: 0.0817\n",
            "\n",
            "Epoch 09755: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9756/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0816 - val_mse: 0.0816\n",
            "\n",
            "Epoch 09756: loss did not improve from 0.00362\n",
            "Epoch 9757/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0817 - val_mse: 0.0817\n",
            "\n",
            "Epoch 09757: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9758/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0816 - val_mse: 0.0816\n",
            "\n",
            "Epoch 09758: loss did not improve from 0.00362\n",
            "Epoch 9759/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0817 - val_mse: 0.0817\n",
            "\n",
            "Epoch 09759: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9760/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0817 - val_mse: 0.0817\n",
            "\n",
            "Epoch 09760: loss did not improve from 0.00362\n",
            "Epoch 9761/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0817 - val_mse: 0.0817\n",
            "\n",
            "Epoch 09761: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9762/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0817 - val_mse: 0.0817\n",
            "\n",
            "Epoch 09762: loss did not improve from 0.00362\n",
            "Epoch 9763/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0818 - val_mse: 0.0818\n",
            "\n",
            "Epoch 09763: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9764/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0817 - val_mse: 0.0817\n",
            "\n",
            "Epoch 09764: loss did not improve from 0.00362\n",
            "Epoch 9765/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0818 - val_mse: 0.0818\n",
            "\n",
            "Epoch 09765: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9766/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0817 - val_mse: 0.0817\n",
            "\n",
            "Epoch 09766: loss did not improve from 0.00362\n",
            "Epoch 9767/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0818 - val_mse: 0.0818\n",
            "\n",
            "Epoch 09767: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9768/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0817 - val_mse: 0.0817\n",
            "\n",
            "Epoch 09768: loss did not improve from 0.00362\n",
            "Epoch 9769/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0818 - val_mse: 0.0818\n",
            "\n",
            "Epoch 09769: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9770/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0817 - val_mse: 0.0817\n",
            "\n",
            "Epoch 09770: loss did not improve from 0.00362\n",
            "Epoch 9771/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0818 - val_mse: 0.0818\n",
            "\n",
            "Epoch 09771: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9772/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0817 - val_mse: 0.0817\n",
            "\n",
            "Epoch 09772: loss did not improve from 0.00362\n",
            "Epoch 9773/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0818 - val_mse: 0.0818\n",
            "\n",
            "Epoch 09773: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9774/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0817 - val_mse: 0.0817\n",
            "\n",
            "Epoch 09774: loss did not improve from 0.00362\n",
            "Epoch 9775/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0818 - val_mse: 0.0818\n",
            "\n",
            "Epoch 09775: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9776/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0817 - val_mse: 0.0817\n",
            "\n",
            "Epoch 09776: loss did not improve from 0.00362\n",
            "Epoch 9777/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0818 - val_mse: 0.0818\n",
            "\n",
            "Epoch 09777: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9778/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0818 - val_mse: 0.0818\n",
            "\n",
            "Epoch 09778: loss did not improve from 0.00362\n",
            "Epoch 9779/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0818 - val_mse: 0.0818\n",
            "\n",
            "Epoch 09779: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9780/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0818 - val_mse: 0.0818\n",
            "\n",
            "Epoch 09780: loss did not improve from 0.00362\n",
            "Epoch 9781/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0818 - val_mse: 0.0818\n",
            "\n",
            "Epoch 09781: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9782/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0818 - val_mse: 0.0818\n",
            "\n",
            "Epoch 09782: loss did not improve from 0.00362\n",
            "Epoch 9783/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0819 - val_mse: 0.0819\n",
            "\n",
            "Epoch 09783: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9784/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0818 - val_mse: 0.0818\n",
            "\n",
            "Epoch 09784: loss did not improve from 0.00362\n",
            "Epoch 9785/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0819 - val_mse: 0.0819\n",
            "\n",
            "Epoch 09785: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9786/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0818 - val_mse: 0.0818\n",
            "\n",
            "Epoch 09786: loss did not improve from 0.00362\n",
            "Epoch 9787/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0819 - val_mse: 0.0819\n",
            "\n",
            "Epoch 09787: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9788/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0818 - val_mse: 0.0818\n",
            "\n",
            "Epoch 09788: loss did not improve from 0.00362\n",
            "Epoch 9789/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0819 - val_mse: 0.0819\n",
            "\n",
            "Epoch 09789: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9790/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0818 - val_mse: 0.0818\n",
            "\n",
            "Epoch 09790: loss did not improve from 0.00362\n",
            "Epoch 9791/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0819 - val_mse: 0.0819\n",
            "\n",
            "Epoch 09791: loss improved from 0.00362 to 0.00362, saving model to poids_train.hdf5\n",
            "Epoch 9792/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0818 - val_mse: 0.0818\n",
            "\n",
            "Epoch 09792: loss did not improve from 0.00362\n",
            "Epoch 9793/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0819 - val_mse: 0.0819\n",
            "\n",
            "Epoch 09793: loss improved from 0.00362 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9794/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0818 - val_mse: 0.0818\n",
            "\n",
            "Epoch 09794: loss did not improve from 0.00361\n",
            "Epoch 9795/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0819 - val_mse: 0.0819\n",
            "\n",
            "Epoch 09795: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9796/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0818 - val_mse: 0.0818\n",
            "\n",
            "Epoch 09796: loss did not improve from 0.00361\n",
            "Epoch 9797/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0819 - val_mse: 0.0819\n",
            "\n",
            "Epoch 09797: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9798/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0819 - val_mse: 0.0819\n",
            "\n",
            "Epoch 09798: loss did not improve from 0.00361\n",
            "Epoch 9799/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0819 - val_mse: 0.0819\n",
            "\n",
            "Epoch 09799: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9800/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0819 - val_mse: 0.0819\n",
            "\n",
            "Epoch 09800: loss did not improve from 0.00361\n",
            "Epoch 9801/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0820 - val_mse: 0.0820\n",
            "\n",
            "Epoch 09801: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9802/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0819 - val_mse: 0.0819\n",
            "\n",
            "Epoch 09802: loss did not improve from 0.00361\n",
            "Epoch 9803/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0820 - val_mse: 0.0820\n",
            "\n",
            "Epoch 09803: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9804/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0819 - val_mse: 0.0819\n",
            "\n",
            "Epoch 09804: loss did not improve from 0.00361\n",
            "Epoch 9805/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0820 - val_mse: 0.0820\n",
            "\n",
            "Epoch 09805: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9806/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0819 - val_mse: 0.0819\n",
            "\n",
            "Epoch 09806: loss did not improve from 0.00361\n",
            "Epoch 9807/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0820 - val_mse: 0.0820\n",
            "\n",
            "Epoch 09807: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9808/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0819 - val_mse: 0.0819\n",
            "\n",
            "Epoch 09808: loss did not improve from 0.00361\n",
            "Epoch 9809/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0820 - val_mse: 0.0820\n",
            "\n",
            "Epoch 09809: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9810/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0819 - val_mse: 0.0819\n",
            "\n",
            "Epoch 09810: loss did not improve from 0.00361\n",
            "Epoch 9811/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0820 - val_mse: 0.0820\n",
            "\n",
            "Epoch 09811: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9812/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0819 - val_mse: 0.0819\n",
            "\n",
            "Epoch 09812: loss did not improve from 0.00361\n",
            "Epoch 9813/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0820 - val_mse: 0.0820\n",
            "\n",
            "Epoch 09813: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9814/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0819 - val_mse: 0.0819\n",
            "\n",
            "Epoch 09814: loss did not improve from 0.00361\n",
            "Epoch 9815/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0820 - val_mse: 0.0820\n",
            "\n",
            "Epoch 09815: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9816/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0820 - val_mse: 0.0820\n",
            "\n",
            "Epoch 09816: loss did not improve from 0.00361\n",
            "Epoch 9817/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0820 - val_mse: 0.0820\n",
            "\n",
            "Epoch 09817: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9818/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0820 - val_mse: 0.0820\n",
            "\n",
            "Epoch 09818: loss did not improve from 0.00361\n",
            "Epoch 9819/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0820 - val_mse: 0.0820\n",
            "\n",
            "Epoch 09819: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9820/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0820 - val_mse: 0.0820\n",
            "\n",
            "Epoch 09820: loss did not improve from 0.00361\n",
            "Epoch 9821/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0821 - val_mse: 0.0821\n",
            "\n",
            "Epoch 09821: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9822/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0820 - val_mse: 0.0820\n",
            "\n",
            "Epoch 09822: loss did not improve from 0.00361\n",
            "Epoch 9823/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0821 - val_mse: 0.0821\n",
            "\n",
            "Epoch 09823: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9824/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0820 - val_mse: 0.0820\n",
            "\n",
            "Epoch 09824: loss did not improve from 0.00361\n",
            "Epoch 9825/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0821 - val_mse: 0.0821\n",
            "\n",
            "Epoch 09825: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9826/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0820 - val_mse: 0.0820\n",
            "\n",
            "Epoch 09826: loss did not improve from 0.00361\n",
            "Epoch 9827/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0821 - val_mse: 0.0821\n",
            "\n",
            "Epoch 09827: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9828/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0820 - val_mse: 0.0820\n",
            "\n",
            "Epoch 09828: loss did not improve from 0.00361\n",
            "Epoch 9829/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0821 - val_mse: 0.0821\n",
            "\n",
            "Epoch 09829: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9830/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0820 - val_mse: 0.0820\n",
            "\n",
            "Epoch 09830: loss did not improve from 0.00361\n",
            "Epoch 9831/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0821 - val_mse: 0.0821\n",
            "\n",
            "Epoch 09831: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9832/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0820 - val_mse: 0.0820\n",
            "\n",
            "Epoch 09832: loss did not improve from 0.00361\n",
            "Epoch 9833/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0821 - val_mse: 0.0821\n",
            "\n",
            "Epoch 09833: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9834/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0820 - val_mse: 0.0820\n",
            "\n",
            "Epoch 09834: loss did not improve from 0.00361\n",
            "Epoch 9835/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0821 - val_mse: 0.0821\n",
            "\n",
            "Epoch 09835: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9836/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0821 - val_mse: 0.0821\n",
            "\n",
            "Epoch 09836: loss did not improve from 0.00361\n",
            "Epoch 9837/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0821 - val_mse: 0.0821\n",
            "\n",
            "Epoch 09837: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9838/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0821 - val_mse: 0.0821\n",
            "\n",
            "Epoch 09838: loss did not improve from 0.00361\n",
            "Epoch 9839/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0821 - val_mse: 0.0821\n",
            "\n",
            "Epoch 09839: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9840/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0821 - val_mse: 0.0821\n",
            "\n",
            "Epoch 09840: loss did not improve from 0.00361\n",
            "Epoch 9841/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0822 - val_mse: 0.0822\n",
            "\n",
            "Epoch 09841: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9842/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0821 - val_mse: 0.0821\n",
            "\n",
            "Epoch 09842: loss did not improve from 0.00361\n",
            "Epoch 9843/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0822 - val_mse: 0.0822\n",
            "\n",
            "Epoch 09843: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9844/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0821 - val_mse: 0.0821\n",
            "\n",
            "Epoch 09844: loss did not improve from 0.00361\n",
            "Epoch 9845/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0822 - val_mse: 0.0822\n",
            "\n",
            "Epoch 09845: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9846/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0821 - val_mse: 0.0821\n",
            "\n",
            "Epoch 09846: loss did not improve from 0.00361\n",
            "Epoch 9847/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0822 - val_mse: 0.0822\n",
            "\n",
            "Epoch 09847: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9848/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0821 - val_mse: 0.0821\n",
            "\n",
            "Epoch 09848: loss did not improve from 0.00361\n",
            "Epoch 9849/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0822 - val_mse: 0.0822\n",
            "\n",
            "Epoch 09849: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9850/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0821 - val_mse: 0.0821\n",
            "\n",
            "Epoch 09850: loss did not improve from 0.00361\n",
            "Epoch 9851/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0822 - val_mse: 0.0822\n",
            "\n",
            "Epoch 09851: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9852/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0821 - val_mse: 0.0821\n",
            "\n",
            "Epoch 09852: loss did not improve from 0.00361\n",
            "Epoch 9853/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0822 - val_mse: 0.0822\n",
            "\n",
            "Epoch 09853: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9854/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0821 - val_mse: 0.0821\n",
            "\n",
            "Epoch 09854: loss did not improve from 0.00361\n",
            "Epoch 9855/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0822 - val_mse: 0.0822\n",
            "\n",
            "Epoch 09855: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9856/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0822 - val_mse: 0.0822\n",
            "\n",
            "Epoch 09856: loss did not improve from 0.00361\n",
            "Epoch 9857/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0822 - val_mse: 0.0822\n",
            "\n",
            "Epoch 09857: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9858/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0822 - val_mse: 0.0822\n",
            "\n",
            "Epoch 09858: loss did not improve from 0.00361\n",
            "Epoch 9859/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0823 - val_mse: 0.0823\n",
            "\n",
            "Epoch 09859: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9860/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0822 - val_mse: 0.0822\n",
            "\n",
            "Epoch 09860: loss did not improve from 0.00361\n",
            "Epoch 9861/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0823 - val_mse: 0.0823\n",
            "\n",
            "Epoch 09861: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9862/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0822 - val_mse: 0.0822\n",
            "\n",
            "Epoch 09862: loss did not improve from 0.00361\n",
            "Epoch 9863/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0823 - val_mse: 0.0823\n",
            "\n",
            "Epoch 09863: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9864/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0822 - val_mse: 0.0822\n",
            "\n",
            "Epoch 09864: loss did not improve from 0.00361\n",
            "Epoch 9865/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0823 - val_mse: 0.0823\n",
            "\n",
            "Epoch 09865: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9866/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0822 - val_mse: 0.0822\n",
            "\n",
            "Epoch 09866: loss did not improve from 0.00361\n",
            "Epoch 9867/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0823 - val_mse: 0.0823\n",
            "\n",
            "Epoch 09867: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9868/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0822 - val_mse: 0.0822\n",
            "\n",
            "Epoch 09868: loss did not improve from 0.00361\n",
            "Epoch 9869/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0823 - val_mse: 0.0823\n",
            "\n",
            "Epoch 09869: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9870/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0822 - val_mse: 0.0822\n",
            "\n",
            "Epoch 09870: loss did not improve from 0.00361\n",
            "Epoch 9871/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0823 - val_mse: 0.0823\n",
            "\n",
            "Epoch 09871: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9872/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0822 - val_mse: 0.0822\n",
            "\n",
            "Epoch 09872: loss did not improve from 0.00361\n",
            "Epoch 9873/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0823 - val_mse: 0.0823\n",
            "\n",
            "Epoch 09873: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9874/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0823 - val_mse: 0.0823\n",
            "\n",
            "Epoch 09874: loss did not improve from 0.00361\n",
            "Epoch 9875/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0823 - val_mse: 0.0823\n",
            "\n",
            "Epoch 09875: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9876/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0823 - val_mse: 0.0823\n",
            "\n",
            "Epoch 09876: loss did not improve from 0.00361\n",
            "Epoch 9877/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0823 - val_mse: 0.0823\n",
            "\n",
            "Epoch 09877: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9878/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0823 - val_mse: 0.0823\n",
            "\n",
            "Epoch 09878: loss did not improve from 0.00361\n",
            "Epoch 9879/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0824 - val_mse: 0.0824\n",
            "\n",
            "Epoch 09879: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9880/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0823 - val_mse: 0.0823\n",
            "\n",
            "Epoch 09880: loss did not improve from 0.00361\n",
            "Epoch 9881/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0824 - val_mse: 0.0824\n",
            "\n",
            "Epoch 09881: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9882/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0823 - val_mse: 0.0823\n",
            "\n",
            "Epoch 09882: loss did not improve from 0.00361\n",
            "Epoch 9883/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0824 - val_mse: 0.0824\n",
            "\n",
            "Epoch 09883: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9884/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0823 - val_mse: 0.0823\n",
            "\n",
            "Epoch 09884: loss did not improve from 0.00361\n",
            "Epoch 9885/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0824 - val_mse: 0.0824\n",
            "\n",
            "Epoch 09885: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9886/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0823 - val_mse: 0.0823\n",
            "\n",
            "Epoch 09886: loss did not improve from 0.00361\n",
            "Epoch 9887/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0824 - val_mse: 0.0824\n",
            "\n",
            "Epoch 09887: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9888/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0823 - val_mse: 0.0823\n",
            "\n",
            "Epoch 09888: loss did not improve from 0.00361\n",
            "Epoch 9889/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0824 - val_mse: 0.0824\n",
            "\n",
            "Epoch 09889: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9890/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0823 - val_mse: 0.0823\n",
            "\n",
            "Epoch 09890: loss did not improve from 0.00361\n",
            "Epoch 9891/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0824 - val_mse: 0.0824\n",
            "\n",
            "Epoch 09891: loss improved from 0.00361 to 0.00361, saving model to poids_train.hdf5\n",
            "Epoch 9892/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0823 - val_mse: 0.0823\n",
            "\n",
            "Epoch 09892: loss did not improve from 0.00361\n",
            "Epoch 9893/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0824 - val_mse: 0.0824\n",
            "\n",
            "Epoch 09893: loss improved from 0.00361 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9894/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0824 - val_mse: 0.0824\n",
            "\n",
            "Epoch 09894: loss did not improve from 0.00360\n",
            "Epoch 9895/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0824 - val_mse: 0.0824\n",
            "\n",
            "Epoch 09895: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9896/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0824 - val_mse: 0.0824\n",
            "\n",
            "Epoch 09896: loss did not improve from 0.00360\n",
            "Epoch 9897/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0824 - val_mse: 0.0824\n",
            "\n",
            "Epoch 09897: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9898/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0824 - val_mse: 0.0824\n",
            "\n",
            "Epoch 09898: loss did not improve from 0.00360\n",
            "Epoch 9899/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0825 - val_mse: 0.0825\n",
            "\n",
            "Epoch 09899: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9900/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0824 - val_mse: 0.0824\n",
            "\n",
            "Epoch 09900: loss did not improve from 0.00360\n",
            "Epoch 9901/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0825 - val_mse: 0.0825\n",
            "\n",
            "Epoch 09901: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9902/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0824 - val_mse: 0.0824\n",
            "\n",
            "Epoch 09902: loss did not improve from 0.00360\n",
            "Epoch 9903/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0825 - val_mse: 0.0825\n",
            "\n",
            "Epoch 09903: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9904/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0824 - val_mse: 0.0824\n",
            "\n",
            "Epoch 09904: loss did not improve from 0.00360\n",
            "Epoch 9905/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0825 - val_mse: 0.0825\n",
            "\n",
            "Epoch 09905: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9906/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0824 - val_mse: 0.0824\n",
            "\n",
            "Epoch 09906: loss did not improve from 0.00360\n",
            "Epoch 9907/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0825 - val_mse: 0.0825\n",
            "\n",
            "Epoch 09907: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9908/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0824 - val_mse: 0.0824\n",
            "\n",
            "Epoch 09908: loss did not improve from 0.00360\n",
            "Epoch 9909/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0825 - val_mse: 0.0825\n",
            "\n",
            "Epoch 09909: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9910/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0824 - val_mse: 0.0824\n",
            "\n",
            "Epoch 09910: loss did not improve from 0.00360\n",
            "Epoch 9911/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0825 - val_mse: 0.0825\n",
            "\n",
            "Epoch 09911: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9912/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0825 - val_mse: 0.0825\n",
            "\n",
            "Epoch 09912: loss did not improve from 0.00360\n",
            "Epoch 9913/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0825 - val_mse: 0.0825\n",
            "\n",
            "Epoch 09913: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9914/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0825 - val_mse: 0.0825\n",
            "\n",
            "Epoch 09914: loss did not improve from 0.00360\n",
            "Epoch 9915/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0825 - val_mse: 0.0825\n",
            "\n",
            "Epoch 09915: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9916/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0825 - val_mse: 0.0825\n",
            "\n",
            "Epoch 09916: loss did not improve from 0.00360\n",
            "Epoch 9917/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0826 - val_mse: 0.0826\n",
            "\n",
            "Epoch 09917: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9918/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0825 - val_mse: 0.0825\n",
            "\n",
            "Epoch 09918: loss did not improve from 0.00360\n",
            "Epoch 9919/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0826 - val_mse: 0.0826\n",
            "\n",
            "Epoch 09919: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9920/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0825 - val_mse: 0.0825\n",
            "\n",
            "Epoch 09920: loss did not improve from 0.00360\n",
            "Epoch 9921/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0826 - val_mse: 0.0826\n",
            "\n",
            "Epoch 09921: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9922/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0825 - val_mse: 0.0825\n",
            "\n",
            "Epoch 09922: loss did not improve from 0.00360\n",
            "Epoch 9923/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0826 - val_mse: 0.0826\n",
            "\n",
            "Epoch 09923: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9924/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0825 - val_mse: 0.0825\n",
            "\n",
            "Epoch 09924: loss did not improve from 0.00360\n",
            "Epoch 9925/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0826 - val_mse: 0.0826\n",
            "\n",
            "Epoch 09925: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9926/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0825 - val_mse: 0.0825\n",
            "\n",
            "Epoch 09926: loss did not improve from 0.00360\n",
            "Epoch 9927/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0826 - val_mse: 0.0826\n",
            "\n",
            "Epoch 09927: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9928/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0825 - val_mse: 0.0825\n",
            "\n",
            "Epoch 09928: loss did not improve from 0.00360\n",
            "Epoch 9929/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0826 - val_mse: 0.0826\n",
            "\n",
            "Epoch 09929: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9930/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0825 - val_mse: 0.0825\n",
            "\n",
            "Epoch 09930: loss did not improve from 0.00360\n",
            "Epoch 9931/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0826 - val_mse: 0.0826\n",
            "\n",
            "Epoch 09931: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9932/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0826 - val_mse: 0.0826\n",
            "\n",
            "Epoch 09932: loss did not improve from 0.00360\n",
            "Epoch 9933/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0826 - val_mse: 0.0826\n",
            "\n",
            "Epoch 09933: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9934/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0826 - val_mse: 0.0826\n",
            "\n",
            "Epoch 09934: loss did not improve from 0.00360\n",
            "Epoch 9935/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0826 - val_mse: 0.0826\n",
            "\n",
            "Epoch 09935: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9936/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0826 - val_mse: 0.0826\n",
            "\n",
            "Epoch 09936: loss did not improve from 0.00360\n",
            "Epoch 9937/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0827 - val_mse: 0.0827\n",
            "\n",
            "Epoch 09937: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9938/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0826 - val_mse: 0.0826\n",
            "\n",
            "Epoch 09938: loss did not improve from 0.00360\n",
            "Epoch 9939/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0827 - val_mse: 0.0827\n",
            "\n",
            "Epoch 09939: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9940/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0826 - val_mse: 0.0826\n",
            "\n",
            "Epoch 09940: loss did not improve from 0.00360\n",
            "Epoch 9941/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0827 - val_mse: 0.0827\n",
            "\n",
            "Epoch 09941: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9942/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0826 - val_mse: 0.0826\n",
            "\n",
            "Epoch 09942: loss did not improve from 0.00360\n",
            "Epoch 9943/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0827 - val_mse: 0.0827\n",
            "\n",
            "Epoch 09943: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9944/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0826 - val_mse: 0.0826\n",
            "\n",
            "Epoch 09944: loss did not improve from 0.00360\n",
            "Epoch 9945/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0827 - val_mse: 0.0827\n",
            "\n",
            "Epoch 09945: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9946/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0826 - val_mse: 0.0826\n",
            "\n",
            "Epoch 09946: loss did not improve from 0.00360\n",
            "Epoch 9947/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0827 - val_mse: 0.0827\n",
            "\n",
            "Epoch 09947: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9948/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0826 - val_mse: 0.0826\n",
            "\n",
            "Epoch 09948: loss did not improve from 0.00360\n",
            "Epoch 9949/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0827 - val_mse: 0.0827\n",
            "\n",
            "Epoch 09949: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9950/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0826 - val_mse: 0.0826\n",
            "\n",
            "Epoch 09950: loss did not improve from 0.00360\n",
            "Epoch 9951/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0827 - val_mse: 0.0827\n",
            "\n",
            "Epoch 09951: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9952/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0827 - val_mse: 0.0827\n",
            "\n",
            "Epoch 09952: loss did not improve from 0.00360\n",
            "Epoch 9953/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0827 - val_mse: 0.0827\n",
            "\n",
            "Epoch 09953: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9954/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0827 - val_mse: 0.0827\n",
            "\n",
            "Epoch 09954: loss did not improve from 0.00360\n",
            "Epoch 9955/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0827 - val_mse: 0.0827\n",
            "\n",
            "Epoch 09955: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9956/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0827 - val_mse: 0.0827\n",
            "\n",
            "Epoch 09956: loss did not improve from 0.00360\n",
            "Epoch 9957/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0828 - val_mse: 0.0828\n",
            "\n",
            "Epoch 09957: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9958/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0827 - val_mse: 0.0827\n",
            "\n",
            "Epoch 09958: loss did not improve from 0.00360\n",
            "Epoch 9959/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0828 - val_mse: 0.0828\n",
            "\n",
            "Epoch 09959: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9960/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0827 - val_mse: 0.0827\n",
            "\n",
            "Epoch 09960: loss did not improve from 0.00360\n",
            "Epoch 9961/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0828 - val_mse: 0.0828\n",
            "\n",
            "Epoch 09961: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9962/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0827 - val_mse: 0.0827\n",
            "\n",
            "Epoch 09962: loss did not improve from 0.00360\n",
            "Epoch 9963/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0828 - val_mse: 0.0828\n",
            "\n",
            "Epoch 09963: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9964/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0827 - val_mse: 0.0827\n",
            "\n",
            "Epoch 09964: loss did not improve from 0.00360\n",
            "Epoch 9965/10000\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0828 - val_mse: 0.0828\n",
            "\n",
            "Epoch 09965: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9966/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0827 - val_mse: 0.0827\n",
            "\n",
            "Epoch 09966: loss did not improve from 0.00360\n",
            "Epoch 9967/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0828 - val_mse: 0.0828\n",
            "\n",
            "Epoch 09967: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9968/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0827 - val_mse: 0.0827\n",
            "\n",
            "Epoch 09968: loss did not improve from 0.00360\n",
            "Epoch 9969/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0828 - val_mse: 0.0828\n",
            "\n",
            "Epoch 09969: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9970/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0827 - val_mse: 0.0827\n",
            "\n",
            "Epoch 09970: loss did not improve from 0.00360\n",
            "Epoch 9971/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0828 - val_mse: 0.0828\n",
            "\n",
            "Epoch 09971: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9972/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0828 - val_mse: 0.0828\n",
            "\n",
            "Epoch 09972: loss did not improve from 0.00360\n",
            "Epoch 9973/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0828 - val_mse: 0.0828\n",
            "\n",
            "Epoch 09973: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9974/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0828 - val_mse: 0.0828\n",
            "\n",
            "Epoch 09974: loss did not improve from 0.00360\n",
            "Epoch 9975/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0829 - val_mse: 0.0829\n",
            "\n",
            "Epoch 09975: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9976/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0828 - val_mse: 0.0828\n",
            "\n",
            "Epoch 09976: loss did not improve from 0.00360\n",
            "Epoch 9977/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0829 - val_mse: 0.0829\n",
            "\n",
            "Epoch 09977: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9978/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0828 - val_mse: 0.0828\n",
            "\n",
            "Epoch 09978: loss did not improve from 0.00360\n",
            "Epoch 9979/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0829 - val_mse: 0.0829\n",
            "\n",
            "Epoch 09979: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9980/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0828 - val_mse: 0.0828\n",
            "\n",
            "Epoch 09980: loss did not improve from 0.00360\n",
            "Epoch 9981/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0829 - val_mse: 0.0829\n",
            "\n",
            "Epoch 09981: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9982/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0828 - val_mse: 0.0828\n",
            "\n",
            "Epoch 09982: loss did not improve from 0.00360\n",
            "Epoch 9983/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0829 - val_mse: 0.0829\n",
            "\n",
            "Epoch 09983: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9984/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0828 - val_mse: 0.0828\n",
            "\n",
            "Epoch 09984: loss did not improve from 0.00360\n",
            "Epoch 9985/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0829 - val_mse: 0.0829\n",
            "\n",
            "Epoch 09985: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9986/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0828 - val_mse: 0.0828\n",
            "\n",
            "Epoch 09986: loss did not improve from 0.00360\n",
            "Epoch 9987/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0829 - val_mse: 0.0829\n",
            "\n",
            "Epoch 09987: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9988/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0828 - val_mse: 0.0828\n",
            "\n",
            "Epoch 09988: loss did not improve from 0.00360\n",
            "Epoch 9989/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0829 - val_mse: 0.0829\n",
            "\n",
            "Epoch 09989: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9990/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0829 - val_mse: 0.0829\n",
            "\n",
            "Epoch 09990: loss did not improve from 0.00360\n",
            "Epoch 9991/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0829 - val_mse: 0.0829\n",
            "\n",
            "Epoch 09991: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9992/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0829 - val_mse: 0.0829\n",
            "\n",
            "Epoch 09992: loss did not improve from 0.00360\n",
            "Epoch 9993/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0829 - val_mse: 0.0829\n",
            "\n",
            "Epoch 09993: loss improved from 0.00360 to 0.00360, saving model to poids_train.hdf5\n",
            "Epoch 9994/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0829 - val_mse: 0.0829\n",
            "\n",
            "Epoch 09994: loss did not improve from 0.00360\n",
            "Epoch 9995/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0830 - val_mse: 0.0830\n",
            "\n",
            "Epoch 09995: loss improved from 0.00360 to 0.00359, saving model to poids_train.hdf5\n",
            "Epoch 9996/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0829 - val_mse: 0.0829\n",
            "\n",
            "Epoch 09996: loss did not improve from 0.00359\n",
            "Epoch 9997/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0830 - val_mse: 0.0830\n",
            "\n",
            "Epoch 09997: loss improved from 0.00359 to 0.00359, saving model to poids_train.hdf5\n",
            "Epoch 9998/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0829 - val_mse: 0.0829\n",
            "\n",
            "Epoch 09998: loss did not improve from 0.00359\n",
            "Epoch 9999/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0830 - val_mse: 0.0830\n",
            "\n",
            "Epoch 09999: loss improved from 0.00359 to 0.00359, saving model to poids_train.hdf5\n",
            "Epoch 10000/10000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0829 - val_mse: 0.0829\n",
            "\n",
            "Epoch 10000: loss did not improve from 0.00359\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfbomV0LS9LD"
      },
      "source": [
        "model.load_weights(\"poids_train.hdf5\")"
      ],
      "execution_count": 570,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MDY8O1-l6kN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "b89478db-b4c3-4a57-f0fb-9855caebb2dc"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "#erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "#plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": 441,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, \"Evolution de l'erreur en fonction de la période\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 441
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEuSDQ6vZnBm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d663ba72-ca26-42ee-8e1a-8a595d40d8b4"
      },
      "source": [
        "# Evaluation du modèle\n",
        "\n",
        "model.evaluate(dataset)\n",
        "#model.evaluate(dataset_val)\n"
      ],
      "execution_count": 353,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0035 - mse: 0.0035\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0035034799948334694, 0.0035034799948334694]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 353
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbzro22hgt4b"
      },
      "source": [
        "**3. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMmVn1e5zEAm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a3a194d-dd29-4318-e543-74230777607b"
      },
      "source": [
        "# Création des instants d'entrainement et de validation\n",
        "y_train_timing = serie_entrainement.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_train)]\n",
        "y_val_timing = serie_test.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_val)]\n",
        "\n",
        "# Calcul des prédictions\n",
        "pred_ent = model.predict(x_train, verbose=1)\n",
        "pred_val = model.predict(x_val, verbose=1)"
      ],
      "execution_count": 554,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12/12 [==============================] - 0s 1ms/step\n",
            "1/1 [==============================] - 0s 11ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvAclrEVLW97"
      },
      "source": [
        "from statsmodels.tsa.statespace.tools import diff\n",
        "\n",
        "predictions=[]\n",
        "\n",
        "for t in range(0,serie_entrainement.size-taille_fenetre):\n",
        "  data_to_predict = serie_entrainement[t:t+taille_fenetre]\n",
        "\n",
        "  data_to_predict = tf.expand_dims(data_to_predict,axis=1)\n",
        "  data_to_predict = tf.expand_dims(data_to_predict,axis=0)\n",
        "  pred = model.predict(data_to_predict)\n",
        "  predictions.append(pred[0])"
      ],
      "execution_count": 571,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jAO9WGrWsuk",
        "outputId": "be717358-3d41-4d45-a6c9-8d05a4139212",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tmax"
      ],
      "execution_count": 542,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 542
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ODOI0qEW2DX",
        "outputId": "dc4e9d23-d056-42ae-b6a6-b10d13a162ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.asarray(predictions)[:,0]"
      ],
      "execution_count": 543,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.30793598], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 543
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TACq-1J4MNRy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "e3a614f5-cba1-43be-ff5d-c0c9da0c4d81"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "tmax = len(np.asarray(predictions)[:,0])\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=df_paris.index[taille_fenetre+horizon:taille_fenetre+tmax+horizon],y=np.asarray(predictions)[:,0],line=dict(color='red', width=1),name=\"prediction\"))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_paris.index,y=serie_entrainement,line=dict(color='black', width=1),name=\"true\"))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": 572,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"f3bc3810-1f8c-4c27-bf95-1e877d969e76\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"f3bc3810-1f8c-4c27-bf95-1e877d969e76\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'f3bc3810-1f8c-4c27-bf95-1e877d969e76',\n",
              "                        [{\"line\": {\"color\": \"red\", \"width\": 1}, \"name\": \"prediction\", \"type\": \"scatter\", \"x\": [\"2020-03-28T00:00:00\", \"2020-03-29T00:00:00\", \"2020-03-30T00:00:00\", \"2020-03-31T00:00:00\", \"2020-04-01T00:00:00\", \"2020-04-02T00:00:00\", \"2020-04-03T00:00:00\", \"2020-04-04T00:00:00\", \"2020-04-05T00:00:00\", \"2020-04-06T00:00:00\", \"2020-04-07T00:00:00\", \"2020-04-08T00:00:00\", \"2020-04-09T00:00:00\", \"2020-04-10T00:00:00\", \"2020-04-11T00:00:00\", \"2020-04-12T00:00:00\", \"2020-04-13T00:00:00\", \"2020-04-14T00:00:00\", \"2020-04-15T00:00:00\", \"2020-04-16T00:00:00\", \"2020-04-17T00:00:00\", \"2020-04-18T00:00:00\", \"2020-04-19T00:00:00\", \"2020-04-20T00:00:00\", \"2020-04-21T00:00:00\", \"2020-04-22T00:00:00\", \"2020-04-23T00:00:00\", \"2020-04-24T00:00:00\", \"2020-04-25T00:00:00\", \"2020-04-26T00:00:00\", \"2020-04-27T00:00:00\", \"2020-04-28T00:00:00\", \"2020-04-29T00:00:00\", \"2020-04-30T00:00:00\", \"2020-05-01T00:00:00\", \"2020-05-02T00:00:00\", \"2020-05-03T00:00:00\", \"2020-05-04T00:00:00\", \"2020-05-05T00:00:00\", \"2020-05-06T00:00:00\", \"2020-05-07T00:00:00\", \"2020-05-08T00:00:00\", \"2020-05-09T00:00:00\", \"2020-05-10T00:00:00\", \"2020-05-11T00:00:00\", \"2020-05-12T00:00:00\", \"2020-05-13T00:00:00\", \"2020-05-14T00:00:00\", \"2020-05-15T00:00:00\", \"2020-05-16T00:00:00\", \"2020-05-17T00:00:00\", \"2020-05-18T00:00:00\", \"2020-05-19T00:00:00\", \"2020-05-20T00:00:00\", \"2020-05-21T00:00:00\", \"2020-05-22T00:00:00\", \"2020-05-23T00:00:00\", \"2020-05-24T00:00:00\", \"2020-05-25T00:00:00\", \"2020-05-26T00:00:00\", \"2020-05-27T00:00:00\", \"2020-05-28T00:00:00\", \"2020-05-29T00:00:00\", \"2020-05-30T00:00:00\", \"2020-05-31T00:00:00\", \"2020-06-01T00:00:00\", \"2020-06-02T00:00:00\", \"2020-06-03T00:00:00\", \"2020-06-04T00:00:00\", \"2020-06-05T00:00:00\", \"2020-06-06T00:00:00\", \"2020-06-07T00:00:00\", \"2020-06-08T00:00:00\", \"2020-06-09T00:00:00\", \"2020-06-10T00:00:00\", \"2020-06-11T00:00:00\", \"2020-06-12T00:00:00\", \"2020-06-13T00:00:00\", \"2020-06-14T00:00:00\", \"2020-06-15T00:00:00\", \"2020-06-16T00:00:00\", \"2020-06-17T00:00:00\", \"2020-06-18T00:00:00\", \"2020-06-19T00:00:00\", \"2020-06-20T00:00:00\", \"2020-06-21T00:00:00\", \"2020-06-22T00:00:00\", \"2020-06-23T00:00:00\", \"2020-06-24T00:00:00\", \"2020-06-25T00:00:00\", \"2020-06-26T00:00:00\", \"2020-06-27T00:00:00\", \"2020-06-28T00:00:00\", \"2020-06-29T00:00:00\", \"2020-06-30T00:00:00\", \"2020-07-01T00:00:00\", \"2020-07-02T00:00:00\", \"2020-07-03T00:00:00\", \"2020-07-04T00:00:00\", \"2020-07-05T00:00:00\", \"2020-07-06T00:00:00\", \"2020-07-07T00:00:00\", \"2020-07-08T00:00:00\", \"2020-07-09T00:00:00\", \"2020-07-10T00:00:00\", \"2020-07-11T00:00:00\", \"2020-07-12T00:00:00\", \"2020-07-13T00:00:00\", \"2020-07-14T00:00:00\", \"2020-07-15T00:00:00\", \"2020-07-16T00:00:00\", \"2020-07-17T00:00:00\", \"2020-07-18T00:00:00\", \"2020-07-19T00:00:00\", \"2020-07-20T00:00:00\", \"2020-07-21T00:00:00\", \"2020-07-22T00:00:00\", \"2020-07-23T00:00:00\", \"2020-07-24T00:00:00\", \"2020-07-25T00:00:00\", \"2020-07-26T00:00:00\", \"2020-07-27T00:00:00\", \"2020-07-28T00:00:00\", \"2020-07-29T00:00:00\", \"2020-07-30T00:00:00\", \"2020-07-31T00:00:00\", \"2020-08-01T00:00:00\", \"2020-08-02T00:00:00\", \"2020-08-03T00:00:00\", \"2020-08-04T00:00:00\", \"2020-08-05T00:00:00\", \"2020-08-06T00:00:00\", \"2020-08-07T00:00:00\", \"2020-08-08T00:00:00\", \"2020-08-09T00:00:00\", \"2020-08-10T00:00:00\", \"2020-08-11T00:00:00\", \"2020-08-12T00:00:00\", \"2020-08-13T00:00:00\", \"2020-08-14T00:00:00\", \"2020-08-15T00:00:00\", \"2020-08-16T00:00:00\", \"2020-08-17T00:00:00\", \"2020-08-18T00:00:00\", \"2020-08-19T00:00:00\", \"2020-08-20T00:00:00\", \"2020-08-21T00:00:00\", \"2020-08-22T00:00:00\", \"2020-08-23T00:00:00\", \"2020-08-24T00:00:00\", \"2020-08-25T00:00:00\", \"2020-08-26T00:00:00\", \"2020-08-27T00:00:00\", \"2020-08-28T00:00:00\", \"2020-08-29T00:00:00\", \"2020-08-30T00:00:00\", \"2020-08-31T00:00:00\", \"2020-09-01T00:00:00\", \"2020-09-02T00:00:00\", \"2020-09-03T00:00:00\", \"2020-09-04T00:00:00\", \"2020-09-05T00:00:00\", \"2020-09-06T00:00:00\", \"2020-09-07T00:00:00\", \"2020-09-08T00:00:00\", \"2020-09-09T00:00:00\", \"2020-09-10T00:00:00\", \"2020-09-11T00:00:00\", \"2020-09-12T00:00:00\", \"2020-09-13T00:00:00\", \"2020-09-14T00:00:00\", \"2020-09-15T00:00:00\", \"2020-09-16T00:00:00\", \"2020-09-17T00:00:00\", \"2020-09-18T00:00:00\", \"2020-09-19T00:00:00\", \"2020-09-20T00:00:00\", \"2020-09-21T00:00:00\", \"2020-09-22T00:00:00\", \"2020-09-23T00:00:00\", \"2020-09-24T00:00:00\", \"2020-09-25T00:00:00\", \"2020-09-26T00:00:00\", \"2020-09-27T00:00:00\", \"2020-09-28T00:00:00\", \"2020-09-29T00:00:00\", \"2020-09-30T00:00:00\", \"2020-10-01T00:00:00\", \"2020-10-02T00:00:00\", \"2020-10-03T00:00:00\", \"2020-10-04T00:00:00\", \"2020-10-05T00:00:00\", \"2020-10-06T00:00:00\", \"2020-10-07T00:00:00\", \"2020-10-08T00:00:00\", \"2020-10-09T00:00:00\", \"2020-10-10T00:00:00\", \"2020-10-11T00:00:00\", \"2020-10-12T00:00:00\", \"2020-10-13T00:00:00\", \"2020-10-14T00:00:00\", \"2020-10-15T00:00:00\", \"2020-10-16T00:00:00\", \"2020-10-17T00:00:00\", \"2020-10-18T00:00:00\", \"2020-10-19T00:00:00\", \"2020-10-20T00:00:00\", \"2020-10-21T00:00:00\", \"2020-10-22T00:00:00\", \"2020-10-23T00:00:00\", \"2020-10-24T00:00:00\", \"2020-10-25T00:00:00\", \"2020-10-26T00:00:00\", \"2020-10-27T00:00:00\", \"2020-10-28T00:00:00\", \"2020-10-29T00:00:00\", \"2020-10-30T00:00:00\", \"2020-10-31T00:00:00\", \"2020-11-01T00:00:00\", \"2020-11-02T00:00:00\", \"2020-11-03T00:00:00\", \"2020-11-04T00:00:00\", \"2020-11-05T00:00:00\", \"2020-11-06T00:00:00\", \"2020-11-07T00:00:00\", \"2020-11-08T00:00:00\", \"2020-11-09T00:00:00\", \"2020-11-10T00:00:00\", \"2020-11-11T00:00:00\", \"2020-11-12T00:00:00\", \"2020-11-13T00:00:00\", \"2020-11-14T00:00:00\", \"2020-11-15T00:00:00\", \"2020-11-16T00:00:00\", \"2020-11-17T00:00:00\", \"2020-11-18T00:00:00\", \"2020-11-19T00:00:00\", \"2020-11-20T00:00:00\", \"2020-11-21T00:00:00\", \"2020-11-22T00:00:00\", \"2020-11-23T00:00:00\", \"2020-11-24T00:00:00\", \"2020-11-25T00:00:00\", \"2020-11-26T00:00:00\", \"2020-11-27T00:00:00\", \"2020-11-28T00:00:00\", \"2020-11-29T00:00:00\", \"2020-11-30T00:00:00\", \"2020-12-01T00:00:00\", \"2020-12-02T00:00:00\", \"2020-12-03T00:00:00\", \"2020-12-04T00:00:00\", \"2020-12-05T00:00:00\", \"2020-12-06T00:00:00\", \"2020-12-07T00:00:00\", \"2020-12-08T00:00:00\", \"2020-12-09T00:00:00\", \"2020-12-10T00:00:00\", \"2020-12-11T00:00:00\", \"2020-12-12T00:00:00\", \"2020-12-13T00:00:00\", \"2020-12-14T00:00:00\", \"2020-12-15T00:00:00\", \"2020-12-16T00:00:00\", \"2020-12-17T00:00:00\", \"2020-12-18T00:00:00\", \"2020-12-19T00:00:00\", \"2020-12-20T00:00:00\", \"2020-12-21T00:00:00\", \"2020-12-22T00:00:00\", \"2020-12-23T00:00:00\", \"2020-12-24T00:00:00\", \"2020-12-25T00:00:00\", \"2020-12-26T00:00:00\", \"2020-12-27T00:00:00\", \"2020-12-28T00:00:00\", \"2020-12-29T00:00:00\", \"2020-12-30T00:00:00\", \"2020-12-31T00:00:00\", \"2021-01-01T00:00:00\", \"2021-01-02T00:00:00\", \"2021-01-03T00:00:00\", \"2021-01-04T00:00:00\", \"2021-01-05T00:00:00\", \"2021-01-06T00:00:00\", \"2021-01-07T00:00:00\", \"2021-01-08T00:00:00\", \"2021-01-09T00:00:00\", \"2021-01-10T00:00:00\", \"2021-01-11T00:00:00\", \"2021-01-12T00:00:00\", \"2021-01-13T00:00:00\", \"2021-01-14T00:00:00\", \"2021-01-15T00:00:00\", \"2021-01-16T00:00:00\", \"2021-01-17T00:00:00\", \"2021-01-18T00:00:00\", \"2021-01-19T00:00:00\", \"2021-01-20T00:00:00\", \"2021-01-21T00:00:00\", \"2021-01-22T00:00:00\", \"2021-01-23T00:00:00\", \"2021-01-24T00:00:00\", \"2021-01-25T00:00:00\", \"2021-01-26T00:00:00\", \"2021-01-27T00:00:00\", \"2021-01-28T00:00:00\", \"2021-01-29T00:00:00\", \"2021-01-30T00:00:00\", \"2021-01-31T00:00:00\", \"2021-02-01T00:00:00\", \"2021-02-02T00:00:00\", \"2021-02-03T00:00:00\", \"2021-02-04T00:00:00\", \"2021-02-05T00:00:00\", \"2021-02-06T00:00:00\", \"2021-02-07T00:00:00\", \"2021-02-08T00:00:00\", \"2021-02-09T00:00:00\", \"2021-02-10T00:00:00\", \"2021-02-11T00:00:00\", \"2021-02-12T00:00:00\", \"2021-02-13T00:00:00\", \"2021-02-14T00:00:00\", \"2021-02-15T00:00:00\", \"2021-02-16T00:00:00\", \"2021-02-17T00:00:00\", \"2021-02-18T00:00:00\", \"2021-02-19T00:00:00\", \"2021-02-20T00:00:00\", \"2021-02-21T00:00:00\", \"2021-02-22T00:00:00\", \"2021-02-23T00:00:00\", \"2021-02-24T00:00:00\", \"2021-02-25T00:00:00\", \"2021-02-26T00:00:00\", \"2021-02-27T00:00:00\", \"2021-02-28T00:00:00\", \"2021-03-01T00:00:00\", \"2021-03-02T00:00:00\", \"2021-03-03T00:00:00\", \"2021-03-04T00:00:00\", \"2021-03-05T00:00:00\", \"2021-03-06T00:00:00\", \"2021-03-07T00:00:00\", \"2021-03-08T00:00:00\", \"2021-03-09T00:00:00\", \"2021-03-10T00:00:00\", \"2021-03-11T00:00:00\", \"2021-03-12T00:00:00\", \"2021-03-13T00:00:00\", \"2021-03-14T00:00:00\", \"2021-03-15T00:00:00\", \"2021-03-16T00:00:00\", \"2021-03-17T00:00:00\", \"2021-03-18T00:00:00\", \"2021-03-19T00:00:00\", \"2021-03-20T00:00:00\", \"2021-03-21T00:00:00\", \"2021-03-22T00:00:00\", \"2021-03-23T00:00:00\", \"2021-03-24T00:00:00\", \"2021-03-25T00:00:00\", \"2021-03-26T00:00:00\", \"2021-03-27T00:00:00\", \"2021-03-28T00:00:00\", \"2021-03-29T00:00:00\", \"2021-03-30T00:00:00\", \"2021-03-31T00:00:00\", \"2021-04-01T00:00:00\", \"2021-04-02T00:00:00\", \"2021-04-03T00:00:00\", \"2021-04-04T00:00:00\", \"2021-04-05T00:00:00\", \"2021-04-06T00:00:00\"], \"y\": [-0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.3062305450439453, -0.01495051383972168, 0.3064644932746887, 0.4846632480621338, 0.5617161393165588, 0.7947181463241577, 0.9770809412002563, 1.0568907260894775, 1.056180715560913, 1.2690300941467285, 1.599179744720459, 1.8948800563812256, 2.1741743087768555, 2.5386838912963867, 2.663766860961914, 2.7012579441070557, 3.0399649143218994, 3.3338522911071777, 3.593520402908325, 3.779583215713501, 3.813877820968628, 3.842672824859619, 3.8457815647125244, 3.9580397605895996, 4.01577615737915, 3.857938051223755, 3.731748104095459, 3.5441396236419678, 3.450899124145508, 3.4071011543273926, 3.0631089210510254, 2.6001172065734863, 1.9349024295806885, 1.64268159866333, 1.4278826713562012, 1.3367252349853516, 1.3399238586425781, 1.109433650970459, 0.868767261505127, 1.0578830242156982, 0.8139693737030029, 0.6171067357063293, 0.5523952841758728, 0.538101315498352, 0.34445488452911377, 0.22071579098701477, 0.13344994187355042, 0.09775850176811218, 0.046529412269592285, 0.05960020422935486, 0.058290958404541016, -0.024810075759887695, -0.05438429117202759, -0.07465839385986328, -0.08425724506378174, 0.037557125091552734, -0.05334830284118652, -0.12984824180603027, -0.09936845302581787, -0.05753517150878906, -0.07528644800186157, -0.08939653635025024, -0.09793496131896973, -0.042054831981658936, -0.03631335496902466, -0.07414299249649048, -0.0024156570434570312, 0.05667760968208313, 0.08449268341064453, 0.09656727313995361, 0.10980549454689026, 0.12587189674377441, 0.1256657838821411, 0.16137725114822388, 0.2112177014350891, 0.22939106822013855, -0.01787710189819336, -0.08770877122879028, -0.15805399417877197, -0.12218809127807617, 0.058708369731903076, -0.18176448345184326, -0.11328744888305664, -0.1205054521560669, -0.09677112102508545, -0.11716878414154053, -0.06994032859802246, -0.05390727519989014, -0.057033538818359375, -0.02052319049835205, 0.06122016906738281, 0.14870065450668335, 0.17933854460716248, 0.1909325122833252, 0.14451143145561218, 0.24845916032791138, 0.15582135319709778, 0.1308670938014984, 0.13968998193740845, 0.002810955047607422, 0.1625724732875824, 0.1589023470878601, 0.08703920245170593, 0.1447332501411438, 0.10043308138847351, 0.2145296335220337, 0.23634719848632812, 0.16522502899169922, 0.23803004622459412, 0.20732420682907104, 0.24031203985214233, 0.27208927273750305, 0.31755462288856506, 0.4427395462989807, 0.38145527243614197, 0.31071966886520386, 0.39551010727882385, 0.3330550193786621, 0.3155716061592102, 0.27856385707855225, 0.23430213332176208, 0.32619616389274597, 0.28051891922950745, 0.21195143461227417, 0.17713552713394165, 0.1331695318222046, 0.0800936222076416, 0.15316346287727356, 0.12733757495880127, 0.07904002070426941, 0.0558093786239624, 0.12199446558952332, 0.10245871543884277, 0.12059205770492554, 0.04191869497299194, 0.034682393074035645, 0.09961593151092529, 0.05254703760147095, 0.05717390775680542, 0.12381023168563843, 0.1379069685935974, 0.17783451080322266, 0.2624990940093994, 0.07772761583328247, 0.1693110167980194, 0.10727107524871826, 0.10343959927558899, -0.0051572322845458984, 0.03131812810897827, 0.0018818378448486328, -0.05158722400665283, -0.03188437223434448, 0.002152740955352783, 0.0667329728603363, 0.10618582367897034, 0.17441046237945557, 0.18369290232658386, 0.2529377043247223, 0.19129842519760132, 0.3638562560081482, 0.40747448801994324, 0.3545718491077423, 0.39270684123039246, 0.46179336309432983, 0.47647085785865784, 0.6029756665229797, 0.6338682174682617, 0.7576678991317749, 0.7857702970504761, 0.790477991104126, 0.8958937525749207, 0.9845747947692871, 1.0142678022384644, 1.1243863105773926, 1.1246908903121948, 1.1484830379486084, 1.1469541788101196, 0.7694979906082153, 0.8672513365745544, 0.8420654535293579, 0.7395786046981812, 0.6932218074798584, 0.6891535520553589, 0.6888476610183716, 1.0195186138153076, 0.8232522010803223, 0.791942834854126, 0.7794068455696106, 0.7124945521354675, 0.6889176368713379, 0.678371787071228, 0.6407275795936584, 0.29251372814178467, -0.05346941947937012, -0.4030361771583557, -0.7581536769866943, -0.7351583242416382, -0.7261821031570435, -0.7027316093444824, -0.659115195274353, -0.6501214504241943, -0.6450697183609009, -0.6619338989257812, -0.6916975975036621, -0.6469341516494751, -0.705716609954834, -0.739158034324646, -0.7908047437667847, -0.7853078842163086, -0.7822195291519165, -0.786850094795227, -0.8279469013214111, -0.8661080598831177, -0.8900889158248901, -0.8910869359970093, -0.8715789318084717, -0.8461931943893433, -0.8323228359222412, -0.8396780490875244, -0.8674895763397217, -0.8789184093475342, -0.8730307817459106, -0.8710663318634033, -0.881859302520752, -0.8705891370773315, -0.8521590232849121, -0.8409194946289062, -0.8460893630981445, -0.8479971885681152, -0.8429336547851562, -0.8383591175079346, -0.8482533693313599, -0.8602985143661499, -0.8728792667388916, -0.8784298896789551, -0.8823074102401733, -0.8843333721160889, -0.8865747451782227, -0.8847247362136841, -0.8832395076751709, -0.884005069732666, -0.886127233505249, -0.8880336284637451, -0.8873865604400635, -0.8873481750488281, -0.8905273675918579, -0.8961261510848999, -0.8991140127182007, -0.8930110931396484, -0.8884049654006958, -0.8904705047607422, -0.8954834938049316, -0.8927698135375977, -0.8908154964447021, -0.8870874643325806, -0.8864866495132446, -0.8902742862701416, -0.8889076709747314, -0.8840563297271729, -0.8814427852630615, -0.873151421546936, -0.8668071031570435, -0.8685939311981201, -0.8638298511505127, -0.8541885614395142, -0.8530024290084839, -0.8524855375289917, -0.8515444993972778, -0.8513652086257935, -0.8552193641662598, -0.8624593019485474, -0.8728455305099487, -0.876562237739563, -0.881759762763977, -0.8884774446487427, -0.8938767910003662, -0.8911274671554565, -0.8894689083099365, -0.89080810546875, -0.8915477991104126, -0.8913729190826416, -0.8966777324676514, -0.9019625186920166, -0.9045531749725342, -0.9042713642120361, -0.901100754737854, -0.8971948623657227, -0.8965785503387451, -0.8861092329025269, -0.8737586736679077, -0.8663312196731567, -0.8500279188156128, -0.8341842889785767, -0.8315980434417725, -0.822777271270752, -0.8073934316635132, -0.8050668239593506, -0.7956396341323853, -0.785735011100769, -0.7848497629165649, -0.7844047546386719, -0.7829937934875488, -0.7845861911773682, -0.7787826061248779, -0.7766550779342651, -0.7861416339874268, -0.789804220199585, -0.7884478569030762, -0.7770774364471436, -0.772446870803833, -0.7673598527908325, -0.7656570672988892, -0.747235894203186, -0.7419294118881226, -0.7447249889373779, -0.7431496381759644, -0.7271602153778076, -0.7075583934783936, -0.6851691007614136, -0.6555362939834595, -0.632521390914917, -0.4255605936050415, -0.5272955894470215, -0.6314147710800171, -0.6124645471572876, -0.6110992431640625, -0.5711411237716675, -0.5432686805725098, -0.817900538444519, -0.6467094421386719, -0.5073976516723633, -0.5697132349014282, -0.6408571004867554, -0.5740582942962646, -0.5391765832901001]}, {\"line\": {\"color\": \"black\", \"width\": 1}, \"name\": \"true\", \"type\": \"scatter\", \"x\": [\"2020-03-18T00:00:00\", \"2020-03-19T00:00:00\", \"2020-03-20T00:00:00\", \"2020-03-21T00:00:00\", \"2020-03-22T00:00:00\", \"2020-03-23T00:00:00\", \"2020-03-24T00:00:00\", \"2020-03-25T00:00:00\", \"2020-03-26T00:00:00\", \"2020-03-27T00:00:00\", \"2020-03-28T00:00:00\", \"2020-03-29T00:00:00\", \"2020-03-30T00:00:00\", \"2020-03-31T00:00:00\", \"2020-04-01T00:00:00\", \"2020-04-02T00:00:00\", \"2020-04-03T00:00:00\", \"2020-04-04T00:00:00\", \"2020-04-05T00:00:00\", \"2020-04-06T00:00:00\", \"2020-04-07T00:00:00\", \"2020-04-08T00:00:00\", \"2020-04-09T00:00:00\", \"2020-04-10T00:00:00\", \"2020-04-11T00:00:00\", \"2020-04-12T00:00:00\", \"2020-04-13T00:00:00\", \"2020-04-14T00:00:00\", \"2020-04-15T00:00:00\", \"2020-04-16T00:00:00\", \"2020-04-17T00:00:00\", \"2020-04-18T00:00:00\", \"2020-04-19T00:00:00\", \"2020-04-20T00:00:00\", \"2020-04-21T00:00:00\", \"2020-04-22T00:00:00\", \"2020-04-23T00:00:00\", \"2020-04-24T00:00:00\", \"2020-04-25T00:00:00\", \"2020-04-26T00:00:00\", \"2020-04-27T00:00:00\", \"2020-04-28T00:00:00\", \"2020-04-29T00:00:00\", \"2020-04-30T00:00:00\", \"2020-05-01T00:00:00\", \"2020-05-02T00:00:00\", \"2020-05-03T00:00:00\", \"2020-05-04T00:00:00\", \"2020-05-05T00:00:00\", \"2020-05-06T00:00:00\", \"2020-05-07T00:00:00\", \"2020-05-08T00:00:00\", \"2020-05-09T00:00:00\", \"2020-05-10T00:00:00\", \"2020-05-11T00:00:00\", \"2020-05-12T00:00:00\", \"2020-05-13T00:00:00\", \"2020-05-14T00:00:00\", \"2020-05-15T00:00:00\", \"2020-05-16T00:00:00\", \"2020-05-17T00:00:00\", \"2020-05-18T00:00:00\", \"2020-05-19T00:00:00\", \"2020-05-20T00:00:00\", \"2020-05-21T00:00:00\", \"2020-05-22T00:00:00\", \"2020-05-23T00:00:00\", \"2020-05-24T00:00:00\", \"2020-05-25T00:00:00\", \"2020-05-26T00:00:00\", \"2020-05-27T00:00:00\", \"2020-05-28T00:00:00\", \"2020-05-29T00:00:00\", \"2020-05-30T00:00:00\", \"2020-05-31T00:00:00\", \"2020-06-01T00:00:00\", \"2020-06-02T00:00:00\", \"2020-06-03T00:00:00\", \"2020-06-04T00:00:00\", \"2020-06-05T00:00:00\", \"2020-06-06T00:00:00\", \"2020-06-07T00:00:00\", \"2020-06-08T00:00:00\", \"2020-06-09T00:00:00\", \"2020-06-10T00:00:00\", \"2020-06-11T00:00:00\", \"2020-06-12T00:00:00\", \"2020-06-13T00:00:00\", \"2020-06-14T00:00:00\", \"2020-06-15T00:00:00\", \"2020-06-16T00:00:00\", \"2020-06-17T00:00:00\", \"2020-06-18T00:00:00\", \"2020-06-19T00:00:00\", \"2020-06-20T00:00:00\", \"2020-06-21T00:00:00\", \"2020-06-22T00:00:00\", \"2020-06-23T00:00:00\", \"2020-06-24T00:00:00\", \"2020-06-25T00:00:00\", \"2020-06-26T00:00:00\", \"2020-06-27T00:00:00\", \"2020-06-28T00:00:00\", \"2020-06-29T00:00:00\", \"2020-06-30T00:00:00\", \"2020-07-01T00:00:00\", \"2020-07-02T00:00:00\", \"2020-07-03T00:00:00\", \"2020-07-04T00:00:00\", \"2020-07-05T00:00:00\", \"2020-07-06T00:00:00\", \"2020-07-07T00:00:00\", \"2020-07-08T00:00:00\", \"2020-07-09T00:00:00\", \"2020-07-10T00:00:00\", \"2020-07-11T00:00:00\", \"2020-07-12T00:00:00\", \"2020-07-13T00:00:00\", \"2020-07-14T00:00:00\", \"2020-07-15T00:00:00\", \"2020-07-16T00:00:00\", \"2020-07-17T00:00:00\", \"2020-07-18T00:00:00\", \"2020-07-19T00:00:00\", \"2020-07-20T00:00:00\", \"2020-07-21T00:00:00\", \"2020-07-22T00:00:00\", \"2020-07-23T00:00:00\", \"2020-07-24T00:00:00\", \"2020-07-25T00:00:00\", \"2020-07-26T00:00:00\", \"2020-07-27T00:00:00\", \"2020-07-28T00:00:00\", \"2020-07-29T00:00:00\", \"2020-07-30T00:00:00\", \"2020-07-31T00:00:00\", \"2020-08-01T00:00:00\", \"2020-08-02T00:00:00\", \"2020-08-03T00:00:00\", \"2020-08-04T00:00:00\", \"2020-08-05T00:00:00\", \"2020-08-06T00:00:00\", \"2020-08-07T00:00:00\", \"2020-08-08T00:00:00\", \"2020-08-09T00:00:00\", \"2020-08-10T00:00:00\", \"2020-08-11T00:00:00\", \"2020-08-12T00:00:00\", \"2020-08-13T00:00:00\", \"2020-08-14T00:00:00\", \"2020-08-15T00:00:00\", \"2020-08-16T00:00:00\", \"2020-08-17T00:00:00\", \"2020-08-18T00:00:00\", \"2020-08-19T00:00:00\", \"2020-08-20T00:00:00\", \"2020-08-21T00:00:00\", \"2020-08-22T00:00:00\", \"2020-08-23T00:00:00\", \"2020-08-24T00:00:00\", \"2020-08-25T00:00:00\", \"2020-08-26T00:00:00\", \"2020-08-27T00:00:00\", \"2020-08-28T00:00:00\", \"2020-08-29T00:00:00\", \"2020-08-30T00:00:00\", \"2020-08-31T00:00:00\", \"2020-09-01T00:00:00\", \"2020-09-02T00:00:00\", \"2020-09-03T00:00:00\", \"2020-09-04T00:00:00\", \"2020-09-05T00:00:00\", \"2020-09-06T00:00:00\", \"2020-09-07T00:00:00\", \"2020-09-08T00:00:00\", \"2020-09-09T00:00:00\", \"2020-09-10T00:00:00\", \"2020-09-11T00:00:00\", \"2020-09-12T00:00:00\", \"2020-09-13T00:00:00\", \"2020-09-14T00:00:00\", \"2020-09-15T00:00:00\", \"2020-09-16T00:00:00\", \"2020-09-17T00:00:00\", \"2020-09-18T00:00:00\", \"2020-09-19T00:00:00\", \"2020-09-20T00:00:00\", \"2020-09-21T00:00:00\", \"2020-09-22T00:00:00\", \"2020-09-23T00:00:00\", \"2020-09-24T00:00:00\", \"2020-09-25T00:00:00\", \"2020-09-26T00:00:00\", \"2020-09-27T00:00:00\", \"2020-09-28T00:00:00\", \"2020-09-29T00:00:00\", \"2020-09-30T00:00:00\", \"2020-10-01T00:00:00\", \"2020-10-02T00:00:00\", \"2020-10-03T00:00:00\", \"2020-10-04T00:00:00\", \"2020-10-05T00:00:00\", \"2020-10-06T00:00:00\", \"2020-10-07T00:00:00\", \"2020-10-08T00:00:00\", \"2020-10-09T00:00:00\", \"2020-10-10T00:00:00\", \"2020-10-11T00:00:00\", \"2020-10-12T00:00:00\", \"2020-10-13T00:00:00\", \"2020-10-14T00:00:00\", \"2020-10-15T00:00:00\", \"2020-10-16T00:00:00\", \"2020-10-17T00:00:00\", \"2020-10-18T00:00:00\", \"2020-10-19T00:00:00\", \"2020-10-20T00:00:00\", \"2020-10-21T00:00:00\", \"2020-10-22T00:00:00\", \"2020-10-23T00:00:00\", \"2020-10-24T00:00:00\", \"2020-10-25T00:00:00\", \"2020-10-26T00:00:00\", \"2020-10-27T00:00:00\", \"2020-10-28T00:00:00\", \"2020-10-29T00:00:00\", \"2020-10-30T00:00:00\", \"2020-10-31T00:00:00\", \"2020-11-01T00:00:00\", \"2020-11-02T00:00:00\", \"2020-11-03T00:00:00\", \"2020-11-04T00:00:00\", \"2020-11-05T00:00:00\", \"2020-11-06T00:00:00\", \"2020-11-07T00:00:00\", \"2020-11-08T00:00:00\", \"2020-11-09T00:00:00\", \"2020-11-10T00:00:00\", \"2020-11-11T00:00:00\", \"2020-11-12T00:00:00\", \"2020-11-13T00:00:00\", \"2020-11-14T00:00:00\", \"2020-11-15T00:00:00\", \"2020-11-16T00:00:00\", \"2020-11-17T00:00:00\", \"2020-11-18T00:00:00\", \"2020-11-19T00:00:00\", \"2020-11-20T00:00:00\", \"2020-11-21T00:00:00\", \"2020-11-22T00:00:00\", \"2020-11-23T00:00:00\", \"2020-11-24T00:00:00\", \"2020-11-25T00:00:00\", \"2020-11-26T00:00:00\", \"2020-11-27T00:00:00\", \"2020-11-28T00:00:00\", \"2020-11-29T00:00:00\", \"2020-11-30T00:00:00\", \"2020-12-01T00:00:00\", \"2020-12-02T00:00:00\", \"2020-12-03T00:00:00\", \"2020-12-04T00:00:00\", \"2020-12-05T00:00:00\", \"2020-12-06T00:00:00\", \"2020-12-07T00:00:00\", \"2020-12-08T00:00:00\", \"2020-12-09T00:00:00\", \"2020-12-10T00:00:00\", \"2020-12-11T00:00:00\", \"2020-12-12T00:00:00\", \"2020-12-13T00:00:00\", \"2020-12-14T00:00:00\", \"2020-12-15T00:00:00\", \"2020-12-16T00:00:00\", \"2020-12-17T00:00:00\", \"2020-12-18T00:00:00\", \"2020-12-19T00:00:00\", \"2020-12-20T00:00:00\", \"2020-12-21T00:00:00\", \"2020-12-22T00:00:00\", \"2020-12-23T00:00:00\", \"2020-12-24T00:00:00\", \"2020-12-25T00:00:00\", \"2020-12-26T00:00:00\", \"2020-12-27T00:00:00\", \"2020-12-28T00:00:00\", \"2020-12-29T00:00:00\", \"2020-12-30T00:00:00\", \"2020-12-31T00:00:00\", \"2021-01-01T00:00:00\", \"2021-01-02T00:00:00\", \"2021-01-03T00:00:00\", \"2021-01-04T00:00:00\", \"2021-01-05T00:00:00\", \"2021-01-06T00:00:00\", \"2021-01-07T00:00:00\", \"2021-01-08T00:00:00\", \"2021-01-09T00:00:00\", \"2021-01-10T00:00:00\", \"2021-01-11T00:00:00\", \"2021-01-12T00:00:00\", \"2021-01-13T00:00:00\", \"2021-01-14T00:00:00\", \"2021-01-15T00:00:00\", \"2021-01-16T00:00:00\", \"2021-01-17T00:00:00\", \"2021-01-18T00:00:00\", \"2021-01-19T00:00:00\", \"2021-01-20T00:00:00\", \"2021-01-21T00:00:00\", \"2021-01-22T00:00:00\", \"2021-01-23T00:00:00\", \"2021-01-24T00:00:00\", \"2021-01-25T00:00:00\", \"2021-01-26T00:00:00\", \"2021-01-27T00:00:00\", \"2021-01-28T00:00:00\", \"2021-01-29T00:00:00\", \"2021-01-30T00:00:00\", \"2021-01-31T00:00:00\", \"2021-02-01T00:00:00\", \"2021-02-02T00:00:00\", \"2021-02-03T00:00:00\", \"2021-02-04T00:00:00\", \"2021-02-05T00:00:00\", \"2021-02-06T00:00:00\", \"2021-02-07T00:00:00\", \"2021-02-08T00:00:00\", \"2021-02-09T00:00:00\", \"2021-02-10T00:00:00\", \"2021-02-11T00:00:00\", \"2021-02-12T00:00:00\", \"2021-02-13T00:00:00\", \"2021-02-14T00:00:00\", \"2021-02-15T00:00:00\", \"2021-02-16T00:00:00\", \"2021-02-17T00:00:00\", \"2021-02-18T00:00:00\", \"2021-02-19T00:00:00\", \"2021-02-20T00:00:00\", \"2021-02-21T00:00:00\", \"2021-02-22T00:00:00\", \"2021-02-23T00:00:00\", \"2021-02-24T00:00:00\", \"2021-02-25T00:00:00\", \"2021-02-26T00:00:00\", \"2021-02-27T00:00:00\", \"2021-02-28T00:00:00\", \"2021-03-01T00:00:00\", \"2021-03-02T00:00:00\", \"2021-03-03T00:00:00\", \"2021-03-04T00:00:00\", \"2021-03-05T00:00:00\", \"2021-03-06T00:00:00\", \"2021-03-07T00:00:00\", \"2021-03-08T00:00:00\", \"2021-03-09T00:00:00\", \"2021-03-10T00:00:00\", \"2021-03-11T00:00:00\", \"2021-03-12T00:00:00\", \"2021-03-13T00:00:00\", \"2021-03-14T00:00:00\", \"2021-03-15T00:00:00\", \"2021-03-16T00:00:00\", \"2021-03-17T00:00:00\", \"2021-03-18T00:00:00\", \"2021-03-19T00:00:00\", \"2021-03-20T00:00:00\", \"2021-03-21T00:00:00\", \"2021-03-22T00:00:00\", \"2021-03-23T00:00:00\", \"2021-03-24T00:00:00\", \"2021-03-25T00:00:00\", \"2021-03-26T00:00:00\", \"2021-03-27T00:00:00\", \"2021-03-28T00:00:00\", \"2021-03-29T00:00:00\", \"2021-03-30T00:00:00\", \"2021-03-31T00:00:00\", \"2021-04-01T00:00:00\", \"2021-04-02T00:00:00\", \"2021-04-03T00:00:00\", \"2021-04-04T00:00:00\", \"2021-04-05T00:00:00\", \"2021-04-06T00:00:00\", \"2021-04-07T00:00:00\", \"2021-04-08T00:00:00\", \"2021-04-09T00:00:00\", \"2021-04-10T00:00:00\", \"2021-04-11T00:00:00\", \"2021-04-12T00:00:00\", \"2021-04-13T00:00:00\", \"2021-04-14T00:00:00\", \"2021-04-15T00:00:00\", \"2021-04-16T00:00:00\", \"2021-04-17T00:00:00\", \"2021-04-18T00:00:00\", \"2021-04-19T00:00:00\", \"2021-04-20T00:00:00\", \"2021-04-21T00:00:00\", \"2021-04-22T00:00:00\"], \"y\": [-0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.3459504085799503, -0.2806675658097864, -0.17855591684007094, -0.06024968322644321, 0.06491780743258048, 0.09715562546205099, 0.09947634475681714, 0.3052636056992427, 0.43966352311787993, 0.5724994775335225, 0.7875191652355615, 0.9710577911999018, 1.048650536316217, 1.059396475659374, 1.269825175191332, 1.5992664142092383, 1.9018175796595265, 2.1759651589586535, 2.5453631301821003, 2.667453145254586, 2.7035756455818176, 3.039171835772799, 3.33481129375824, 3.592814738832906, 3.7794308403837844, 3.814796584419244, 3.8439569268621767, 3.847791158740486, 3.9583784781780436, 4.012108174893828, 3.8654488055484904, 3.72953537554631, 3.5436760302872035, 3.4430778939010316, 3.415431064041642, 3.0599069581673413, 2.6106459729522644, 1.926386933932378, 1.6529961109250222, 1.4226394957097448, 1.3450467505934294, 1.336621530545039, 1.1108559034998433, 0.8704596548137296, 1.0578325126563792, 0.8144092388031793, 0.6193679172039107, 0.5502508425554374, 0.5395049032122806, 0.3490545697830929, 0.22232311612107453, 0.13247091907805927, 0.10098985734036041, 0.04645295391335331, 0.05951961255127639, 0.05644213696473846, -0.02110015773212549, -0.05565869505636215, -0.047990231299743155, -0.12326225712129234, -0.14551089209937754, -0.16165502632383844, -0.16165502632383844, -0.13708567205098693, -0.10252713472675026, -0.12018478153475441, -0.10479740360206502, -0.1009631717237556, -0.09178119538359347, -0.09021723238059881, -0.09480822055067988, -0.07947129303744202, -0.0763938174509041, -0.04723347500797152, 0.02960251381657219, 0.11022228409997392, 0.19467628626168523, 0.1831735906267568, 0.2208096035375314, 0.23306905546423146, 0.17626188316190947, -0.019586645148582223, -0.08713975679406087, -0.16548925820214785, -0.1470244046829207, -0.17623519754530478, -0.1424334165128396, -0.10404064731029353, -0.10943884219159762, -0.09944965914021246, -0.0932947079671366, -0.03184609707528228, 0.005789915835492316, -0.011867730972511831, 0.019613330765187037, 0.2108204204861461, 0.19316277367814197, 0.20698618860783669, 0.16551594381875268, 0.1271231746162066, 0.14937180959429178, 0.14629433400775385, 0.13247091907805927, 0.14321685842121606, 0.11637723527304965, 0.16011774893744857, 0.1877645787968379, 0.19008529809160418, 0.20925645748315158, 0.20774294489960832, 0.23387626217545449, 0.24462220151861128, 0.27686001954808165, 0.3121753131640899, 0.3121753131640899, 0.30909783757755216, 0.341386106026474, 0.34522033790478346, 0.340578899315251, 0.3490545697830929, 0.3267554843855563, 0.36746897288286867, 0.3874473389856391, 0.36671221659109704, 0.3352311548533983, 0.33058971626386574, 0.2369032873425409, 0.2369032873425409, 0.19467628626168523, 0.14937180959429178, 0.11557002856182662, 0.11481327227005513, 0.11557002856182662, 0.12631596790498342, 0.08408896682412789, 0.05568538067296682, 0.07182951489742784, 0.06950879560266154, 0.07490699048396564, 0.081818697948813, 0.11637723527304965, 0.12787993090797808, 0.16551594381875268, 0.14629433400775385, 0.16168171194044323, 0.16011774893744857, 0.14937180959429178, 0.09564211287850771, 0.060276368843048024, 0.009624147713801741, -0.017265925853815922, -0.038757804540129635, -0.04032176754312416, -0.038001048248358, -0.032602853367053916, 0.004982709124269286, 0.012701623300339675, 0.053364661378200666, 0.10174661363213204, 0.12021146715135922, 0.12863668719974972, 0.1854943099215231, 0.2592023727400775, 0.3021861301127048, 0.35364555795317393, 0.38896085156918225, 0.41741488813979455, 0.42044191330688113, 0.4803770116151923, 0.49339321983366385, 0.5878868554662119, 0.6600814057012231, 0.7583588227926291, 0.7982651045787184, 0.799829067581713, 0.8935154965030379, 0.9795334616677438, 1.028672170213447, 1.1162036479616961, 1.1338612947697002, 1.1446576845323084, 1.150005428994161, 0.7706687251387806, 0.874293886692039, 0.8512884954221822, 0.7445354078629344, 0.69463994302546, 0.6869210288493893, 0.688484991852384, 1.0202469501650562, 0.8228849092710212, 0.7936741164086374, 0.7798507014789428, 0.712297589833464, 0.691562467438922, 0.6823300406793085, 0.6454507840603055, 0.29546361172080043, -0.05452356061870463, -0.40451073295820966, -0.7544979052977148, -0.7352762954867161, -0.7237735998517876, -0.7015249648737024, -0.6592475133733953, -0.6508222933250047, -0.6454240984437006, -0.6639743295958456, -0.6825245607479907, -0.7010747919001358, -0.7196250230522807, -0.7381752542044258, -0.7567254853565707, -0.7752757165087157, -0.7938259476608607, -0.8123761788130058, -0.8309264099651508, -0.8494766411172958, -0.8680268722694409, -0.8865771034215858, -0.8796653959567384, -0.875831164078429, -0.875831164078429, -0.875831164078429, -0.867355493610587, -0.8604437861457397, -0.8589302735621965, -0.8650852247352723, -0.8635212617322776, -0.8620077491487343, -0.8620077491487343, -0.86816270032181, -0.8735104447836627, -0.8789086396649669, -0.8789086396649669, -0.8796653959567384, -0.8796653959567384, -0.8789086396649669, -0.8781518833731953, -0.8773446766619722, -0.8789086396649669, -0.8796653959567384, -0.8804221522485101, -0.8796653959567384, -0.8796653959567384, -0.8827428715432764, -0.8842563841268195, -0.8850635908380426, -0.8789086396649669, -0.8796653959567384, -0.8819861152515047, -0.8819861152515047, -0.8819861152515047, -0.8804221522485101, -0.8773446766619722, -0.8804221522485101, -0.8811789085402817, -0.8781518833731953, -0.8789086396649669, -0.8750744077866573, -0.8696762129053532, -0.8719969322001196, -0.871240175908348, -0.867355493610587, -0.8658419810270439, -0.8658419810270439, -0.8650852247352723, -0.8665987373188153, -0.8658419810270439, -0.8696762129053532, -0.8735104447836627, -0.8773446766619722, -0.8773446766619722, -0.8804221522485101, -0.8827428715432764, -0.8827428715432764, -0.8804221522485101, -0.8804221522485101, -0.8811789085402817, -0.8804221522485101, -0.8819861152515047, -0.8858203471298142, -0.8858203471298142, -0.8880906160051291, -0.8858203471298142, -0.8827428715432764, -0.883499627835048, -0.8811789085402817, -0.8735104447836627, -0.8719969322001196, -0.867355493610587, -0.8558527979756586, -0.8574167609786533, -0.8558527979756586, -0.8481843342190397, -0.8481843342190397, -0.846620371216045, -0.8381951511676544, -0.8412726267541923, -0.838951907459426, -0.838951907459426, -0.842029383045964, -0.838951907459426, -0.8366816385841113, -0.8381951511676544, -0.8443501023407302, -0.842029383045964, -0.8427861393377355, -0.8327969562863503, -0.8351176755811166, -0.831283443702807, -0.8305266874110355, -0.8182167850648842, -0.8236149799461883, -0.8243717362379599, -0.8189735413566556, -0.8097915650164935, -0.80055913825688, -0.7874924796189569, -0.7683213202274095, -0.756011417881258, -0.4657197043576698, -0.4381233249177319, -0.4319683737446562, -0.5302457908360622, -0.5310025471278338, -0.5302457908360622, -0.5133449003198296, -0.6454240984437006, -0.6108655611194639, -0.5840259379712976, -0.574793511211684, -0.5640475718685273, -0.5509809132306042, -0.5387214613039042, -0.537914254592681]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"rangeslider\": {\"visible\": true}}, \"yaxis\": {\"autorange\": true, \"fixedrange\": false}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f3bc3810-1f8c-4c27-bf95-1e877d969e76');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}