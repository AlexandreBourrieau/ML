{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bitcoin.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/PredictionSeries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVuCzMYKDp3z"
      },
      "source": [
        "# Chargement des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3nLyKtnliCW"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eo2qVS1lrzuX"
      },
      "source": [
        "On télécharge un script depuis Github permettant de télécharger un fichier stocké sur GoogleDrive, puis on utilise ce script écrit en Python pour télécharger le fichier `bitcoin.zip`. Enfin, on décompresse les données pour obtenir le fichier `bitstampUSD_1-min_data_2012-01-01_to_2021-03-31.csv` :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sOdetggHqKE"
      },
      "source": [
        "# Récupération des données au format .csv\n",
        "\n",
        "!git clone https://github.com/chentinghao/download_google_drive.git\n",
        "!python download_google_drive/download_gdrive.py \"1FZsEdpBm-AQ2L9n_pMnm6336-O_IVo7z\" \"/content/bitcoin.zip\"\n",
        "!unzip bitcoin.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPJ6nnrRsUBm"
      },
      "source": [
        "Charge la série sous Pandas et affiche les informations du fichier :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEB_JjihEYTP"
      },
      "source": [
        "# Création de la série sous Pandas\n",
        "serie = pd.read_csv(\"bitstampUSD_1-min_data_2012-01-01_to_2021-03-31.csv\")\n",
        "serie"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF62FumBty9H"
      },
      "source": [
        "# Pré-traitement des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2oeGZgr1UUP"
      },
      "source": [
        "**1. Recherche des erreurs dans les données**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2d3HD1YwmIS"
      },
      "source": [
        "On commence par vérifier qu'il ne manque pas de dates. Pour cela, on vérifie qu'il y a bien 60 secondes entre deux Timestamp. Si on trouve un décalage non cohérent, on enregistre les informations dans une liste."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCXSGY9Swltw"
      },
      "source": [
        "# Fonction permettant de vérifier si chaque intervalle est bien de 60s\n",
        "def recherche_erreur(fenetre):\n",
        "  if fenetre.values[1] - fenetre.values[0] != 60:\n",
        "    Timestamp_Errors.append(fenetre.values)\n",
        "  return 0\n",
        "\n",
        "# Définit une liste pour sauvegarder le résultat des recherches\n",
        "Timestamp_Errors = []\n",
        "\n",
        "# Applique la fonction sur une fenêtre glissante des données\n",
        "serie.Timestamp.rolling(2).apply(recherche_erreur)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l1fTgb_1O8b"
      },
      "source": [
        "On affiche les erreurs trouvées :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZERkJS40sHP"
      },
      "source": [
        "# Affiche les informations sur les erreurs trouvées\n",
        "\n",
        "for erreur in Timestamp_Errors:\n",
        "  print (pd.to_datetime(Timestamp_Errors[0],unit=\"s\"))\n",
        "  print((Timestamp_Errors[0][1] - Timestamp_Errors[0][0])/60 - 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHOSRv8_1aYa"
      },
      "source": [
        "On observe qu'il manque des données entre le 5 janvier 2015 à 9:12:00 et le 9 janvier 2015 à 21:05:00, soit 6472 données."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tjbmho3h2nB7"
      },
      "source": [
        "Recherchons maintenant le nombre de données manquantes :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUzjd-qN2s-T"
      },
      "source": [
        "# Affichage du nombre total de données manquantes\n",
        "\n",
        "data_manquantes = sum(np.isnan(serie['Open']))\n",
        "print (\"Nombre de données manquantes : %s\" %data_manquantes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D41XKoGE3O60"
      },
      "source": [
        "On a donc en tout : 6472 + 1243608 = 1250080 données manquantes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GefH2TaM3a0S"
      },
      "source": [
        "**2. Identification des erreurs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XivWgphwsdIm"
      },
      "source": [
        "On convertit maintenant les `Timestamp` (mesure de temps exprimé en seconde écoulé depuis le 01/01/1970 - 00:00:00 UTC) en format plus standard :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdLSr2IWSOnX"
      },
      "source": [
        "# Conversion des timestamp en date\n",
        "serie.Timestamp = pd.to_datetime(serie['Timestamp'], unit=\"s\")\n",
        "serie"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgHcZyd13gCa"
      },
      "source": [
        "On demande maintenant à échantillonner les données sur 60 secondes :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVNlzo9QWNmP"
      },
      "source": [
        "# Echantillonnage de la série sur 1min\n",
        "serie_minute = serie.set_index('Timestamp').resample('60s').asfreq()\n",
        "\n",
        "# Récupère le nombre de données sans valeurs numériques\n",
        "data_manquantes = sum(np.isnan(serie_minute['Open']))\n",
        "\n",
        "# Affiche le nombre de données manquantes et la série sur 1min \n",
        "print (\"Nombre de données manquantes : %s\" %data_manquantes)\n",
        "serie_minute"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqFOnDlU3tey"
      },
      "source": [
        "On obtient en tout 4863849 données après échantillonnage, soit (4863849-4857377) =  6472 données supplémentaires. Ceci est cohérent avec ce qu'on avait trouvé avant. Il manque 1250080 données. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tkbi7uuBnUD3"
      },
      "source": [
        "**3. Correction des données**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IQuSqAknduG"
      },
      "source": [
        "Pour corriger les données, on va tout simplement utiliser la fonction [fillna](https://pandas.pydata.org/docs/reference/api/pandas.Series.fillna.html) de Pandas avec la fonctionnalité de type `backfill` :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2Oav6gin5aP"
      },
      "source": [
        "# Applique la fonction de remplissage automatique des données non numérique avec l'option backfill\n",
        "serie_minute = serie_minute.interpolate(method=\"slinear\")\n",
        "serie_minute = serie_minute.fillna(method=\"backfill\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfOvQ-BKn80v"
      },
      "source": [
        "# Récupère le nombre de données non numériques et affiche les informations\n",
        "\n",
        "data_manquantes = sum(np.isnan(serie_minute['Open']))\n",
        "print (\"Nombre de données manquantes : %s\" %data_manquantes)\n",
        "serie_minute"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v05rWWccJI26"
      },
      "source": [
        "**4. Affichage des données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1QKMBThNQni"
      },
      "source": [
        "# Affiche la série\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(serie_minute.index, serie_minute.Open)\n",
        "plt.title(\"Evolution du prix du BTC\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU7u1mA1E6jk"
      },
      "source": [
        "# Préparation des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhIs2GS7vu8k"
      },
      "source": [
        "Nous allons réaliser des modélisations sur la série journalière, et pour une période allant du 1er avril 2013 au 31 mars 2021."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeKrafzHv9gd"
      },
      "source": [
        "**1. Création de la série horaire pour la modélisation globale**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUlNO0pswBpr"
      },
      "source": [
        "On va réaliser des prédictions à l'aide d'une série à fréquence journalière. On commence par tenter d'estimer les données manquantes à l'aide d'une interpolation linéaire à l'aide de la fonction [interpolate](https://pandas.pydata.org/docs/reference/api/pandas.Series.interpolate.html#pandas.Series.interpolate) de Pandas, puis on complète avec la méthode `backfill` si nécessaire."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir4BkaUtuiXX"
      },
      "source": [
        "# Echantillonne la série sur 1 heure\n",
        "serie_heure = serie.set_index('Timestamp').resample('1H').asfreq()\n",
        "\n",
        "# Remplissage des données non numériques par interpolation linéraire\n",
        "serie_heure = serie_heure.interpolate(method=\"slinear\")\n",
        "\n",
        "# Remplissage des données non numériques restantes par backfill\n",
        "serie_heure = serie_heure.fillna(method=\"backfill\")\n",
        "\n",
        "# Affiche les informations\n",
        "data_manquantes = sum(np.isnan(serie_heure['Open']))\n",
        "print (\"Nombre de données manquantes : %s\" %data_manquantes)\n",
        "serie_heure"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epIrgQTvca6p"
      },
      "source": [
        "# Affiche la série\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(serie_heure.index, serie_heure.Open)\n",
        "plt.title(\"Evolution du prix du BTC\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hsbej4XvckFD"
      },
      "source": [
        "On construit une nouvelle série avec les dates retenues pour le début et la fin :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMiDRe1ZcqMR"
      },
      "source": [
        "# Définition des dates de début et de fin\n",
        "\n",
        "date_debut = \"2013-04-01 00:00:00\"\n",
        "date_fin = \"2021-03-31 00:00:00\"\n",
        "\n",
        "serie_etude = serie_heure.loc[date_debut:date_fin].copy()\n",
        "serie_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUD1yl1wxkla"
      },
      "source": [
        "serie_etude['x'] = np.linspace(0,serie_etude.index.size-1,serie_etude.index.size)\n",
        "serie_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwOeFLtLSPnv"
      },
      "source": [
        "**2. Détection des anomalies dans la série \"horaire\"**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1joYv2Kd7Js"
      },
      "source": [
        "Les anomalies sont fréquentes dans les séries temporelles, et la performance des prédictions est souvent améliorée lorsque ces anomalies sont traitées.  \n",
        "Pour avoir un apperçu de ces éventuelles anomalies, nous allons utiliser la méthode [\"Isolation Forest\"](https://scikit-learn.org/stable/modules/outlier_detection.html#isolation-forest) disponnible dans Scikit-learn.  \n",
        "\n",
        "Les paramètres utilisés sont les suivants :\n",
        " - **n_estimators** : C'est le nombre de sous-groupes d'échantillons à utiliser. Une valeur de 128 ou 256 est préconnisée dans le document de recherche.\n",
        " - **max_samples** : C'est le nombre d'échantillons maximum à utiliser. Nous utiliserons l'ensemble des échantillons.\n",
        " - **max_features** :  C'est le nombre de motifs aléatoirement choisis sur chaque noeud de l'arbre. Nous choisirons un seul motif.\n",
        " - **contamination** : C'est le pourcentage estimé d'anomalies dans les données. Ce paramètre permet de régler la sensibilité de l'algorithme. On va commencer avec 5% et affiner si nécessaire par la suite."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHag65S4dH7x"
      },
      "source": [
        "# Initialise le modèle\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "clf = IsolationForest(n_estimators=256,max_samples=serie_etude['Open'].size, contamination=0.05,max_features=1, verbose=1)\n",
        "clf.fit(serie_etude['Open'].values.reshape(-1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAPFfAaffb4h"
      },
      "source": [
        "# Réalise les prédictions\n",
        "pred = clf.predict(serie_etude['Open'].values.reshape(-1,1))\n",
        "pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU0TN1UEBqR2"
      },
      "source": [
        "On ajoute maintenant ces informations dans la série journalière et on affiche les informations :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWg0uUb9G5Ws"
      },
      "source": [
        "# Ajoute une colonne \"Anomalie\" dans la série\n",
        "serie_etude['Anomalies']=pred\n",
        "serie_etude['Anomalies'] = serie_etude['Anomalies'].apply(lambda x: 1 if (x==-1) else 0)\n",
        "serie_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "105qNoy1EwWd"
      },
      "source": [
        "# Affiche les informations sur les anomalies\n",
        "print(serie_etude['Anomalies'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaV_MfJyFXkF"
      },
      "source": [
        "**3. Affichage des anomalies sur le graphique**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2idYKYImFh8v"
      },
      "source": [
        "# Affiche la série\n",
        "\n",
        "fig = px.line(x=serie_etude.index,y=serie_etude['Open'],title=\"Evolution du prix du BTC\")\n",
        "fig.add_trace(px.scatter(x=serie_etude.index,y=serie_etude['Anomalies']*serie_etude['Open'],color=serie_etude['Anomalies'].astype(np.bool)).data[0])\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVAWVQioe-kS"
      },
      "source": [
        "Comme les anomalies détectées ne sembles pas cohérentes, nous n'allons pas les traiter..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vDYEK-cxE0C"
      },
      "source": [
        "# Analyse de la série"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGY4fCB3xdUx"
      },
      "source": [
        "serie_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBCbjPSNxWXy"
      },
      "source": [
        "**1. ACF & PACF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CleN4htOxYqZ"
      },
      "source": [
        "# ACF & PACF du bruit blanc\n",
        "\n",
        "serie = serie_etude['Open']\n",
        "\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "f1, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "f1.subplots_adjust(hspace=0.3,wspace=0.2)\n",
        "\n",
        "plot_acf(serie, ax=ax1, lags = range(0,50))\n",
        "ax1.set_title(\"Autocorrélation du bruit blanc\")\n",
        "\n",
        "plot_pacf(serie, ax=ax2, lags = range(0, 50))\n",
        "ax2.set_title(\"Autocorrélation partielle du bruit blanc\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o66MqHZTyHX5"
      },
      "source": [
        "**2. Test de Dickey-Fuller**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm5VXMhkyLSN"
      },
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "serie_test = serie\n",
        "\n",
        "adf, p, usedlag, nobs, cvs, aic = sm.tsa.stattools.adfuller(serie_test)\n",
        "\n",
        "adf_results_string = 'ADF: {}\\np-value: {},\\nN: {}, \\ncritical values: {}'\n",
        "print(adf_results_string.format(adf, p, nobs, cvs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moHyQgGfyTi4"
      },
      "source": [
        "**3. Suppression de la tendance non linéaire et test de sationnarité**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43AcGds6y0pI"
      },
      "source": [
        "from scipy.stats import boxcox\n",
        "\n",
        "serie_log, lam = boxcox(serie)\n",
        "\n",
        "f1, (ax1,ax2) = plt.subplots(2, 1, figsize=(15, 5))\n",
        "ax1.plot(serie_etude.index,serie_log)\n",
        "ax2.plot(serie_etude.index,serie)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fRPXCkO0DUh"
      },
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "serie_test = serie_log\n",
        "\n",
        "adf, p, usedlag, nobs, cvs, aic = sm.tsa.stattools.adfuller(serie_test)\n",
        "\n",
        "adf_results_string = 'ADF: {}\\np-value: {},\\nN: {}, \\ncritical values: {}'\n",
        "print(adf_results_string.format(adf, p, nobs, cvs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI8sp_Rlz6GT"
      },
      "source": [
        "***4. Suppression de la tendance linéaire et test de stationnarité***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iHrAWJH0TdT"
      },
      "source": [
        "f1, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 5))\n",
        "\n",
        "# Calcul des coefficients\n",
        "x = np.linspace(0,len(serie_log),len(serie_log))\n",
        "coefs = np.polyfit(x,serie_log,1)\n",
        "\n",
        "# Calcul de la tendance non linéaire\n",
        "trend = coefs[0]*np.power(x,1) + coefs[1]\n",
        "\n",
        "# Calcul de la série sans tendance\n",
        "serie_log_detrend = serie_log - trend\n",
        "\n",
        "# Affiche les résultats\n",
        "ax1.plot(trend)\n",
        "ax1.plot(serie_log)\n",
        "ax1.set_title(\"Série originale et tendance non linéaire\")\n",
        "\n",
        "ax2.plot(serie_log_detrend)\n",
        "ax2.set_title(\"Série avec tendance non linéaire supprimée\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNXs9Fm--Kcl"
      },
      "source": [
        "# ACF & PACF du bruit blanc\n",
        "\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "f1, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "f1.subplots_adjust(hspace=0.3,wspace=0.2)\n",
        "\n",
        "plot_acf(serie_log_detrend, ax=ax1, lags = range(0,50))\n",
        "ax1.set_title(\"Autocorrélation du bruit blanc\")\n",
        "\n",
        "plot_pacf(serie_log_detrend, ax=ax2, lags = range(0, 50))\n",
        "ax2.set_title(\"Autocorrélation partielle du bruit blanc\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_r01cDgq0xaJ"
      },
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "serie_test = serie_log_detrend\n",
        "\n",
        "adf, p, usedlag, nobs, cvs, aic = sm.tsa.stattools.adfuller(serie_test)\n",
        "\n",
        "adf_results_string = 'ADF: {}\\np-value: {},\\nN: {}, \\ncritical values: {}'\n",
        "print(adf_results_string.format(adf, p, nobs, cvs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ychdf1RxMPDD"
      },
      "source": [
        "**5. Différentiation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qaHkgtQMOqJ"
      },
      "source": [
        "# Différenciation d'odre 1 et saisonnale à l'odre 1 et de période 12\n",
        "\n",
        "from statsmodels.tsa.statespace.tools import diff\n",
        "\n",
        "serie_log_detrend_diff1 = diff(serie_log_detrend,1)       # diff=1 ; diff_saison=1 ; periode = 12\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(serie_log_detrend_diff1)\n",
        "plt.title(\"Signal différencié d'ordre 1 + saisonalité\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFLlzv0JMks5"
      },
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "serie_test = serie_log_detrend_diff1\n",
        "\n",
        "adf, p, usedlag, nobs, cvs, aic = sm.tsa.stattools.adfuller(serie_test)\n",
        "\n",
        "adf_results_string = 'ADF: {}\\np-value: {},\\nN: {}, \\ncritical values: {}'\n",
        "print(adf_results_string.format(adf, p, nobs, cvs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0Oqd_7XMqaZ"
      },
      "source": [
        "# ACF & PACF du bruit blanc\n",
        "\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "f1, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "f1.subplots_adjust(hspace=0.3,wspace=0.2)\n",
        "\n",
        "plot_acf(serie_log_detrend_diff1, ax=ax1, lags = range(0,50))\n",
        "ax1.set_title(\"Autocorrélation du bruit blanc\")\n",
        "\n",
        "plot_pacf(serie_log_detrend_diff1, ax=ax2, lags = range(0, 50))\n",
        "ax2.set_title(\"Autocorrélation partielle du bruit blanc\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5oIY2Yl5Tlt"
      },
      "source": [
        "**5. Enregistrement des données dans le dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjFSWhdeM4KM"
      },
      "source": [
        "serie_log_detrend_diff1 = np.insert(serie_log_detrend_diff1,0,0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ele3kFOp5TTW"
      },
      "source": [
        "serie_etude['diff'] = serie_log_detrend_diff1\n",
        "serie_etude['diff'][0] = \"Nan\"\n",
        "serie_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3q8CAozN-Gt"
      },
      "source": [
        "# Prépartion des datasets diff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWrUXYyFN-Gu"
      },
      "source": [
        "serie_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp724KcgN-Gw"
      },
      "source": [
        "**1. Séparation des données en données pour l'entrainement et la validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIWQZUvVN-Gw"
      },
      "source": [
        "On réserve 20% des données pour l'entrainement et le reste pour la validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZaGwL4YN-Gx"
      },
      "source": [
        "# Sépare les données en entrainement et tests\n",
        "pourcentage = 0.8\n",
        "temps_separation = int(len(serie_etude) * pourcentage)\n",
        "date_separation = serie_etude.index[temps_separation]\n",
        "\n",
        "serie_entrainement = serie_etude['diff'].iloc[1:temps_separation]\n",
        "serie_test = serie_etude['diff'].iloc[temps_separation:]\n",
        "\n",
        "print(\"Taille de l'entrainement : %d\" %len(serie_entrainement))\n",
        "print(\"Taille de la validation : %d\" %len(serie_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-YXKb5TN-Gy"
      },
      "source": [
        "On normalise les données :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcszCbQQN-Gy"
      },
      "source": [
        "# Calcul de la moyenne et de l'écart type de la série\n",
        "mean = tf.math.reduce_mean(np.asarray(serie_entrainement))\n",
        "std = tf.math.reduce_std(np.asarray((serie_entrainement)))\n",
        "\n",
        "# Normalisation des données\n",
        "serie_entrainement = (serie_entrainement-mean)/std\n",
        "serie_test = (serie_test-mean)/std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixzH25StN-Gz"
      },
      "source": [
        "# Affiche la série\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "ax.plot(serie_entrainement, label=\"Entrainement\")\n",
        "ax.plot(serie_test,label=\"Validation\")\n",
        "\n",
        "ax.set_title(\"Evolution du prix du BTC\")\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsIJ_4OON-Gz"
      },
      "source": [
        "**2. Création des datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlwKgWqiN-G0"
      },
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "\n",
        "def prepare_dataset_XY(serie, taille_fenetre, horizon, batch_size):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie)\n",
        "  dataset = dataset.window(taille_fenetre+horizon, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre + horizon))\n",
        "  dataset = dataset.map(lambda x: (tf.expand_dims(x[0:taille_fenetre],axis=1),x[-1:]))\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9qP2YSBN-G1"
      },
      "source": [
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "taille_fenetre = 500\n",
        "horizon = 1\n",
        "batch_size = 100\n",
        "\n",
        "# Création du dataset\n",
        "dataset = prepare_dataset_XY(serie_entrainement,taille_fenetre,horizon,batch_size)\n",
        "dataset_val = prepare_dataset_XY(serie_test,taille_fenetre,horizon,batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFRs-cLHN-HA"
      },
      "source": [
        "print(len(list(dataset.as_numpy_iterator())))\n",
        "for element in dataset.take(1):\n",
        "  print(element[0].shape)\n",
        "  print(element[1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ti9mbzgN-HA"
      },
      "source": [
        "On extrait maintenant les deux tenseurs (X,Y) pour l'entrainement :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIwGFoF7N-HB"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "#56x((1000,4,1),(1000,1)) => (56*1000,4,1) ; (56*1000,1)\n",
        "\n",
        "x,y = tuple(zip(*dataset))\n",
        "\n",
        "# Recombine les données\n",
        "# (56,1000,4,1) => (56*128,4,1)\n",
        "# (56,1000,1) => (56*128,1)\n",
        "x_train = np.asarray(tf.reshape(np.asarray(x,dtype=np.float32),shape=(np.asarray(x).shape[0]*np.asarray(x).shape[1],taille_fenetre,1)))\n",
        "y_train = np.asarray(tf.reshape(np.asarray(y,dtype=np.float32),shape=(np.asarray(y).shape[0]*np.asarray(y).shape[1])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4-eznwiN-HC"
      },
      "source": [
        "Puis la même chose pour les données de validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e25gJ8ymN-HC"
      },
      "source": [
        "# Extrait les X,Y du dataset_val\n",
        "\n",
        "x,y = tuple(zip(*dataset_val))\n",
        "\n",
        "# Recombine les données\n",
        "\n",
        "x_val = np.asarray(tf.reshape(np.asarray(x,dtype=np.float32),shape=(np.asarray(x).shape[0]*np.asarray(x).shape[1],taille_fenetre,1)))\n",
        "y_val = np.asarray(tf.reshape(np.asarray(y,dtype=np.float32),shape=(np.asarray(y).shape[0]*np.asarray(y).shape[1])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG5Fyx5O5oe5"
      },
      "source": [
        "# Prépartion des datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7cGUeWb5oe7"
      },
      "source": [
        "**1. Séparation des données en données pour l'entrainement et la validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob-EAw_j5oe8"
      },
      "source": [
        "On réserve 20% des données pour l'entrainement et le reste pour la validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5AWeK_Z5oe8"
      },
      "source": [
        "# Sépare les données en entrainement et tests\n",
        "pourcentage = 0.8\n",
        "temps_separation = int(len(serie_etude) * pourcentage)\n",
        "date_separation = serie_etude.index[temps_separation]\n",
        "\n",
        "serie_entrainement = serie_etude['Open'].iloc[:temps_separation]\n",
        "serie_test = serie_etude['Open'].iloc[temps_separation:]\n",
        "\n",
        "print(\"Taille de l'entrainement : %d\" %len(serie_entrainement))\n",
        "print(\"Taille de la validation : %d\" %len(serie_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZUMMMro5oe9"
      },
      "source": [
        "On normalise les données :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu_YxoSI5oe9"
      },
      "source": [
        "# Calcul de la moyenne et de l'écart type de la série\n",
        "mean = tf.math.reduce_mean(np.asarray(serie_entrainement))\n",
        "std = tf.math.reduce_std(np.asarray((serie_entrainement)))\n",
        "\n",
        "# Normalisation des données\n",
        "serie_entrainement = (serie_entrainement-mean)/std\n",
        "serie_test = (serie_test-mean)/std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_4OZJ-p5oe9"
      },
      "source": [
        "# Affiche la série\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "ax.plot(serie_entrainement, label=\"Entrainement\")\n",
        "ax.plot(serie_test,label=\"Validation\")\n",
        "\n",
        "ax.set_title(\"Evolution du prix du BTC\")\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-bANnT35oe-"
      },
      "source": [
        "**2. Création des datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_EweLDJ5oe-"
      },
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "\n",
        "def prepare_dataset_XY(serie, taille_fenetre, horizon, batch_size):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie)\n",
        "  dataset = dataset.window(taille_fenetre+horizon, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre + horizon))\n",
        "  dataset = dataset.map(lambda x: (tf.expand_dims(x[0:taille_fenetre],axis=1),x[-1:]))\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Jh1RZYo5oe_"
      },
      "source": [
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "taille_fenetre = 500\n",
        "horizon = 1\n",
        "batch_size = 1000\n",
        "\n",
        "# Création du dataset\n",
        "dataset = prepare_dataset_XY(serie_entrainement,taille_fenetre,horizon,batch_size)\n",
        "dataset_val = prepare_dataset_XY(serie_test,taille_fenetre,horizon,batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr2pbMox5oe_"
      },
      "source": [
        "print(len(list(dataset.as_numpy_iterator())))\n",
        "for element in dataset.take(1):\n",
        "  print(element[0].shape)\n",
        "  print(element[1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHCcYn6i5oe_"
      },
      "source": [
        "On extrait maintenant les deux tenseurs (X,Y) pour l'entrainement :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3ZhLIK15ofA"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "#56x((1000,4,1),(1000,1)) => (56*1000,4,1) ; (56*1000,1)\n",
        "\n",
        "x,y = tuple(zip(*dataset))\n",
        "\n",
        "# Recombine les données\n",
        "# (56,1000,4,1) => (56*128,4,1)\n",
        "# (56,1000,1) => (56*128,1)\n",
        "x_train = np.asarray(tf.reshape(np.asarray(x,dtype=np.float32),shape=(np.asarray(x).shape[0]*np.asarray(x).shape[1],taille_fenetre,1)))\n",
        "y_train = np.asarray(tf.reshape(np.asarray(y,dtype=np.float32),shape=(np.asarray(y).shape[0]*np.asarray(y).shape[1])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llnKyLvl5ofA"
      },
      "source": [
        "Puis la même chose pour les données de validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hadrKVrZ5ofB"
      },
      "source": [
        "# Extrait les X,Y du dataset_val\n",
        "\n",
        "x,y = tuple(zip(*dataset_val))\n",
        "\n",
        "# Recombine les données\n",
        "\n",
        "x_val = np.asarray(tf.reshape(np.asarray(x,dtype=np.float32),shape=(np.asarray(x).shape[0]*np.asarray(x).shape[1],taille_fenetre,1)))\n",
        "y_val = np.asarray(tf.reshape(np.asarray(y,dtype=np.float32),shape=(np.asarray(y).shape[0]*np.asarray(y).shape[1])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Htl_PyCdqKMK"
      },
      "source": [
        "# Prépartion des datasets X/Y(log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RqAZOY-WM3Y"
      },
      "source": [
        "**1. Séparation des données en données pour l'entrainement et la validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QL65znv4N1l2"
      },
      "source": [
        "On réserve 20% des données pour l'entrainement et le reste pour la validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouWe7AKpNi6m"
      },
      "source": [
        "# Sépare les données en entrainement et tests\n",
        "pourcentage = 0.8\n",
        "temps_separation = int(len(serie_etude) * pourcentage)\n",
        "date_separation = serie_etude.index[temps_separation]\n",
        "\n",
        "serie_entrainement_x = serie_etude['Open'].iloc[:temps_separation]\n",
        "serie_entrainement_y = serie_etude['Open_log'].iloc[:temps_separation]\n",
        "\n",
        "serie_test_x = serie_etude['Open'].iloc[temps_separation:]\n",
        "serie_test_y = serie_etude['Open_log'].iloc[temps_separation:]\n",
        "\n",
        "print(\"Taille de l'entrainement : %d\" %len(serie_entrainement_x))\n",
        "print(\"Taille de la validation : %d\" %len(serie_test_x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFPjmI7-N3FH"
      },
      "source": [
        "On normalise les labels :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJI0xJlQNsTW"
      },
      "source": [
        "# Calcul de la moyenne et de l'écart type de la série\n",
        "mean = tf.math.reduce_mean(np.asarray(serie_entrainement_y))\n",
        "std = tf.math.reduce_std(np.asarray((serie_entrainement_y)))\n",
        "\n",
        "# Normalisation des données\n",
        "serie_entrainement_y = (serie_entrainement_y-mean)/std\n",
        "serie_test_y = (serie_test_y-mean)/std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXcAk4BjNyCp"
      },
      "source": [
        "# Affiche la série\n",
        "f1, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 5))\n",
        "\n",
        "ax1.plot(serie_entrainement_x, label=\"Entrainement\")\n",
        "ax1.plot(serie_test_x,label=\"Validation\")\n",
        "\n",
        "ax2.plot(serie_entrainement_y, label=\"Entrainement (log)\")\n",
        "ax2.plot(serie_test_y,label=\"Validation (log)\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Gewbn6bOPTH"
      },
      "source": [
        "**2. Création des datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hen7VrB260G_"
      },
      "source": [
        "def prepare_dataset_XY(serie_x,serie_y, taille_fenetre, horizon, batch_size):\n",
        "  dataset_x = tf.data.Dataset.from_tensor_slices(serie_x)\n",
        "  dataset_x = dataset_x.window(taille_fenetre, shift=1, drop_remainder=True)\n",
        "  dataset_x = dataset_x.flat_map(lambda x: x.batch(taille_fenetre))\n",
        "  dataset_x = dataset_x.map(lambda x: tf.expand_dims(x[0:taille_fenetre],axis=1))\n",
        "\n",
        "  dataset_y = tf.data.Dataset.from_tensor_slices(serie_y)\n",
        "  dataset_y = dataset_y.window(taille_fenetre+horizon, shift=1, drop_remainder=True)\n",
        "  dataset_y = dataset_y.flat_map(lambda x: x.batch(taille_fenetre+horizon))\n",
        "  dataset_y = dataset_y.map(lambda x: (x[-1:]))\n",
        "\n",
        "  dataset = tf.data.Dataset.zip((dataset_x,dataset_y))\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXDZCZF9-0_V"
      },
      "source": [
        "  dataset = dataset.map(lambda x: (tf.expand_dims(x[0:taille_fenetre],axis=1),x[-1:]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oltK9vwW7B1W"
      },
      "source": [
        "x = np.linspace(0,1000,1001)\n",
        "y = np.linspace(1001,2000,1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdh-lOfb7fcG"
      },
      "source": [
        "toto = prepare_dataset_XY(x,x,10,1,5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjZmE59v7lJG"
      },
      "source": [
        "print(len(list(toto.as_numpy_iterator())))\n",
        "for element in toto.take(1):\n",
        "  print(element)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BDjHf5NN--b"
      },
      "source": [
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "taille_fenetre = 50\n",
        "horizon = 1\n",
        "batch_size = 1000\n",
        "\n",
        "# Création du dataset\n",
        "dataset = prepare_dataset_XY(serie_entrainement_x,serie_entrainement_y,taille_fenetre,horizon,batch_size)\n",
        "dataset_val = prepare_dataset_XY(serie_test_x,serie_test_y,taille_fenetre,horizon,batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c_BOKBJQksv"
      },
      "source": [
        "print(len(list(dataset.as_numpy_iterator())))\n",
        "for element in dataset.take(1):\n",
        "  print(element[0].shape)\n",
        "  print(element[1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VpY_YzzQRy9"
      },
      "source": [
        "On extrait maintenant les deux tenseurs (X,Y) pour l'entrainement :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwEhjWtWOdFf"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "#56x((1000,50,1),(1000,1)) => (56*1000,50,1) ; (56*1000,1)\n",
        "\n",
        "x,y = tuple(zip(*dataset))\n",
        "\n",
        "# Recombine les données\n",
        "# (56,1000,50,1) => (56*1000,50,1)\n",
        "# (56,1000,1) => (56*1000,1)\n",
        "x_train = np.asarray(tf.reshape(np.asarray(x,dtype=np.float32),shape=(np.asarray(x).shape[0]*np.asarray(x).shape[1],taille_fenetre,1)))\n",
        "y_train = np.asarray(tf.reshape(np.asarray(y,dtype=np.float32),shape=(np.asarray(y).shape[0]*np.asarray(y).shape[1])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T70G6av-jV1r"
      },
      "source": [
        "Puis la même chose pour les données de validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNIUOBLJjVQr"
      },
      "source": [
        "# Extrait les X,Y du dataset_val\n",
        "\n",
        "x,y = tuple(zip(*dataset_val))\n",
        "\n",
        "# Recombine les données\n",
        "\n",
        "x_val = np.asarray(tf.reshape(np.asarray(x,dtype=np.float32),shape=(np.asarray(x).shape[0]*np.asarray(x).shape[1],taille_fenetre,1)))\n",
        "y_val = np.asarray(tf.reshape(np.asarray(y,dtype=np.float32),shape=(np.asarray(y).shape[0]*np.asarray(y).shape[1])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "083QISTMM3AM"
      },
      "source": [
        "# Optimisation des hyperparamètres"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VzGM7ODMf8e"
      },
      "source": [
        "**1. Création de la série horaire pour l'optimisation des hyperparamètres**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKW0cGrbMl7u"
      },
      "source": [
        "# Définition des dates de début et de fin\n",
        "\n",
        "date_debut = \"2020-01-01 00:00:00\"\n",
        "date_fin = \"2021-03-31 00:00:00\"\n",
        "\n",
        "serie_opti = serie_etude['Open'].loc[date_debut:date_fin].copy()\n",
        "serie_opti"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJy04evcNBST"
      },
      "source": [
        "# Affiche la série\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(serie_opti)\n",
        "plt.title(\"Evolution du prix du BTC\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K5rZtb0Nc8-"
      },
      "source": [
        "**1. Préparation des données**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP9rTvLRhrsq"
      },
      "source": [
        "On normalise les données :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYoSqpbuhFHQ"
      },
      "source": [
        "# Calcul de la moyenne et de l'écart type de la série\n",
        "mean = tf.math.reduce_mean(np.asarray(serie_opti))\n",
        "std = tf.math.reduce_std(np.asarray((serie_opti)))\n",
        "\n",
        "# Normalisation des données\n",
        "serie_opti = (serie_opti-mean)/std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AB4yOw4Ohwvw"
      },
      "source": [
        "**2. Création du dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCBTKbgkhzL-"
      },
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "\n",
        "def prepare_dataset_XY(serie, taille_fenetre, horizon, batch_size):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie)\n",
        "  dataset = dataset.window(taille_fenetre+horizon, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre + horizon))\n",
        "  dataset = dataset.map(lambda x: (tf.expand_dims(x[0:taille_fenetre],axis=1),x[-1:]))\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "  return dataset\n",
        "\n",
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "taille_fenetre = 4\n",
        "horizon = 1\n",
        "batch_size = 32\n",
        "\n",
        "# Création du dataset\n",
        "dataset = prepare_dataset_XY(serie_entrainement,taille_fenetre,horizon,batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyzkXx5eh_To"
      },
      "source": [
        "print(len(list(dataset.as_numpy_iterator())))\n",
        "for element in dataset.take(1):\n",
        "  print(element[0].shape)\n",
        "  print(element[1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt06fAi0iz0m"
      },
      "source": [
        "On extriat maintenant les données X et les labels Y du dataset :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qt19cKylizLA"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "# 272x((32,4,1),(32,1)) => (272*32,4,1) ; (272*32,1)\n",
        "\n",
        "x,y = tuple(zip(*dataset))\n",
        "\n",
        "# Recombine les données\n",
        "# (272,32,4,1) => (272*32,4,1)\n",
        "# (272,32,1) => (272*32,1)\n",
        "x_train = np.asarray(tf.reshape(np.asarray(x,dtype=np.float32),shape=(np.asarray(x).shape[0]*np.asarray(x).shape[1],taille_fenetre,1)))\n",
        "y_train = np.asarray(tf.reshape(np.asarray(y,dtype=np.float32),shape=(np.asarray(y).shape[0]*np.asarray(y).shape[1],1)))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUpvJmTQiNk-"
      },
      "source": [
        "**3. Définition du modèle**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuAyb8pciUle"
      },
      "source": [
        "Dans le modèle, les paramètres dim_LSTM, l1_reg, l2_reg seront optimisés :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCFNM5i2iP0Q"
      },
      "source": [
        "def ModelLSTM(dim_LSTM = 10, l1_reg=0, l2_reg=0):\n",
        "\n",
        "  entrees = tf.keras.layers.Input(shape=(taille_fenetre,1))\n",
        "\n",
        "  # Encodeur\n",
        "  s_encodeur = tf.keras.layers.LSTM(dim_LSTM, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg,l2=l2_reg))(entrees)\n",
        "  \n",
        "  # Générateur\n",
        "  sortie = tf.keras.layers.Dense(1,kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg,l2=l2_reg))(s_encodeur)\n",
        "\n",
        "  # Construction du modèle\n",
        "  model = tf.keras.Model(entrees,sortie)\n",
        "  model.compile(loss='mse', optimizer='adam')\n",
        "  return(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYTuqrcYii9w"
      },
      "source": [
        "**4. Cross-validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVCNBdgcihuv"
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import KFold, TimeSeriesSplit, GridSearchCV\n",
        "\n",
        "# Définitions des paramètres\n",
        "dim_LSTM = [5,10,15,20,30,40]\n",
        "l1_reg = [0,0.001,0.01,0.1]\n",
        "l2_reg = [0,0.001,0.01,0.1]\n",
        "batch_size = [32]\n",
        "\n",
        "param_grid = {'dim_LSTM': dim_LSTM, 'l1_reg': l1_reg, 'l2_reg': l2_reg, 'batch_size': batch_size}\n",
        "param_grid = {'dim_LSTM': dim_LSTM, 'batch_size': batch_size}\n",
        "\n",
        "max_periodes = 5\n",
        "\n",
        "# Surveillance de l'entrainement\n",
        "es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=50, min_delta=1e-7, restore_best_weights=True)\n",
        "\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits = 5)\n",
        "model = KerasRegressor(build_fn=ModelLSTM, epochs=max_periodes, verbose=2)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=tscv, n_jobs=-1, verbose=3)\n",
        "\n",
        "grid_result = grid.fit(x_train, y_train,callbacks=[es])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWTWBh8DjJpP"
      },
      "source": [
        "# Affiche les résultats\n",
        "print(\"Meilleur résultat : %f avec %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params_ = grid_result.cv_results_['params']\n",
        "for mean, stdev, param_ in zip(means, stds, params_):\n",
        "  print(\"%f (%f) with %r\" % (mean, stdev, param_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHfiXLFZhrPs"
      },
      "source": [
        "# Création du modèle LSTM de type encodeur-décodeur"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd9AzWFtjnjs"
      },
      "source": [
        "**1. Création du réseau**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9OCzL7UjAhL"
      },
      "source": [
        "Par défaut, la dimension des vecteurs cachés est de 10 et aucune régularisation n'est utilisée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnFw_FPPhxiE"
      },
      "source": [
        "dim_LSTM = 100\n",
        "l1_reg = 0.0\n",
        "l2_reg = 0.0\n",
        "\n",
        "# Définition de l'entrée du modèle\n",
        "entrees = tf.keras.layers.Input(shape=(taille_fenetre,1),batch_size=batch_size)\n",
        "\n",
        "# Encodeur\n",
        "s_encodeur = tf.keras.layers.LSTM(dim_LSTM, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg,l2=l2_reg),stateful=True)(entrees)\n",
        "\n",
        "# Décodeur\n",
        "s_decodeur = tf.keras.layers.Dense(dim_LSTM,activation=\"tanh\",kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg,l2=l2_reg))(s_encodeur)\n",
        "s_decodeur = tf.keras.layers.Concatenate()([s_decodeur,s_encodeur])\n",
        "\n",
        "# Générateur\n",
        "sortie = tf.keras.layers.Dense(1,kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg,l2=l2_reg))(s_decodeur)\n",
        "\n",
        "# Construction du modèle\n",
        "model = tf.keras.Model(entrees,sortie)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D75XXP5J_nzs"
      },
      "source": [
        "dim_LSTM = 100\n",
        "l1_reg = 0.0\n",
        "l2_reg = 0.0\n",
        "\n",
        "\n",
        "class PenalizedLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self,reduction=tf.keras.losses.Reduction.AUTO,name='Penalized_loss'):\n",
        "    self.reduction = reduction\n",
        "    self.name = name\n",
        "    self.n = 0\n",
        "\n",
        "  def __call__(self, y_true, y_pred, sample_weight=None):\n",
        "    print(y_true.shape)\n",
        "    print(y_pred.shape)\n",
        "    if self.n == 0:\n",
        "      self.n = 1\n",
        "      error = (tf.keras.metrics.mse(y_true,y_pred))\n",
        "      return (error)\n",
        "    else:\n",
        "      error = (tf.keras.metrics.mse(y_true,y_pred))\n",
        "      return (error)\n",
        "\n",
        "def make_model():\n",
        "  # loss\n",
        "  loss = PenalizedLoss()\n",
        "\n",
        "  # Définition de l'entrée du modèle\n",
        "  entrees = tf.keras.layers.Input(shape=(taille_fenetre,1))\n",
        "\n",
        "  # Encodeur\n",
        "  s_encodeur = tf.keras.layers.LSTM(dim_LSTM, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg,l2=l2_reg))(entrees)\n",
        "\n",
        "  # Décodeur\n",
        "  s_decodeur = tf.keras.layers.Dense(dim_LSTM,activation=\"tanh\",kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg,l2=l2_reg))(s_encodeur)\n",
        "  s_decodeur = tf.keras.layers.Concatenate()([s_decodeur,s_encodeur])\n",
        "\n",
        "  # Générateur\n",
        "  sortie = tf.keras.layers.Dense(1,kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg,l2=l2_reg))(s_decodeur)\n",
        "\n",
        "  # Construction du modèle\n",
        "  model = tf.keras.Model(entrees,sortie)\n",
        "  optimiseur=tf.keras.optimizers.Adam()\n",
        "  model.compile(loss=loss, optimizer=optimiseur)\n",
        "  return model\n",
        "\n",
        "model = make_model()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_azfJaeUo2nU"
      },
      "source": [
        "**2. Optimisation de l'apprentissage**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf3lwaQBnjxL"
      },
      "source": [
        "Pour accélérer le traitement des données, nous n'allons pas utiliser l'intégralité des données pendant la mise à jour du gradient, comme cela a été fait jusqu'à présent (en utilisant le dataset).  \n",
        "Cette fois-ci, nous allons forcer les mises à jour du gradient à se produire de manière moins fréquente en attribuant la valeur du batch_size à prendre en compte lors de la regression du modèle.  \n",
        "Pour cela, on utilise l'argument \"batch_size\" dans la méthode fit. En précisant un batch_size=1000, cela signifie que :\n",
        " - Sur notre total de 56000 échantillons, 56 seront utilisés pour les calculs du gradient\n",
        " - Il y aura également 56 itérations à chaque période.\n",
        "  \n",
        "    \n",
        "    \n",
        "Si nous avions pris le dataset comme entrée, nous aurions eu :\n",
        "- Un total de 56000 échantillons également\n",
        "- Chaque période aurait également pris 56 itérations pour se compléter\n",
        "- Mais 1000 échantillons auraient été utilisés pour le calcul du gradient, au lieu de 56 avec la méthode utilisée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6Z35rNWj5SA"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam()\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"mse\", optimizer=optimiseur, metrics=\"mse\")\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(x=x_train,y=y_train,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint],batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2aP9J3TkNGG"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 1])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZxCgpuYkQ2Q"
      },
      "source": [
        "# Chargement des poids sauvegardés\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmdbo23qkTKE"
      },
      "source": [
        "max_periodes = 10000\n",
        "\n",
        "# Classe permettant d'arrêter l'entrainement si la variation\n",
        "# devient plus petite qu'une valeur à choisir sur un nombre\n",
        "# de périodes à choisir\n",
        "class StopTrain(keras.callbacks.Callback):\n",
        "    def __init__(self, delta=0.01,periodes=100, term=\"loss\", logs={}):\n",
        "      self.n_periodes = 0\n",
        "      self.periodes = periodes\n",
        "      self.loss_1 = 100\n",
        "      self.delta = delta\n",
        "      self.term = term\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "      diff_loss = abs(self.loss_1 - logs[self.term])\n",
        "      self.loss_1 = logs[self.term]\n",
        "      if (diff_loss < self.delta):\n",
        "        self.n_periodes = self.n_periodes + 1\n",
        "      else:\n",
        "        self.n_periodes = 0\n",
        "      if (self.n_periodes == self.periodes):\n",
        "        print(\"Arrêt de l'entrainement...\")\n",
        "        self.model.stop_training = True\n",
        "\n",
        "\n",
        "class PenalizedLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self,reduction=tf.keras.losses.Reduction.AUTO,name='Penalized_loss'):\n",
        "    self.reduction = reduction\n",
        "    self.name = name\n",
        "    self.y_true_1 = np.linspace(1,1000,1000)\n",
        "    self.n = 0\n",
        "\n",
        "  def __call__(self, y_true, y_pred, sample_weight=None):\n",
        "    if self.n == 0:\n",
        "      self.n = 1\n",
        "      error = (tf.keras.metrics.mse(y_true[:,0],y_pred[:,0]))\n",
        "      self.y_true_1 = y_true[:,0]\n",
        "      return (error)\n",
        "    else:\n",
        "      penalized = tf.keras.metrics.mse(y_true_1,y_pred[:,0])\n",
        "      error = (tf.keras.metrics.mse(y_true[:,0],y_pred[:,0]))\n",
        "      return (error + 1/penalized)\n",
        "\n",
        "def  My_MSE(y_true,y_pred):\n",
        "  return(tf.keras.metrics.mse(y_true,y_pred)*std.numpy()+mean.numpy())\n",
        "\n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.001,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0.01)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "#optimiseur=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=PenalizedLoss(), optimizer=optimiseur, metrics=[\"mse\",My_MSE])\n",
        "\n",
        "# Entraine le modèle, avec une réduction des calculs du gradient\n",
        "#historique = model.fit(x=x_train,y=y_train,validation_data=(x_val,y_val), epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=1e-7,periodes = 10, term=\"My_MSE\")],batch_size=batch_size)\n",
        "\n",
        "# Entraine le modèle sans réduction de calculs\n",
        "historique = model.fit(dataset,validation_data=dataset_val, epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=1e-8,periodes = 10, term=\"val_My_MSE\")])\n",
        "#historique = model.fit(dataset, epochs=max_periodes)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfbomV0LS9LD"
      },
      "source": [
        "model.load_weights(\"poids_train.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MDY8O1-l6kN"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEuSDQ6vZnBm"
      },
      "source": [
        "# Evaluation du modèle\n",
        "\n",
        "model.evaluate(dataset)\n",
        "model.evaluate(dataset_val)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOrzs53kvrIw"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('poids_train.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbzro22hgt4b"
      },
      "source": [
        "**3. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMmVn1e5zEAm"
      },
      "source": [
        "# Création des instants d'entrainement et de validation\n",
        "y_train_timing = serie_entrainement.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_train)]\n",
        "y_val_timing = serie_test.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_val)]\n",
        "\n",
        "# Calcul des prédictions\n",
        "pred_ent = model.predict(x_train, verbose=1)\n",
        "pred_val = model.predict(x_val, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvAclrEVLW97"
      },
      "source": [
        "from statsmodels.tsa.statespace.tools import diff\n",
        "\n",
        "predictions=[]\n",
        "\n",
        "for t in range(0,100):\n",
        "  data_to_predict = serie_entrainement[t:t+taille_fenetre]\n",
        "\n",
        "  data_to_predict = tf.expand_dims(data_to_predict,axis=1)\n",
        "  data_to_predict = tf.expand_dims(data_to_predict,axis=0)\n",
        "  pred = model.predict(data_to_predict)\n",
        "  predictions.append(pred[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8nID2lULofD"
      },
      "source": [
        "predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TACq-1J4MNRy"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "tmax = len(np.asarray(predictions)[:,0])\n",
        "\n",
        "# Courbe originale\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index,y=serie_entrainement[0:taille_fenetre],line=dict(color='blue', width=1),name=\"diff_reel\"))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "#fig.add_trace(go.Scatter(x=serie_etude.index,y=data_to_predict[:,:,0][0],line=dict(color='green', width=1),name=\"data_to_predict\"))\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index[taille_fenetre:taille_fenetre+tmax],y=np.asarray(predictions)[:,0],line=dict(color='red', width=1),name=\"prediction\"))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index[taille_fenetre:taille_fenetre+tmax],y=serie_entrainement[taille_fenetre:taille_fenetre+tmax],line=dict(color='black', width=1),name=\"true\"))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsT_RRS6iFhA"
      },
      "source": [
        "**4.Affichage sur une période de 1 heure**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoGhoCJWHOuj"
      },
      "source": [
        "Création d'une série contenant les valeurs originales et les prédictions, synchronisées dans le temps :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBWDeZVBfynP"
      },
      "source": [
        "serie_btc_ent_ori = serie_entrainement*std+mean\n",
        "serie_btc_val_ori = serie_test*std+mean\n",
        "\n",
        "serie_btc_ent_pred = pd.Series(data=(pred_ent[:,0]*std+mean),index=y_train_timing)\n",
        "serie_btc_val_pred = pd.Series(data=(pred_val[:,0]*std+mean),index=y_val_timing)\n",
        "\n",
        "serie_btc_ori = pd.concat([serie_btc_ent_ori,serie_btc_val_ori])\n",
        "serie_btc_pred = pd.concat([serie_btc_ent_pred,serie_btc_val_pred])\n",
        "\n",
        "serie_btc_ori = serie_btc_ori.fillna(method=\"backfill\")\n",
        "serie_btc_pred = serie_btc_pred.fillna(method=\"backfill\")\n",
        "\n",
        "serie_btc_ent_ori = serie_btc_ent_ori.fillna(method=\"backfill\")\n",
        "serie_btc_val_ori = serie_btc_val_ori.fillna(method=\"backfill\")\n",
        "serie_btc_ent_pred = serie_btc_ent_pred.fillna(method=\"backfill\")\n",
        "serie_btc_val_pred = serie_btc_val_pred.fillna(method=\"backfill\")\n",
        "\n",
        "frame = {'BTC_ALL' : serie_btc_ori, 'BTC_ENT': serie_btc_ent_ori, 'BTC_VAL' : serie_btc_val_ori, 'BTC_PRED' : serie_btc_pred, 'BTC_PRED_ENT' : serie_btc_ent_pred, 'BTC_PRED_VAL':serie_btc_val_pred}\n",
        "df_resultats = pd.DataFrame(frame)\n",
        "df_resultats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pFnvAVy-3lM"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbe originale\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_ALL'],line=dict(color='blue', width=1),name=\"Prix BTC\"))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_ENT'],line=dict(color='green', width=1),name=\"Entrainement\"))\n",
        "\n",
        "# Courbe de validation\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_VAL'],line=dict(color='red', width=1),name=\"Validation\"))\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNUg-0NAR0gv"
      },
      "source": [
        "serie_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0SkDEbHS2my"
      },
      "source": [
        "from scipy.special import inv_boxcox"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Nv-2zsjTTJ6"
      },
      "source": [
        "from scipy.integrate import cumtrapz\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def f(x):\n",
        "    return serie_etude['Open'][0:taille_fenetre+1]\n",
        "\n",
        "f = np.vectorize(f)\n",
        "\n",
        "X = np.linspace(0,taille_fenetre+1,taille_fenetre+2)\n",
        "\n",
        "fv = f(X)\n",
        "plt.plot(fv)\n",
        "\n",
        "F = cumtrapz(fv, x=X, initial=0)\n",
        "plt.plot(F);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_C5qDpsRkrP"
      },
      "source": [
        "from statsmodels.tsa.statespace.tools import diff\n",
        "\n",
        "predictions=[]\n",
        "\n",
        "for t in range(0,200):\n",
        "  data_to_predict = serie_etude['Open'][t:t+taille_fenetre+1]\n",
        "  data_to_predict = boxcox(data_to_predict,lam)\n",
        "  data_to_predict = data_to_predict - trend[t:t+taille_fenetre+1]\n",
        "  data_to_predict = diff(data_to_predict,1)\n",
        "  data_to_predict = np.insert(data_to_predict,0,0)\n",
        "\n",
        "  data_to_predict = tf.expand_dims(data_to_predict,axis=1)\n",
        "  data_to_predict = tf.expand_dims(data_to_predict,axis=0)\n",
        "  pred = model.predict(data_to_predict[:,1:,:])\n",
        "  predictions.append(pred[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Zp30SLWZ7cG"
      },
      "source": [
        "np.asarray(predictions)[:,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OP2cvVBUW36R"
      },
      "source": [
        "serie_etude.index[taille_fenetre+1:taille_fenetre+2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7n75-EQyUfk8"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "tmax = len(np.asarray(predictions)[:,0])\n",
        "\n",
        "# Courbe originale\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index,y=serie_etude['diff'][0:taille_fenetre+1],line=dict(color='blue', width=1),name=\"diff_reel\"))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "#fig.add_trace(go.Scatter(x=serie_etude.index,y=data_to_predict[:,:,0][0],line=dict(color='green', width=1),name=\"data_to_predict\"))\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index[taille_fenetre+1:taille_fenetre+1+tmax],y=np.asarray(predictions)[:,0],line=dict(color='red', width=1),name=\"prediction\"))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index[taille_fenetre+1:taille_fenetre+1+tmax],y=serie_etude['diff'][taille_fenetre+1:taille_fenetre+1+tmax],line=dict(color='black', width=1),name=\"true\"))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T72zWjfGrtQu"
      },
      "source": [
        "# Création du modèle LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr-AD-INrtQv"
      },
      "source": [
        "**1. Création du réseau**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipNxd-RNrtQv"
      },
      "source": [
        "Par défaut, la dimension des vecteurs cachés est de 10 et aucune régularisation n'est utilisée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52Iv3nK0rtQ9"
      },
      "source": [
        "dim_LSTM = 40\n",
        "l1_reg = 0.0\n",
        "l2_reg = 0.0\n",
        "\n",
        "entrees = tf.keras.layers.Input(shape=(taille_fenetre,1))\n",
        "\n",
        "# Encodeur\n",
        "s_encodeur = tf.keras.layers.LSTM(dim_LSTM, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg,l2=l2_reg))(entrees)\n",
        "  \n",
        "# Générateur\n",
        "sortie = tf.keras.layers.Dense(1,kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg,l2=l2_reg))(s_encodeur)\n",
        "\n",
        "# Construction du modèle\n",
        "model = tf.keras.Model(entrees,sortie)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qv4ZaPfrtQ-"
      },
      "source": [
        "**2. Optimisation de l'apprentissage**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlrGDGQ-rtQ-"
      },
      "source": [
        "Pour accélérer le traitement des données, nous n'allons pas utiliser l'intégralité des données pendant la mise à jour du gradient, comme cela a été fait jusqu'à présent (en utilisant le dataset).  \n",
        "Cette fois-ci, nous allons forcer les mises à jour du gradient à se produire de manière moins fréquente en attribuant la valeur du batch_size à prendre en compte lors de la regression du modèle.  \n",
        "Pour cela, on utilise l'argument \"batch_size\" dans la méthode fit. En précisant un batch_size=1000, cela signifie que :\n",
        " - Sur notre total de 56000 échantillons, 56 seront utilisés pour les calculs du gradient\n",
        " - Il y aura également 56 itérations à chaque période.\n",
        "  \n",
        "    \n",
        "    \n",
        "Si nous avions pris le dataset comme entrée, nous aurions eu :\n",
        "- Un total de 56000 échantillons également\n",
        "- Chaque période aurait également pris 56 itérations pour se compléter\n",
        "- Mais 1000 échantillons auraient été utilisés pour le calcul du gradient, au lieu de 56 avec la méthode utilisée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOnTaeMSrtQ_"
      },
      "source": [
        "batch_size = 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APgK7pSyrtQ_"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam()\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"logcosh\", optimizer=optimiseur, metrics=\"mse\")\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(x=x_train,y=y_train,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint],batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFX_PudkrtRA"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[30], taux[99], 0, 0.04])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8ptlq-JrtRA"
      },
      "source": [
        "# Chargement des poids sauvegardés\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ_TJVE_rtRB"
      },
      "source": [
        "max_periodes = 1000\n",
        "\n",
        "# Classe permettant d'arrêter l'entrainement si la variation\n",
        "# devient plus petite qu'une valeur à choisir sur un nombre\n",
        "# de périodes à choisir\n",
        "class StopTrain(keras.callbacks.Callback):\n",
        "    def __init__(self, delta=0.01,periodes=100, term=\"loss\", logs={}):\n",
        "      self.n_periodes = 0\n",
        "      self.periodes = periodes\n",
        "      self.loss_1 = 100\n",
        "      self.delta = delta\n",
        "      self.term = term\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "      diff_loss = abs(self.loss_1 - logs[self.term])\n",
        "      self.loss_1 = logs[self.term]\n",
        "      if (diff_loss < self.delta):\n",
        "        self.n_periodes = self.n_periodes + 1\n",
        "      else:\n",
        "        self.n_periodes = 0\n",
        "      if (self.n_periodes == self.periodes):\n",
        "        print(\"Arrêt de l'entrainement...\")\n",
        "        self.model.stop_training = True\n",
        "\n",
        "def  My_MSE(y_true,y_pred):\n",
        "  return(tf.keras.metrics.mse(y_true,y_pred)*std.numpy()+mean.numpy())\n",
        "  \n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.02,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0.01)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"logcosh\", optimizer=optimiseur, metrics=[\"mse\",My_MSE])\n",
        "\n",
        "# Entraine le modèle, avec une réduction des calculs du gradient\n",
        "historique = model.fit(x=x_train,y=y_train,validation_data=(x_val,y_val), epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=0.005,periodes = 10, term=\"My_MSE\")],batch_size=batch_size)\n",
        "\n",
        "# Entraine le modèle sans réduction de calculs\n",
        "#historique = model.fit(dataset,validation_data=dataset_val, epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=0.1,periodes = 10, term=\"val_My_MSE\")])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5ec3f0brtRC"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZFhBaeYrtRC"
      },
      "source": [
        "# Evaluation du modèle\n",
        "# Avec le modèle type encodeur/décodeur :\n",
        "# 56/56 [==============================] - 7s 113ms/step - loss: 1.1795e-04 - mse: 2.3664e-04 - My_MSE: 2712.2415\n",
        "# 14/14 [==============================] - 2s 113ms/step - loss: 0.3602 - mse: 1.9360 - My_MSE: 9650.2637\n",
        "model.evaluate(dataset)\n",
        "model.evaluate(dataset_val)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy8gdKf3rtRC"
      },
      "source": [
        "**3. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iodlvc6frtRD"
      },
      "source": [
        "# Création des instants d'entrainement et de validation\n",
        "y_train_timing = serie_entrainement.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_train)]\n",
        "y_val_timing = serie_test.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_val)]\n",
        "\n",
        "# Calcul des prédictions\n",
        "pred_ent = model.predict(x_train, verbose=1)\n",
        "pred_val = model.predict(x_val, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7URVy7qrtRD"
      },
      "source": [
        "**4.Affichage sur une période de 1 heure**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqONHLBbrtRD"
      },
      "source": [
        "Création d'une série contenant les valeurs originales et les prédictions, synchronisées dans le temps :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDgJ20qOrtRE"
      },
      "source": [
        "serie_btc_ent_ori = serie_entrainement*std+mean\n",
        "serie_btc_val_ori = serie_test*std+mean\n",
        "\n",
        "serie_btc_ent_pred = pd.Series(data=(pred_ent[:,0]*std+mean),index=y_train_timing)\n",
        "serie_btc_val_pred = pd.Series(data=(pred_val[:,0]*std+mean),index=y_val_timing)\n",
        "\n",
        "serie_btc_ori = pd.concat([serie_btc_ent_ori,serie_btc_val_ori])\n",
        "serie_btc_pred = pd.concat([serie_btc_ent_pred,serie_btc_val_pred])\n",
        "\n",
        "serie_btc_ori = serie_btc_ori.fillna(method=\"backfill\")\n",
        "serie_btc_pred = serie_btc_pred.fillna(method=\"backfill\")\n",
        "\n",
        "serie_btc_ent_ori = serie_btc_ent_ori.fillna(method=\"backfill\")\n",
        "serie_btc_val_ori = serie_btc_val_ori.fillna(method=\"backfill\")\n",
        "serie_btc_ent_pred = serie_btc_ent_pred.fillna(method=\"backfill\")\n",
        "serie_btc_val_pred = serie_btc_val_pred.fillna(method=\"backfill\")\n",
        "\n",
        "frame = {'BTC_ALL' : serie_btc_ori, 'BTC_ENT': serie_btc_ent_ori, 'BTC_VAL' : serie_btc_val_ori, 'BTC_PRED' : serie_btc_pred, 'BTC_PRED_ENT' : serie_btc_ent_pred, 'BTC_PRED_VAL':serie_btc_val_pred}\n",
        "df_resultats = pd.DataFrame(frame)\n",
        "df_resultats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7W2aoMPBrtRE"
      },
      "source": [
        "date_separation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfObUyVjrtRF"
      },
      "source": [
        "df_resultats.loc[date_separation-pd.Timedelta(hours=7):date_separation+pd.Timedelta(hours=7)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wv1MZoxPrtRF"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbe originale\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_ALL'],line=dict(color='blue', width=1),name=\"Prix BTC\"))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_ENT'],line=dict(color='green', width=1),name=\"Entrainement\"))\n",
        "\n",
        "# Courbe de validation\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_VAL'],line=dict(color='red', width=1),name=\"Validation\"))\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aq51_Kk6rtRK"
      },
      "source": [
        "**5. Détection de l'augmentation des erreurs dans la zone de validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2-WtViDrtRK"
      },
      "source": [
        "On peut imaginer devoir suivre en temps réel l'évolution des prédictions afin de détecter lorsque le modèle n'est plus valide."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6hb5UnZrtRK"
      },
      "source": [
        "# Détection de la date où commencent les anomalies\n",
        "# sur la période de validation\n",
        "\n",
        "# Construit les dataframe pour calculer les erreurs\n",
        "df_error_btc_ent = df_resultats[['BTC_ENT','BTC_PRED_ENT']].dropna()\n",
        "df_error_btc_val = df_resultats[['BTC_VAL','BTC_PRED_VAL']].dropna()\n",
        "\n",
        "# Calcul des erreurs relatives sur les entrainements et les validations\n",
        "erreur_ent = abs(np.asarray(df_error_btc_ent['BTC_ENT']) - np.asarray(df_error_btc_ent['BTC_PRED_ENT']))/np.asarray(df_error_btc_ent['BTC_ENT'])*100.0\n",
        "erreur_val = abs(np.asarray(df_error_btc_val['BTC_VAL']) - np.asarray(df_error_btc_val['BTC_PRED_VAL']))/np.asarray(df_error_btc_val['BTC_VAL'])*100.0\n",
        "\n",
        "# Erreur relative moyenne sur les entrainements\n",
        "erreur_ent_mape = tf.keras.metrics.mape(np.asarray(df_error_btc_ent['BTC_ENT']),np.asarray(df_error_btc_ent['BTC_PRED_ENT']))\n",
        "erreur_ent_mape_max = np.amax(erreur_ent)\n",
        "\n",
        "# Calcul le nombre d'anomalies sur les entrainements avec le ratio spécifié\n",
        "nbr_anomalies_ent = np.count_nonzero(erreur_ent>erreur_ent_mape)\n",
        "\n",
        "# Calcul du ratio d'anomalies sur la période d'entrainement\n",
        "ratio_ent = np.count_nonzero(erreur_ent>erreur_ent_mape)/erreur_ent.size\n",
        "\n",
        "# Recherche de la date à partir de laquelle on dépasse l\n",
        "n_erreurs = 0\n",
        "for i in range(5,erreur_val.size):\n",
        "  erreur_val_ = abs(np.asarray(df_error_btc_val['BTC_VAL'][i]) - np.asarray(df_error_btc_val['BTC_PRED_VAL'][i]))/np.asarray(df_error_btc_val['BTC_VAL'][i])*100.0\n",
        "  seuil = erreur_ent_mape_max*1\n",
        "  if erreur_val_ > seuil:\n",
        "    n_erreurs = n_erreurs + 1\n",
        "    if n_erreurs == 5:\n",
        "      index = df_error_btc_val.index[i]\n",
        "      break\n",
        "  else:\n",
        "    n_erreurs = 0\n",
        "print(index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBcqczT7rtRL"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbe originale\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_ALL'],line=dict(color='blue', width=1),name=\"Prix BTC\"))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_ENT'],line=dict(color='green', width=1),name=\"Entrainement\"))\n",
        "\n",
        "# Courbe de validation\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_VAL'],line=dict(color='red', width=1),name=\"Validation\"))\n",
        "\n",
        "# Délimite la zone d'erreur\n",
        "fig.add_shape(type=\"line\",x0=index,y0=0,x1=index,y1=np.amax(df_resultats['BTC_ALL']),name=\"Zone d'erreur\")\n",
        "\n",
        "# Affiche les courbes\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlsQSJqmrtRR"
      },
      "source": [
        "**5.Affichage sur une période de 1 jour**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P20ThOgSrtRR"
      },
      "source": [
        "# Echantillonage des données\n",
        "time = \"1D\"\n",
        "\n",
        "df_resultats = df_resultats.resample(time).asfreq()\n",
        "df_resultats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhUmbb8TrtRS"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbe originale\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_ALL'],line=dict(color='blue', width=1),name=\"Prix BTC\"))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_ENT'],line=dict(color='green', width=1),name=\"Entrainement\"))\n",
        "\n",
        "# Courbe de validation\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_VAL'],line=dict(color='red', width=1),name=\"Validation\"))\n",
        "\n",
        "# Délimite la zone d'erreur\n",
        "fig.add_shape(type=\"line\",x0=index,y0=0,x1=index,y1=np.amax(df_resultats['BTC_ALL']),name=\"Zone d'erreur\")\n",
        "\n",
        "# Affiche les courbes\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFgrzd6NrtRT"
      },
      "source": [
        "**6.Synthèse des erreurs sur les différentes zones**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIdnnWaYrtRU"
      },
      "source": [
        "# Echantillonage des données\n",
        "time = \"1H\"\n",
        "\n",
        "df_resultats = df_resultats.resample(time).asfreq()\n",
        "df_resultats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpdOSJcZrtRU"
      },
      "source": [
        "# Détection de la date où commencnte les anomalies\n",
        "# sur la période de validation\n",
        "\n",
        "# Construit les dataframe pour calculer les erreurs\n",
        "df_error_btc_ent = df_resultats[['BTC_ENT','BTC_PRED_ENT']].dropna()\n",
        "df_error_btc_val = df_resultats[['BTC_VAL','BTC_PRED_VAL']].dropna()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nou-vexFrtRU"
      },
      "source": [
        "# Erreurs d'entrainement\n",
        "mae_ent = tf.keras.metrics.mean_absolute_error(np.asarray(df_error_btc_ent['BTC_ENT']),np.asarray(df_error_btc_ent['BTC_PRED_ENT'])).numpy()\n",
        "mse_ent = tf.keras.metrics.mean_squared_error(np.asarray(df_error_btc_ent['BTC_ENT']),np.asarray(df_error_btc_ent['BTC_PRED_ENT'])).numpy()\n",
        "mape_ent = tf.keras.metrics.mape(np.asarray(df_error_btc_ent['BTC_ENT']),np.asarray(df_error_btc_ent['BTC_PRED_ENT'])).numpy()\n",
        "\n",
        "# Erreurs de validation\n",
        "mae_val = tf.keras.metrics.mean_absolute_error(np.asarray(df_error_btc_val['BTC_VAL']),np.asarray(df_error_btc_val['BTC_PRED_VAL'])).numpy()\n",
        "mse_val = tf.keras.metrics.mean_squared_error(np.asarray(df_error_btc_val['BTC_VAL']),np.asarray(df_error_btc_val['BTC_PRED_VAL'])).numpy()\n",
        "mape_val = tf.keras.metrics.mape(np.asarray(df_error_btc_val['BTC_VAL']),np.asarray(df_error_btc_val['BTC_PRED_VAL'])).numpy()\n",
        "\n",
        "# Erreurs sur la zone valide de validation\n",
        "mae_val_ok = tf.keras.metrics.mean_absolute_error(df_error_btc_val['BTC_VAL'][:index],np.asarray(df_error_btc_val['BTC_PRED_VAL'][:index])).numpy()\n",
        "mse_val_ok = tf.keras.metrics.mean_squared_error(df_error_btc_val['BTC_VAL'][:index],np.asarray(df_error_btc_val['BTC_PRED_VAL'][:index])).numpy()\n",
        "mape_val_ok = tf.keras.metrics.mape(df_error_btc_val['BTC_VAL'][:index],np.asarray(df_error_btc_val['BTC_PRED_VAL'][:index])).numpy()\n",
        "\n",
        "\n",
        "print(\"Erreur mae entrainement %s\" %mae_ent)\n",
        "print(\"Erreur mse entrainement : %s\" %mse_ent)\n",
        "print(\"Erreur mape entrainement : %s\\n\" %mape_ent)\n",
        "\n",
        "print(\"Erreur mae validation %s\" %mae_val)\n",
        "print(\"Erreur mse validation : %s\" %mse_val)\n",
        "print(\"Erreur mape entrainement : %s\\n\" %mape_val)\n",
        "\n",
        "print(\"Erreur mae validation zone OK %s\" %mae_val_ok)\n",
        "print(\"Erreur mse validation zone OK: %s\" %mse_val_ok)\n",
        "print(\"Erreur mape validation zone OK: %s\" %mape_val_ok)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PSUZ4SN2WRM"
      },
      "source": [
        "# Création du modèle LSTM avec auto-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC6DrlYT2WRS"
      },
      "source": [
        "**1. Création du réseau**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUF802Z22WRS"
      },
      "source": [
        "Par défaut, la dimension des vecteurs cachés est de 10 et aucune régularisation n'est utilisée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGtT8icQ7HBN"
      },
      "source": [
        "# Classe d'auto-attention\n",
        "# Applique les poids de la matrice d'attention sur les vecteurs de la couche récurrente\n",
        "\n",
        "# Importe le Backend de Keras\n",
        "from keras import backend as K\n",
        "\n",
        "# Définit une nouvelle classe Couche_Attention\n",
        "# Héritée de la classe Layer de Keras\n",
        "\n",
        "class Couche_Auto_Attention(tf.keras.layers.Layer):\n",
        "  # Fonction d'initialisation de la classe d'attention\n",
        "  def __init__(self,dim_att,nbr_hop):\n",
        "    self.dim_att = dim_att          # Dimension du vecteur d'attention\n",
        "    self.nbr_hop = nbr_hop\n",
        "    super().__init__()              # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.W = self.add_weight(shape=(self.dim_att,input_shape[2]),initializer='glorot_uniform',name=\"W\")\n",
        "    self.U = self.add_weight(shape=(self.nbr_hop,self.dim_att),initializer='glorot_uniform',name=\"U\")\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Définit la logique de la couche d'attention\n",
        "  # Arguments :   x : Tenseur d'entrée de dimension (None, nbr_v,dim)\n",
        "  def call(self,x):\n",
        "    # Calcul de la matrice XH contenant les\n",
        "    # représentations cachées des vecteurs\n",
        "    # issus de la couche GRU\n",
        "    xt = tf.transpose(x,perm=[0,2,1])           # (None,20,40) => (None,40,20)\n",
        "    Xh = tf.matmul(self.W,xt)                   # (#Att,40)x(None,40,20) = (None,#Att,20)\n",
        "    Xh = K.tanh(Xh)                             # Xh = (None,#Att,20)\n",
        "\n",
        "    # Calcul de la matrice des poids d'attention normalisés\n",
        "    A = tf.matmul(self.U,Xh)                    # (#hop,#Att)x(None,#Att,20) = (None,#Att,20)\n",
        "    A = tf.keras.activations.softmax(A,axis=2)  # (None,#Att,20)\n",
        "\n",
        "    # Calcul de la matrice des vecteur d'attentions\n",
        "    sortie = tf.matmul(A,x)                     # (None,#Att,20)x(None,20,40) = (None,#Att,40)\n",
        "    return tf.keras.layers.Flatten()(sortie)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uMyWVKe2WRT"
      },
      "source": [
        "dim_LSTM = 100\n",
        "l1_reg = 0.0\n",
        "l2_reg = 0.00\n",
        "nbr_hop = 20\n",
        "\n",
        "# Définition de l'entrée du modèle\n",
        "entrees = tf.keras.layers.Input(shape=(taille_fenetre,1))\n",
        "\n",
        "# Encodeur\n",
        "s_encodeur = tf.keras.layers.LSTM(dim_LSTM,return_sequences=True,recurrent_regularizer=tf.keras.regularizers.l2(1e-5))(entrees)\n",
        "s_attention = Couche_Auto_Attention(dim_att=dim_LSTM,nbr_hop=nbr_hop)(s_encodeur)\n",
        "\n",
        "# Décodeur\n",
        "s_decodeur = tf.keras.layers.Dense(dim_LSTM*nbr_hop,activation=\"tanh\")(s_attention)\n",
        "s_decodeur = tf.keras.layers.Concatenate()([s_decodeur,s_attention])\n",
        "\n",
        "# Générateur\n",
        "sortie = tf.keras.layers.Dense(1)(s_decodeur)\n",
        "\n",
        "# Construction du modèle\n",
        "model = tf.keras.Model(entrees,sortie)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwZnK5dl2WRV"
      },
      "source": [
        "**2. Optimisation de l'apprentissage**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwVSsLyv2WRV"
      },
      "source": [
        "Pour accélérer le traitement des données, nous n'allons pas utiliser l'intégralité des données pendant la mise à jour du gradient, comme cela a été fait jusqu'à présent (en utilisant le dataset).  \n",
        "Cette fois-ci, nous allons forcer les mises à jour du gradient à se produire de manière moins fréquente en attribuant la valeur du batch_size à prendre en compte lors de la regression du modèle.  \n",
        "Pour cela, on utilise l'argument \"batch_size\" dans la méthode fit. En précisant un batch_size=1000, cela signifie que :\n",
        " - Sur notre total de 56000 échantillons, 56 seront utilisés pour les calculs du gradient\n",
        " - Il y aura également 56 itérations à chaque période.\n",
        "  \n",
        "    \n",
        "    \n",
        "Si nous avions pris le dataset comme entrée, nous aurions eu :\n",
        "- Un total de 56000 échantillons également\n",
        "- Chaque période aurait également pris 56 itérations pour se compléter\n",
        "- Mais 1000 échantillons auraient été utilisés pour le calcul du gradient, au lieu de 56 avec la méthode utilisée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCl_AlQc2WRW"
      },
      "source": [
        "batch_size = 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ka2d6HB2WRW"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam()\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"mse\", optimizer=optimiseur, metrics=\"mse\")\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(x=x_train,y=y_train,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint],batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr6bQEZq2WRX"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 2])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryUpPIEF2WRX"
      },
      "source": [
        "# Chargement des poids sauvegardés\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhV93IpO2WRY"
      },
      "source": [
        "max_periodes = 1000\n",
        "\n",
        "# Classe permettant d'arrêter l'entrainement si la variation\n",
        "# devient plus petite qu'une valeur à choisir sur un nombre\n",
        "# de périodes à choisir\n",
        "class StopTrain(keras.callbacks.Callback):\n",
        "    def __init__(self, delta=0.01,periodes=100, term=\"loss\", logs={}):\n",
        "      self.n_periodes = 0\n",
        "      self.periodes = periodes\n",
        "      self.loss_1 = 100\n",
        "      self.delta = delta\n",
        "      self.term = term\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "      diff_loss = abs(self.loss_1 - logs[self.term])\n",
        "      self.loss_1 = logs[self.term]\n",
        "      if (diff_loss < self.delta):\n",
        "        self.n_periodes = self.n_periodes + 1\n",
        "      else:\n",
        "        self.n_periodes = 0\n",
        "      if (self.n_periodes == self.periodes):\n",
        "        print(\"Arrêt de l'entrainement...\")\n",
        "        self.model.stop_training = True\n",
        "\n",
        "def  My_MSE(y_true,y_pred):\n",
        "  return(tf.keras.metrics.mse(y_true,y_pred)*std.numpy()+mean.numpy())\n",
        "  \n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.001,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0.1)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"mse\", optimizer=optimiseur, metrics=[\"mse\",My_MSE])\n",
        "\n",
        "# Entraine le modèle, avec une réduction des calculs du gradient\n",
        "historique = model.fit(x=x_train,y=y_train,validation_data=(x_val,y_val), epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=0.0005,periodes = 10, term=\"My_MSE\")],batch_size=batch_size)\n",
        "\n",
        "# Entraine le modèle sans réduction de calculs\n",
        "#historique = model.fit(dataset,validation_data=dataset_val, epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=0.1,periodes = 10, term=\"val_My_MSE\")])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dgf1eJPPay1b"
      },
      "source": [
        "model.load_weights('poids_train.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JM-dtd32WRZ"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQj5FkF72WRa"
      },
      "source": [
        "# Evaluation du modèle\n",
        "\n",
        "model.evaluate(dataset)\n",
        "model.evaluate(dataset_val)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rExhkpy-2WRa"
      },
      "source": [
        "**3. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ii9KzNlZ2WRb"
      },
      "source": [
        "# Création des instants d'entrainement et de validation\n",
        "y_train_timing = serie_entrainement_x.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_train)]\n",
        "y_val_timing = serie_test_x.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_val)]\n",
        "\n",
        "# Calcul des prédictions\n",
        "pred_ent = model.predict(x_train, verbose=1)\n",
        "pred_val = model.predict(x_val, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEBjS2gG2WRb"
      },
      "source": [
        "**4.Affichage sur une période de 1 heure**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee3YihHo2WRb"
      },
      "source": [
        "Création d'une série contenant les valeurs originales et les prédictions, synchronisées dans le temps :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbyOs-He2WRc"
      },
      "source": [
        "serie_btc_ent_ori = serie_entrainement_y*std+mean\n",
        "serie_btc_val_ori = serie_test_y*std+mean\n",
        "\n",
        "serie_btc_ent_pred = pd.Series(data=(pred_ent[:,0]*std+mean),index=y_train_timing)\n",
        "serie_btc_val_pred = pd.Series(data=(pred_val[:,0]*std+mean),index=y_val_timing)\n",
        "\n",
        "serie_btc_ori = pd.concat([serie_btc_ent_ori,serie_btc_val_ori])\n",
        "serie_btc_pred = pd.concat([serie_btc_ent_pred,serie_btc_val_pred])\n",
        "\n",
        "serie_btc_ori = serie_btc_ori.fillna(method=\"backfill\")\n",
        "serie_btc_pred = serie_btc_pred.fillna(method=\"backfill\")\n",
        "\n",
        "serie_btc_ent_ori = serie_btc_ent_ori.fillna(method=\"backfill\")\n",
        "serie_btc_val_ori = serie_btc_val_ori.fillna(method=\"backfill\")\n",
        "serie_btc_ent_pred = serie_btc_ent_pred.fillna(method=\"backfill\")\n",
        "serie_btc_val_pred = serie_btc_val_pred.fillna(method=\"backfill\")\n",
        "\n",
        "frame = {'BTC_ALL' : serie_btc_ori, 'BTC_ENT': serie_btc_ent_ori, 'BTC_VAL' : serie_btc_val_ori, 'BTC_PRED' : serie_btc_pred, 'BTC_PRED_ENT' : serie_btc_ent_pred, 'BTC_PRED_VAL':serie_btc_val_pred}\n",
        "df_resultats = pd.DataFrame(frame)\n",
        "df_resultats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMvz38v02WRc"
      },
      "source": [
        "date_separation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3tE5FK-2WRd"
      },
      "source": [
        "df_resultats.loc[date_separation-pd.Timedelta(hours=7):date_separation+pd.Timedelta(hours=7)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvF19ok42WRd"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbe originale\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_ALL'],line=dict(color='blue', width=1),name=\"Prix BTC\"))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_ENT'],line=dict(color='green', width=1),name=\"Entrainement\"))\n",
        "\n",
        "# Courbe de validation\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_VAL'],line=dict(color='red', width=1),name=\"Validation\"))\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmyOVcdD2WRh"
      },
      "source": [
        "**5. Détection de l'augmentation des erreurs dans la zone de validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvS4fipz2WRi"
      },
      "source": [
        "On peut imaginer devoir suivre en temps réel l'évolution des prédictions afin de détecter lorsque le modèle n'est plus valide."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_aSn4Gg2WRi"
      },
      "source": [
        "# Détection de la date où commencent les anomalies\n",
        "# sur la période de validation\n",
        "\n",
        "# Construit les dataframe pour calculer les erreurs\n",
        "df_error_btc_ent = df_resultats[['BTC_ENT','BTC_PRED_ENT']].dropna()\n",
        "df_error_btc_val = df_resultats[['BTC_VAL','BTC_PRED_VAL']].dropna()\n",
        "\n",
        "# Calcul des erreurs relatives sur les entrainements et les validations\n",
        "erreur_ent = abs(np.asarray(df_error_btc_ent['BTC_ENT']) - np.asarray(df_error_btc_ent['BTC_PRED_ENT']))/np.asarray(df_error_btc_ent['BTC_ENT'])*100.0\n",
        "erreur_val = abs(np.asarray(df_error_btc_val['BTC_VAL']) - np.asarray(df_error_btc_val['BTC_PRED_VAL']))/np.asarray(df_error_btc_val['BTC_VAL'])*100.0\n",
        "\n",
        "# Erreur relative moyenne sur les entrainements\n",
        "erreur_ent_mape = tf.keras.metrics.mape(np.asarray(df_error_btc_ent['BTC_ENT']),np.asarray(df_error_btc_ent['BTC_PRED_ENT']))\n",
        "erreur_ent_mape_max = np.amax(erreur_ent)\n",
        "\n",
        "# Calcul le nombre d'anomalies sur les entrainements avec le ratio spécifié\n",
        "nbr_anomalies_ent = np.count_nonzero(erreur_ent>erreur_ent_mape)\n",
        "\n",
        "# Calcul du ratio d'anomalies sur la période d'entrainement\n",
        "ratio_ent = np.count_nonzero(erreur_ent>erreur_ent_mape)/erreur_ent.size\n",
        "\n",
        "# Recherche de la date à partir de laquelle on dépasse l\n",
        "n_erreurs = 0\n",
        "for i in range(5,erreur_val.size):\n",
        "  erreur_val_ = abs(np.asarray(df_error_btc_val['BTC_VAL'][i]) - np.asarray(df_error_btc_val['BTC_PRED_VAL'][i]))/np.asarray(df_error_btc_val['BTC_VAL'][i])*100.0\n",
        "  seuil = erreur_ent_mape_max*1\n",
        "  if erreur_val_ > seuil:\n",
        "    n_erreurs = n_erreurs + 1\n",
        "    if n_erreurs == 1:\n",
        "      index = df_error_btc_val.index[i]\n",
        "      break\n",
        "  else:\n",
        "    n_erreurs = 0\n",
        "print(index)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXx0vqSX2WRj"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbe originale\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_ALL'],line=dict(color='blue', width=1),name=\"Prix BTC\"))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_ENT'],line=dict(color='green', width=1),name=\"Entrainement\"))\n",
        "\n",
        "# Courbe de validation\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_VAL'],line=dict(color='red', width=1),name=\"Validation\"))\n",
        "\n",
        "# Délimite la zone d'erreur\n",
        "fig.add_shape(type=\"line\",x0=index,y0=0,x1=index,y1=np.amax(df_resultats['BTC_ALL']),name=\"Zone d'erreur\")\n",
        "\n",
        "# Affiche les courbes\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "st8Ynp3T2WRo"
      },
      "source": [
        "**5.Affichage sur une période de 1 jour**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZe5LHTL2WRo"
      },
      "source": [
        "# Echantillonage des données\n",
        "time = \"1D\"\n",
        "\n",
        "df_resultats = df_resultats.resample(time).asfreq()\n",
        "df_resultats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG6DOtBk2WRo"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbe originale\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_ALL'],line=dict(color='blue', width=1),name=\"Prix BTC\"))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_ENT'],line=dict(color='green', width=1),name=\"Entrainement\"))\n",
        "\n",
        "# Courbe de validation\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_VAL'],line=dict(color='red', width=1),name=\"Validation\"))\n",
        "\n",
        "# Délimite la zone d'erreur\n",
        "fig.add_shape(type=\"line\",x0=index,y0=0,x1=index,y1=np.amax(df_resultats['BTC_ALL']),name=\"Zone d'erreur\")\n",
        "\n",
        "# Affiche les courbes\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRKI61PK2WRp"
      },
      "source": [
        "**6.Synthèse des erreurs sur les différentes zones**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Hxnr0oT2WRq"
      },
      "source": [
        "# Echantillonage des données\n",
        "time = \"1H\"\n",
        "\n",
        "df_resultats = df_resultats.resample(time).asfreq()\n",
        "df_resultats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMW1t-kp2WRq"
      },
      "source": [
        "# Détection de la date où commencnte les anomalies\n",
        "# sur la période de validation\n",
        "\n",
        "# Construit les dataframe pour calculer les erreurs\n",
        "df_error_btc_ent = df_resultats[['BTC_ENT','BTC_PRED_ENT']].dropna()\n",
        "df_error_btc_val = df_resultats[['BTC_VAL','BTC_PRED_VAL']].dropna()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bxhjwQK2WRq"
      },
      "source": [
        "# Erreurs d'entrainement\n",
        "mae_ent = tf.keras.metrics.mean_absolute_error(np.asarray(df_error_btc_ent['BTC_ENT']),np.asarray(df_error_btc_ent['BTC_PRED_ENT'])).numpy()\n",
        "mse_ent = tf.keras.metrics.mean_squared_error(np.asarray(df_error_btc_ent['BTC_ENT']),np.asarray(df_error_btc_ent['BTC_PRED_ENT'])).numpy()\n",
        "mape_ent = tf.keras.metrics.mape(np.asarray(df_error_btc_ent['BTC_ENT']),np.asarray(df_error_btc_ent['BTC_PRED_ENT'])).numpy()\n",
        "\n",
        "# Erreurs de validation\n",
        "mae_val = tf.keras.metrics.mean_absolute_error(np.asarray(df_error_btc_val['BTC_VAL']),np.asarray(df_error_btc_val['BTC_PRED_VAL'])).numpy()\n",
        "mse_val = tf.keras.metrics.mean_squared_error(np.asarray(df_error_btc_val['BTC_VAL']),np.asarray(df_error_btc_val['BTC_PRED_VAL'])).numpy()\n",
        "mape_val = tf.keras.metrics.mape(np.asarray(df_error_btc_val['BTC_VAL']),np.asarray(df_error_btc_val['BTC_PRED_VAL'])).numpy()\n",
        "\n",
        "# Erreurs sur la zone valide de validation\n",
        "mae_val_ok = tf.keras.metrics.mean_absolute_error(df_error_btc_val['BTC_VAL'][:index],np.asarray(df_error_btc_val['BTC_PRED_VAL'][:index])).numpy()\n",
        "mse_val_ok = tf.keras.metrics.mean_squared_error(df_error_btc_val['BTC_VAL'][:index],np.asarray(df_error_btc_val['BTC_PRED_VAL'][:index])).numpy()\n",
        "mape_val_ok = tf.keras.metrics.mape(df_error_btc_val['BTC_VAL'][:index],np.asarray(df_error_btc_val['BTC_PRED_VAL'][:index])).numpy()\n",
        "\n",
        "\n",
        "print(\"Erreur mae entrainement %s\" %mae_ent)\n",
        "print(\"Erreur mse entrainement : %s\" %mse_ent)\n",
        "print(\"Erreur mape entrainement : %s\\n\" %mape_ent)\n",
        "\n",
        "print(\"Erreur mae validation %s\" %mae_val)\n",
        "print(\"Erreur mse validation : %s\" %mse_val)\n",
        "print(\"Erreur mape entrainement : %s\\n\" %mape_val)\n",
        "\n",
        "print(\"Erreur mae validation zone OK %s\" %mae_val_ok)\n",
        "print(\"Erreur mse validation zone OK: %s\" %mse_val_ok)\n",
        "print(\"Erreur mape validation zone OK: %s\" %mape_val_ok)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ntshp_muADWt"
      },
      "source": [
        "# Création du modèle End-To-End Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vddiuNuhAliy"
      },
      "source": [
        "def prepare_dataset_XY(serie, taille_fenetre, batch_size, nbr_sequences):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie)\n",
        "  dataset = dataset.window(taille_fenetre+1, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre + 1))\n",
        "  dataset = dataset.window(nbr_sequences+1, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(nbr_sequences+1,drop_remainder=True))\n",
        "  dataset = dataset.map(lambda x: [(tf.slice(x,[0,0],[nbr_sequences,taille_fenetre]),                           # (30;20)       [((30,20),(20)),(1)]\n",
        "                                   tf.squeeze(tf.slice(x,[nbr_sequences,0],[1,taille_fenetre]),axis=0)),        # (20)\n",
        "                                   tf.squeeze(tf.slice(x,[nbr_sequences,taille_fenetre],[1,1]),axis=0)])        # (1)\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WG2KIkJApxh"
      },
      "source": [
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "taille_fenetre = 15\n",
        "batch_size = 1000\n",
        "Nbr_Sequences = 10\n",
        "\n",
        "dataset = prepare_dataset_XY(serie_entrainement,taille_fenetre,batch_size,Nbr_Sequences)              # 56x((1000,10,15),(1000,15)),(1000,1)\n",
        "dataset_val = prepare_dataset_XY(serie_test,taille_fenetre,batch_size,Nbr_Sequences)              # 56x((1000,10,15),(1000,15)),(1000,1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQEziTjSCFDg"
      },
      "source": [
        "len(list(dataset.as_numpy_iterator()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4z2Y65dCJg4"
      },
      "source": [
        "for element in dataset.take(1):\n",
        "  print(element)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9JXgCGOCiiP"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "#56x((1000,4,1),(1000,1)) => (56*1000,4,1) ; (56*1000,1)\n",
        "\n",
        "x,y = tuple(zip(*dataset))\n",
        "\n",
        "# Recombine les données\n",
        "# (56,1000,4,1) => (56*128,4,1)\n",
        "# (56,1000,1) => (56*128,1)\n",
        "x_train = np.asarray(tf.reshape(np.asarray(x,dtype=np.float32),shape=(np.asarray(x).shape[0]*np.asarray(x).shape[1],taille_fenetre,1)))\n",
        "y_train = np.asarray(tf.reshape(np.asarray(y,dtype=np.float32),shape=(np.asarray(y).shape[0]*np.asarray(y).shape[1])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW2AKgnLADW9"
      },
      "source": [
        "**1. Création du réseau**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNx-iQ3bADW9"
      },
      "source": [
        "Par défaut, la dimension des vecteurs cachés est de 10 et aucune régularisation n'est utilisée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1P16yDVAM0B"
      },
      "source": [
        "# Définition du de la couche du modèle\n",
        "# End-to-End Memory Network\n",
        "# Epaquetage des données avec le dernier état caché d'une couche GRU\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "class Couche_End_to_End_MN(tf.keras.layers.Layer):\n",
        "  # Fonction d'initialisation de la classe d'attention\n",
        "  # dim_GRU : Dimension des vecteurs GRU\n",
        "  # x : Séquences à mémoriser (batch_size, Nbr_Sequence, taille_fenetre)\n",
        "  # Fonction de la couche lambda d'entrée\n",
        "  def __init__(self,dim_GRU,regul=0.0):\n",
        "    self.dim_GRU = dim_GRU\n",
        "    self.regul = regul\n",
        "    super().__init__()          # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    # Définition des couches GRU pour traiter les séquences d'entrée\n",
        "    self.couche_GRU_A = tf.keras.layers.GRU(self.dim_GRU,kernel_regularizer=tf.keras.regularizers.l2(self.regul))\n",
        "    self.couche_GRU_B = tf.keras.layers.GRU(self.dim_GRU,kernel_regularizer=tf.keras.regularizers.l2(self.regul))\n",
        "    self.couche_GRU_C = tf.keras.layers.GRU(self.dim_GRU,kernel_regularizer=tf.keras.regularizers.l2(self.regul))\n",
        "\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Définit la logique de la couche d'attention\n",
        "  # Arguments :     x : (batch_size, Nbr_Sequence, taille_fenetre)\n",
        "  #                 y : (batch_size, taille_fenetre)\n",
        "  # Exemple :   batch_size = 32\n",
        "  #             Nbr_Sequence =30\n",
        "  #             taille_fenetre = 20\n",
        "  #             dim_GRU = 40 \n",
        "  def call(self,x,y):\n",
        "    # Création des vecteurs mi dans le tenseur M\n",
        "    M = tf.expand_dims(x,axis=-1)                                   # (32,30,20) => (32,30,20,1)\n",
        "    M = tf.keras.layers.TimeDistributed(self.couche_GRU_A)(M)       # (32,30,20,1) => (32,30,40) : TimeStep = 30 : (32,20,1) envoyé\n",
        "    M = K.tanh(M)\n",
        "\n",
        "    # Création du vecteur d'état u\n",
        "    u = tf.expand_dims(y,axis=-1)                                   # (32,20) => (32,20,1)\n",
        "    u = self.couche_GRU_B(u)                                        # (32,20,1) => (32,40)\n",
        "    u = tf.expand_dims(u,axis=-1)                                   # (32,40) => (32,40,1)\n",
        "    u = K.tanh(u)                                                   # (32,40,1)\n",
        "\n",
        "    # Calcul des poids d'attention\n",
        "    p = tf.matmul(M,u)                                              # (32,30,40)x(32,40,1)=(32,30,1)\n",
        "    p = tf.keras.activations.softmax(p,axis=1)                      # (32,30,1)\n",
        "\n",
        "    # Création des vecteurs ci dans le tenseur C\n",
        "    C = tf.expand_dims(x,axis=-1)                                   # (32,30,20) => (32,30,20,1)\n",
        "    C = tf.keras.layers.TimeDistributed(self.couche_GRU_C)(C)       # (32,30,20,1) => (32,30,40) : TimeStep = 30 : (32,20,1) envoyé\n",
        "    C = K.tanh(C)\n",
        "\n",
        "    # Calcul du vecteur réponse issu de la mémoire\n",
        "    o = tf.multiply(C,p)                                            # (32,30,40)_x_(32,30,1) = (32,30,40)\n",
        "    o = K.sum(o, axis=1)                                            # (32,40)\n",
        "    o = K.tanh(o)                                                   # (32,40)\n",
        "    \n",
        "    # Retourne le vecteur d'attention\n",
        "    return (o+tf.squeeze(u,axis=2))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygj7fwujADW-"
      },
      "source": [
        "dim_LSTM = 40\n",
        "l1_reg = 0.0\n",
        "l2_reg = 0.0\n",
        "\n",
        "# Définition des entrées du modèle\n",
        "entrees_sequences = tf.keras.layers.Input(shape=(Nbr_Sequences,taille_fenetre))\n",
        "entrees_entrainement = tf.keras.layers.Input(shape=(taille_fenetre))\n",
        "\n",
        "# Encodeur\n",
        "s_encodeur = Couche_End_to_End_MN(dim_GRU=dim_LSTM,regul=0.0)(entrees_sequences,entrees_entrainement)\n",
        "\n",
        "# Décodeur\n",
        "s_decodeur = tf.keras.layers.Dense(dim_LSTM,activation=\"tanh\")(s_encodeur)\n",
        "s_decodeur = tf.keras.layers.Concatenate()([s_decodeur,s_encodeur])\n",
        "\n",
        "# Générateur\n",
        "sortie = tf.keras.layers.Dense(1)(s_decodeur)\n",
        "\n",
        "# Construction du modèle\n",
        "model = tf.keras.Model([entrees_sequences,entrees_entrainement],sortie)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFNCcD1BADW_"
      },
      "source": [
        "**2. Optimisation de l'apprentissage**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTQVpBLpADW_"
      },
      "source": [
        "Pour accélérer le traitement des données, nous n'allons pas utiliser l'intégralité des données pendant la mise à jour du gradient, comme cela a été fait jusqu'à présent (en utilisant le dataset).  \n",
        "Cette fois-ci, nous allons forcer les mises à jour du gradient à se produire de manière moins fréquente en attribuant la valeur du batch_size à prendre en compte lors de la regression du modèle.  \n",
        "Pour cela, on utilise l'argument \"batch_size\" dans la méthode fit. En précisant un batch_size=1000, cela signifie que :\n",
        " - Sur notre total de 56000 échantillons, 56 seront utilisés pour les calculs du gradient\n",
        " - Il y aura également 56 itérations à chaque période.\n",
        "  \n",
        "    \n",
        "    \n",
        "Si nous avions pris le dataset comme entrée, nous aurions eu :\n",
        "- Un total de 56000 échantillons également\n",
        "- Chaque période aurait également pris 56 itérations pour se compléter\n",
        "- Mais 1000 échantillons auraient été utilisés pour le calcul du gradient, au lieu de 56 avec la méthode utilisée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atvCPpf-ADXA"
      },
      "source": [
        "batch_size = 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZm9WrqVADXA"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam()\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"logcosh\", optimizer=optimiseur, metrics=\"mse\")\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint],batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2zHrpiAADXB"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[30], taux[99], 0, 0.04])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha5ZDQyOADXC"
      },
      "source": [
        "# Chargement des poids sauvegardés\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SzpxsrQADXC"
      },
      "source": [
        "max_periodes = 1000\n",
        "\n",
        "# Classe permettant d'arrêter l'entrainement si la variation\n",
        "# devient plus petite qu'une valeur à choisir sur un nombre\n",
        "# de périodes à choisir\n",
        "class StopTrain(keras.callbacks.Callback):\n",
        "    def __init__(self, delta=0.01,periodes=100, term=\"loss\", logs={}):\n",
        "      self.n_periodes = 0\n",
        "      self.periodes = periodes\n",
        "      self.loss_1 = 100\n",
        "      self.delta = delta\n",
        "      self.term = term\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "      diff_loss = abs(self.loss_1 - logs[self.term])\n",
        "      self.loss_1 = logs[self.term]\n",
        "      if (diff_loss < self.delta):\n",
        "        self.n_periodes = self.n_periodes + 1\n",
        "      else:\n",
        "        self.n_periodes = 0\n",
        "      if (self.n_periodes == self.periodes):\n",
        "        print(\"Arrêt de l'entrainement...\")\n",
        "        self.model.stop_training = True\n",
        "\n",
        "def  My_MSE(y_true,y_pred):\n",
        "  return(tf.keras.metrics.mse(y_true,y_pred)*std.numpy()+mean.numpy())\n",
        "  \n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.001,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0.01)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"logcosh\", optimizer=optimiseur, metrics=[\"mse\",My_MSE])\n",
        "\n",
        "# Entraine le modèle, avec une réduction des calculs du gradient\n",
        "#historique = model.fit(x=x_train,y=y_train,validation_data=(x_val,y_val), epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=0.005,periodes = 10, term=\"My_MSE\")],batch_size=batch_size)\n",
        "\n",
        "# Entraine le modèle sans réduction de calculs\n",
        "historique = model.fit(dataset,validation_data=dataset_val, epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=0.1,periodes = 10, term=\"val_My_MSE\")])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xiy0xyuYJpC8"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('poids_train.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nehoz4YADXD"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7fBX8FzADXE"
      },
      "source": [
        "# Evaluation du modèle\n",
        "# Avec le modèle type encodeur/décodeur :\n",
        "# 56/56 [==============================] - 7s 113ms/step - loss: 1.1795e-04 - mse: 2.3664e-04 - My_MSE: 2712.2415\n",
        "# 14/14 [==============================] - 2s 113ms/step - loss: 0.3602 - mse: 1.9360 - My_MSE: 9650.2637\n",
        "model.evaluate(dataset)\n",
        "model.evaluate(dataset_val)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibjW2608ADXE"
      },
      "source": [
        "**3. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4XVlUVqADXF"
      },
      "source": [
        "horizon = 1\n",
        "# Création des instants d'entrainement et de validation\n",
        "y_train_timing = serie_entrainement.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_train)]\n",
        "y_val_timing = serie_test.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_val)]\n",
        "\n",
        "# Calcul des prédictions\n",
        "pred_ent = model.predict(x_train, verbose=1)\n",
        "pred_val = model.predict(x_val, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qssOW-x2ADXF"
      },
      "source": [
        "**4.Affichage sur une période de 1 heure**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLGHAwjIADXF"
      },
      "source": [
        "Création d'une série contenant les valeurs originales et les prédictions, synchronisées dans le temps :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoyojCloADXG"
      },
      "source": [
        "serie_btc_ent_ori = serie_entrainement*std+mean\n",
        "serie_btc_val_ori = serie_test*std+mean\n",
        "\n",
        "serie_btc_ent_pred = pd.Series(data=(pred_ent[:,0]*std+mean),index=y_train_timing)\n",
        "serie_btc_val_pred = pd.Series(data=(pred_val[:,0]*std+mean),index=y_val_timing)\n",
        "\n",
        "serie_btc_ori = pd.concat([serie_btc_ent_ori,serie_btc_val_ori])\n",
        "serie_btc_pred = pd.concat([serie_btc_ent_pred,serie_btc_val_pred])\n",
        "\n",
        "serie_btc_ori = serie_btc_ori.fillna(method=\"backfill\")\n",
        "serie_btc_pred = serie_btc_pred.fillna(method=\"backfill\")\n",
        "\n",
        "serie_btc_ent_ori = serie_btc_ent_ori.fillna(method=\"backfill\")\n",
        "serie_btc_val_ori = serie_btc_val_ori.fillna(method=\"backfill\")\n",
        "serie_btc_ent_pred = serie_btc_ent_pred.fillna(method=\"backfill\")\n",
        "serie_btc_val_pred = serie_btc_val_pred.fillna(method=\"backfill\")\n",
        "\n",
        "frame = {'BTC_ALL' : serie_btc_ori, 'BTC_ENT': serie_btc_ent_ori, 'BTC_VAL' : serie_btc_val_ori, 'BTC_PRED' : serie_btc_pred, 'BTC_PRED_ENT' : serie_btc_ent_pred, 'BTC_PRED_VAL':serie_btc_val_pred}\n",
        "df_resultats = pd.DataFrame(frame)\n",
        "df_resultats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SwwQkjiADXG"
      },
      "source": [
        "date_separation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqvOoeoYADXG"
      },
      "source": [
        "df_resultats.loc[date_separation-pd.Timedelta(hours=7):date_separation+pd.Timedelta(hours=7)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNqabyNzADXH"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbe originale\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_ALL'],line=dict(color='blue', width=1),name=\"Prix BTC\"))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_ENT'],line=dict(color='green', width=1),name=\"Entrainement\"))\n",
        "\n",
        "# Courbe de validation\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_VAL'],line=dict(color='red', width=1),name=\"Validation\"))\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCTa2-ZEADXM"
      },
      "source": [
        "**5. Détection de l'augmentation des erreurs dans la zone de validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3cjdfFLADXN"
      },
      "source": [
        "On peut imaginer devoir suivre en temps réel l'évolution des prédictions afin de détecter lorsque le modèle n'est plus valide."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snd-tMArADXN"
      },
      "source": [
        "# Détection de la date où commencent les anomalies\n",
        "# sur la période de validation\n",
        "\n",
        "# Construit les dataframe pour calculer les erreurs\n",
        "df_error_btc_ent = df_resultats[['BTC_ENT','BTC_PRED_ENT']].dropna()\n",
        "df_error_btc_val = df_resultats[['BTC_VAL','BTC_PRED_VAL']].dropna()\n",
        "\n",
        "# Calcul des erreurs relatives sur les entrainements et les validations\n",
        "erreur_ent = abs(np.asarray(df_error_btc_ent['BTC_ENT']) - np.asarray(df_error_btc_ent['BTC_PRED_ENT']))/np.asarray(df_error_btc_ent['BTC_ENT'])*100.0\n",
        "erreur_val = abs(np.asarray(df_error_btc_val['BTC_VAL']) - np.asarray(df_error_btc_val['BTC_PRED_VAL']))/np.asarray(df_error_btc_val['BTC_VAL'])*100.0\n",
        "\n",
        "# Erreur relative moyenne sur les entrainements\n",
        "erreur_ent_mape = tf.keras.metrics.mape(np.asarray(df_error_btc_ent['BTC_ENT']),np.asarray(df_error_btc_ent['BTC_PRED_ENT']))\n",
        "erreur_ent_mape_max = np.amax(erreur_ent)\n",
        "\n",
        "# Calcul le nombre d'anomalies sur les entrainements avec le ratio spécifié\n",
        "nbr_anomalies_ent = np.count_nonzero(erreur_ent>erreur_ent_mape)\n",
        "\n",
        "# Calcul du ratio d'anomalies sur la période d'entrainement\n",
        "ratio_ent = np.count_nonzero(erreur_ent>erreur_ent_mape)/erreur_ent.size\n",
        "\n",
        "# Recherche de la date à partir de laquelle on dépasse l\n",
        "n_erreurs = 0\n",
        "for i in range(5,erreur_val.size):\n",
        "  erreur_val_ = abs(np.asarray(df_error_btc_val['BTC_VAL'][i]) - np.asarray(df_error_btc_val['BTC_PRED_VAL'][i]))/np.asarray(df_error_btc_val['BTC_VAL'][i])*100.0\n",
        "  seuil = erreur_ent_mape_max*1\n",
        "  if erreur_val_ > seuil:\n",
        "    n_erreurs = n_erreurs + 1\n",
        "    if n_erreurs == 5:\n",
        "      index = df_error_btc_val.index[i]\n",
        "      break\n",
        "  else:\n",
        "    n_erreurs = 0\n",
        "print(index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubGN_i0fADXO"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbe originale\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_ALL'],line=dict(color='blue', width=1),name=\"Prix BTC\"))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_ENT'],line=dict(color='green', width=1),name=\"Entrainement\"))\n",
        "\n",
        "# Courbe de validation\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_VAL'],line=dict(color='red', width=1),name=\"Validation\"))\n",
        "\n",
        "# Délimite la zone d'erreur\n",
        "fig.add_shape(type=\"line\",x0=index,y0=0,x1=index,y1=np.amax(df_resultats['BTC_ALL']),name=\"Zone d'erreur\")\n",
        "\n",
        "# Affiche les courbes\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emkQ01bfADXT"
      },
      "source": [
        "**5.Affichage sur une période de 1 jour**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShhfXg8yADXU"
      },
      "source": [
        "# Echantillonage des données\n",
        "time = \"1D\"\n",
        "\n",
        "df_resultats = df_resultats.resample(time).asfreq()\n",
        "df_resultats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btEN2yyyADXU"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbe originale\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_ALL'],line=dict(color='blue', width=1),name=\"Prix BTC\"))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_ENT'],line=dict(color='green', width=1),name=\"Entrainement\"))\n",
        "\n",
        "# Courbe de validation\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_VAL'],line=dict(color='red', width=1),name=\"Validation\"))\n",
        "\n",
        "# Délimite la zone d'erreur\n",
        "fig.add_shape(type=\"line\",x0=index,y0=0,x1=index,y1=np.amax(df_resultats['BTC_ALL']),name=\"Zone d'erreur\")\n",
        "\n",
        "# Affiche les courbes\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0eIXJrhADXV"
      },
      "source": [
        "**6.Synthèse des erreurs sur les différentes zones**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nP8wjWErADXV"
      },
      "source": [
        "# Echantillonage des données\n",
        "time = \"1H\"\n",
        "\n",
        "df_resultats = df_resultats.resample(time).asfreq()\n",
        "df_resultats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuofOZLJADXW"
      },
      "source": [
        "# Détection de la date où commencnte les anomalies\n",
        "# sur la période de validation\n",
        "\n",
        "# Construit les dataframe pour calculer les erreurs\n",
        "df_error_btc_ent = df_resultats[['BTC_ENT','BTC_PRED_ENT']].dropna()\n",
        "df_error_btc_val = df_resultats[['BTC_VAL','BTC_PRED_VAL']].dropna()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAyfDmMLADXW"
      },
      "source": [
        "# Erreurs d'entrainement\n",
        "mae_ent = tf.keras.metrics.mean_absolute_error(np.asarray(df_error_btc_ent['BTC_ENT']),np.asarray(df_error_btc_ent['BTC_PRED_ENT'])).numpy()\n",
        "mse_ent = tf.keras.metrics.mean_squared_error(np.asarray(df_error_btc_ent['BTC_ENT']),np.asarray(df_error_btc_ent['BTC_PRED_ENT'])).numpy()\n",
        "mape_ent = tf.keras.metrics.mape(np.asarray(df_error_btc_ent['BTC_ENT']),np.asarray(df_error_btc_ent['BTC_PRED_ENT'])).numpy()\n",
        "\n",
        "# Erreurs de validation\n",
        "mae_val = tf.keras.metrics.mean_absolute_error(np.asarray(df_error_btc_val['BTC_VAL']),np.asarray(df_error_btc_val['BTC_PRED_VAL'])).numpy()\n",
        "mse_val = tf.keras.metrics.mean_squared_error(np.asarray(df_error_btc_val['BTC_VAL']),np.asarray(df_error_btc_val['BTC_PRED_VAL'])).numpy()\n",
        "mape_val = tf.keras.metrics.mape(np.asarray(df_error_btc_val['BTC_VAL']),np.asarray(df_error_btc_val['BTC_PRED_VAL'])).numpy()\n",
        "\n",
        "# Erreurs sur la zone valide de validation\n",
        "mae_val_ok = tf.keras.metrics.mean_absolute_error(df_error_btc_val['BTC_VAL'][:index],np.asarray(df_error_btc_val['BTC_PRED_VAL'][:index])).numpy()\n",
        "mse_val_ok = tf.keras.metrics.mean_squared_error(df_error_btc_val['BTC_VAL'][:index],np.asarray(df_error_btc_val['BTC_PRED_VAL'][:index])).numpy()\n",
        "mape_val_ok = tf.keras.metrics.mape(df_error_btc_val['BTC_VAL'][:index],np.asarray(df_error_btc_val['BTC_PRED_VAL'][:index])).numpy()\n",
        "\n",
        "\n",
        "print(\"Erreur mae entrainement %s\" %mae_ent)\n",
        "print(\"Erreur mse entrainement : %s\" %mse_ent)\n",
        "print(\"Erreur mape entrainement : %s\\n\" %mape_ent)\n",
        "\n",
        "print(\"Erreur mae validation %s\" %mae_val)\n",
        "print(\"Erreur mse validation : %s\" %mse_val)\n",
        "print(\"Erreur mape entrainement : %s\\n\" %mape_val)\n",
        "\n",
        "print(\"Erreur mae validation zone OK %s\" %mae_val_ok)\n",
        "print(\"Erreur mse validation zone OK: %s\" %mse_val_ok)\n",
        "print(\"Erreur mape validation zone OK: %s\" %mape_val_ok)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}