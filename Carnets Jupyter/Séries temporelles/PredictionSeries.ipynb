{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PredictionsSeries.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/PredictionSeries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVuCzMYKDp3z"
      },
      "source": [
        "# Chargement des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3nLyKtnliCW"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fln1XUm2bxPx",
        "outputId": "7f4772e4-9a23-4ce5-bb69-1486d2128d2b"
      },
      "source": [
        "!wget --no-check-certificate --content-disposition \"https://raw.githubusercontent.com/AlexandreBourrieau/ML/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/data/table-indicateurs-open-data-dep-serie.csv\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-24 08:53:40--  https://raw.githubusercontent.com/AlexandreBourrieau/ML/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/data/table-indicateurs-open-data-dep-serie.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4340764 (4.1M) [text/plain]\n",
            "Saving to: ‘table-indicateurs-open-data-dep-serie.csv’\n",
            "\n",
            "table-indicateurs-o 100%[===================>]   4.14M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-04-24 08:53:41 (34.0 MB/s) - ‘table-indicateurs-open-data-dep-serie.csv’ saved [4340764/4340764]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPJ6nnrRsUBm"
      },
      "source": [
        "Charge la série sous Pandas et affiche les informations du fichier :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEB_JjihEYTP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        },
        "outputId": "293f27e4-6334-4315-a9db-3dd41d3f5880"
      },
      "source": [
        "# Création de la série sous Pandas\n",
        "serie = pd.read_csv(\"table-indicateurs-open-data-dep-serie.csv\")\n",
        "serie"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>extract_date</th>\n",
              "      <th>departement</th>\n",
              "      <th>region</th>\n",
              "      <th>libelle_reg</th>\n",
              "      <th>libelle_dep</th>\n",
              "      <th>tx_incid</th>\n",
              "      <th>R</th>\n",
              "      <th>taux_occupation_sae</th>\n",
              "      <th>tx_pos</th>\n",
              "      <th>tx_incid_couleur</th>\n",
              "      <th>R_couleur</th>\n",
              "      <th>taux_occupation_sae_couleur</th>\n",
              "      <th>tx_pos_couleur</th>\n",
              "      <th>nb_orange</th>\n",
              "      <th>nb_rouge</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-03-20</td>\n",
              "      <td>01</td>\n",
              "      <td>84</td>\n",
              "      <td>Auvergne Rhône Alpes</td>\n",
              "      <td>Ain</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15.6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>vert</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-03-21</td>\n",
              "      <td>01</td>\n",
              "      <td>84</td>\n",
              "      <td>Auvergne Rhône Alpes</td>\n",
              "      <td>Ain</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15.7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>vert</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-03-19</td>\n",
              "      <td>01</td>\n",
              "      <td>84</td>\n",
              "      <td>Auvergne Rhône Alpes</td>\n",
              "      <td>Ain</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>14.1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>vert</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-03-18</td>\n",
              "      <td>01</td>\n",
              "      <td>84</td>\n",
              "      <td>Auvergne Rhône Alpes</td>\n",
              "      <td>Ain</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>vert</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-04-28</td>\n",
              "      <td>01</td>\n",
              "      <td>84</td>\n",
              "      <td>Auvergne Rhône Alpes</td>\n",
              "      <td>Ain</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>72.6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>rouge</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40496</th>\n",
              "      <td>2021-04-15</td>\n",
              "      <td>84</td>\n",
              "      <td>93</td>\n",
              "      <td>Provence Alpes Côte d'Azur</td>\n",
              "      <td>Vaucluse</td>\n",
              "      <td>403.39</td>\n",
              "      <td>0.92</td>\n",
              "      <td>124.6</td>\n",
              "      <td>12.075129</td>\n",
              "      <td>rouge</td>\n",
              "      <td>vert</td>\n",
              "      <td>rouge</td>\n",
              "      <td>rouge</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40497</th>\n",
              "      <td>2021-04-16</td>\n",
              "      <td>84</td>\n",
              "      <td>93</td>\n",
              "      <td>Provence Alpes Côte d'Azur</td>\n",
              "      <td>Vaucluse</td>\n",
              "      <td>408.38</td>\n",
              "      <td>NaN</td>\n",
              "      <td>127.4</td>\n",
              "      <td>12.472100</td>\n",
              "      <td>rouge</td>\n",
              "      <td>NaN</td>\n",
              "      <td>rouge</td>\n",
              "      <td>rouge</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40498</th>\n",
              "      <td>2021-04-13</td>\n",
              "      <td>84</td>\n",
              "      <td>93</td>\n",
              "      <td>Provence Alpes Côte d'Azur</td>\n",
              "      <td>Vaucluse</td>\n",
              "      <td>434.58</td>\n",
              "      <td>NaN</td>\n",
              "      <td>124.3</td>\n",
              "      <td>12.066320</td>\n",
              "      <td>rouge</td>\n",
              "      <td>NaN</td>\n",
              "      <td>rouge</td>\n",
              "      <td>rouge</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40499</th>\n",
              "      <td>2021-04-18</td>\n",
              "      <td>84</td>\n",
              "      <td>93</td>\n",
              "      <td>Provence Alpes Côte d'Azur</td>\n",
              "      <td>Vaucluse</td>\n",
              "      <td>396.26</td>\n",
              "      <td>NaN</td>\n",
              "      <td>124.3</td>\n",
              "      <td>12.487361</td>\n",
              "      <td>rouge</td>\n",
              "      <td>NaN</td>\n",
              "      <td>rouge</td>\n",
              "      <td>rouge</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40500</th>\n",
              "      <td>2021-04-17</td>\n",
              "      <td>84</td>\n",
              "      <td>93</td>\n",
              "      <td>Provence Alpes Côte d'Azur</td>\n",
              "      <td>Vaucluse</td>\n",
              "      <td>403.75</td>\n",
              "      <td>NaN</td>\n",
              "      <td>124.3</td>\n",
              "      <td>12.593128</td>\n",
              "      <td>rouge</td>\n",
              "      <td>NaN</td>\n",
              "      <td>rouge</td>\n",
              "      <td>rouge</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>40501 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      extract_date departement  region  ... tx_pos_couleur nb_orange  nb_rouge\n",
              "0       2020-03-20          01      84  ...            NaN         0         0\n",
              "1       2020-03-21          01      84  ...            NaN         0         0\n",
              "2       2020-03-19          01      84  ...            NaN         0         0\n",
              "3       2020-03-18          01      84  ...            NaN         0         0\n",
              "4       2020-04-28          01      84  ...            NaN         0         1\n",
              "...            ...         ...     ...  ...            ...       ...       ...\n",
              "40496   2021-04-15          84      93  ...          rouge         0         3\n",
              "40497   2021-04-16          84      93  ...          rouge         0         3\n",
              "40498   2021-04-13          84      93  ...          rouge         0         3\n",
              "40499   2021-04-18          84      93  ...          rouge         0         3\n",
              "40500   2021-04-17          84      93  ...          rouge         0         3\n",
              "\n",
              "[40501 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "id": "cbYvPt63d-GS",
        "outputId": "1dbd00a8-8017-489b-90aa-d7dde8839689"
      },
      "source": [
        "serie.groupby(by=\"region\").agg(['count'])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>extract_date</th>\n",
              "      <th>departement</th>\n",
              "      <th>libelle_reg</th>\n",
              "      <th>libelle_dep</th>\n",
              "      <th>tx_incid</th>\n",
              "      <th>R</th>\n",
              "      <th>taux_occupation_sae</th>\n",
              "      <th>tx_pos</th>\n",
              "      <th>tx_incid_couleur</th>\n",
              "      <th>R_couleur</th>\n",
              "      <th>taux_occupation_sae_couleur</th>\n",
              "      <th>tx_pos_couleur</th>\n",
              "      <th>nb_orange</th>\n",
              "      <th>nb_rouge</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>count</th>\n",
              "      <th>count</th>\n",
              "      <th>count</th>\n",
              "      <th>count</th>\n",
              "      <th>count</th>\n",
              "      <th>count</th>\n",
              "      <th>count</th>\n",
              "      <th>count</th>\n",
              "      <th>count</th>\n",
              "      <th>count</th>\n",
              "      <th>count</th>\n",
              "      <th>count</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>region</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "      <td>342</td>\n",
              "      <td>54</td>\n",
              "      <td>401</td>\n",
              "      <td>336</td>\n",
              "      <td>342</td>\n",
              "      <td>54</td>\n",
              "      <td>401</td>\n",
              "      <td>336</td>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "      <td>342</td>\n",
              "      <td>56</td>\n",
              "      <td>401</td>\n",
              "      <td>336</td>\n",
              "      <td>342</td>\n",
              "      <td>56</td>\n",
              "      <td>401</td>\n",
              "      <td>336</td>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "      <td>342</td>\n",
              "      <td>68</td>\n",
              "      <td>401</td>\n",
              "      <td>336</td>\n",
              "      <td>342</td>\n",
              "      <td>68</td>\n",
              "      <td>401</td>\n",
              "      <td>336</td>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "      <td>342</td>\n",
              "      <td>55</td>\n",
              "      <td>401</td>\n",
              "      <td>336</td>\n",
              "      <td>342</td>\n",
              "      <td>55</td>\n",
              "      <td>401</td>\n",
              "      <td>336</td>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "      <td>342</td>\n",
              "      <td>35</td>\n",
              "      <td>401</td>\n",
              "      <td>336</td>\n",
              "      <td>342</td>\n",
              "      <td>35</td>\n",
              "      <td>401</td>\n",
              "      <td>336</td>\n",
              "      <td>401</td>\n",
              "      <td>401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>3208</td>\n",
              "      <td>3208</td>\n",
              "      <td>3208</td>\n",
              "      <td>3208</td>\n",
              "      <td>2736</td>\n",
              "      <td>544</td>\n",
              "      <td>3208</td>\n",
              "      <td>2688</td>\n",
              "      <td>2736</td>\n",
              "      <td>544</td>\n",
              "      <td>3208</td>\n",
              "      <td>2688</td>\n",
              "      <td>3208</td>\n",
              "      <td>3208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>2406</td>\n",
              "      <td>2406</td>\n",
              "      <td>2406</td>\n",
              "      <td>2406</td>\n",
              "      <td>2052</td>\n",
              "      <td>408</td>\n",
              "      <td>2406</td>\n",
              "      <td>2016</td>\n",
              "      <td>2052</td>\n",
              "      <td>408</td>\n",
              "      <td>2406</td>\n",
              "      <td>2016</td>\n",
              "      <td>2406</td>\n",
              "      <td>2406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>3208</td>\n",
              "      <td>3208</td>\n",
              "      <td>3208</td>\n",
              "      <td>3208</td>\n",
              "      <td>2736</td>\n",
              "      <td>544</td>\n",
              "      <td>3208</td>\n",
              "      <td>2688</td>\n",
              "      <td>2736</td>\n",
              "      <td>544</td>\n",
              "      <td>3208</td>\n",
              "      <td>2688</td>\n",
              "      <td>3208</td>\n",
              "      <td>3208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>2005</td>\n",
              "      <td>2005</td>\n",
              "      <td>2005</td>\n",
              "      <td>2005</td>\n",
              "      <td>1710</td>\n",
              "      <td>340</td>\n",
              "      <td>2005</td>\n",
              "      <td>1680</td>\n",
              "      <td>1710</td>\n",
              "      <td>340</td>\n",
              "      <td>2005</td>\n",
              "      <td>1680</td>\n",
              "      <td>2005</td>\n",
              "      <td>2005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>2005</td>\n",
              "      <td>2005</td>\n",
              "      <td>2005</td>\n",
              "      <td>2005</td>\n",
              "      <td>1710</td>\n",
              "      <td>340</td>\n",
              "      <td>2005</td>\n",
              "      <td>1680</td>\n",
              "      <td>1710</td>\n",
              "      <td>340</td>\n",
              "      <td>2005</td>\n",
              "      <td>1680</td>\n",
              "      <td>2005</td>\n",
              "      <td>2005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>4010</td>\n",
              "      <td>4010</td>\n",
              "      <td>4010</td>\n",
              "      <td>4010</td>\n",
              "      <td>3420</td>\n",
              "      <td>680</td>\n",
              "      <td>4010</td>\n",
              "      <td>3360</td>\n",
              "      <td>3420</td>\n",
              "      <td>680</td>\n",
              "      <td>4010</td>\n",
              "      <td>3360</td>\n",
              "      <td>4010</td>\n",
              "      <td>4010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>2005</td>\n",
              "      <td>2005</td>\n",
              "      <td>2005</td>\n",
              "      <td>2005</td>\n",
              "      <td>1710</td>\n",
              "      <td>340</td>\n",
              "      <td>2005</td>\n",
              "      <td>1680</td>\n",
              "      <td>1710</td>\n",
              "      <td>340</td>\n",
              "      <td>2005</td>\n",
              "      <td>1680</td>\n",
              "      <td>2005</td>\n",
              "      <td>2005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>1604</td>\n",
              "      <td>1604</td>\n",
              "      <td>1604</td>\n",
              "      <td>1604</td>\n",
              "      <td>1368</td>\n",
              "      <td>272</td>\n",
              "      <td>1604</td>\n",
              "      <td>1344</td>\n",
              "      <td>1368</td>\n",
              "      <td>272</td>\n",
              "      <td>1604</td>\n",
              "      <td>1344</td>\n",
              "      <td>1604</td>\n",
              "      <td>1604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>4812</td>\n",
              "      <td>4812</td>\n",
              "      <td>4812</td>\n",
              "      <td>4812</td>\n",
              "      <td>4104</td>\n",
              "      <td>816</td>\n",
              "      <td>4812</td>\n",
              "      <td>4032</td>\n",
              "      <td>4104</td>\n",
              "      <td>816</td>\n",
              "      <td>4812</td>\n",
              "      <td>4032</td>\n",
              "      <td>4812</td>\n",
              "      <td>4812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>5213</td>\n",
              "      <td>5213</td>\n",
              "      <td>5213</td>\n",
              "      <td>5213</td>\n",
              "      <td>4446</td>\n",
              "      <td>884</td>\n",
              "      <td>5213</td>\n",
              "      <td>4368</td>\n",
              "      <td>4446</td>\n",
              "      <td>884</td>\n",
              "      <td>5213</td>\n",
              "      <td>4368</td>\n",
              "      <td>5213</td>\n",
              "      <td>5213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>4812</td>\n",
              "      <td>4812</td>\n",
              "      <td>4812</td>\n",
              "      <td>4812</td>\n",
              "      <td>4104</td>\n",
              "      <td>816</td>\n",
              "      <td>4812</td>\n",
              "      <td>4032</td>\n",
              "      <td>4104</td>\n",
              "      <td>816</td>\n",
              "      <td>4812</td>\n",
              "      <td>4032</td>\n",
              "      <td>4812</td>\n",
              "      <td>4812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>2406</td>\n",
              "      <td>2406</td>\n",
              "      <td>2406</td>\n",
              "      <td>2406</td>\n",
              "      <td>2052</td>\n",
              "      <td>408</td>\n",
              "      <td>2406</td>\n",
              "      <td>2016</td>\n",
              "      <td>2052</td>\n",
              "      <td>408</td>\n",
              "      <td>2406</td>\n",
              "      <td>2016</td>\n",
              "      <td>2406</td>\n",
              "      <td>2406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>802</td>\n",
              "      <td>802</td>\n",
              "      <td>802</td>\n",
              "      <td>802</td>\n",
              "      <td>684</td>\n",
              "      <td>104</td>\n",
              "      <td>802</td>\n",
              "      <td>672</td>\n",
              "      <td>684</td>\n",
              "      <td>104</td>\n",
              "      <td>802</td>\n",
              "      <td>672</td>\n",
              "      <td>802</td>\n",
              "      <td>802</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       extract_date departement libelle_reg  ... tx_pos_couleur nb_orange nb_rouge\n",
              "              count       count       count  ...          count     count    count\n",
              "region                                       ...                                  \n",
              "1               401         401         401  ...            336       401      401\n",
              "2               401         401         401  ...            336       401      401\n",
              "3               401         401         401  ...            336       401      401\n",
              "4               401         401         401  ...            336       401      401\n",
              "6               401         401         401  ...            336       401      401\n",
              "11             3208        3208        3208  ...           2688      3208     3208\n",
              "24             2406        2406        2406  ...           2016      2406     2406\n",
              "27             3208        3208        3208  ...           2688      3208     3208\n",
              "28             2005        2005        2005  ...           1680      2005     2005\n",
              "32             2005        2005        2005  ...           1680      2005     2005\n",
              "44             4010        4010        4010  ...           3360      4010     4010\n",
              "52             2005        2005        2005  ...           1680      2005     2005\n",
              "53             1604        1604        1604  ...           1344      1604     1604\n",
              "75             4812        4812        4812  ...           4032      4812     4812\n",
              "76             5213        5213        5213  ...           4368      5213     5213\n",
              "84             4812        4812        4812  ...           4032      4812     4812\n",
              "93             2406        2406        2406  ...           2016      2406     2406\n",
              "94              802         802         802  ...            672       802      802\n",
              "\n",
              "[18 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJLcuFiNevyy"
      },
      "source": [
        "Regardons l'évolution du taux d'incidence sur paris :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "Q17wIXPLe2i_",
        "outputId": "94d00339-3f8d-401a-dab0-96f7c7f383be"
      },
      "source": [
        "serie_paris = serie.loc[serie['region']==84]\n",
        "serie_paris = serie_paris[['extract_date','tx_incid']]\n",
        "\n",
        "serie_paris"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>extract_date</th>\n",
              "      <th>tx_incid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-03-20</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-03-21</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-03-19</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-03-18</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-04-28</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4807</th>\n",
              "      <td>2021-04-16</td>\n",
              "      <td>291.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4808</th>\n",
              "      <td>2021-04-13</td>\n",
              "      <td>330.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4809</th>\n",
              "      <td>2021-04-18</td>\n",
              "      <td>281.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4810</th>\n",
              "      <td>2021-04-19</td>\n",
              "      <td>264.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4811</th>\n",
              "      <td>2021-04-17</td>\n",
              "      <td>280.78</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4812 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     extract_date  tx_incid\n",
              "0      2020-03-20       NaN\n",
              "1      2020-03-21       NaN\n",
              "2      2020-03-19       NaN\n",
              "3      2020-03-18       NaN\n",
              "4      2020-04-28       NaN\n",
              "...           ...       ...\n",
              "4807   2021-04-16    291.40\n",
              "4808   2021-04-13    330.15\n",
              "4809   2021-04-18    281.02\n",
              "4810   2021-04-19    264.48\n",
              "4811   2021-04-17    280.78\n",
              "\n",
              "[4812 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "cTSg8XwLknp9",
        "outputId": "f844b4eb-58ca-4266-ce16-de72727c65e4"
      },
      "source": [
        "df_paris = pd.DataFrame(data={'taux' : serie_paris['tx_incid'].values},index=serie_paris['extract_date'])\n",
        "df_paris.index = pd.to_datetime(df_paris.index)\n",
        "df_paris = df_paris[~df_paris.index.duplicated(keep='first')]\n",
        "df_paris"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>taux</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>extract_date</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2020-03-20</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-21</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-19</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-18</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-04-28</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-27</th>\n",
              "      <td>80.22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-28</th>\n",
              "      <td>75.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-29</th>\n",
              "      <td>75.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-30</th>\n",
              "      <td>74.59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-10-01</th>\n",
              "      <td>77.78</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>401 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               taux\n",
              "extract_date       \n",
              "2020-03-20      NaN\n",
              "2020-03-21      NaN\n",
              "2020-03-19      NaN\n",
              "2020-03-18      NaN\n",
              "2020-04-28      NaN\n",
              "...             ...\n",
              "2020-09-27    80.22\n",
              "2020-09-28    75.20\n",
              "2020-09-29    75.50\n",
              "2020-09-30    74.59\n",
              "2020-10-01    77.78\n",
              "\n",
              "[401 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "m23cEXjcny_q",
        "outputId": "4ebda16d-2ddd-4506-a142-66b1bc1af086"
      },
      "source": [
        "df_paris.index = df_paris.index.sort_values()\n",
        "df_paris"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>taux</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>extract_date</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2020-03-18</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-19</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-20</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-21</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-22</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-18</th>\n",
              "      <td>80.22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-19</th>\n",
              "      <td>75.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-20</th>\n",
              "      <td>75.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-21</th>\n",
              "      <td>74.59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-22</th>\n",
              "      <td>77.78</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>401 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               taux\n",
              "extract_date       \n",
              "2020-03-18      NaN\n",
              "2020-03-19      NaN\n",
              "2020-03-20      NaN\n",
              "2020-03-21      NaN\n",
              "2020-03-22      NaN\n",
              "...             ...\n",
              "2021-04-18    80.22\n",
              "2021-04-19    75.20\n",
              "2021-04-20    75.50\n",
              "2021-04-21    74.59\n",
              "2021-04-22    77.78\n",
              "\n",
              "[401 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "qYpzzcbGi5FE",
        "outputId": "d15d503e-0d4f-463b-dbe0-ce1224b3b924"
      },
      "source": [
        "plt.plot(df_paris)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fe6352b70d0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF62FumBty9H"
      },
      "source": [
        "# Pré-traitement des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUzjd-qN2s-T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b04d743-fb0a-4743-8e71-c60483722894"
      },
      "source": [
        "# Affichage du nombre total de données manquantes\n",
        "\n",
        "data_manquantes = sum(np.isnan(df_paris['taux']))\n",
        "print (\"Nombre de données manquantes : %s\" %data_manquantes)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nombre de données manquantes : 59\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tkbi7uuBnUD3"
      },
      "source": [
        "**3. Correction des données**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IQuSqAknduG"
      },
      "source": [
        "Pour corriger les données, on va tout simplement utiliser la fonction [fillna](https://pandas.pydata.org/docs/reference/api/pandas.Series.fillna.html) de Pandas avec la fonctionnalité de type `backfill` :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2Oav6gin5aP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42d2dddc-9b75-4a3d-8e0f-018147db83e8"
      },
      "source": [
        "# Applique la fonction de remplissage automatique des données non numérique avec l'option backfill\n",
        "df_paris = df_paris.interpolate(method=\"slinear\")\n",
        "data_manquantes = sum(np.isnan(df_paris['taux']))\n",
        "print (\"Nombre de données manquantes : %s\" %data_manquantes)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nombre de données manquantes : 44\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n84L5raJo72w",
        "outputId": "adf29dfe-d6d1-4cb0-9fd4-de193aca58ac"
      },
      "source": [
        "df_paris = df_paris.fillna(method=\"backfill\")\n",
        "data_manquantes = sum(np.isnan(df_paris['taux']))\n",
        "print (\"Nombre de données manquantes : %s\" %data_manquantes)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nombre de données manquantes : 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v05rWWccJI26"
      },
      "source": [
        "**4. Affichage des données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1QKMBThNQni",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "43fe626a-a5bd-41ec-d95c-491ba29e8cba"
      },
      "source": [
        "# Affiche la série\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(df_paris)\n",
        "plt.title(\"Evolution du prix du BTC\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Evolution du prix du BTC')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU7u1mA1E6jk"
      },
      "source": [
        "# Préparation des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "_IbnwRJJF4Ox",
        "outputId": "8731d705-378d-4b5d-accb-14545cba9ee4"
      },
      "source": [
        "# Définition des dates de début et de fin\n",
        "\n",
        "date_debut = \"2020-03-18\"\n",
        "date_fin = \"2021-04-22\"\n",
        "\n",
        "serie_etude = df_paris.loc[date_debut:date_fin].copy()\n",
        "serie_etude"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>taux</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>extract_date</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2020-03-18</th>\n",
              "      <td>108.53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-19</th>\n",
              "      <td>108.53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-20</th>\n",
              "      <td>108.53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-21</th>\n",
              "      <td>108.53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-22</th>\n",
              "      <td>108.53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-18</th>\n",
              "      <td>80.22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-19</th>\n",
              "      <td>75.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-20</th>\n",
              "      <td>75.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-21</th>\n",
              "      <td>74.59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-22</th>\n",
              "      <td>77.78</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>401 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                taux\n",
              "extract_date        \n",
              "2020-03-18    108.53\n",
              "2020-03-19    108.53\n",
              "2020-03-20    108.53\n",
              "2020-03-21    108.53\n",
              "2020-03-22    108.53\n",
              "...              ...\n",
              "2021-04-18     80.22\n",
              "2021-04-19     75.20\n",
              "2021-04-20     75.50\n",
              "2021-04-21     74.59\n",
              "2021-04-22     77.78\n",
              "\n",
              "[401 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwOeFLtLSPnv"
      },
      "source": [
        "**2. Détection des anomalies dans la série \"horaire\"**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1joYv2Kd7Js"
      },
      "source": [
        "Les anomalies sont fréquentes dans les séries temporelles, et la performance des prédictions est souvent améliorée lorsque ces anomalies sont traitées.  \n",
        "Pour avoir un apperçu de ces éventuelles anomalies, nous allons utiliser la méthode [\"Isolation Forest\"](https://scikit-learn.org/stable/modules/outlier_detection.html#isolation-forest) disponnible dans Scikit-learn.  \n",
        "\n",
        "Les paramètres utilisés sont les suivants :\n",
        " - **n_estimators** : C'est le nombre de sous-groupes d'échantillons à utiliser. Une valeur de 128 ou 256 est préconnisée dans le document de recherche.\n",
        " - **max_samples** : C'est le nombre d'échantillons maximum à utiliser. Nous utiliserons l'ensemble des échantillons.\n",
        " - **max_features** :  C'est le nombre de motifs aléatoirement choisis sur chaque noeud de l'arbre. Nous choisirons un seul motif.\n",
        " - **contamination** : C'est le pourcentage estimé d'anomalies dans les données. Ce paramètre permet de régler la sensibilité de l'algorithme. On va commencer avec 5% et affiner si nécessaire par la suite."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHag65S4dH7x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d692ae8a-1629-4ef5-a8df-7724f30963d7"
      },
      "source": [
        "# Initialise le modèle\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "clf = IsolationForest(n_estimators=256,max_samples=df_paris['taux'].size, contamination=0.01,max_features=1, verbose=1)\n",
        "clf.fit(df_paris['taux'].values.reshape(-1,1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "IsolationForest(behaviour='deprecated', bootstrap=False, contamination=0.01,\n",
              "                max_features=1, max_samples=401, n_estimators=256, n_jobs=None,\n",
              "                random_state=None, verbose=1, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAPFfAaffb4h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f58e3100-2138-4c20-d770-6f295f3b6037"
      },
      "source": [
        "# Réalise les prédictions\n",
        "pred = clf.predict(df_paris['taux'].values.reshape(-1,1))\n",
        "pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1, -1,  1,\n",
              "        1,  1,  1,  1, -1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 222
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU0TN1UEBqR2"
      },
      "source": [
        "On ajoute maintenant ces informations dans la série journalière et on affiche les informations :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWg0uUb9G5Ws",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "outputId": "d153748e-0a25-4baf-bb60-3c38c01bd28c"
      },
      "source": [
        "# Ajoute une colonne \"Anomalie\" dans la série\n",
        "df_paris['Anomalies']=pred\n",
        "df_paris['Anomalies'] = df_paris['Anomalies'].apply(lambda x: 1 if (x==-1) else 0)\n",
        "df_paris"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>taux</th>\n",
              "      <th>Anomalies</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>extract_date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2020-03-18</th>\n",
              "      <td>108.53</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-19</th>\n",
              "      <td>108.53</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-20</th>\n",
              "      <td>108.53</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-21</th>\n",
              "      <td>108.53</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-22</th>\n",
              "      <td>108.53</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-18</th>\n",
              "      <td>80.22</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-19</th>\n",
              "      <td>75.20</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-20</th>\n",
              "      <td>75.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-21</th>\n",
              "      <td>74.59</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-22</th>\n",
              "      <td>77.78</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>401 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                taux  Anomalies\n",
              "extract_date                   \n",
              "2020-03-18    108.53          0\n",
              "2020-03-19    108.53          0\n",
              "2020-03-20    108.53          0\n",
              "2020-03-21    108.53          0\n",
              "2020-03-22    108.53          0\n",
              "...              ...        ...\n",
              "2021-04-18     80.22          0\n",
              "2021-04-19     75.20          0\n",
              "2021-04-20     75.50          0\n",
              "2021-04-21     74.59          0\n",
              "2021-04-22     77.78          0\n",
              "\n",
              "[401 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "105qNoy1EwWd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "601ff73b-c009-4c42-bc82-13b085516c0e"
      },
      "source": [
        "# Affiche les informations sur les anomalies\n",
        "print(df_paris['Anomalies'].value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    397\n",
            "1      4\n",
            "Name: Anomalies, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaV_MfJyFXkF"
      },
      "source": [
        "**3. Affichage des anomalies sur le graphique**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2idYKYImFh8v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "08e15cca-91bf-4f40-cabb-19d12301e390"
      },
      "source": [
        "# Affiche la série\n",
        "\n",
        "fig = px.line(x=df_paris.index,y=df_paris['taux'],title=\"Evolution du prix du BTC\")\n",
        "fig.add_trace(px.scatter(x=df_paris.index,y=df_paris['Anomalies']*df_paris['taux'],color=df_paris['Anomalies'].astype(np.bool)).data[0])\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"d761acf0-dc52-4b7e-9227-37df53b1147a\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"d761acf0-dc52-4b7e-9227-37df53b1147a\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'd761acf0-dc52-4b7e-9227-37df53b1147a',\n",
              "                        [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"x=%{x}<br>y=%{y}\", \"legendgroup\": \"\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [\"2020-03-18T00:00:00\", \"2020-03-19T00:00:00\", \"2020-03-20T00:00:00\", \"2020-03-21T00:00:00\", \"2020-03-22T00:00:00\", \"2020-03-23T00:00:00\", \"2020-03-24T00:00:00\", \"2020-03-25T00:00:00\", \"2020-03-26T00:00:00\", \"2020-03-27T00:00:00\", \"2020-03-28T00:00:00\", \"2020-03-29T00:00:00\", \"2020-03-30T00:00:00\", \"2020-03-31T00:00:00\", \"2020-04-01T00:00:00\", \"2020-04-02T00:00:00\", \"2020-04-03T00:00:00\", \"2020-04-04T00:00:00\", \"2020-04-05T00:00:00\", \"2020-04-06T00:00:00\", \"2020-04-07T00:00:00\", \"2020-04-08T00:00:00\", \"2020-04-09T00:00:00\", \"2020-04-10T00:00:00\", \"2020-04-11T00:00:00\", \"2020-04-12T00:00:00\", \"2020-04-13T00:00:00\", \"2020-04-14T00:00:00\", \"2020-04-15T00:00:00\", \"2020-04-16T00:00:00\", \"2020-04-17T00:00:00\", \"2020-04-18T00:00:00\", \"2020-04-19T00:00:00\", \"2020-04-20T00:00:00\", \"2020-04-21T00:00:00\", \"2020-04-22T00:00:00\", \"2020-04-23T00:00:00\", \"2020-04-24T00:00:00\", \"2020-04-25T00:00:00\", \"2020-04-26T00:00:00\", \"2020-04-27T00:00:00\", \"2020-04-28T00:00:00\", \"2020-04-29T00:00:00\", \"2020-04-30T00:00:00\", \"2020-05-01T00:00:00\", \"2020-05-02T00:00:00\", \"2020-05-03T00:00:00\", \"2020-05-04T00:00:00\", \"2020-05-05T00:00:00\", \"2020-05-06T00:00:00\", \"2020-05-07T00:00:00\", \"2020-05-08T00:00:00\", \"2020-05-09T00:00:00\", \"2020-05-10T00:00:00\", \"2020-05-11T00:00:00\", \"2020-05-12T00:00:00\", \"2020-05-13T00:00:00\", \"2020-05-14T00:00:00\", \"2020-05-15T00:00:00\", \"2020-05-16T00:00:00\", \"2020-05-17T00:00:00\", \"2020-05-18T00:00:00\", \"2020-05-19T00:00:00\", \"2020-05-20T00:00:00\", \"2020-05-21T00:00:00\", \"2020-05-22T00:00:00\", \"2020-05-23T00:00:00\", \"2020-05-24T00:00:00\", \"2020-05-25T00:00:00\", \"2020-05-26T00:00:00\", \"2020-05-27T00:00:00\", \"2020-05-28T00:00:00\", \"2020-05-29T00:00:00\", \"2020-05-30T00:00:00\", \"2020-05-31T00:00:00\", \"2020-06-01T00:00:00\", \"2020-06-02T00:00:00\", \"2020-06-03T00:00:00\", \"2020-06-04T00:00:00\", \"2020-06-05T00:00:00\", \"2020-06-06T00:00:00\", \"2020-06-07T00:00:00\", \"2020-06-08T00:00:00\", \"2020-06-09T00:00:00\", \"2020-06-10T00:00:00\", \"2020-06-11T00:00:00\", \"2020-06-12T00:00:00\", \"2020-06-13T00:00:00\", \"2020-06-14T00:00:00\", \"2020-06-15T00:00:00\", \"2020-06-16T00:00:00\", \"2020-06-17T00:00:00\", \"2020-06-18T00:00:00\", \"2020-06-19T00:00:00\", \"2020-06-20T00:00:00\", \"2020-06-21T00:00:00\", \"2020-06-22T00:00:00\", \"2020-06-23T00:00:00\", \"2020-06-24T00:00:00\", \"2020-06-25T00:00:00\", \"2020-06-26T00:00:00\", \"2020-06-27T00:00:00\", \"2020-06-28T00:00:00\", \"2020-06-29T00:00:00\", \"2020-06-30T00:00:00\", \"2020-07-01T00:00:00\", \"2020-07-02T00:00:00\", \"2020-07-03T00:00:00\", \"2020-07-04T00:00:00\", \"2020-07-05T00:00:00\", \"2020-07-06T00:00:00\", \"2020-07-07T00:00:00\", \"2020-07-08T00:00:00\", \"2020-07-09T00:00:00\", \"2020-07-10T00:00:00\", \"2020-07-11T00:00:00\", \"2020-07-12T00:00:00\", \"2020-07-13T00:00:00\", \"2020-07-14T00:00:00\", \"2020-07-15T00:00:00\", \"2020-07-16T00:00:00\", \"2020-07-17T00:00:00\", \"2020-07-18T00:00:00\", \"2020-07-19T00:00:00\", \"2020-07-20T00:00:00\", \"2020-07-21T00:00:00\", \"2020-07-22T00:00:00\", \"2020-07-23T00:00:00\", \"2020-07-24T00:00:00\", \"2020-07-25T00:00:00\", \"2020-07-26T00:00:00\", \"2020-07-27T00:00:00\", \"2020-07-28T00:00:00\", \"2020-07-29T00:00:00\", \"2020-07-30T00:00:00\", \"2020-07-31T00:00:00\", \"2020-08-01T00:00:00\", \"2020-08-02T00:00:00\", \"2020-08-03T00:00:00\", \"2020-08-04T00:00:00\", \"2020-08-05T00:00:00\", \"2020-08-06T00:00:00\", \"2020-08-07T00:00:00\", \"2020-08-08T00:00:00\", \"2020-08-09T00:00:00\", \"2020-08-10T00:00:00\", \"2020-08-11T00:00:00\", \"2020-08-12T00:00:00\", \"2020-08-13T00:00:00\", \"2020-08-14T00:00:00\", \"2020-08-15T00:00:00\", \"2020-08-16T00:00:00\", \"2020-08-17T00:00:00\", \"2020-08-18T00:00:00\", \"2020-08-19T00:00:00\", \"2020-08-20T00:00:00\", \"2020-08-21T00:00:00\", \"2020-08-22T00:00:00\", \"2020-08-23T00:00:00\", \"2020-08-24T00:00:00\", \"2020-08-25T00:00:00\", \"2020-08-26T00:00:00\", \"2020-08-27T00:00:00\", \"2020-08-28T00:00:00\", \"2020-08-29T00:00:00\", \"2020-08-30T00:00:00\", \"2020-08-31T00:00:00\", \"2020-09-01T00:00:00\", \"2020-09-02T00:00:00\", \"2020-09-03T00:00:00\", \"2020-09-04T00:00:00\", \"2020-09-05T00:00:00\", \"2020-09-06T00:00:00\", \"2020-09-07T00:00:00\", \"2020-09-08T00:00:00\", \"2020-09-09T00:00:00\", \"2020-09-10T00:00:00\", \"2020-09-11T00:00:00\", \"2020-09-12T00:00:00\", \"2020-09-13T00:00:00\", \"2020-09-14T00:00:00\", \"2020-09-15T00:00:00\", \"2020-09-16T00:00:00\", \"2020-09-17T00:00:00\", \"2020-09-18T00:00:00\", \"2020-09-19T00:00:00\", \"2020-09-20T00:00:00\", \"2020-09-21T00:00:00\", \"2020-09-22T00:00:00\", \"2020-09-23T00:00:00\", \"2020-09-24T00:00:00\", \"2020-09-25T00:00:00\", \"2020-09-26T00:00:00\", \"2020-09-27T00:00:00\", \"2020-09-28T00:00:00\", \"2020-09-29T00:00:00\", \"2020-09-30T00:00:00\", \"2020-10-01T00:00:00\", \"2020-10-02T00:00:00\", \"2020-10-03T00:00:00\", \"2020-10-04T00:00:00\", \"2020-10-05T00:00:00\", \"2020-10-06T00:00:00\", \"2020-10-07T00:00:00\", \"2020-10-08T00:00:00\", \"2020-10-09T00:00:00\", \"2020-10-10T00:00:00\", \"2020-10-11T00:00:00\", \"2020-10-12T00:00:00\", \"2020-10-13T00:00:00\", \"2020-10-14T00:00:00\", \"2020-10-15T00:00:00\", \"2020-10-16T00:00:00\", \"2020-10-17T00:00:00\", \"2020-10-18T00:00:00\", \"2020-10-19T00:00:00\", \"2020-10-20T00:00:00\", \"2020-10-21T00:00:00\", \"2020-10-22T00:00:00\", \"2020-10-23T00:00:00\", \"2020-10-24T00:00:00\", \"2020-10-25T00:00:00\", \"2020-10-26T00:00:00\", \"2020-10-27T00:00:00\", \"2020-10-28T00:00:00\", \"2020-10-29T00:00:00\", \"2020-10-30T00:00:00\", \"2020-10-31T00:00:00\", \"2020-11-01T00:00:00\", \"2020-11-02T00:00:00\", \"2020-11-03T00:00:00\", \"2020-11-04T00:00:00\", \"2020-11-05T00:00:00\", \"2020-11-06T00:00:00\", \"2020-11-07T00:00:00\", \"2020-11-08T00:00:00\", \"2020-11-09T00:00:00\", \"2020-11-10T00:00:00\", \"2020-11-11T00:00:00\", \"2020-11-12T00:00:00\", \"2020-11-13T00:00:00\", \"2020-11-14T00:00:00\", \"2020-11-15T00:00:00\", \"2020-11-16T00:00:00\", \"2020-11-17T00:00:00\", \"2020-11-18T00:00:00\", \"2020-11-19T00:00:00\", \"2020-11-20T00:00:00\", \"2020-11-21T00:00:00\", \"2020-11-22T00:00:00\", \"2020-11-23T00:00:00\", \"2020-11-24T00:00:00\", \"2020-11-25T00:00:00\", \"2020-11-26T00:00:00\", \"2020-11-27T00:00:00\", \"2020-11-28T00:00:00\", \"2020-11-29T00:00:00\", \"2020-11-30T00:00:00\", \"2020-12-01T00:00:00\", \"2020-12-02T00:00:00\", \"2020-12-03T00:00:00\", \"2020-12-04T00:00:00\", \"2020-12-05T00:00:00\", \"2020-12-06T00:00:00\", \"2020-12-07T00:00:00\", \"2020-12-08T00:00:00\", \"2020-12-09T00:00:00\", \"2020-12-10T00:00:00\", \"2020-12-11T00:00:00\", \"2020-12-12T00:00:00\", \"2020-12-13T00:00:00\", \"2020-12-14T00:00:00\", \"2020-12-15T00:00:00\", \"2020-12-16T00:00:00\", \"2020-12-17T00:00:00\", \"2020-12-18T00:00:00\", \"2020-12-19T00:00:00\", \"2020-12-20T00:00:00\", \"2020-12-21T00:00:00\", \"2020-12-22T00:00:00\", \"2020-12-23T00:00:00\", \"2020-12-24T00:00:00\", \"2020-12-25T00:00:00\", \"2020-12-26T00:00:00\", \"2020-12-27T00:00:00\", \"2020-12-28T00:00:00\", \"2020-12-29T00:00:00\", \"2020-12-30T00:00:00\", \"2020-12-31T00:00:00\", \"2021-01-01T00:00:00\", \"2021-01-02T00:00:00\", \"2021-01-03T00:00:00\", \"2021-01-04T00:00:00\", \"2021-01-05T00:00:00\", \"2021-01-06T00:00:00\", \"2021-01-07T00:00:00\", \"2021-01-08T00:00:00\", \"2021-01-09T00:00:00\", \"2021-01-10T00:00:00\", \"2021-01-11T00:00:00\", \"2021-01-12T00:00:00\", \"2021-01-13T00:00:00\", \"2021-01-14T00:00:00\", \"2021-01-15T00:00:00\", \"2021-01-16T00:00:00\", \"2021-01-17T00:00:00\", \"2021-01-18T00:00:00\", \"2021-01-19T00:00:00\", \"2021-01-20T00:00:00\", \"2021-01-21T00:00:00\", \"2021-01-22T00:00:00\", \"2021-01-23T00:00:00\", \"2021-01-24T00:00:00\", \"2021-01-25T00:00:00\", \"2021-01-26T00:00:00\", \"2021-01-27T00:00:00\", \"2021-01-28T00:00:00\", \"2021-01-29T00:00:00\", \"2021-01-30T00:00:00\", \"2021-01-31T00:00:00\", \"2021-02-01T00:00:00\", \"2021-02-02T00:00:00\", \"2021-02-03T00:00:00\", \"2021-02-04T00:00:00\", \"2021-02-05T00:00:00\", \"2021-02-06T00:00:00\", \"2021-02-07T00:00:00\", \"2021-02-08T00:00:00\", \"2021-02-09T00:00:00\", \"2021-02-10T00:00:00\", \"2021-02-11T00:00:00\", \"2021-02-12T00:00:00\", \"2021-02-13T00:00:00\", \"2021-02-14T00:00:00\", \"2021-02-15T00:00:00\", \"2021-02-16T00:00:00\", \"2021-02-17T00:00:00\", \"2021-02-18T00:00:00\", \"2021-02-19T00:00:00\", \"2021-02-20T00:00:00\", \"2021-02-21T00:00:00\", \"2021-02-22T00:00:00\", \"2021-02-23T00:00:00\", \"2021-02-24T00:00:00\", \"2021-02-25T00:00:00\", \"2021-02-26T00:00:00\", \"2021-02-27T00:00:00\", \"2021-02-28T00:00:00\", \"2021-03-01T00:00:00\", \"2021-03-02T00:00:00\", \"2021-03-03T00:00:00\", \"2021-03-04T00:00:00\", \"2021-03-05T00:00:00\", \"2021-03-06T00:00:00\", \"2021-03-07T00:00:00\", \"2021-03-08T00:00:00\", \"2021-03-09T00:00:00\", \"2021-03-10T00:00:00\", \"2021-03-11T00:00:00\", \"2021-03-12T00:00:00\", \"2021-03-13T00:00:00\", \"2021-03-14T00:00:00\", \"2021-03-15T00:00:00\", \"2021-03-16T00:00:00\", \"2021-03-17T00:00:00\", \"2021-03-18T00:00:00\", \"2021-03-19T00:00:00\", \"2021-03-20T00:00:00\", \"2021-03-21T00:00:00\", \"2021-03-22T00:00:00\", \"2021-03-23T00:00:00\", \"2021-03-24T00:00:00\", \"2021-03-25T00:00:00\", \"2021-03-26T00:00:00\", \"2021-03-27T00:00:00\", \"2021-03-28T00:00:00\", \"2021-03-29T00:00:00\", \"2021-03-30T00:00:00\", \"2021-03-31T00:00:00\", \"2021-04-01T00:00:00\", \"2021-04-02T00:00:00\", \"2021-04-03T00:00:00\", \"2021-04-04T00:00:00\", \"2021-04-05T00:00:00\", \"2021-04-06T00:00:00\", \"2021-04-07T00:00:00\", \"2021-04-08T00:00:00\", \"2021-04-09T00:00:00\", \"2021-04-10T00:00:00\", \"2021-04-11T00:00:00\", \"2021-04-12T00:00:00\", \"2021-04-13T00:00:00\", \"2021-04-14T00:00:00\", \"2021-04-15T00:00:00\", \"2021-04-16T00:00:00\", \"2021-04-17T00:00:00\", \"2021-04-18T00:00:00\", \"2021-04-19T00:00:00\", \"2021-04-20T00:00:00\", \"2021-04-21T00:00:00\", \"2021-04-22T00:00:00\"], \"xaxis\": \"x\", \"y\": [108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 108.53, 121.47, 141.71, 165.16, 189.97, 196.36, 196.82, 237.61, 264.25, 290.58, 333.2, 369.58, 384.96, 387.09, 428.8, 494.1, 554.07, 608.41, 681.63, 705.83, 712.99, 779.51, 838.11, 889.25, 926.24, 933.25, 939.03, 939.79, 961.71, 972.36, 943.29, 916.35, 879.51, 859.57, 854.09, 783.62, 694.57, 558.94, 504.75, 459.09, 443.71, 442.04, 397.29, 349.64, 386.78, 338.53, 299.87, 286.17, 284.04, 246.29, 221.17, 203.36, 197.12, 186.31, 188.9, 188.29, 172.92, 166.07, 167.59, 152.67, 148.26, 145.06, 145.06, 149.93, 156.78, 153.28, 156.33, 157.09, 158.91, 159.22, 158.31, 161.35, 161.96, 167.74, 182.97, 198.95, 215.69, 213.41, 220.87, 223.3, 212.04, 173.22, 159.83, 144.3, 147.96, 142.17, 148.87, 156.48, 155.41, 157.39, 158.61, 170.79, 178.25, 174.75, 180.99, 218.89, 215.39, 218.13, 209.91, 202.3, 206.71, 206.1, 203.36, 205.49, 200.17, 208.84, 214.32, 214.78, 218.58, 218.28, 223.46, 225.59, 231.98, 238.98, 238.98, 238.37, 244.77, 245.53, 244.61, 246.29, 241.87, 249.94, 253.9, 249.79, 243.55, 242.63, 224.06, 224.06, 215.69, 206.71, 200.01, 199.86, 200.01, 202.14, 193.77, 188.14, 191.34, 190.88, 191.95, 193.32, 200.17, 202.45, 209.91, 206.1, 209.15, 208.84, 206.71, 196.06, 189.05, 179.01, 173.68, 169.42, 169.11, 169.57, 170.64, 178.09, 179.62, 187.68, 197.27, 200.93, 202.6, 213.87, 228.48, 237.0, 247.2, 254.2, 259.84, 260.44, 272.32, 274.9, 293.63, 307.94, 327.42, 335.33, 335.64, 354.21, 371.26, 381.0, 398.35, 401.85, 403.99, 405.05, 329.86, 350.4, 345.84, 324.68, 314.79, 313.26, 313.57, 379.33, 340.21, 334.42, 331.68, 318.29, 314.18, 312.35, 305.04, 235.66750000000002, 166.29500000000002, 96.92250000000001, 27.55, 31.36, 33.64, 38.05, 46.43, 48.1, 49.17, 45.49307692307692, 41.81615384615384, 38.139230769230764, 34.46230769230769, 30.78538461538461, 27.108461538461537, 23.43153846153846, 19.75461538461538, 16.077692307692306, 12.40076923076923, 8.723846153846154, 5.046923076923076, 1.37, 2.74, 3.5, 3.5, 3.5, 5.18, 6.55, 6.85, 5.63, 5.94, 6.24, 6.24, 5.02, 3.96, 2.89, 2.89, 2.74, 2.74, 2.89, 3.04, 3.2, 2.89, 2.74, 2.59, 2.74, 2.74, 2.13, 1.83, 1.67, 2.89, 2.74, 2.28, 2.28, 2.28, 2.59, 3.2, 2.59, 2.44, 3.04, 2.89, 3.65, 4.72, 4.26, 4.41, 5.18, 5.48, 5.48, 5.63, 5.33, 5.48, 4.72, 3.96, 3.2, 3.2, 2.59, 2.13, 2.13, 2.59, 2.59, 2.44, 2.59, 2.28, 1.52, 1.52, 1.07, 1.52, 2.13, 1.98, 2.44, 3.96, 4.26, 5.18, 7.46, 7.15, 7.46, 8.98, 8.98, 9.29, 10.96, 10.35, 10.81, 10.81, 10.2, 10.81, 11.26, 10.96, 9.74, 10.2, 10.05, 12.03, 11.57, 12.33, 12.48, 14.92, 13.85, 13.7, 14.77, 16.59, 18.42, 21.01, 24.81, 27.25, 84.79, 90.26, 91.48, 72.0, 71.85, 72.0, 75.35, 49.17, 56.02, 61.34, 63.17, 65.3, 67.89, 70.32, 70.48, 69.11, 102.14, 94.53, 86.61, 84.48, 75.8, 76.11, 80.37, 89.81, 102.9, 102.29, 102.75, 100.62, 100.46, 81.28, 80.68, 80.22, 75.2, 75.5, 74.59, 77.78], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"x=%{x}<br>y=%{y}<br>color=%{marker.color}\", \"legendgroup\": \"\", \"marker\": {\"color\": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], \"coloraxis\": \"coloraxis\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [\"2020-03-18T00:00:00\", \"2020-03-19T00:00:00\", \"2020-03-20T00:00:00\", \"2020-03-21T00:00:00\", \"2020-03-22T00:00:00\", \"2020-03-23T00:00:00\", \"2020-03-24T00:00:00\", \"2020-03-25T00:00:00\", \"2020-03-26T00:00:00\", \"2020-03-27T00:00:00\", \"2020-03-28T00:00:00\", \"2020-03-29T00:00:00\", \"2020-03-30T00:00:00\", \"2020-03-31T00:00:00\", \"2020-04-01T00:00:00\", \"2020-04-02T00:00:00\", \"2020-04-03T00:00:00\", \"2020-04-04T00:00:00\", \"2020-04-05T00:00:00\", \"2020-04-06T00:00:00\", \"2020-04-07T00:00:00\", \"2020-04-08T00:00:00\", \"2020-04-09T00:00:00\", \"2020-04-10T00:00:00\", \"2020-04-11T00:00:00\", \"2020-04-12T00:00:00\", \"2020-04-13T00:00:00\", \"2020-04-14T00:00:00\", \"2020-04-15T00:00:00\", \"2020-04-16T00:00:00\", \"2020-04-17T00:00:00\", \"2020-04-18T00:00:00\", \"2020-04-19T00:00:00\", \"2020-04-20T00:00:00\", \"2020-04-21T00:00:00\", \"2020-04-22T00:00:00\", \"2020-04-23T00:00:00\", \"2020-04-24T00:00:00\", \"2020-04-25T00:00:00\", \"2020-04-26T00:00:00\", \"2020-04-27T00:00:00\", \"2020-04-28T00:00:00\", \"2020-04-29T00:00:00\", \"2020-04-30T00:00:00\", \"2020-05-01T00:00:00\", \"2020-05-02T00:00:00\", \"2020-05-03T00:00:00\", \"2020-05-04T00:00:00\", \"2020-05-05T00:00:00\", \"2020-05-06T00:00:00\", \"2020-05-07T00:00:00\", \"2020-05-08T00:00:00\", \"2020-05-09T00:00:00\", \"2020-05-10T00:00:00\", \"2020-05-11T00:00:00\", \"2020-05-12T00:00:00\", \"2020-05-13T00:00:00\", \"2020-05-14T00:00:00\", \"2020-05-15T00:00:00\", \"2020-05-16T00:00:00\", \"2020-05-17T00:00:00\", \"2020-05-18T00:00:00\", \"2020-05-19T00:00:00\", \"2020-05-20T00:00:00\", \"2020-05-21T00:00:00\", \"2020-05-22T00:00:00\", \"2020-05-23T00:00:00\", \"2020-05-24T00:00:00\", \"2020-05-25T00:00:00\", \"2020-05-26T00:00:00\", \"2020-05-27T00:00:00\", \"2020-05-28T00:00:00\", \"2020-05-29T00:00:00\", \"2020-05-30T00:00:00\", \"2020-05-31T00:00:00\", \"2020-06-01T00:00:00\", \"2020-06-02T00:00:00\", \"2020-06-03T00:00:00\", \"2020-06-04T00:00:00\", \"2020-06-05T00:00:00\", \"2020-06-06T00:00:00\", \"2020-06-07T00:00:00\", \"2020-06-08T00:00:00\", \"2020-06-09T00:00:00\", \"2020-06-10T00:00:00\", \"2020-06-11T00:00:00\", \"2020-06-12T00:00:00\", \"2020-06-13T00:00:00\", \"2020-06-14T00:00:00\", \"2020-06-15T00:00:00\", \"2020-06-16T00:00:00\", \"2020-06-17T00:00:00\", \"2020-06-18T00:00:00\", \"2020-06-19T00:00:00\", \"2020-06-20T00:00:00\", \"2020-06-21T00:00:00\", \"2020-06-22T00:00:00\", \"2020-06-23T00:00:00\", \"2020-06-24T00:00:00\", \"2020-06-25T00:00:00\", \"2020-06-26T00:00:00\", \"2020-06-27T00:00:00\", \"2020-06-28T00:00:00\", \"2020-06-29T00:00:00\", \"2020-06-30T00:00:00\", \"2020-07-01T00:00:00\", \"2020-07-02T00:00:00\", \"2020-07-03T00:00:00\", \"2020-07-04T00:00:00\", \"2020-07-05T00:00:00\", \"2020-07-06T00:00:00\", \"2020-07-07T00:00:00\", \"2020-07-08T00:00:00\", \"2020-07-09T00:00:00\", \"2020-07-10T00:00:00\", \"2020-07-11T00:00:00\", \"2020-07-12T00:00:00\", \"2020-07-13T00:00:00\", \"2020-07-14T00:00:00\", \"2020-07-15T00:00:00\", \"2020-07-16T00:00:00\", \"2020-07-17T00:00:00\", \"2020-07-18T00:00:00\", \"2020-07-19T00:00:00\", \"2020-07-20T00:00:00\", \"2020-07-21T00:00:00\", \"2020-07-22T00:00:00\", \"2020-07-23T00:00:00\", \"2020-07-24T00:00:00\", \"2020-07-25T00:00:00\", \"2020-07-26T00:00:00\", \"2020-07-27T00:00:00\", \"2020-07-28T00:00:00\", \"2020-07-29T00:00:00\", \"2020-07-30T00:00:00\", \"2020-07-31T00:00:00\", \"2020-08-01T00:00:00\", \"2020-08-02T00:00:00\", \"2020-08-03T00:00:00\", \"2020-08-04T00:00:00\", \"2020-08-05T00:00:00\", \"2020-08-06T00:00:00\", \"2020-08-07T00:00:00\", \"2020-08-08T00:00:00\", \"2020-08-09T00:00:00\", \"2020-08-10T00:00:00\", \"2020-08-11T00:00:00\", \"2020-08-12T00:00:00\", \"2020-08-13T00:00:00\", \"2020-08-14T00:00:00\", \"2020-08-15T00:00:00\", \"2020-08-16T00:00:00\", \"2020-08-17T00:00:00\", \"2020-08-18T00:00:00\", \"2020-08-19T00:00:00\", \"2020-08-20T00:00:00\", \"2020-08-21T00:00:00\", \"2020-08-22T00:00:00\", \"2020-08-23T00:00:00\", \"2020-08-24T00:00:00\", \"2020-08-25T00:00:00\", \"2020-08-26T00:00:00\", \"2020-08-27T00:00:00\", \"2020-08-28T00:00:00\", \"2020-08-29T00:00:00\", \"2020-08-30T00:00:00\", \"2020-08-31T00:00:00\", \"2020-09-01T00:00:00\", \"2020-09-02T00:00:00\", \"2020-09-03T00:00:00\", \"2020-09-04T00:00:00\", \"2020-09-05T00:00:00\", \"2020-09-06T00:00:00\", \"2020-09-07T00:00:00\", \"2020-09-08T00:00:00\", \"2020-09-09T00:00:00\", \"2020-09-10T00:00:00\", \"2020-09-11T00:00:00\", \"2020-09-12T00:00:00\", \"2020-09-13T00:00:00\", \"2020-09-14T00:00:00\", \"2020-09-15T00:00:00\", \"2020-09-16T00:00:00\", \"2020-09-17T00:00:00\", \"2020-09-18T00:00:00\", \"2020-09-19T00:00:00\", \"2020-09-20T00:00:00\", \"2020-09-21T00:00:00\", \"2020-09-22T00:00:00\", \"2020-09-23T00:00:00\", \"2020-09-24T00:00:00\", \"2020-09-25T00:00:00\", \"2020-09-26T00:00:00\", \"2020-09-27T00:00:00\", \"2020-09-28T00:00:00\", \"2020-09-29T00:00:00\", \"2020-09-30T00:00:00\", \"2020-10-01T00:00:00\", \"2020-10-02T00:00:00\", \"2020-10-03T00:00:00\", \"2020-10-04T00:00:00\", \"2020-10-05T00:00:00\", \"2020-10-06T00:00:00\", \"2020-10-07T00:00:00\", \"2020-10-08T00:00:00\", \"2020-10-09T00:00:00\", \"2020-10-10T00:00:00\", \"2020-10-11T00:00:00\", \"2020-10-12T00:00:00\", \"2020-10-13T00:00:00\", \"2020-10-14T00:00:00\", \"2020-10-15T00:00:00\", \"2020-10-16T00:00:00\", \"2020-10-17T00:00:00\", \"2020-10-18T00:00:00\", \"2020-10-19T00:00:00\", \"2020-10-20T00:00:00\", \"2020-10-21T00:00:00\", \"2020-10-22T00:00:00\", \"2020-10-23T00:00:00\", \"2020-10-24T00:00:00\", \"2020-10-25T00:00:00\", \"2020-10-26T00:00:00\", \"2020-10-27T00:00:00\", \"2020-10-28T00:00:00\", \"2020-10-29T00:00:00\", \"2020-10-30T00:00:00\", \"2020-10-31T00:00:00\", \"2020-11-01T00:00:00\", \"2020-11-02T00:00:00\", \"2020-11-03T00:00:00\", \"2020-11-04T00:00:00\", \"2020-11-05T00:00:00\", \"2020-11-06T00:00:00\", \"2020-11-07T00:00:00\", \"2020-11-08T00:00:00\", \"2020-11-09T00:00:00\", \"2020-11-10T00:00:00\", \"2020-11-11T00:00:00\", \"2020-11-12T00:00:00\", \"2020-11-13T00:00:00\", \"2020-11-14T00:00:00\", \"2020-11-15T00:00:00\", \"2020-11-16T00:00:00\", \"2020-11-17T00:00:00\", \"2020-11-18T00:00:00\", \"2020-11-19T00:00:00\", \"2020-11-20T00:00:00\", \"2020-11-21T00:00:00\", \"2020-11-22T00:00:00\", \"2020-11-23T00:00:00\", \"2020-11-24T00:00:00\", \"2020-11-25T00:00:00\", \"2020-11-26T00:00:00\", \"2020-11-27T00:00:00\", \"2020-11-28T00:00:00\", \"2020-11-29T00:00:00\", \"2020-11-30T00:00:00\", \"2020-12-01T00:00:00\", \"2020-12-02T00:00:00\", \"2020-12-03T00:00:00\", \"2020-12-04T00:00:00\", \"2020-12-05T00:00:00\", \"2020-12-06T00:00:00\", \"2020-12-07T00:00:00\", \"2020-12-08T00:00:00\", \"2020-12-09T00:00:00\", \"2020-12-10T00:00:00\", \"2020-12-11T00:00:00\", \"2020-12-12T00:00:00\", \"2020-12-13T00:00:00\", \"2020-12-14T00:00:00\", \"2020-12-15T00:00:00\", \"2020-12-16T00:00:00\", \"2020-12-17T00:00:00\", \"2020-12-18T00:00:00\", \"2020-12-19T00:00:00\", \"2020-12-20T00:00:00\", \"2020-12-21T00:00:00\", \"2020-12-22T00:00:00\", \"2020-12-23T00:00:00\", \"2020-12-24T00:00:00\", \"2020-12-25T00:00:00\", \"2020-12-26T00:00:00\", \"2020-12-27T00:00:00\", \"2020-12-28T00:00:00\", \"2020-12-29T00:00:00\", \"2020-12-30T00:00:00\", \"2020-12-31T00:00:00\", \"2021-01-01T00:00:00\", \"2021-01-02T00:00:00\", \"2021-01-03T00:00:00\", \"2021-01-04T00:00:00\", \"2021-01-05T00:00:00\", \"2021-01-06T00:00:00\", \"2021-01-07T00:00:00\", \"2021-01-08T00:00:00\", \"2021-01-09T00:00:00\", \"2021-01-10T00:00:00\", \"2021-01-11T00:00:00\", \"2021-01-12T00:00:00\", \"2021-01-13T00:00:00\", \"2021-01-14T00:00:00\", \"2021-01-15T00:00:00\", \"2021-01-16T00:00:00\", \"2021-01-17T00:00:00\", \"2021-01-18T00:00:00\", \"2021-01-19T00:00:00\", \"2021-01-20T00:00:00\", \"2021-01-21T00:00:00\", \"2021-01-22T00:00:00\", \"2021-01-23T00:00:00\", \"2021-01-24T00:00:00\", \"2021-01-25T00:00:00\", \"2021-01-26T00:00:00\", \"2021-01-27T00:00:00\", \"2021-01-28T00:00:00\", \"2021-01-29T00:00:00\", \"2021-01-30T00:00:00\", \"2021-01-31T00:00:00\", \"2021-02-01T00:00:00\", \"2021-02-02T00:00:00\", \"2021-02-03T00:00:00\", \"2021-02-04T00:00:00\", \"2021-02-05T00:00:00\", \"2021-02-06T00:00:00\", \"2021-02-07T00:00:00\", \"2021-02-08T00:00:00\", \"2021-02-09T00:00:00\", \"2021-02-10T00:00:00\", \"2021-02-11T00:00:00\", \"2021-02-12T00:00:00\", \"2021-02-13T00:00:00\", \"2021-02-14T00:00:00\", \"2021-02-15T00:00:00\", \"2021-02-16T00:00:00\", \"2021-02-17T00:00:00\", \"2021-02-18T00:00:00\", \"2021-02-19T00:00:00\", \"2021-02-20T00:00:00\", \"2021-02-21T00:00:00\", \"2021-02-22T00:00:00\", \"2021-02-23T00:00:00\", \"2021-02-24T00:00:00\", \"2021-02-25T00:00:00\", \"2021-02-26T00:00:00\", \"2021-02-27T00:00:00\", \"2021-02-28T00:00:00\", \"2021-03-01T00:00:00\", \"2021-03-02T00:00:00\", \"2021-03-03T00:00:00\", \"2021-03-04T00:00:00\", \"2021-03-05T00:00:00\", \"2021-03-06T00:00:00\", \"2021-03-07T00:00:00\", \"2021-03-08T00:00:00\", \"2021-03-09T00:00:00\", \"2021-03-10T00:00:00\", \"2021-03-11T00:00:00\", \"2021-03-12T00:00:00\", \"2021-03-13T00:00:00\", \"2021-03-14T00:00:00\", \"2021-03-15T00:00:00\", \"2021-03-16T00:00:00\", \"2021-03-17T00:00:00\", \"2021-03-18T00:00:00\", \"2021-03-19T00:00:00\", \"2021-03-20T00:00:00\", \"2021-03-21T00:00:00\", \"2021-03-22T00:00:00\", \"2021-03-23T00:00:00\", \"2021-03-24T00:00:00\", \"2021-03-25T00:00:00\", \"2021-03-26T00:00:00\", \"2021-03-27T00:00:00\", \"2021-03-28T00:00:00\", \"2021-03-29T00:00:00\", \"2021-03-30T00:00:00\", \"2021-03-31T00:00:00\", \"2021-04-01T00:00:00\", \"2021-04-02T00:00:00\", \"2021-04-03T00:00:00\", \"2021-04-04T00:00:00\", \"2021-04-05T00:00:00\", \"2021-04-06T00:00:00\", \"2021-04-07T00:00:00\", \"2021-04-08T00:00:00\", \"2021-04-09T00:00:00\", \"2021-04-10T00:00:00\", \"2021-04-11T00:00:00\", \"2021-04-12T00:00:00\", \"2021-04-13T00:00:00\", \"2021-04-14T00:00:00\", \"2021-04-15T00:00:00\", \"2021-04-16T00:00:00\", \"2021-04-17T00:00:00\", \"2021-04-18T00:00:00\", \"2021-04-19T00:00:00\", \"2021-04-20T00:00:00\", \"2021-04-21T00:00:00\", \"2021-04-22T00:00:00\"], \"xaxis\": \"x\", \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 608.41, 0.0, 0.0, 0.0, 0.0, 838.11, 0.0, 0.0, 0.0, 0.0, 0.0, 961.71, 972.36, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], \"yaxis\": \"y\"}],\n",
              "                        {\"legend\": {\"tracegroupgap\": 0}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Evolution du prix du BTC\"}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"rangeslider\": {\"visible\": true}, \"title\": {\"text\": \"x\"}}, \"yaxis\": {\"anchor\": \"x\", \"autorange\": true, \"domain\": [0.0, 1.0], \"fixedrange\": false, \"title\": {\"text\": \"y\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('d761acf0-dc52-4b7e-9227-37df53b1147a');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVAWVQioe-kS"
      },
      "source": [
        "Comme les anomalies détectées ne sembles pas cohérentes, nous n'allons pas les traiter..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vDYEK-cxE0C"
      },
      "source": [
        "# Analyse de la série"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGY4fCB3xdUx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "outputId": "89d1ab50-bfa2-402c-be37-c9da63377843"
      },
      "source": [
        "df_paris"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>taux</th>\n",
              "      <th>Anomalies</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>extract_date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2020-03-18</th>\n",
              "      <td>108.53</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-19</th>\n",
              "      <td>108.53</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-20</th>\n",
              "      <td>108.53</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-21</th>\n",
              "      <td>108.53</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-22</th>\n",
              "      <td>108.53</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-18</th>\n",
              "      <td>80.22</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-19</th>\n",
              "      <td>75.20</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-20</th>\n",
              "      <td>75.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-21</th>\n",
              "      <td>74.59</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-22</th>\n",
              "      <td>77.78</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>401 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                taux  Anomalies\n",
              "extract_date                   \n",
              "2020-03-18    108.53          0\n",
              "2020-03-19    108.53          0\n",
              "2020-03-20    108.53          0\n",
              "2020-03-21    108.53          0\n",
              "2020-03-22    108.53          0\n",
              "...              ...        ...\n",
              "2021-04-18     80.22          0\n",
              "2021-04-19     75.20          0\n",
              "2021-04-20     75.50          0\n",
              "2021-04-21     74.59          0\n",
              "2021-04-22     77.78          0\n",
              "\n",
              "[401 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 226
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBCbjPSNxWXy"
      },
      "source": [
        "**1. ACF & PACF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CleN4htOxYqZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "9475a8ce-da01-412b-f3c2-f7a298297cf3"
      },
      "source": [
        "# ACF & PACF du bruit blanc\n",
        "\n",
        "serie = serie_etude\n",
        "\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "f1, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "f1.subplots_adjust(hspace=0.3,wspace=0.2)\n",
        "\n",
        "plot_acf(serie, ax=ax1, lags = range(0,15))\n",
        "ax1.set_title(\"Autocorrélation du bruit blanc\")\n",
        "\n",
        "plot_pacf(serie, ax=ax2, lags = range(0, 15))\n",
        "ax2.set_title(\"Autocorrélation partielle du bruit blanc\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/regression/linear_model.py:1358: RuntimeWarning:\n",
            "\n",
            "invalid value encountered in sqrt\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Autocorrélation partielle du bruit blanc')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o66MqHZTyHX5"
      },
      "source": [
        "**2. Test de Dickey-Fuller**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm5VXMhkyLSN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b308f77f-468a-4ea8-b763-98e964bf9ce1"
      },
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "serie_test = serie_etude['taux']\n",
        "\n",
        "adf, p, usedlag, nobs, cvs, aic = sm.tsa.stattools.adfuller(serie_test)\n",
        "\n",
        "adf_results_string = 'ADF: {}\\np-value: {},\\nN: {}, \\ncritical values: {}'\n",
        "print(adf_results_string.format(adf, p, nobs, cvs))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ADF: -2.564352316207272\n",
            "p-value: 0.10059098552499757,\n",
            "N: 113, \n",
            "critical values: {'1%': -3.489589552580676, '5%': -2.887477210140433, '10%': -2.580604145195395}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moHyQgGfyTi4"
      },
      "source": [
        "**3. Suppression de la tendance non linéaire et test de sationnarité**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43AcGds6y0pI"
      },
      "source": [
        "from scipy.stats import boxcox\n",
        "\n",
        "serie_log, lam = boxcox(serie)\n",
        "\n",
        "f1, (ax1,ax2) = plt.subplots(2, 1, figsize=(15, 5))\n",
        "ax1.plot(serie_etude.index,serie_log)\n",
        "ax2.plot(serie_etude.index,serie)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fRPXCkO0DUh"
      },
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "serie_test = serie_log\n",
        "\n",
        "adf, p, usedlag, nobs, cvs, aic = sm.tsa.stattools.adfuller(serie_test)\n",
        "\n",
        "adf_results_string = 'ADF: {}\\np-value: {},\\nN: {}, \\ncritical values: {}'\n",
        "print(adf_results_string.format(adf, p, nobs, cvs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI8sp_Rlz6GT"
      },
      "source": [
        "***4. Suppression de la tendance linéaire et test de stationnarité***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iHrAWJH0TdT"
      },
      "source": [
        "f1, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 5))\n",
        "\n",
        "# Calcul des coefficients\n",
        "x = np.linspace(0,len(serie_log),len(serie_log))\n",
        "coefs = np.polyfit(x,serie_log,1)\n",
        "\n",
        "# Calcul de la tendance non linéaire\n",
        "trend = coefs[0]*np.power(x,1) + coefs[1]\n",
        "\n",
        "# Calcul de la série sans tendance\n",
        "serie_log_detrend = serie_log - trend\n",
        "\n",
        "# Affiche les résultats\n",
        "ax1.plot(trend)\n",
        "ax1.plot(serie_log)\n",
        "ax1.set_title(\"Série originale et tendance non linéaire\")\n",
        "\n",
        "ax2.plot(serie_log_detrend)\n",
        "ax2.set_title(\"Série avec tendance non linéaire supprimée\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNXs9Fm--Kcl"
      },
      "source": [
        "# ACF & PACF du bruit blanc\n",
        "\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "f1, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "f1.subplots_adjust(hspace=0.3,wspace=0.2)\n",
        "\n",
        "plot_acf(serie_log_detrend, ax=ax1, lags = range(0,50))\n",
        "ax1.set_title(\"Autocorrélation du bruit blanc\")\n",
        "\n",
        "plot_pacf(serie_log_detrend, ax=ax2, lags = range(0, 50))\n",
        "ax2.set_title(\"Autocorrélation partielle du bruit blanc\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_r01cDgq0xaJ"
      },
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "serie_test = serie_log_detrend\n",
        "\n",
        "adf, p, usedlag, nobs, cvs, aic = sm.tsa.stattools.adfuller(serie_test)\n",
        "\n",
        "adf_results_string = 'ADF: {}\\np-value: {},\\nN: {}, \\ncritical values: {}'\n",
        "print(adf_results_string.format(adf, p, nobs, cvs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ychdf1RxMPDD"
      },
      "source": [
        "**5. Différentiation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qaHkgtQMOqJ"
      },
      "source": [
        "# Différenciation d'odre 1 et saisonnale à l'odre 1 et de période 12\n",
        "\n",
        "from statsmodels.tsa.statespace.tools import diff\n",
        "\n",
        "serie_log_detrend_diff1 = diff(serie_log_detrend,1)       # diff=1 ; diff_saison=1 ; periode = 12\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(serie_log_detrend_diff1)\n",
        "plt.title(\"Signal différencié d'ordre 1 + saisonalité\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFLlzv0JMks5"
      },
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "serie_test = serie_log_detrend_diff1\n",
        "\n",
        "adf, p, usedlag, nobs, cvs, aic = sm.tsa.stattools.adfuller(serie_test)\n",
        "\n",
        "adf_results_string = 'ADF: {}\\np-value: {},\\nN: {}, \\ncritical values: {}'\n",
        "print(adf_results_string.format(adf, p, nobs, cvs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0Oqd_7XMqaZ"
      },
      "source": [
        "# ACF & PACF du bruit blanc\n",
        "\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "f1, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "f1.subplots_adjust(hspace=0.3,wspace=0.2)\n",
        "\n",
        "plot_acf(serie_log_detrend_diff1, ax=ax1, lags = range(0,50))\n",
        "ax1.set_title(\"Autocorrélation du bruit blanc\")\n",
        "\n",
        "plot_pacf(serie_log_detrend_diff1, ax=ax2, lags = range(0, 50))\n",
        "ax2.set_title(\"Autocorrélation partielle du bruit blanc\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5oIY2Yl5Tlt"
      },
      "source": [
        "**5. Enregistrement des données dans le dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjFSWhdeM4KM"
      },
      "source": [
        "serie_log_detrend_diff1 = np.insert(serie_log_detrend_diff1,0,0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ele3kFOp5TTW"
      },
      "source": [
        "serie_etude['diff'] = serie_log_detrend_diff1\n",
        "serie_etude['diff'][0] = \"Nan\"\n",
        "serie_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG5Fyx5O5oe5"
      },
      "source": [
        "# Prépartion des datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7cGUeWb5oe7"
      },
      "source": [
        "**1. Séparation des données en données pour l'entrainement et la validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob-EAw_j5oe8"
      },
      "source": [
        "On réserve 20% des données pour l'entrainement et le reste pour la validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5AWeK_Z5oe8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e9b5dae-25b0-4cf0-c979-8e3070e9144e"
      },
      "source": [
        "# Sépare les données en entrainement et tests\n",
        "pourcentage = 0.5\n",
        "temps_separation = int(len(serie_etude['taux']) * pourcentage)\n",
        "date_separation = serie_etude.index[temps_separation]\n",
        "\n",
        "serie_entrainement = serie_etude['taux'].iloc[:temps_separation]\n",
        "serie_test = serie_etude['taux'].iloc[temps_separation:]\n",
        "\n",
        "print(\"Taille de l'entrainement : %d\" %len(serie_entrainement))\n",
        "print(\"Taille de la validation : %d\" %len(serie_test))"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Taille de l'entrainement : 200\n",
            "Taille de la validation : 201\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZUMMMro5oe9"
      },
      "source": [
        "On normalise les données :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu_YxoSI5oe9"
      },
      "source": [
        "# Calcul de la moyenne et de l'écart type de la série\n",
        "mean = tf.math.reduce_mean(np.asarray(serie_entrainement))\n",
        "std = tf.math.reduce_std(np.asarray((serie_entrainement)))\n",
        "\n",
        "# Normalisation des données\n",
        "serie_entrainement = (serie_entrainement-mean)/std\n",
        "serie_test = (serie_test-mean)/std"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_4OZJ-p5oe9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "d0ec96ac-b127-4c00-8fb0-1a6629e2337c"
      },
      "source": [
        "# Affiche la série\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "ax.plot(serie_entrainement, label=\"Entrainement\")\n",
        "ax.plot(serie_test,label=\"Validation\")\n",
        "\n",
        "ax.set_title(\"Evolution du prix du BTC\")\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-bANnT35oe-"
      },
      "source": [
        "**2. Création des datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_EweLDJ5oe-"
      },
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "\n",
        "def prepare_dataset_XY(serie, taille_fenetre, horizon, batch_size):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie)\n",
        "  dataset = dataset.window(taille_fenetre+horizon, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre + horizon))\n",
        "  dataset = dataset.map(lambda x: (tf.expand_dims(x[0:taille_fenetre],axis=1),x[-1:]))\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "  return dataset"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Jh1RZYo5oe_"
      },
      "source": [
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "taille_fenetre = 10\n",
        "horizon = 5\n",
        "batch_size = 1\n",
        "\n",
        "# Création du dataset\n",
        "dataset = prepare_dataset_XY(serie_entrainement,taille_fenetre,horizon,batch_size)\n",
        "dataset_val = prepare_dataset_XY(serie_test,taille_fenetre,horizon,batch_size)"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr2pbMox5oe_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f02b3218-12e8-479d-d0aa-fc66dac79a4a"
      },
      "source": [
        "print(len(list(dataset.as_numpy_iterator())))\n",
        "for element in dataset.take(1):\n",
        "  print(element[0].shape)\n",
        "  print(element[1].shape)"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "186\n",
            "(1, 10, 1)\n",
            "(1, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itll4lU9q67C",
        "outputId": "f9e69fa2-27db-4fd8-f300-926e0874f407"
      },
      "source": [
        "print(len(list(dataset_val.as_numpy_iterator())))\n",
        "for element in dataset_val.take(1):\n",
        "  print(element[0].shape)\n",
        "  print(element[1].shape)"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "187\n",
            "(1, 10, 1)\n",
            "(1, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHCcYn6i5oe_"
      },
      "source": [
        "On extrait maintenant les deux tenseurs (X,Y) pour l'entrainement :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3ZhLIK15ofA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "964c2da4-03ec-4268-9d2c-2d2eee1796a9"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "#56x((1000,4,1),(1000,1)) => (56*1000,4,1) ; (56*1000,1)\n",
        "\n",
        "x,y = tuple(zip(*dataset))\n",
        "\n",
        "# Recombine les données\n",
        "# (56,1000,4,1) => (56*128,4,1)\n",
        "# (56,1000,1) => (56*128,1)\n",
        "x_train = np.asarray(tf.reshape(np.asarray(x,dtype=np.float32),shape=(np.asarray(x).shape[0]*np.asarray(x).shape[1],taille_fenetre,1)))\n",
        "y_train = np.asarray(tf.reshape(np.asarray(y,dtype=np.float32),shape=(np.asarray(y).shape[0]*np.asarray(y).shape[1])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(186, 10, 1)\n",
            "(186,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llnKyLvl5ofA"
      },
      "source": [
        "Puis la même chose pour les données de validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hadrKVrZ5ofB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4101254e-81d9-4a94-e9e8-bd8e3d35f2a3"
      },
      "source": [
        "# Extrait les X,Y du dataset_val\n",
        "\n",
        "x,y = tuple(zip(*dataset_val))\n",
        "\n",
        "# Recombine les données\n",
        "\n",
        "x_val = np.asarray(tf.reshape(np.asarray(x,dtype=np.float32),shape=(np.asarray(x).shape[0]*np.asarray(x).shape[1],taille_fenetre,1)))\n",
        "y_val = np.asarray(tf.reshape(np.asarray(y,dtype=np.float32),shape=(np.asarray(y).shape[0]*np.asarray(y).shape[1])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(187, 10, 1)\n",
            "(187,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "083QISTMM3AM"
      },
      "source": [
        "# Optimisation des hyperparamètres"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VzGM7ODMf8e"
      },
      "source": [
        "**1. Création de la série horaire pour l'optimisation des hyperparamètres**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUpvJmTQiNk-"
      },
      "source": [
        "**3. Définition du modèle**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuAyb8pciUle"
      },
      "source": [
        "Dans le modèle, les paramètres dim_LSTM, l1_reg, l2_reg seront optimisés :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRY7JTnlrTMf"
      },
      "source": [
        "def ModelLSTM(dim_LSTM = 10, l1_reg=0, l2_reg=0):\n",
        "  # Définition de l'entrée du modèle\n",
        "  entrees = tf.keras.layers.Input(shape=(taille_fenetre,1))\n",
        "\n",
        "  # Encodeur\n",
        "  s_encodeur = tf.keras.layers.LSTM(dim_LSTM, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg,l2=l2_reg))(entrees)\n",
        "\n",
        "  # Décodeur\n",
        "  s_decodeur = tf.keras.layers.Dense(dim_LSTM,activation=\"tanh\",kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg,l2=l2_reg))(s_encodeur)\n",
        "  s_decodeur = tf.keras.layers.Concatenate()([s_decodeur,s_encodeur])\n",
        "\n",
        "  # Générateur\n",
        "  sortie = tf.keras.layers.Dense(1,kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg,l2=l2_reg))(s_decodeur)\n",
        "\n",
        "  # Construction du modèle\n",
        "  model = tf.keras.Model(entrees,sortie)\n",
        "  optimiseur=tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "  model.compile(loss=\"mse\", optimizer=optimiseur,metrics=[\"mse\"])\n",
        "  return(model)"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYTuqrcYii9w"
      },
      "source": [
        "**4. Cross-validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_g-t1CbmsOkn",
        "outputId": "4a06d263-9c0d-40aa-fe20-072280b9b588"
      },
      "source": [
        "batch_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 474
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVCNBdgcihuv"
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import KFold, TimeSeriesSplit, GridSearchCV\n",
        "\n",
        "# Définitions des paramètres\n",
        "dim_LSTM = [5,10,20,40,60]\n",
        "l1_reg = [0,0.001,0.01]\n",
        "l2_reg = [0,0.001,0.01]\n",
        "\n",
        "param_grid = {'dim_LSTM': dim_LSTM, 'l1_reg': l1_reg, 'l2_reg': l2_reg}\n",
        "\n",
        "max_periodes = 5\n",
        "\n",
        "# Surveillance de l'entrainement\n",
        "es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=50, min_delta=1e-7, restore_best_weights=True)\n",
        "\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits = 5)\n",
        "model = KerasRegressor(build_fn=ModelLSTM, epochs=max_periodes, verbose=3,batch_size=batch_size)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=tscv, n_jobs=1, verbose=3)\n",
        "\n",
        "grid_result = grid.fit(x_train, y_train,callbacks=[es])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWTWBh8DjJpP"
      },
      "source": [
        "# Affiche les résultats\n",
        "print(\"Meilleur résultat : %f avec %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params_ = grid_result.cv_results_['params']\n",
        "for mean, stdev, param_ in zip(means, stds, params_):\n",
        "  print(\"%f (%f) with %r\" % (mean, stdev, param_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHfiXLFZhrPs"
      },
      "source": [
        "# Création du modèle LSTM de type encodeur-décodeur"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd9AzWFtjnjs"
      },
      "source": [
        "**1. Création du réseau**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9OCzL7UjAhL"
      },
      "source": [
        "Par défaut, la dimension des vecteurs cachés est de 10 et aucune régularisation n'est utilisée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnFw_FPPhxiE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b8fa345-e19b-4ec8-9d48-715231b3b161"
      },
      "source": [
        "dim_LSTM = 100\n",
        "l1_reg = 0.0\n",
        "l2_reg = 0.0\n",
        "\n",
        "# Définition de l'entrée du modèle\n",
        "entrees = tf.keras.layers.Input(shape=(taille_fenetre,1))\n",
        "\n",
        "# Encodeur\n",
        "s_encodeur = tf.keras.layers.LSTM(dim_LSTM, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg,l2=l2_reg),)(entrees)\n",
        "\n",
        "# Décodeur\n",
        "s_decodeur = tf.keras.layers.Dense(dim_LSTM,activation=\"tanh\",kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg,l2=l2_reg))(s_encodeur)\n",
        "s_decodeur = tf.keras.layers.Concatenate()([s_decodeur,s_encodeur])\n",
        "\n",
        "# Générateur\n",
        "sortie = tf.keras.layers.Dense(1,kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg,l2=l2_reg))(s_decodeur)\n",
        "\n",
        "# Construction du modèle\n",
        "model = tf.keras.Model(entrees,sortie)\n",
        "model.summary()"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            [(None, 10, 1)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 100)          40800       input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 100)          10100       lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 200)          0           dense_10[0][0]                   \n",
            "                                                                 lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 1)            201         concatenate_5[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 51,101\n",
            "Trainable params: 51,101\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_azfJaeUo2nU"
      },
      "source": [
        "**2. Optimisation de l'apprentissage**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf3lwaQBnjxL"
      },
      "source": [
        "Pour accélérer le traitement des données, nous n'allons pas utiliser l'intégralité des données pendant la mise à jour du gradient, comme cela a été fait jusqu'à présent (en utilisant le dataset).  \n",
        "Cette fois-ci, nous allons forcer les mises à jour du gradient à se produire de manière moins fréquente en attribuant la valeur du batch_size à prendre en compte lors de la regression du modèle.  \n",
        "Pour cela, on utilise l'argument \"batch_size\" dans la méthode fit. En précisant un batch_size=1000, cela signifie que :\n",
        " - Sur notre total de 56000 échantillons, 56 seront utilisés pour les calculs du gradient\n",
        " - Il y aura également 56 itérations à chaque période.\n",
        "  \n",
        "    \n",
        "    \n",
        "Si nous avions pris le dataset comme entrée, nous aurions eu :\n",
        "- Un total de 56000 échantillons également\n",
        "- Chaque période aurait également pris 56 itérations pour se compléter\n",
        "- Mais 1000 échantillons auraient été utilisés pour le calcul du gradient, au lieu de 56 avec la méthode utilisée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6Z35rNWj5SA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef1efc39-6d32-477f-a0c4-f3aad7e44066"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD()\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"mse\", optimizer=optimiseur, metrics=\"mse\")\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint],batch_size=batch_size)"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "186/186 [==============================] - 2s 3ms/step - loss: 1.1215 - mse: 1.1215\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.88770, saving model to poids.hdf5\n",
            "Epoch 2/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 1.1215 - mse: 1.1215\n",
            "\n",
            "Epoch 00002: loss improved from 0.88770 to 0.88769, saving model to poids.hdf5\n",
            "Epoch 3/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 1.1215 - mse: 1.1215\n",
            "\n",
            "Epoch 00003: loss improved from 0.88769 to 0.88768, saving model to poids.hdf5\n",
            "Epoch 4/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 1.1215 - mse: 1.1215\n",
            "\n",
            "Epoch 00004: loss improved from 0.88768 to 0.88767, saving model to poids.hdf5\n",
            "Epoch 5/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 1.1215 - mse: 1.1215\n",
            "\n",
            "Epoch 00005: loss improved from 0.88767 to 0.88765, saving model to poids.hdf5\n",
            "Epoch 6/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 1.1214 - mse: 1.1214\n",
            "\n",
            "Epoch 00006: loss improved from 0.88765 to 0.88762, saving model to poids.hdf5\n",
            "Epoch 7/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 1.1214 - mse: 1.1214\n",
            "\n",
            "Epoch 00007: loss improved from 0.88762 to 0.88759, saving model to poids.hdf5\n",
            "Epoch 8/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 1.1213 - mse: 1.1213\n",
            "\n",
            "Epoch 00008: loss improved from 0.88759 to 0.88754, saving model to poids.hdf5\n",
            "Epoch 9/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 1.1213 - mse: 1.1213\n",
            "\n",
            "Epoch 00009: loss improved from 0.88754 to 0.88749, saving model to poids.hdf5\n",
            "Epoch 10/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 1.1212 - mse: 1.1212\n",
            "\n",
            "Epoch 00010: loss improved from 0.88749 to 0.88741, saving model to poids.hdf5\n",
            "Epoch 11/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 1.1211 - mse: 1.1211\n",
            "\n",
            "Epoch 00011: loss improved from 0.88741 to 0.88732, saving model to poids.hdf5\n",
            "Epoch 12/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 1.1209 - mse: 1.1209\n",
            "\n",
            "Epoch 00012: loss improved from 0.88732 to 0.88720, saving model to poids.hdf5\n",
            "Epoch 13/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 1.1207 - mse: 1.1207\n",
            "\n",
            "Epoch 00013: loss improved from 0.88720 to 0.88705, saving model to poids.hdf5\n",
            "Epoch 14/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 1.1205 - mse: 1.1205\n",
            "\n",
            "Epoch 00014: loss improved from 0.88705 to 0.88687, saving model to poids.hdf5\n",
            "Epoch 15/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 1.1202 - mse: 1.1202\n",
            "\n",
            "Epoch 00015: loss improved from 0.88687 to 0.88663, saving model to poids.hdf5\n",
            "Epoch 16/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 1.1198 - mse: 1.1198\n",
            "\n",
            "Epoch 00016: loss improved from 0.88663 to 0.88632, saving model to poids.hdf5\n",
            "Epoch 17/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 1.1194 - mse: 1.1194\n",
            "\n",
            "Epoch 00017: loss improved from 0.88632 to 0.88594, saving model to poids.hdf5\n",
            "Epoch 18/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 1.1188 - mse: 1.1188\n",
            "\n",
            "Epoch 00018: loss improved from 0.88594 to 0.88546, saving model to poids.hdf5\n",
            "Epoch 19/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 1.1180 - mse: 1.1180\n",
            "\n",
            "Epoch 00019: loss improved from 0.88546 to 0.88484, saving model to poids.hdf5\n",
            "Epoch 20/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 1.1170 - mse: 1.1170\n",
            "\n",
            "Epoch 00020: loss improved from 0.88484 to 0.88407, saving model to poids.hdf5\n",
            "Epoch 21/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 1.1158 - mse: 1.1158\n",
            "\n",
            "Epoch 00021: loss improved from 0.88407 to 0.88310, saving model to poids.hdf5\n",
            "Epoch 22/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 1.1143 - mse: 1.1143\n",
            "\n",
            "Epoch 00022: loss improved from 0.88310 to 0.88188, saving model to poids.hdf5\n",
            "Epoch 23/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 1.1124 - mse: 1.1124\n",
            "\n",
            "Epoch 00023: loss improved from 0.88188 to 0.88035, saving model to poids.hdf5\n",
            "Epoch 24/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 1.1100 - mse: 1.1100\n",
            "\n",
            "Epoch 00024: loss improved from 0.88035 to 0.87842, saving model to poids.hdf5\n",
            "Epoch 25/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 1.1070 - mse: 1.1070\n",
            "\n",
            "Epoch 00025: loss improved from 0.87842 to 0.87600, saving model to poids.hdf5\n",
            "Epoch 26/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 1.1032 - mse: 1.1032\n",
            "\n",
            "Epoch 00026: loss improved from 0.87600 to 0.87296, saving model to poids.hdf5\n",
            "Epoch 27/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 1.0985 - mse: 1.0985\n",
            "\n",
            "Epoch 00027: loss improved from 0.87296 to 0.86917, saving model to poids.hdf5\n",
            "Epoch 28/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 1.0926 - mse: 1.0926\n",
            "\n",
            "Epoch 00028: loss improved from 0.86917 to 0.86443, saving model to poids.hdf5\n",
            "Epoch 29/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 1.0853 - mse: 1.0853\n",
            "\n",
            "Epoch 00029: loss improved from 0.86443 to 0.85852, saving model to poids.hdf5\n",
            "Epoch 30/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 1.0761 - mse: 1.0761\n",
            "\n",
            "Epoch 00030: loss improved from 0.85852 to 0.85118, saving model to poids.hdf5\n",
            "Epoch 31/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 1.0648 - mse: 1.0648\n",
            "\n",
            "Epoch 00031: loss improved from 0.85118 to 0.84209, saving model to poids.hdf5\n",
            "Epoch 32/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 1.0508 - mse: 1.0508\n",
            "\n",
            "Epoch 00032: loss improved from 0.84209 to 0.83088, saving model to poids.hdf5\n",
            "Epoch 33/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 1.0336 - mse: 1.0336\n",
            "\n",
            "Epoch 00033: loss improved from 0.83088 to 0.81714, saving model to poids.hdf5\n",
            "Epoch 34/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 1.0126 - mse: 1.0126\n",
            "\n",
            "Epoch 00034: loss improved from 0.81714 to 0.80039, saving model to poids.hdf5\n",
            "Epoch 35/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.9872 - mse: 0.9872\n",
            "\n",
            "Epoch 00035: loss improved from 0.80039 to 0.78015, saving model to poids.hdf5\n",
            "Epoch 36/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.9567 - mse: 0.9567\n",
            "\n",
            "Epoch 00036: loss improved from 0.78015 to 0.75595, saving model to poids.hdf5\n",
            "Epoch 37/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.9206 - mse: 0.9206\n",
            "\n",
            "Epoch 00037: loss improved from 0.75595 to 0.72739, saving model to poids.hdf5\n",
            "Epoch 38/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.8785 - mse: 0.8785\n",
            "\n",
            "Epoch 00038: loss improved from 0.72739 to 0.69424, saving model to poids.hdf5\n",
            "Epoch 39/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.8303 - mse: 0.8303\n",
            "\n",
            "Epoch 00039: loss improved from 0.69424 to 0.65654, saving model to poids.hdf5\n",
            "Epoch 40/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.7764 - mse: 0.7764\n",
            "\n",
            "Epoch 00040: loss improved from 0.65654 to 0.61480, saving model to poids.hdf5\n",
            "Epoch 41/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.7181 - mse: 0.7181\n",
            "\n",
            "Epoch 00041: loss improved from 0.61480 to 0.57012, saving model to poids.hdf5\n",
            "Epoch 42/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.6574 - mse: 0.6574\n",
            "\n",
            "Epoch 00042: loss improved from 0.57012 to 0.52435, saving model to poids.hdf5\n",
            "Epoch 43/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.5974 - mse: 0.5974\n",
            "\n",
            "Epoch 00043: loss improved from 0.52435 to 0.48007, saving model to poids.hdf5\n",
            "Epoch 44/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.5419 - mse: 0.5419\n",
            "\n",
            "Epoch 00044: loss improved from 0.48007 to 0.44025, saving model to poids.hdf5\n",
            "Epoch 45/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.4944 - mse: 0.4944\n",
            "\n",
            "Epoch 00045: loss improved from 0.44025 to 0.40752, saving model to poids.hdf5\n",
            "Epoch 46/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.4573 - mse: 0.4573\n",
            "\n",
            "Epoch 00046: loss improved from 0.40752 to 0.38313, saving model to poids.hdf5\n",
            "Epoch 47/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.4305 - mse: 0.4305\n",
            "\n",
            "Epoch 00047: loss improved from 0.38313 to 0.36622, saving model to poids.hdf5\n",
            "Epoch 48/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.4119 - mse: 0.4119\n",
            "\n",
            "Epoch 00048: loss improved from 0.36622 to 0.35432, saving model to poids.hdf5\n",
            "Epoch 49/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.3983 - mse: 0.3983\n",
            "\n",
            "Epoch 00049: loss improved from 0.35432 to 0.34476, saving model to poids.hdf5\n",
            "Epoch 50/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.3876 - mse: 0.3876\n",
            "\n",
            "Epoch 00050: loss improved from 0.34476 to 0.33578, saving model to poids.hdf5\n",
            "Epoch 51/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.3778 - mse: 0.3778\n",
            "\n",
            "Epoch 00051: loss improved from 0.33578 to 0.32642, saving model to poids.hdf5\n",
            "Epoch 52/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.3677 - mse: 0.3677\n",
            "\n",
            "Epoch 00052: loss improved from 0.32642 to 0.31593, saving model to poids.hdf5\n",
            "Epoch 53/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.3556 - mse: 0.3556\n",
            "\n",
            "Epoch 00053: loss improved from 0.31593 to 0.30344, saving model to poids.hdf5\n",
            "Epoch 54/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.3403 - mse: 0.3403\n",
            "\n",
            "Epoch 00054: loss improved from 0.30344 to 0.28803, saving model to poids.hdf5\n",
            "Epoch 55/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.3205 - mse: 0.3205\n",
            "\n",
            "Epoch 00055: loss improved from 0.28803 to 0.26883, saving model to poids.hdf5\n",
            "Epoch 56/100\n",
            "186/186 [==============================] - 1s 4ms/step - loss: 0.2947 - mse: 0.2947\n",
            "\n",
            "Epoch 00056: loss improved from 0.26883 to 0.24505, saving model to poids.hdf5\n",
            "Epoch 57/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.2617 - mse: 0.2617\n",
            "\n",
            "Epoch 00057: loss improved from 0.24505 to 0.21615, saving model to poids.hdf5\n",
            "Epoch 58/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.2206 - mse: 0.2206\n",
            "\n",
            "Epoch 00058: loss improved from 0.21615 to 0.18204, saving model to poids.hdf5\n",
            "Epoch 59/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.1729 - mse: 0.1729\n",
            "\n",
            "Epoch 00059: loss improved from 0.18204 to 0.14358, saving model to poids.hdf5\n",
            "Epoch 60/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.1229 - mse: 0.1229\n",
            "\n",
            "Epoch 00060: loss improved from 0.14358 to 0.10315, saving model to poids.hdf5\n",
            "Epoch 61/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.0773 - mse: 0.0773\n",
            "\n",
            "Epoch 00061: loss improved from 0.10315 to 0.06554, saving model to poids.hdf5\n",
            "Epoch 62/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.0457 - mse: 0.0457\n",
            "\n",
            "Epoch 00062: loss improved from 0.06554 to 0.03907, saving model to poids.hdf5\n",
            "Epoch 63/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.0955 - mse: 0.0955\n",
            "\n",
            "Epoch 00063: loss did not improve from 0.03907\n",
            "Epoch 64/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.0283 - mse: 0.0283\n",
            "\n",
            "Epoch 00064: loss improved from 0.03907 to 0.03148, saving model to poids.hdf5\n",
            "Epoch 65/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.1139 - mse: 0.1139\n",
            "\n",
            "Epoch 00065: loss did not improve from 0.03148\n",
            "Epoch 66/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.0596 - mse: 0.0596\n",
            "\n",
            "Epoch 00066: loss did not improve from 0.03148\n",
            "Epoch 67/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.0565 - mse: 0.0565\n",
            "\n",
            "Epoch 00067: loss did not improve from 0.03148\n",
            "Epoch 68/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.1051 - mse: 0.1051\n",
            "\n",
            "Epoch 00068: loss did not improve from 0.03148\n",
            "Epoch 69/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.1576 - mse: 0.1576\n",
            "\n",
            "Epoch 00069: loss did not improve from 0.03148\n",
            "Epoch 70/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.2095 - mse: 0.2095\n",
            "\n",
            "Epoch 00070: loss did not improve from 0.03148\n",
            "Epoch 71/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.1817 - mse: 0.1817\n",
            "\n",
            "Epoch 00071: loss did not improve from 0.03148\n",
            "Epoch 72/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.1964 - mse: 0.1964\n",
            "\n",
            "Epoch 00072: loss did not improve from 0.03148\n",
            "Epoch 73/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.0991 - mse: 0.0991\n",
            "\n",
            "Epoch 00073: loss did not improve from 0.03148\n",
            "Epoch 74/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: 0.1076 - mse: 0.1076\n",
            "\n",
            "Epoch 00074: loss did not improve from 0.03148\n",
            "Epoch 75/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: nan - mse: nan\n",
            "\n",
            "Epoch 00075: loss did not improve from 0.03148\n",
            "Epoch 76/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: nan - mse: nan\n",
            "\n",
            "Epoch 00076: loss did not improve from 0.03148\n",
            "Epoch 77/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: nan - mse: nan\n",
            "\n",
            "Epoch 00077: loss did not improve from 0.03148\n",
            "Epoch 78/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: nan - mse: nan\n",
            "\n",
            "Epoch 00078: loss did not improve from 0.03148\n",
            "Epoch 79/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: nan - mse: nan\n",
            "\n",
            "Epoch 00079: loss did not improve from 0.03148\n",
            "Epoch 80/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: nan - mse: nan\n",
            "\n",
            "Epoch 00080: loss did not improve from 0.03148\n",
            "Epoch 81/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: nan - mse: nan\n",
            "\n",
            "Epoch 00081: loss did not improve from 0.03148\n",
            "Epoch 82/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: nan - mse: nan\n",
            "\n",
            "Epoch 00082: loss did not improve from 0.03148\n",
            "Epoch 83/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: nan - mse: nan\n",
            "\n",
            "Epoch 00083: loss did not improve from 0.03148\n",
            "Epoch 84/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: nan - mse: nan\n",
            "\n",
            "Epoch 00084: loss did not improve from 0.03148\n",
            "Epoch 85/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: nan - mse: nan\n",
            "\n",
            "Epoch 00085: loss did not improve from 0.03148\n",
            "Epoch 86/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: nan - mse: nan\n",
            "\n",
            "Epoch 00086: loss did not improve from 0.03148\n",
            "Epoch 87/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: nan - mse: nan\n",
            "\n",
            "Epoch 00087: loss did not improve from 0.03148\n",
            "Epoch 88/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: nan - mse: nan\n",
            "\n",
            "Epoch 00088: loss did not improve from 0.03148\n",
            "Epoch 89/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: nan - mse: nan\n",
            "\n",
            "Epoch 00089: loss did not improve from 0.03148\n",
            "Epoch 90/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: nan - mse: nan\n",
            "\n",
            "Epoch 00090: loss did not improve from 0.03148\n",
            "Epoch 91/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: nan - mse: nan\n",
            "\n",
            "Epoch 00091: loss did not improve from 0.03148\n",
            "Epoch 92/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: nan - mse: nan\n",
            "\n",
            "Epoch 00092: loss did not improve from 0.03148\n",
            "Epoch 93/100\n",
            "186/186 [==============================] - 1s 4ms/step - loss: nan - mse: nan\n",
            "\n",
            "Epoch 00093: loss did not improve from 0.03148\n",
            "Epoch 94/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: nan - mse: nan\n",
            "\n",
            "Epoch 00094: loss did not improve from 0.03148\n",
            "Epoch 95/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: nan - mse: nan\n",
            "\n",
            "Epoch 00095: loss did not improve from 0.03148\n",
            "Epoch 96/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: nan - mse: nan\n",
            "\n",
            "Epoch 00096: loss did not improve from 0.03148\n",
            "Epoch 97/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: nan - mse: nan\n",
            "\n",
            "Epoch 00097: loss did not improve from 0.03148\n",
            "Epoch 98/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: nan - mse: nan\n",
            "\n",
            "Epoch 00098: loss did not improve from 0.03148\n",
            "Epoch 99/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: nan - mse: nan\n",
            "\n",
            "Epoch 00099: loss did not improve from 0.03148\n",
            "Epoch 100/100\n",
            "186/186 [==============================] - 1s 3ms/step - loss: nan - mse: nan\n",
            "\n",
            "Epoch 00100: loss did not improve from 0.03148\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2aP9J3TkNGG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "ef9a25ba-a1df-475f-a9ae-3ac6f7f8af0a"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 1])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, \"Evolution de l'erreur en fonction du taux d'apprentissage\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 188
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZxCgpuYkQ2Q"
      },
      "source": [
        "# Chargement des poids sauvegardés\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmdbo23qkTKE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a522010-8bcc-4673-cb79-e8275f63b96b"
      },
      "source": [
        "max_periodes = 500\n",
        "\n",
        "# Classe permettant d'arrêter l'entrainement si la variation\n",
        "# devient plus petite qu'une valeur à choisir sur un nombre\n",
        "# de périodes à choisir\n",
        "class StopTrain(keras.callbacks.Callback):\n",
        "    def __init__(self, delta=0.01,periodes=100, term=\"loss\", logs={}):\n",
        "      self.n_periodes = 0\n",
        "      self.periodes = periodes\n",
        "      self.loss_1 = 100\n",
        "      self.delta = delta\n",
        "      self.term = term\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "      diff_loss = abs(self.loss_1 - logs[self.term])\n",
        "      self.loss_1 = logs[self.term]\n",
        "      if (diff_loss < self.delta):\n",
        "        self.n_periodes = self.n_periodes + 1\n",
        "      else:\n",
        "        self.n_periodes = 0\n",
        "      if (self.n_periodes == self.periodes):\n",
        "        print(\"Arrêt de l'entrainement...\")\n",
        "        self.model.stop_training = True\n",
        "\n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.01,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0.01)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(learning_rate=lr_schedule,momentum=0.9)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"mse\", optimizer=optimiseur, metrics=[\"mse\"])\n",
        "\n",
        "# Entraine le modèle, avec une réduction des calculs du gradient\n",
        "#historique = model.fit(x=x_train,y=y_train,validation_data=(x_val,y_val), epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=1e-7,periodes = 10, term=\"My_MSE\")],batch_size=batch_size)\n",
        "\n",
        "# Entraine le modèle sans réduction de calculs\n",
        "historique = model.fit(dataset,validation_data=dataset_val, epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=1e-5,periodes = 10, term=\"loss\")])\n",
        "#historique = model.fit(dataset, epochs=max_periodes)\n"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "186/186 [==============================] - 2s 7ms/step - loss: 0.0475 - mse: 0.0475 - val_loss: 0.1472 - val_mse: 0.1472\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.08009, saving model to poids_train.hdf5\n",
            "Epoch 2/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.3585 - mse: 0.3585 - val_loss: 0.3513 - val_mse: 0.3513\n",
            "\n",
            "Epoch 00002: loss did not improve from 0.08009\n",
            "Epoch 3/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.1981 - mse: 0.1981 - val_loss: 0.3962 - val_mse: 0.3962\n",
            "\n",
            "Epoch 00003: loss did not improve from 0.08009\n",
            "Epoch 4/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0418 - mse: 0.0418 - val_loss: 0.4899 - val_mse: 0.4899\n",
            "\n",
            "Epoch 00004: loss improved from 0.08009 to 0.03982, saving model to poids_train.hdf5\n",
            "Epoch 5/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0440 - mse: 0.0440 - val_loss: 2.7232 - val_mse: 2.7232\n",
            "\n",
            "Epoch 00005: loss did not improve from 0.03982\n",
            "Epoch 6/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.3821 - mse: 0.3821 - val_loss: 29.0290 - val_mse: 29.0290\n",
            "\n",
            "Epoch 00006: loss did not improve from 0.03982\n",
            "Epoch 7/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.1123 - mse: 0.1123 - val_loss: 36.4339 - val_mse: 36.4339\n",
            "\n",
            "Epoch 00007: loss did not improve from 0.03982\n",
            "Epoch 8/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 3.0887 - mse: 3.0887 - val_loss: 1.5760 - val_mse: 1.5760\n",
            "\n",
            "Epoch 00008: loss did not improve from 0.03982\n",
            "Epoch 9/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.5833 - mse: 0.5833 - val_loss: 0.8938 - val_mse: 0.8938\n",
            "\n",
            "Epoch 00009: loss did not improve from 0.03982\n",
            "Epoch 10/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.3022 - mse: 0.3022 - val_loss: 4.3367 - val_mse: 4.3367\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.03982\n",
            "Epoch 11/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0981 - mse: 0.0981 - val_loss: 6.7475 - val_mse: 6.7475\n",
            "\n",
            "Epoch 00011: loss did not improve from 0.03982\n",
            "Epoch 12/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.2543 - mse: 0.2543 - val_loss: 19.2604 - val_mse: 19.2604\n",
            "\n",
            "Epoch 00012: loss did not improve from 0.03982\n",
            "Epoch 13/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.7865 - mse: 0.7865 - val_loss: 1.5636 - val_mse: 1.5636\n",
            "\n",
            "Epoch 00013: loss did not improve from 0.03982\n",
            "Epoch 14/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.4023 - mse: 0.4023 - val_loss: 5.5710 - val_mse: 5.5710\n",
            "\n",
            "Epoch 00014: loss did not improve from 0.03982\n",
            "Epoch 15/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 1.5540 - mse: 1.5540 - val_loss: 11.5346 - val_mse: 11.5346\n",
            "\n",
            "Epoch 00015: loss did not improve from 0.03982\n",
            "Epoch 16/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 1.2191 - mse: 1.2191 - val_loss: 7.4711 - val_mse: 7.4711\n",
            "\n",
            "Epoch 00016: loss did not improve from 0.03982\n",
            "Epoch 17/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 1.0411 - mse: 1.0411 - val_loss: 11.5587 - val_mse: 11.5587\n",
            "\n",
            "Epoch 00017: loss did not improve from 0.03982\n",
            "Epoch 18/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.4725 - mse: 0.4725 - val_loss: 4.7561 - val_mse: 4.7561\n",
            "\n",
            "Epoch 00018: loss did not improve from 0.03982\n",
            "Epoch 19/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 1.1547 - mse: 1.1547 - val_loss: 3.2992 - val_mse: 3.2992\n",
            "\n",
            "Epoch 00019: loss did not improve from 0.03982\n",
            "Epoch 20/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.3503 - mse: 0.3503 - val_loss: 9.4171 - val_mse: 9.4171\n",
            "\n",
            "Epoch 00020: loss did not improve from 0.03982\n",
            "Epoch 21/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 1.1209 - mse: 1.1209 - val_loss: 11.1635 - val_mse: 11.1635\n",
            "\n",
            "Epoch 00021: loss did not improve from 0.03982\n",
            "Epoch 22/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 1.3845 - mse: 1.3845 - val_loss: 8.5050 - val_mse: 8.5050\n",
            "\n",
            "Epoch 00022: loss did not improve from 0.03982\n",
            "Epoch 23/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 1.0653 - mse: 1.0653 - val_loss: 35.4302 - val_mse: 35.4302\n",
            "\n",
            "Epoch 00023: loss did not improve from 0.03982\n",
            "Epoch 24/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 2.0561 - mse: 2.0561 - val_loss: 62.6609 - val_mse: 62.6609\n",
            "\n",
            "Epoch 00024: loss did not improve from 0.03982\n",
            "Epoch 25/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 3.9710 - mse: 3.9710 - val_loss: 142.7658 - val_mse: 142.7658\n",
            "\n",
            "Epoch 00025: loss did not improve from 0.03982\n",
            "Epoch 26/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 3.4226 - mse: 3.4226 - val_loss: 106.1332 - val_mse: 106.1332\n",
            "\n",
            "Epoch 00026: loss did not improve from 0.03982\n",
            "Epoch 27/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 4.7868 - mse: 4.7868 - val_loss: 85.7422 - val_mse: 85.7422\n",
            "\n",
            "Epoch 00027: loss did not improve from 0.03982\n",
            "Epoch 28/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 2.2948 - mse: 2.2948 - val_loss: 45.5345 - val_mse: 45.5345\n",
            "\n",
            "Epoch 00028: loss did not improve from 0.03982\n",
            "Epoch 29/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 4.6017 - mse: 4.6017 - val_loss: 8.9424 - val_mse: 8.9424\n",
            "\n",
            "Epoch 00029: loss did not improve from 0.03982\n",
            "Epoch 30/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 8.4993 - mse: 8.4993 - val_loss: 1.7784 - val_mse: 1.7784\n",
            "\n",
            "Epoch 00030: loss did not improve from 0.03982\n",
            "Epoch 31/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.7418 - mse: 0.7418 - val_loss: 1.4759 - val_mse: 1.4759\n",
            "\n",
            "Epoch 00031: loss did not improve from 0.03982\n",
            "Epoch 32/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 1.7525 - mse: 1.7525 - val_loss: 7.5725 - val_mse: 7.5725\n",
            "\n",
            "Epoch 00032: loss did not improve from 0.03982\n",
            "Epoch 33/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 1.2351 - mse: 1.2351 - val_loss: 21.3389 - val_mse: 21.3389\n",
            "\n",
            "Epoch 00033: loss did not improve from 0.03982\n",
            "Epoch 34/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 4.3964 - mse: 4.3964 - val_loss: 14.4578 - val_mse: 14.4578\n",
            "\n",
            "Epoch 00034: loss did not improve from 0.03982\n",
            "Epoch 35/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 3.6944 - mse: 3.6944 - val_loss: 13.7203 - val_mse: 13.7203\n",
            "\n",
            "Epoch 00035: loss did not improve from 0.03982\n",
            "Epoch 36/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 3.8878 - mse: 3.8878 - val_loss: 27.0837 - val_mse: 27.0837\n",
            "\n",
            "Epoch 00036: loss did not improve from 0.03982\n",
            "Epoch 37/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 5.6233 - mse: 5.6233 - val_loss: 66.1847 - val_mse: 66.1847\n",
            "\n",
            "Epoch 00037: loss did not improve from 0.03982\n",
            "Epoch 38/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 11.4121 - mse: 11.4121 - val_loss: 22.4494 - val_mse: 22.4494\n",
            "\n",
            "Epoch 00038: loss did not improve from 0.03982\n",
            "Epoch 39/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 5.5760 - mse: 5.5760 - val_loss: 25.9924 - val_mse: 25.9924\n",
            "\n",
            "Epoch 00039: loss did not improve from 0.03982\n",
            "Epoch 40/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 7.3044 - mse: 7.3044 - val_loss: 12.2975 - val_mse: 12.2975\n",
            "\n",
            "Epoch 00040: loss did not improve from 0.03982\n",
            "Epoch 41/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 6.1223 - mse: 6.1223 - val_loss: 12.5950 - val_mse: 12.5950\n",
            "\n",
            "Epoch 00041: loss did not improve from 0.03982\n",
            "Epoch 42/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 6.2995 - mse: 6.2995 - val_loss: 13.4588 - val_mse: 13.4588\n",
            "\n",
            "Epoch 00042: loss did not improve from 0.03982\n",
            "Epoch 43/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 8.8093 - mse: 8.8093 - val_loss: 6.9881 - val_mse: 6.9881\n",
            "\n",
            "Epoch 00043: loss did not improve from 0.03982\n",
            "Epoch 44/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 2.1936 - mse: 2.1936 - val_loss: 8.9620 - val_mse: 8.9620\n",
            "\n",
            "Epoch 00044: loss did not improve from 0.03982\n",
            "Epoch 45/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 3.4363 - mse: 3.4363 - val_loss: 1.3132 - val_mse: 1.3132\n",
            "\n",
            "Epoch 00045: loss did not improve from 0.03982\n",
            "Epoch 46/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 5.0552 - mse: 5.0552 - val_loss: 0.8349 - val_mse: 0.8349\n",
            "\n",
            "Epoch 00046: loss did not improve from 0.03982\n",
            "Epoch 47/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 2.7431 - mse: 2.7431 - val_loss: 24.0091 - val_mse: 24.0091\n",
            "\n",
            "Epoch 00047: loss did not improve from 0.03982\n",
            "Epoch 48/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 6.4775 - mse: 6.4775 - val_loss: 3.1207 - val_mse: 3.1207\n",
            "\n",
            "Epoch 00048: loss did not improve from 0.03982\n",
            "Epoch 49/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 1.5210 - mse: 1.5210 - val_loss: 7.4552 - val_mse: 7.4552\n",
            "\n",
            "Epoch 00049: loss did not improve from 0.03982\n",
            "Epoch 50/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 1.6111 - mse: 1.6111 - val_loss: 9.6095 - val_mse: 9.6095\n",
            "\n",
            "Epoch 00050: loss did not improve from 0.03982\n",
            "Epoch 51/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 1.8218 - mse: 1.8218 - val_loss: 1.2772 - val_mse: 1.2772\n",
            "\n",
            "Epoch 00051: loss did not improve from 0.03982\n",
            "Epoch 52/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.3464 - mse: 0.3464 - val_loss: 0.6877 - val_mse: 0.6877\n",
            "\n",
            "Epoch 00052: loss did not improve from 0.03982\n",
            "Epoch 53/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.3334 - mse: 0.3334 - val_loss: 1.1339 - val_mse: 1.1339\n",
            "\n",
            "Epoch 00053: loss did not improve from 0.03982\n",
            "Epoch 54/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 1.4412 - mse: 1.4412 - val_loss: 2.9995 - val_mse: 2.9995\n",
            "\n",
            "Epoch 00054: loss did not improve from 0.03982\n",
            "Epoch 55/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 1.4575 - mse: 1.4575 - val_loss: 1.0939 - val_mse: 1.0939\n",
            "\n",
            "Epoch 00055: loss did not improve from 0.03982\n",
            "Epoch 56/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.6892 - mse: 0.6892 - val_loss: 1.1287 - val_mse: 1.1287\n",
            "\n",
            "Epoch 00056: loss did not improve from 0.03982\n",
            "Epoch 57/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 2.8377 - mse: 2.8377 - val_loss: 1.4808 - val_mse: 1.4808\n",
            "\n",
            "Epoch 00057: loss did not improve from 0.03982\n",
            "Epoch 58/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 2.7898 - mse: 2.7898 - val_loss: 1.8941 - val_mse: 1.8941\n",
            "\n",
            "Epoch 00058: loss did not improve from 0.03982\n",
            "Epoch 59/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 2.3500 - mse: 2.3500 - val_loss: 1.3996 - val_mse: 1.3996\n",
            "\n",
            "Epoch 00059: loss did not improve from 0.03982\n",
            "Epoch 60/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 1.5600 - mse: 1.5600 - val_loss: 0.2334 - val_mse: 0.2334\n",
            "\n",
            "Epoch 00060: loss did not improve from 0.03982\n",
            "Epoch 61/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.1905 - mse: 0.1905 - val_loss: 0.2884 - val_mse: 0.2884\n",
            "\n",
            "Epoch 00061: loss did not improve from 0.03982\n",
            "Epoch 62/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.6838 - mse: 0.6838 - val_loss: 1.7864 - val_mse: 1.7864\n",
            "\n",
            "Epoch 00062: loss did not improve from 0.03982\n",
            "Epoch 63/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 1.9962 - mse: 1.9962 - val_loss: 1.2547 - val_mse: 1.2547\n",
            "\n",
            "Epoch 00063: loss did not improve from 0.03982\n",
            "Epoch 64/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.8498 - mse: 0.8498 - val_loss: 0.5177 - val_mse: 0.5177\n",
            "\n",
            "Epoch 00064: loss did not improve from 0.03982\n",
            "Epoch 65/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.4503 - mse: 0.4503 - val_loss: 0.7454 - val_mse: 0.7454\n",
            "\n",
            "Epoch 00065: loss did not improve from 0.03982\n",
            "Epoch 66/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.2850 - mse: 0.2850 - val_loss: 0.2907 - val_mse: 0.2907\n",
            "\n",
            "Epoch 00066: loss did not improve from 0.03982\n",
            "Epoch 67/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.4930 - mse: 0.4930 - val_loss: 0.3619 - val_mse: 0.3619\n",
            "\n",
            "Epoch 00067: loss did not improve from 0.03982\n",
            "Epoch 68/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.3364 - mse: 0.3364 - val_loss: 0.4811 - val_mse: 0.4811\n",
            "\n",
            "Epoch 00068: loss did not improve from 0.03982\n",
            "Epoch 69/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.4786 - mse: 0.4786 - val_loss: 0.4202 - val_mse: 0.4202\n",
            "\n",
            "Epoch 00069: loss did not improve from 0.03982\n",
            "Epoch 70/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.2823 - mse: 0.2823 - val_loss: 0.8468 - val_mse: 0.8468\n",
            "\n",
            "Epoch 00070: loss did not improve from 0.03982\n",
            "Epoch 71/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.4263 - mse: 0.4263 - val_loss: 0.3925 - val_mse: 0.3925\n",
            "\n",
            "Epoch 00071: loss did not improve from 0.03982\n",
            "Epoch 72/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.1971 - mse: 0.1971 - val_loss: 0.4149 - val_mse: 0.4149\n",
            "\n",
            "Epoch 00072: loss did not improve from 0.03982\n",
            "Epoch 73/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.1165 - mse: 0.1165 - val_loss: 0.5585 - val_mse: 0.5585\n",
            "\n",
            "Epoch 00073: loss did not improve from 0.03982\n",
            "Epoch 74/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.2387 - mse: 0.2387 - val_loss: 0.5770 - val_mse: 0.5770\n",
            "\n",
            "Epoch 00074: loss did not improve from 0.03982\n",
            "Epoch 75/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.1603 - mse: 0.1603 - val_loss: 0.4873 - val_mse: 0.4873\n",
            "\n",
            "Epoch 00075: loss did not improve from 0.03982\n",
            "Epoch 76/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.1374 - mse: 0.1374 - val_loss: 0.6118 - val_mse: 0.6118\n",
            "\n",
            "Epoch 00076: loss did not improve from 0.03982\n",
            "Epoch 77/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.1126 - mse: 0.1126 - val_loss: 0.7429 - val_mse: 0.7429\n",
            "\n",
            "Epoch 00077: loss did not improve from 0.03982\n",
            "Epoch 78/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.1583 - mse: 0.1583 - val_loss: 0.7398 - val_mse: 0.7398\n",
            "\n",
            "Epoch 00078: loss did not improve from 0.03982\n",
            "Epoch 79/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.1372 - mse: 0.1372 - val_loss: 0.7683 - val_mse: 0.7683\n",
            "\n",
            "Epoch 00079: loss did not improve from 0.03982\n",
            "Epoch 80/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.1300 - mse: 0.1300 - val_loss: 0.8898 - val_mse: 0.8898\n",
            "\n",
            "Epoch 00080: loss did not improve from 0.03982\n",
            "Epoch 81/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.1417 - mse: 0.1417 - val_loss: 0.9747 - val_mse: 0.9747\n",
            "\n",
            "Epoch 00081: loss did not improve from 0.03982\n",
            "Epoch 82/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.1589 - mse: 0.1589 - val_loss: 0.9824 - val_mse: 0.9824\n",
            "\n",
            "Epoch 00082: loss did not improve from 0.03982\n",
            "Epoch 83/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.1541 - mse: 0.1541 - val_loss: 1.0281 - val_mse: 1.0281\n",
            "\n",
            "Epoch 00083: loss did not improve from 0.03982\n",
            "Epoch 84/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.1551 - mse: 0.1551 - val_loss: 1.0982 - val_mse: 1.0982\n",
            "\n",
            "Epoch 00084: loss did not improve from 0.03982\n",
            "Epoch 85/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.1683 - mse: 0.1683 - val_loss: 1.1248 - val_mse: 1.1248\n",
            "\n",
            "Epoch 00085: loss did not improve from 0.03982\n",
            "Epoch 86/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.1740 - mse: 0.1740 - val_loss: 1.1247 - val_mse: 1.1247\n",
            "\n",
            "Epoch 00086: loss did not improve from 0.03982\n",
            "Epoch 87/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.1734 - mse: 0.1734 - val_loss: 1.1403 - val_mse: 1.1403\n",
            "\n",
            "Epoch 00087: loss did not improve from 0.03982\n",
            "Epoch 88/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.1765 - mse: 0.1765 - val_loss: 1.1499 - val_mse: 1.1499\n",
            "\n",
            "Epoch 00088: loss did not improve from 0.03982\n",
            "Epoch 89/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.1817 - mse: 0.1817 - val_loss: 1.1339 - val_mse: 1.1339\n",
            "\n",
            "Epoch 00089: loss did not improve from 0.03982\n",
            "Epoch 90/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.1818 - mse: 0.1818 - val_loss: 1.1112 - val_mse: 1.1112\n",
            "\n",
            "Epoch 00090: loss did not improve from 0.03982\n",
            "Epoch 91/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.1803 - mse: 0.1803 - val_loss: 1.0903 - val_mse: 1.0903\n",
            "\n",
            "Epoch 00091: loss did not improve from 0.03982\n",
            "Epoch 92/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.1802 - mse: 0.1802 - val_loss: 1.0606 - val_mse: 1.0606\n",
            "\n",
            "Epoch 00092: loss did not improve from 0.03982\n",
            "Epoch 93/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.1789 - mse: 0.1789 - val_loss: 1.0221 - val_mse: 1.0221\n",
            "\n",
            "Epoch 00093: loss did not improve from 0.03982\n",
            "Epoch 94/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.1755 - mse: 0.1755 - val_loss: 0.9828 - val_mse: 0.9828\n",
            "\n",
            "Epoch 00094: loss did not improve from 0.03982\n",
            "Epoch 95/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.1719 - mse: 0.1719 - val_loss: 0.9427 - val_mse: 0.9427\n",
            "\n",
            "Epoch 00095: loss did not improve from 0.03982\n",
            "Epoch 96/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.1684 - mse: 0.1684 - val_loss: 0.8997 - val_mse: 0.8997\n",
            "\n",
            "Epoch 00096: loss did not improve from 0.03982\n",
            "Epoch 97/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.1640 - mse: 0.1640 - val_loss: 0.8559 - val_mse: 0.8559\n",
            "\n",
            "Epoch 00097: loss did not improve from 0.03982\n",
            "Epoch 98/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.1591 - mse: 0.1591 - val_loss: 0.8134 - val_mse: 0.8134\n",
            "\n",
            "Epoch 00098: loss did not improve from 0.03982\n",
            "Epoch 99/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.1542 - mse: 0.1542 - val_loss: 0.7716 - val_mse: 0.7716\n",
            "\n",
            "Epoch 00099: loss did not improve from 0.03982\n",
            "Epoch 100/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.1492 - mse: 0.1492 - val_loss: 0.7308 - val_mse: 0.7308\n",
            "\n",
            "Epoch 00100: loss did not improve from 0.03982\n",
            "Epoch 101/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.1441 - mse: 0.1441 - val_loss: 0.6918 - val_mse: 0.6918\n",
            "\n",
            "Epoch 00101: loss did not improve from 0.03982\n",
            "Epoch 102/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.1390 - mse: 0.1390 - val_loss: 0.6549 - val_mse: 0.6549\n",
            "\n",
            "Epoch 00102: loss did not improve from 0.03982\n",
            "Epoch 103/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.1341 - mse: 0.1341 - val_loss: 0.6199 - val_mse: 0.6199\n",
            "\n",
            "Epoch 00103: loss did not improve from 0.03982\n",
            "Epoch 104/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.1293 - mse: 0.1293 - val_loss: 0.5870 - val_mse: 0.5870\n",
            "\n",
            "Epoch 00104: loss did not improve from 0.03982\n",
            "Epoch 105/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.1246 - mse: 0.1246 - val_loss: 0.5563 - val_mse: 0.5563\n",
            "\n",
            "Epoch 00105: loss did not improve from 0.03982\n",
            "Epoch 106/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.1201 - mse: 0.1201 - val_loss: 0.5277 - val_mse: 0.5277\n",
            "\n",
            "Epoch 00106: loss did not improve from 0.03982\n",
            "Epoch 107/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.1158 - mse: 0.1158 - val_loss: 0.5011 - val_mse: 0.5011\n",
            "\n",
            "Epoch 00107: loss did not improve from 0.03982\n",
            "Epoch 108/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.1117 - mse: 0.1117 - val_loss: 0.4765 - val_mse: 0.4765\n",
            "\n",
            "Epoch 00108: loss did not improve from 0.03982\n",
            "Epoch 109/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.1079 - mse: 0.1079 - val_loss: 0.4537 - val_mse: 0.4537\n",
            "\n",
            "Epoch 00109: loss did not improve from 0.03982\n",
            "Epoch 110/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.1042 - mse: 0.1042 - val_loss: 0.4327 - val_mse: 0.4327\n",
            "\n",
            "Epoch 00110: loss did not improve from 0.03982\n",
            "Epoch 111/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.1007 - mse: 0.1007 - val_loss: 0.4133 - val_mse: 0.4133\n",
            "\n",
            "Epoch 00111: loss did not improve from 0.03982\n",
            "Epoch 112/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0975 - mse: 0.0975 - val_loss: 0.3954 - val_mse: 0.3954\n",
            "\n",
            "Epoch 00112: loss did not improve from 0.03982\n",
            "Epoch 113/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0944 - mse: 0.0944 - val_loss: 0.3790 - val_mse: 0.3790\n",
            "\n",
            "Epoch 00113: loss did not improve from 0.03982\n",
            "Epoch 114/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0915 - mse: 0.0915 - val_loss: 0.3638 - val_mse: 0.3638\n",
            "\n",
            "Epoch 00114: loss did not improve from 0.03982\n",
            "Epoch 115/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.3498 - val_mse: 0.3498\n",
            "\n",
            "Epoch 00115: loss did not improve from 0.03982\n",
            "Epoch 116/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0862 - mse: 0.0862 - val_loss: 0.3369 - val_mse: 0.3369\n",
            "\n",
            "Epoch 00116: loss did not improve from 0.03982\n",
            "Epoch 117/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0838 - mse: 0.0838 - val_loss: 0.3250 - val_mse: 0.3250\n",
            "\n",
            "Epoch 00117: loss did not improve from 0.03982\n",
            "Epoch 118/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0816 - mse: 0.0816 - val_loss: 0.3140 - val_mse: 0.3140\n",
            "\n",
            "Epoch 00118: loss did not improve from 0.03982\n",
            "Epoch 119/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0794 - mse: 0.0794 - val_loss: 0.3039 - val_mse: 0.3039\n",
            "\n",
            "Epoch 00119: loss did not improve from 0.03982\n",
            "Epoch 120/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0774 - mse: 0.0774 - val_loss: 0.2945 - val_mse: 0.2945\n",
            "\n",
            "Epoch 00120: loss did not improve from 0.03982\n",
            "Epoch 121/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0755 - mse: 0.0755 - val_loss: 0.2858 - val_mse: 0.2858\n",
            "\n",
            "Epoch 00121: loss did not improve from 0.03982\n",
            "Epoch 122/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0737 - mse: 0.0737 - val_loss: 0.2777 - val_mse: 0.2777\n",
            "\n",
            "Epoch 00122: loss did not improve from 0.03982\n",
            "Epoch 123/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0721 - mse: 0.0721 - val_loss: 0.2703 - val_mse: 0.2703\n",
            "\n",
            "Epoch 00123: loss did not improve from 0.03982\n",
            "Epoch 124/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0705 - mse: 0.0705 - val_loss: 0.2633 - val_mse: 0.2633\n",
            "\n",
            "Epoch 00124: loss did not improve from 0.03982\n",
            "Epoch 125/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0690 - mse: 0.0690 - val_loss: 0.2569 - val_mse: 0.2569\n",
            "\n",
            "Epoch 00125: loss did not improve from 0.03982\n",
            "Epoch 126/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0675 - mse: 0.0675 - val_loss: 0.2509 - val_mse: 0.2509\n",
            "\n",
            "Epoch 00126: loss did not improve from 0.03982\n",
            "Epoch 127/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0662 - mse: 0.0662 - val_loss: 0.2453 - val_mse: 0.2453\n",
            "\n",
            "Epoch 00127: loss did not improve from 0.03982\n",
            "Epoch 128/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0649 - mse: 0.0649 - val_loss: 0.2400 - val_mse: 0.2400\n",
            "\n",
            "Epoch 00128: loss did not improve from 0.03982\n",
            "Epoch 129/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0637 - mse: 0.0637 - val_loss: 0.2352 - val_mse: 0.2352\n",
            "\n",
            "Epoch 00129: loss did not improve from 0.03982\n",
            "Epoch 130/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0625 - mse: 0.0625 - val_loss: 0.2306 - val_mse: 0.2306\n",
            "\n",
            "Epoch 00130: loss did not improve from 0.03982\n",
            "Epoch 131/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0614 - mse: 0.0614 - val_loss: 0.2263 - val_mse: 0.2263\n",
            "\n",
            "Epoch 00131: loss did not improve from 0.03982\n",
            "Epoch 132/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0604 - mse: 0.0604 - val_loss: 0.2223 - val_mse: 0.2223\n",
            "\n",
            "Epoch 00132: loss did not improve from 0.03982\n",
            "Epoch 133/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0594 - mse: 0.0594 - val_loss: 0.2186 - val_mse: 0.2186\n",
            "\n",
            "Epoch 00133: loss did not improve from 0.03982\n",
            "Epoch 134/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0584 - mse: 0.0584 - val_loss: 0.2150 - val_mse: 0.2150\n",
            "\n",
            "Epoch 00134: loss did not improve from 0.03982\n",
            "Epoch 135/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0575 - mse: 0.0575 - val_loss: 0.2117 - val_mse: 0.2117\n",
            "\n",
            "Epoch 00135: loss did not improve from 0.03982\n",
            "Epoch 136/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0566 - mse: 0.0566 - val_loss: 0.2086 - val_mse: 0.2086\n",
            "\n",
            "Epoch 00136: loss did not improve from 0.03982\n",
            "Epoch 137/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0558 - mse: 0.0558 - val_loss: 0.2056 - val_mse: 0.2056\n",
            "\n",
            "Epoch 00137: loss did not improve from 0.03982\n",
            "Epoch 138/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0550 - mse: 0.0550 - val_loss: 0.2029 - val_mse: 0.2029\n",
            "\n",
            "Epoch 00138: loss did not improve from 0.03982\n",
            "Epoch 139/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0542 - mse: 0.0542 - val_loss: 0.2002 - val_mse: 0.2002\n",
            "\n",
            "Epoch 00139: loss did not improve from 0.03982\n",
            "Epoch 140/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0535 - mse: 0.0535 - val_loss: 0.1978 - val_mse: 0.1978\n",
            "\n",
            "Epoch 00140: loss did not improve from 0.03982\n",
            "Epoch 141/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0528 - mse: 0.0528 - val_loss: 0.1954 - val_mse: 0.1954\n",
            "\n",
            "Epoch 00141: loss did not improve from 0.03982\n",
            "Epoch 142/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0521 - mse: 0.0521 - val_loss: 0.1932 - val_mse: 0.1932\n",
            "\n",
            "Epoch 00142: loss did not improve from 0.03982\n",
            "Epoch 143/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0515 - mse: 0.0515 - val_loss: 0.1911 - val_mse: 0.1911\n",
            "\n",
            "Epoch 00143: loss did not improve from 0.03982\n",
            "Epoch 144/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0508 - mse: 0.0508 - val_loss: 0.1892 - val_mse: 0.1892\n",
            "\n",
            "Epoch 00144: loss did not improve from 0.03982\n",
            "Epoch 145/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0502 - mse: 0.0502 - val_loss: 0.1873 - val_mse: 0.1873\n",
            "\n",
            "Epoch 00145: loss did not improve from 0.03982\n",
            "Epoch 146/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0496 - mse: 0.0496 - val_loss: 0.1855 - val_mse: 0.1855\n",
            "\n",
            "Epoch 00146: loss did not improve from 0.03982\n",
            "Epoch 147/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0491 - mse: 0.0491 - val_loss: 0.1838 - val_mse: 0.1838\n",
            "\n",
            "Epoch 00147: loss did not improve from 0.03982\n",
            "Epoch 148/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0485 - mse: 0.0485 - val_loss: 0.1822 - val_mse: 0.1822\n",
            "\n",
            "Epoch 00148: loss did not improve from 0.03982\n",
            "Epoch 149/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0480 - mse: 0.0480 - val_loss: 0.1806 - val_mse: 0.1806\n",
            "\n",
            "Epoch 00149: loss did not improve from 0.03982\n",
            "Epoch 150/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0475 - mse: 0.0475 - val_loss: 0.1792 - val_mse: 0.1792\n",
            "\n",
            "Epoch 00150: loss did not improve from 0.03982\n",
            "Epoch 151/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0470 - mse: 0.0470 - val_loss: 0.1778 - val_mse: 0.1778\n",
            "\n",
            "Epoch 00151: loss did not improve from 0.03982\n",
            "Epoch 152/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0465 - mse: 0.0465 - val_loss: 0.1765 - val_mse: 0.1765\n",
            "\n",
            "Epoch 00152: loss did not improve from 0.03982\n",
            "Epoch 153/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0461 - mse: 0.0461 - val_loss: 0.1752 - val_mse: 0.1752\n",
            "\n",
            "Epoch 00153: loss improved from 0.03982 to 0.03956, saving model to poids_train.hdf5\n",
            "Epoch 154/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0456 - mse: 0.0456 - val_loss: 0.1740 - val_mse: 0.1740\n",
            "\n",
            "Epoch 00154: loss improved from 0.03956 to 0.03929, saving model to poids_train.hdf5\n",
            "Epoch 155/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0452 - mse: 0.0452 - val_loss: 0.1729 - val_mse: 0.1729\n",
            "\n",
            "Epoch 00155: loss improved from 0.03929 to 0.03903, saving model to poids_train.hdf5\n",
            "Epoch 156/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0448 - mse: 0.0448 - val_loss: 0.1718 - val_mse: 0.1718\n",
            "\n",
            "Epoch 00156: loss improved from 0.03903 to 0.03877, saving model to poids_train.hdf5\n",
            "Epoch 157/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0443 - mse: 0.0443 - val_loss: 0.1707 - val_mse: 0.1707\n",
            "\n",
            "Epoch 00157: loss improved from 0.03877 to 0.03852, saving model to poids_train.hdf5\n",
            "Epoch 158/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0440 - mse: 0.0440 - val_loss: 0.1697 - val_mse: 0.1697\n",
            "\n",
            "Epoch 00158: loss improved from 0.03852 to 0.03828, saving model to poids_train.hdf5\n",
            "Epoch 159/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0436 - mse: 0.0436 - val_loss: 0.1688 - val_mse: 0.1688\n",
            "\n",
            "Epoch 00159: loss improved from 0.03828 to 0.03804, saving model to poids_train.hdf5\n",
            "Epoch 160/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0432 - mse: 0.0432 - val_loss: 0.1679 - val_mse: 0.1679\n",
            "\n",
            "Epoch 00160: loss improved from 0.03804 to 0.03781, saving model to poids_train.hdf5\n",
            "Epoch 161/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0428 - mse: 0.0428 - val_loss: 0.1670 - val_mse: 0.1670\n",
            "\n",
            "Epoch 00161: loss improved from 0.03781 to 0.03758, saving model to poids_train.hdf5\n",
            "Epoch 162/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0425 - mse: 0.0425 - val_loss: 0.1661 - val_mse: 0.1661\n",
            "\n",
            "Epoch 00162: loss improved from 0.03758 to 0.03736, saving model to poids_train.hdf5\n",
            "Epoch 163/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0421 - mse: 0.0421 - val_loss: 0.1653 - val_mse: 0.1653\n",
            "\n",
            "Epoch 00163: loss improved from 0.03736 to 0.03714, saving model to poids_train.hdf5\n",
            "Epoch 164/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0418 - mse: 0.0418 - val_loss: 0.1646 - val_mse: 0.1646\n",
            "\n",
            "Epoch 00164: loss improved from 0.03714 to 0.03693, saving model to poids_train.hdf5\n",
            "Epoch 165/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0415 - mse: 0.0415 - val_loss: 0.1638 - val_mse: 0.1638\n",
            "\n",
            "Epoch 00165: loss improved from 0.03693 to 0.03672, saving model to poids_train.hdf5\n",
            "Epoch 166/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0412 - mse: 0.0412 - val_loss: 0.1631 - val_mse: 0.1631\n",
            "\n",
            "Epoch 00166: loss improved from 0.03672 to 0.03652, saving model to poids_train.hdf5\n",
            "Epoch 167/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0408 - mse: 0.0408 - val_loss: 0.1624 - val_mse: 0.1624\n",
            "\n",
            "Epoch 00167: loss improved from 0.03652 to 0.03632, saving model to poids_train.hdf5\n",
            "Epoch 168/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0405 - mse: 0.0405 - val_loss: 0.1618 - val_mse: 0.1618\n",
            "\n",
            "Epoch 00168: loss improved from 0.03632 to 0.03612, saving model to poids_train.hdf5\n",
            "Epoch 169/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0402 - mse: 0.0402 - val_loss: 0.1612 - val_mse: 0.1612\n",
            "\n",
            "Epoch 00169: loss improved from 0.03612 to 0.03593, saving model to poids_train.hdf5\n",
            "Epoch 170/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0400 - mse: 0.0400 - val_loss: 0.1606 - val_mse: 0.1606\n",
            "\n",
            "Epoch 00170: loss improved from 0.03593 to 0.03574, saving model to poids_train.hdf5\n",
            "Epoch 171/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0397 - mse: 0.0397 - val_loss: 0.1600 - val_mse: 0.1600\n",
            "\n",
            "Epoch 00171: loss improved from 0.03574 to 0.03556, saving model to poids_train.hdf5\n",
            "Epoch 172/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0394 - mse: 0.0394 - val_loss: 0.1594 - val_mse: 0.1594\n",
            "\n",
            "Epoch 00172: loss improved from 0.03556 to 0.03538, saving model to poids_train.hdf5\n",
            "Epoch 173/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0391 - mse: 0.0391 - val_loss: 0.1589 - val_mse: 0.1589\n",
            "\n",
            "Epoch 00173: loss improved from 0.03538 to 0.03520, saving model to poids_train.hdf5\n",
            "Epoch 174/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0389 - mse: 0.0389 - val_loss: 0.1584 - val_mse: 0.1584\n",
            "\n",
            "Epoch 00174: loss improved from 0.03520 to 0.03503, saving model to poids_train.hdf5\n",
            "Epoch 175/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0386 - mse: 0.0386 - val_loss: 0.1579 - val_mse: 0.1579\n",
            "\n",
            "Epoch 00175: loss improved from 0.03503 to 0.03486, saving model to poids_train.hdf5\n",
            "Epoch 176/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.1574 - val_mse: 0.1574\n",
            "\n",
            "Epoch 00176: loss improved from 0.03486 to 0.03469, saving model to poids_train.hdf5\n",
            "Epoch 177/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.1569 - val_mse: 0.1569\n",
            "\n",
            "Epoch 00177: loss improved from 0.03469 to 0.03453, saving model to poids_train.hdf5\n",
            "Epoch 178/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.1565 - val_mse: 0.1565\n",
            "\n",
            "Epoch 00178: loss improved from 0.03453 to 0.03437, saving model to poids_train.hdf5\n",
            "Epoch 179/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0377 - mse: 0.0377 - val_loss: 0.1561 - val_mse: 0.1561\n",
            "\n",
            "Epoch 00179: loss improved from 0.03437 to 0.03421, saving model to poids_train.hdf5\n",
            "Epoch 180/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0374 - mse: 0.0374 - val_loss: 0.1557 - val_mse: 0.1557\n",
            "\n",
            "Epoch 00180: loss improved from 0.03421 to 0.03405, saving model to poids_train.hdf5\n",
            "Epoch 181/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0372 - mse: 0.0372 - val_loss: 0.1553 - val_mse: 0.1553\n",
            "\n",
            "Epoch 00181: loss improved from 0.03405 to 0.03390, saving model to poids_train.hdf5\n",
            "Epoch 182/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0370 - mse: 0.0370 - val_loss: 0.1549 - val_mse: 0.1549\n",
            "\n",
            "Epoch 00182: loss improved from 0.03390 to 0.03375, saving model to poids_train.hdf5\n",
            "Epoch 183/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0368 - mse: 0.0368 - val_loss: 0.1545 - val_mse: 0.1545\n",
            "\n",
            "Epoch 00183: loss improved from 0.03375 to 0.03360, saving model to poids_train.hdf5\n",
            "Epoch 184/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0366 - mse: 0.0366 - val_loss: 0.1542 - val_mse: 0.1542\n",
            "\n",
            "Epoch 00184: loss improved from 0.03360 to 0.03345, saving model to poids_train.hdf5\n",
            "Epoch 185/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0363 - mse: 0.0363 - val_loss: 0.1538 - val_mse: 0.1538\n",
            "\n",
            "Epoch 00185: loss improved from 0.03345 to 0.03331, saving model to poids_train.hdf5\n",
            "Epoch 186/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0361 - mse: 0.0361 - val_loss: 0.1535 - val_mse: 0.1535\n",
            "\n",
            "Epoch 00186: loss improved from 0.03331 to 0.03317, saving model to poids_train.hdf5\n",
            "Epoch 187/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0359 - mse: 0.0359 - val_loss: 0.1532 - val_mse: 0.1532\n",
            "\n",
            "Epoch 00187: loss improved from 0.03317 to 0.03303, saving model to poids_train.hdf5\n",
            "Epoch 188/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0357 - mse: 0.0357 - val_loss: 0.1529 - val_mse: 0.1529\n",
            "\n",
            "Epoch 00188: loss improved from 0.03303 to 0.03289, saving model to poids_train.hdf5\n",
            "Epoch 189/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0356 - mse: 0.0356 - val_loss: 0.1526 - val_mse: 0.1526\n",
            "\n",
            "Epoch 00189: loss improved from 0.03289 to 0.03276, saving model to poids_train.hdf5\n",
            "Epoch 190/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0354 - mse: 0.0354 - val_loss: 0.1523 - val_mse: 0.1523\n",
            "\n",
            "Epoch 00190: loss improved from 0.03276 to 0.03262, saving model to poids_train.hdf5\n",
            "Epoch 191/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0352 - mse: 0.0352 - val_loss: 0.1520 - val_mse: 0.1520\n",
            "\n",
            "Epoch 00191: loss improved from 0.03262 to 0.03249, saving model to poids_train.hdf5\n",
            "Epoch 192/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0350 - mse: 0.0350 - val_loss: 0.1517 - val_mse: 0.1517\n",
            "\n",
            "Epoch 00192: loss improved from 0.03249 to 0.03236, saving model to poids_train.hdf5\n",
            "Epoch 193/500\n",
            "186/186 [==============================] - 1s 5ms/step - loss: 0.0348 - mse: 0.0348 - val_loss: 0.1515 - val_mse: 0.1515\n",
            "\n",
            "Epoch 00193: loss improved from 0.03236 to 0.03223, saving model to poids_train.hdf5\n",
            "Epoch 194/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0346 - mse: 0.0346 - val_loss: 0.1512 - val_mse: 0.1512\n",
            "\n",
            "Epoch 00194: loss improved from 0.03223 to 0.03211, saving model to poids_train.hdf5\n",
            "Epoch 195/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0345 - mse: 0.0345 - val_loss: 0.1510 - val_mse: 0.1510\n",
            "\n",
            "Epoch 00195: loss improved from 0.03211 to 0.03199, saving model to poids_train.hdf5\n",
            "Epoch 196/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0343 - mse: 0.0343 - val_loss: 0.1507 - val_mse: 0.1507\n",
            "\n",
            "Epoch 00196: loss improved from 0.03199 to 0.03186, saving model to poids_train.hdf5\n",
            "Epoch 197/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0341 - mse: 0.0341 - val_loss: 0.1505 - val_mse: 0.1505\n",
            "\n",
            "Epoch 00197: loss improved from 0.03186 to 0.03174, saving model to poids_train.hdf5\n",
            "Epoch 198/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0340 - mse: 0.0340 - val_loss: 0.1503 - val_mse: 0.1503\n",
            "\n",
            "Epoch 00198: loss improved from 0.03174 to 0.03162, saving model to poids_train.hdf5\n",
            "Epoch 199/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0338 - mse: 0.0338 - val_loss: 0.1501 - val_mse: 0.1501\n",
            "\n",
            "Epoch 00199: loss improved from 0.03162 to 0.03151, saving model to poids_train.hdf5\n",
            "Epoch 200/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0337 - mse: 0.0337 - val_loss: 0.1499 - val_mse: 0.1499\n",
            "\n",
            "Epoch 00200: loss improved from 0.03151 to 0.03139, saving model to poids_train.hdf5\n",
            "Epoch 201/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0335 - mse: 0.0335 - val_loss: 0.1497 - val_mse: 0.1497\n",
            "\n",
            "Epoch 00201: loss improved from 0.03139 to 0.03128, saving model to poids_train.hdf5\n",
            "Epoch 202/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0333 - mse: 0.0333 - val_loss: 0.1495 - val_mse: 0.1495\n",
            "\n",
            "Epoch 00202: loss improved from 0.03128 to 0.03116, saving model to poids_train.hdf5\n",
            "Epoch 203/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0332 - mse: 0.0332 - val_loss: 0.1493 - val_mse: 0.1493\n",
            "\n",
            "Epoch 00203: loss improved from 0.03116 to 0.03105, saving model to poids_train.hdf5\n",
            "Epoch 204/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0330 - mse: 0.0330 - val_loss: 0.1491 - val_mse: 0.1491\n",
            "\n",
            "Epoch 00204: loss improved from 0.03105 to 0.03094, saving model to poids_train.hdf5\n",
            "Epoch 205/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0329 - mse: 0.0329 - val_loss: 0.1489 - val_mse: 0.1489\n",
            "\n",
            "Epoch 00205: loss improved from 0.03094 to 0.03083, saving model to poids_train.hdf5\n",
            "Epoch 206/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0328 - mse: 0.0328 - val_loss: 0.1488 - val_mse: 0.1488\n",
            "\n",
            "Epoch 00206: loss improved from 0.03083 to 0.03072, saving model to poids_train.hdf5\n",
            "Epoch 207/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0326 - mse: 0.0326 - val_loss: 0.1486 - val_mse: 0.1486\n",
            "\n",
            "Epoch 00207: loss improved from 0.03072 to 0.03062, saving model to poids_train.hdf5\n",
            "Epoch 208/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0325 - mse: 0.0325 - val_loss: 0.1485 - val_mse: 0.1485\n",
            "\n",
            "Epoch 00208: loss improved from 0.03062 to 0.03051, saving model to poids_train.hdf5\n",
            "Epoch 209/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0323 - mse: 0.0323 - val_loss: 0.1483 - val_mse: 0.1483\n",
            "\n",
            "Epoch 00209: loss improved from 0.03051 to 0.03041, saving model to poids_train.hdf5\n",
            "Epoch 210/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0322 - mse: 0.0322 - val_loss: 0.1482 - val_mse: 0.1482\n",
            "\n",
            "Epoch 00210: loss improved from 0.03041 to 0.03031, saving model to poids_train.hdf5\n",
            "Epoch 211/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0321 - mse: 0.0321 - val_loss: 0.1480 - val_mse: 0.1480\n",
            "\n",
            "Epoch 00211: loss improved from 0.03031 to 0.03020, saving model to poids_train.hdf5\n",
            "Epoch 212/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0319 - mse: 0.0319 - val_loss: 0.1479 - val_mse: 0.1479\n",
            "\n",
            "Epoch 00212: loss improved from 0.03020 to 0.03010, saving model to poids_train.hdf5\n",
            "Epoch 213/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0318 - mse: 0.0318 - val_loss: 0.1477 - val_mse: 0.1477\n",
            "\n",
            "Epoch 00213: loss improved from 0.03010 to 0.03001, saving model to poids_train.hdf5\n",
            "Epoch 214/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0317 - mse: 0.0317 - val_loss: 0.1476 - val_mse: 0.1476\n",
            "\n",
            "Epoch 00214: loss improved from 0.03001 to 0.02991, saving model to poids_train.hdf5\n",
            "Epoch 215/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0315 - mse: 0.0315 - val_loss: 0.1475 - val_mse: 0.1475\n",
            "\n",
            "Epoch 00215: loss improved from 0.02991 to 0.02981, saving model to poids_train.hdf5\n",
            "Epoch 216/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0314 - mse: 0.0314 - val_loss: 0.1474 - val_mse: 0.1474\n",
            "\n",
            "Epoch 00216: loss improved from 0.02981 to 0.02971, saving model to poids_train.hdf5\n",
            "Epoch 217/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.1472 - val_mse: 0.1472\n",
            "\n",
            "Epoch 00217: loss improved from 0.02971 to 0.02962, saving model to poids_train.hdf5\n",
            "Epoch 218/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.1471 - val_mse: 0.1471\n",
            "\n",
            "Epoch 00218: loss improved from 0.02962 to 0.02953, saving model to poids_train.hdf5\n",
            "Epoch 219/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0311 - mse: 0.0311 - val_loss: 0.1470 - val_mse: 0.1470\n",
            "\n",
            "Epoch 00219: loss improved from 0.02953 to 0.02943, saving model to poids_train.hdf5\n",
            "Epoch 220/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0309 - mse: 0.0309 - val_loss: 0.1469 - val_mse: 0.1469\n",
            "\n",
            "Epoch 00220: loss improved from 0.02943 to 0.02934, saving model to poids_train.hdf5\n",
            "Epoch 221/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0308 - mse: 0.0308 - val_loss: 0.1468 - val_mse: 0.1468\n",
            "\n",
            "Epoch 00221: loss improved from 0.02934 to 0.02925, saving model to poids_train.hdf5\n",
            "Epoch 222/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0307 - mse: 0.0307 - val_loss: 0.1467 - val_mse: 0.1467\n",
            "\n",
            "Epoch 00222: loss improved from 0.02925 to 0.02916, saving model to poids_train.hdf5\n",
            "Epoch 223/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0306 - mse: 0.0306 - val_loss: 0.1466 - val_mse: 0.1466\n",
            "\n",
            "Epoch 00223: loss improved from 0.02916 to 0.02907, saving model to poids_train.hdf5\n",
            "Epoch 224/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0305 - mse: 0.0305 - val_loss: 0.1465 - val_mse: 0.1465\n",
            "\n",
            "Epoch 00224: loss improved from 0.02907 to 0.02898, saving model to poids_train.hdf5\n",
            "Epoch 225/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0304 - mse: 0.0304 - val_loss: 0.1464 - val_mse: 0.1464\n",
            "\n",
            "Epoch 00225: loss improved from 0.02898 to 0.02889, saving model to poids_train.hdf5\n",
            "Epoch 226/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0303 - mse: 0.0303 - val_loss: 0.1464 - val_mse: 0.1464\n",
            "\n",
            "Epoch 00226: loss improved from 0.02889 to 0.02881, saving model to poids_train.hdf5\n",
            "Epoch 227/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0301 - mse: 0.0301 - val_loss: 0.1463 - val_mse: 0.1463\n",
            "\n",
            "Epoch 00227: loss improved from 0.02881 to 0.02872, saving model to poids_train.hdf5\n",
            "Epoch 228/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0300 - mse: 0.0300 - val_loss: 0.1462 - val_mse: 0.1462\n",
            "\n",
            "Epoch 00228: loss improved from 0.02872 to 0.02863, saving model to poids_train.hdf5\n",
            "Epoch 229/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0299 - mse: 0.0299 - val_loss: 0.1461 - val_mse: 0.1461\n",
            "\n",
            "Epoch 00229: loss improved from 0.02863 to 0.02855, saving model to poids_train.hdf5\n",
            "Epoch 230/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0298 - mse: 0.0298 - val_loss: 0.1460 - val_mse: 0.1460\n",
            "\n",
            "Epoch 00230: loss improved from 0.02855 to 0.02847, saving model to poids_train.hdf5\n",
            "Epoch 231/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0297 - mse: 0.0297 - val_loss: 0.1460 - val_mse: 0.1460\n",
            "\n",
            "Epoch 00231: loss improved from 0.02847 to 0.02838, saving model to poids_train.hdf5\n",
            "Epoch 232/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0296 - mse: 0.0296 - val_loss: 0.1459 - val_mse: 0.1459\n",
            "\n",
            "Epoch 00232: loss improved from 0.02838 to 0.02830, saving model to poids_train.hdf5\n",
            "Epoch 233/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0295 - mse: 0.0295 - val_loss: 0.1458 - val_mse: 0.1458\n",
            "\n",
            "Epoch 00233: loss improved from 0.02830 to 0.02822, saving model to poids_train.hdf5\n",
            "Epoch 234/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0294 - mse: 0.0294 - val_loss: 0.1458 - val_mse: 0.1458\n",
            "\n",
            "Epoch 00234: loss improved from 0.02822 to 0.02814, saving model to poids_train.hdf5\n",
            "Epoch 235/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0293 - mse: 0.0293 - val_loss: 0.1457 - val_mse: 0.1457\n",
            "\n",
            "Epoch 00235: loss improved from 0.02814 to 0.02806, saving model to poids_train.hdf5\n",
            "Epoch 236/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0292 - mse: 0.0292 - val_loss: 0.1457 - val_mse: 0.1457\n",
            "\n",
            "Epoch 00236: loss improved from 0.02806 to 0.02798, saving model to poids_train.hdf5\n",
            "Epoch 237/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0291 - mse: 0.0291 - val_loss: 0.1456 - val_mse: 0.1456\n",
            "\n",
            "Epoch 00237: loss improved from 0.02798 to 0.02790, saving model to poids_train.hdf5\n",
            "Epoch 238/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0290 - mse: 0.0290 - val_loss: 0.1456 - val_mse: 0.1456\n",
            "\n",
            "Epoch 00238: loss improved from 0.02790 to 0.02782, saving model to poids_train.hdf5\n",
            "Epoch 239/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0289 - mse: 0.0289 - val_loss: 0.1455 - val_mse: 0.1455\n",
            "\n",
            "Epoch 00239: loss improved from 0.02782 to 0.02775, saving model to poids_train.hdf5\n",
            "Epoch 240/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0288 - mse: 0.0288 - val_loss: 0.1455 - val_mse: 0.1455\n",
            "\n",
            "Epoch 00240: loss improved from 0.02775 to 0.02767, saving model to poids_train.hdf5\n",
            "Epoch 241/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0287 - mse: 0.0287 - val_loss: 0.1454 - val_mse: 0.1454\n",
            "\n",
            "Epoch 00241: loss improved from 0.02767 to 0.02759, saving model to poids_train.hdf5\n",
            "Epoch 242/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0286 - mse: 0.0286 - val_loss: 0.1454 - val_mse: 0.1454\n",
            "\n",
            "Epoch 00242: loss improved from 0.02759 to 0.02752, saving model to poids_train.hdf5\n",
            "Epoch 243/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0286 - mse: 0.0286 - val_loss: 0.1453 - val_mse: 0.1453\n",
            "\n",
            "Epoch 00243: loss improved from 0.02752 to 0.02744, saving model to poids_train.hdf5\n",
            "Epoch 244/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.1453 - val_mse: 0.1453\n",
            "\n",
            "Epoch 00244: loss improved from 0.02744 to 0.02737, saving model to poids_train.hdf5\n",
            "Epoch 245/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0284 - mse: 0.0284 - val_loss: 0.1452 - val_mse: 0.1452\n",
            "\n",
            "Epoch 00245: loss improved from 0.02737 to 0.02730, saving model to poids_train.hdf5\n",
            "Epoch 246/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0283 - mse: 0.0283 - val_loss: 0.1452 - val_mse: 0.1452\n",
            "\n",
            "Epoch 00246: loss improved from 0.02730 to 0.02722, saving model to poids_train.hdf5\n",
            "Epoch 247/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0282 - mse: 0.0282 - val_loss: 0.1452 - val_mse: 0.1452\n",
            "\n",
            "Epoch 00247: loss improved from 0.02722 to 0.02715, saving model to poids_train.hdf5\n",
            "Epoch 248/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0281 - mse: 0.0281 - val_loss: 0.1451 - val_mse: 0.1451\n",
            "\n",
            "Epoch 00248: loss improved from 0.02715 to 0.02708, saving model to poids_train.hdf5\n",
            "Epoch 249/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0280 - mse: 0.0280 - val_loss: 0.1451 - val_mse: 0.1451\n",
            "\n",
            "Epoch 00249: loss improved from 0.02708 to 0.02701, saving model to poids_train.hdf5\n",
            "Epoch 250/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0279 - mse: 0.0279 - val_loss: 0.1451 - val_mse: 0.1451\n",
            "\n",
            "Epoch 00250: loss improved from 0.02701 to 0.02694, saving model to poids_train.hdf5\n",
            "Epoch 251/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0278 - mse: 0.0278 - val_loss: 0.1451 - val_mse: 0.1451\n",
            "\n",
            "Epoch 00251: loss improved from 0.02694 to 0.02687, saving model to poids_train.hdf5\n",
            "Epoch 252/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0278 - mse: 0.0278 - val_loss: 0.1450 - val_mse: 0.1450\n",
            "\n",
            "Epoch 00252: loss improved from 0.02687 to 0.02680, saving model to poids_train.hdf5\n",
            "Epoch 253/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0277 - mse: 0.0277 - val_loss: 0.1450 - val_mse: 0.1450\n",
            "\n",
            "Epoch 00253: loss improved from 0.02680 to 0.02673, saving model to poids_train.hdf5\n",
            "Epoch 254/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0276 - mse: 0.0276 - val_loss: 0.1450 - val_mse: 0.1450\n",
            "\n",
            "Epoch 00254: loss improved from 0.02673 to 0.02666, saving model to poids_train.hdf5\n",
            "Epoch 255/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0275 - mse: 0.0275 - val_loss: 0.1450 - val_mse: 0.1450\n",
            "\n",
            "Epoch 00255: loss improved from 0.02666 to 0.02659, saving model to poids_train.hdf5\n",
            "Epoch 256/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.1450 - val_mse: 0.1450\n",
            "\n",
            "Epoch 00256: loss improved from 0.02659 to 0.02652, saving model to poids_train.hdf5\n",
            "Epoch 257/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.1450 - val_mse: 0.1450\n",
            "\n",
            "Epoch 00257: loss improved from 0.02652 to 0.02646, saving model to poids_train.hdf5\n",
            "Epoch 258/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0273 - mse: 0.0273 - val_loss: 0.1449 - val_mse: 0.1449\n",
            "\n",
            "Epoch 00258: loss improved from 0.02646 to 0.02639, saving model to poids_train.hdf5\n",
            "Epoch 259/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0272 - mse: 0.0272 - val_loss: 0.1449 - val_mse: 0.1449\n",
            "\n",
            "Epoch 00259: loss improved from 0.02639 to 0.02632, saving model to poids_train.hdf5\n",
            "Epoch 260/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0271 - mse: 0.0271 - val_loss: 0.1449 - val_mse: 0.1449\n",
            "\n",
            "Epoch 00260: loss improved from 0.02632 to 0.02626, saving model to poids_train.hdf5\n",
            "Epoch 261/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0270 - mse: 0.0270 - val_loss: 0.1449 - val_mse: 0.1449\n",
            "\n",
            "Epoch 00261: loss improved from 0.02626 to 0.02619, saving model to poids_train.hdf5\n",
            "Epoch 262/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0270 - mse: 0.0270 - val_loss: 0.1449 - val_mse: 0.1449\n",
            "\n",
            "Epoch 00262: loss improved from 0.02619 to 0.02613, saving model to poids_train.hdf5\n",
            "Epoch 263/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0269 - mse: 0.0269 - val_loss: 0.1449 - val_mse: 0.1449\n",
            "\n",
            "Epoch 00263: loss improved from 0.02613 to 0.02606, saving model to poids_train.hdf5\n",
            "Epoch 264/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0268 - mse: 0.0268 - val_loss: 0.1449 - val_mse: 0.1449\n",
            "\n",
            "Epoch 00264: loss improved from 0.02606 to 0.02600, saving model to poids_train.hdf5\n",
            "Epoch 265/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0267 - mse: 0.0267 - val_loss: 0.1449 - val_mse: 0.1449\n",
            "\n",
            "Epoch 00265: loss improved from 0.02600 to 0.02594, saving model to poids_train.hdf5\n",
            "Epoch 266/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0267 - mse: 0.0267 - val_loss: 0.1449 - val_mse: 0.1449\n",
            "\n",
            "Epoch 00266: loss improved from 0.02594 to 0.02587, saving model to poids_train.hdf5\n",
            "Epoch 267/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0266 - mse: 0.0266 - val_loss: 0.1449 - val_mse: 0.1449\n",
            "\n",
            "Epoch 00267: loss improved from 0.02587 to 0.02581, saving model to poids_train.hdf5\n",
            "Epoch 268/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0265 - mse: 0.0265 - val_loss: 0.1449 - val_mse: 0.1449\n",
            "\n",
            "Epoch 00268: loss improved from 0.02581 to 0.02575, saving model to poids_train.hdf5\n",
            "Epoch 269/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0264 - mse: 0.0264 - val_loss: 0.1449 - val_mse: 0.1449\n",
            "\n",
            "Epoch 00269: loss improved from 0.02575 to 0.02568, saving model to poids_train.hdf5\n",
            "Epoch 270/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0264 - mse: 0.0264 - val_loss: 0.1449 - val_mse: 0.1449\n",
            "\n",
            "Epoch 00270: loss improved from 0.02568 to 0.02562, saving model to poids_train.hdf5\n",
            "Epoch 271/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0263 - mse: 0.0263 - val_loss: 0.1449 - val_mse: 0.1449\n",
            "\n",
            "Epoch 00271: loss improved from 0.02562 to 0.02556, saving model to poids_train.hdf5\n",
            "Epoch 272/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0262 - mse: 0.0262 - val_loss: 0.1449 - val_mse: 0.1449\n",
            "\n",
            "Epoch 00272: loss improved from 0.02556 to 0.02550, saving model to poids_train.hdf5\n",
            "Epoch 273/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0262 - mse: 0.0262 - val_loss: 0.1450 - val_mse: 0.1450\n",
            "\n",
            "Epoch 00273: loss improved from 0.02550 to 0.02544, saving model to poids_train.hdf5\n",
            "Epoch 274/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0261 - mse: 0.0261 - val_loss: 0.1450 - val_mse: 0.1450\n",
            "\n",
            "Epoch 00274: loss improved from 0.02544 to 0.02538, saving model to poids_train.hdf5\n",
            "Epoch 275/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0260 - mse: 0.0260 - val_loss: 0.1450 - val_mse: 0.1450\n",
            "\n",
            "Epoch 00275: loss improved from 0.02538 to 0.02532, saving model to poids_train.hdf5\n",
            "Epoch 276/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0259 - mse: 0.0259 - val_loss: 0.1450 - val_mse: 0.1450\n",
            "\n",
            "Epoch 00276: loss improved from 0.02532 to 0.02526, saving model to poids_train.hdf5\n",
            "Epoch 277/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0259 - mse: 0.0259 - val_loss: 0.1450 - val_mse: 0.1450\n",
            "\n",
            "Epoch 00277: loss improved from 0.02526 to 0.02520, saving model to poids_train.hdf5\n",
            "Epoch 278/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0258 - mse: 0.0258 - val_loss: 0.1450 - val_mse: 0.1450\n",
            "\n",
            "Epoch 00278: loss improved from 0.02520 to 0.02514, saving model to poids_train.hdf5\n",
            "Epoch 279/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0257 - mse: 0.0257 - val_loss: 0.1451 - val_mse: 0.1451\n",
            "\n",
            "Epoch 00279: loss improved from 0.02514 to 0.02509, saving model to poids_train.hdf5\n",
            "Epoch 280/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0257 - mse: 0.0257 - val_loss: 0.1451 - val_mse: 0.1451\n",
            "\n",
            "Epoch 00280: loss improved from 0.02509 to 0.02503, saving model to poids_train.hdf5\n",
            "Epoch 281/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0256 - mse: 0.0256 - val_loss: 0.1451 - val_mse: 0.1451\n",
            "\n",
            "Epoch 00281: loss improved from 0.02503 to 0.02497, saving model to poids_train.hdf5\n",
            "Epoch 282/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0255 - mse: 0.0255 - val_loss: 0.1451 - val_mse: 0.1451\n",
            "\n",
            "Epoch 00282: loss improved from 0.02497 to 0.02491, saving model to poids_train.hdf5\n",
            "Epoch 283/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0255 - mse: 0.0255 - val_loss: 0.1452 - val_mse: 0.1452\n",
            "\n",
            "Epoch 00283: loss improved from 0.02491 to 0.02486, saving model to poids_train.hdf5\n",
            "Epoch 284/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0254 - mse: 0.0254 - val_loss: 0.1452 - val_mse: 0.1452\n",
            "\n",
            "Epoch 00284: loss improved from 0.02486 to 0.02480, saving model to poids_train.hdf5\n",
            "Epoch 285/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0253 - mse: 0.0253 - val_loss: 0.1452 - val_mse: 0.1452\n",
            "\n",
            "Epoch 00285: loss improved from 0.02480 to 0.02474, saving model to poids_train.hdf5\n",
            "Epoch 286/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0253 - mse: 0.0253 - val_loss: 0.1453 - val_mse: 0.1453\n",
            "\n",
            "Epoch 00286: loss improved from 0.02474 to 0.02469, saving model to poids_train.hdf5\n",
            "Epoch 287/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0252 - mse: 0.0252 - val_loss: 0.1453 - val_mse: 0.1453\n",
            "\n",
            "Epoch 00287: loss improved from 0.02469 to 0.02463, saving model to poids_train.hdf5\n",
            "Epoch 288/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.1453 - val_mse: 0.1453\n",
            "\n",
            "Epoch 00288: loss improved from 0.02463 to 0.02458, saving model to poids_train.hdf5\n",
            "Epoch 289/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.1454 - val_mse: 0.1454\n",
            "\n",
            "Epoch 00289: loss improved from 0.02458 to 0.02452, saving model to poids_train.hdf5\n",
            "Epoch 290/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.1454 - val_mse: 0.1454\n",
            "\n",
            "Epoch 00290: loss improved from 0.02452 to 0.02447, saving model to poids_train.hdf5\n",
            "Epoch 291/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.1454 - val_mse: 0.1454\n",
            "\n",
            "Epoch 00291: loss improved from 0.02447 to 0.02441, saving model to poids_train.hdf5\n",
            "Epoch 292/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0249 - mse: 0.0249 - val_loss: 0.1455 - val_mse: 0.1455\n",
            "\n",
            "Epoch 00292: loss improved from 0.02441 to 0.02436, saving model to poids_train.hdf5\n",
            "Epoch 293/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.1455 - val_mse: 0.1455\n",
            "\n",
            "Epoch 00293: loss improved from 0.02436 to 0.02430, saving model to poids_train.hdf5\n",
            "Epoch 294/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.1456 - val_mse: 0.1456\n",
            "\n",
            "Epoch 00294: loss improved from 0.02430 to 0.02425, saving model to poids_train.hdf5\n",
            "Epoch 295/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.1456 - val_mse: 0.1456\n",
            "\n",
            "Epoch 00295: loss improved from 0.02425 to 0.02420, saving model to poids_train.hdf5\n",
            "Epoch 296/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.1457 - val_mse: 0.1457\n",
            "\n",
            "Epoch 00296: loss improved from 0.02420 to 0.02414, saving model to poids_train.hdf5\n",
            "Epoch 297/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.1457 - val_mse: 0.1457\n",
            "\n",
            "Epoch 00297: loss improved from 0.02414 to 0.02409, saving model to poids_train.hdf5\n",
            "Epoch 298/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.1457 - val_mse: 0.1457\n",
            "\n",
            "Epoch 00298: loss improved from 0.02409 to 0.02404, saving model to poids_train.hdf5\n",
            "Epoch 299/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.1458 - val_mse: 0.1458\n",
            "\n",
            "Epoch 00299: loss improved from 0.02404 to 0.02398, saving model to poids_train.hdf5\n",
            "Epoch 300/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0244 - mse: 0.0244 - val_loss: 0.1458 - val_mse: 0.1458\n",
            "\n",
            "Epoch 00300: loss improved from 0.02398 to 0.02393, saving model to poids_train.hdf5\n",
            "Epoch 301/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0243 - mse: 0.0243 - val_loss: 0.1459 - val_mse: 0.1459\n",
            "\n",
            "Epoch 00301: loss improved from 0.02393 to 0.02388, saving model to poids_train.hdf5\n",
            "Epoch 302/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0243 - mse: 0.0243 - val_loss: 0.1460 - val_mse: 0.1460\n",
            "\n",
            "Epoch 00302: loss improved from 0.02388 to 0.02383, saving model to poids_train.hdf5\n",
            "Epoch 303/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0242 - mse: 0.0242 - val_loss: 0.1460 - val_mse: 0.1460\n",
            "\n",
            "Epoch 00303: loss improved from 0.02383 to 0.02378, saving model to poids_train.hdf5\n",
            "Epoch 304/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0242 - mse: 0.0242 - val_loss: 0.1461 - val_mse: 0.1461\n",
            "\n",
            "Epoch 00304: loss improved from 0.02378 to 0.02373, saving model to poids_train.hdf5\n",
            "Epoch 305/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0241 - mse: 0.0241 - val_loss: 0.1461 - val_mse: 0.1461\n",
            "\n",
            "Epoch 00305: loss improved from 0.02373 to 0.02368, saving model to poids_train.hdf5\n",
            "Epoch 306/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0241 - mse: 0.0241 - val_loss: 0.1462 - val_mse: 0.1462\n",
            "\n",
            "Epoch 00306: loss improved from 0.02368 to 0.02363, saving model to poids_train.hdf5\n",
            "Epoch 307/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0240 - mse: 0.0240 - val_loss: 0.1462 - val_mse: 0.1462\n",
            "\n",
            "Epoch 00307: loss improved from 0.02363 to 0.02358, saving model to poids_train.hdf5\n",
            "Epoch 308/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0239 - mse: 0.0239 - val_loss: 0.1463 - val_mse: 0.1463\n",
            "\n",
            "Epoch 00308: loss improved from 0.02358 to 0.02353, saving model to poids_train.hdf5\n",
            "Epoch 309/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0239 - mse: 0.0239 - val_loss: 0.1464 - val_mse: 0.1464\n",
            "\n",
            "Epoch 00309: loss improved from 0.02353 to 0.02348, saving model to poids_train.hdf5\n",
            "Epoch 310/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0238 - mse: 0.0238 - val_loss: 0.1464 - val_mse: 0.1464\n",
            "\n",
            "Epoch 00310: loss improved from 0.02348 to 0.02343, saving model to poids_train.hdf5\n",
            "Epoch 311/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0238 - mse: 0.0238 - val_loss: 0.1465 - val_mse: 0.1465\n",
            "\n",
            "Epoch 00311: loss improved from 0.02343 to 0.02338, saving model to poids_train.hdf5\n",
            "Epoch 312/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0237 - mse: 0.0237 - val_loss: 0.1466 - val_mse: 0.1466\n",
            "\n",
            "Epoch 00312: loss improved from 0.02338 to 0.02333, saving model to poids_train.hdf5\n",
            "Epoch 313/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0237 - mse: 0.0237 - val_loss: 0.1466 - val_mse: 0.1466\n",
            "\n",
            "Epoch 00313: loss improved from 0.02333 to 0.02328, saving model to poids_train.hdf5\n",
            "Epoch 314/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0236 - mse: 0.0236 - val_loss: 0.1467 - val_mse: 0.1467\n",
            "\n",
            "Epoch 00314: loss improved from 0.02328 to 0.02323, saving model to poids_train.hdf5\n",
            "Epoch 315/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0236 - mse: 0.0236 - val_loss: 0.1468 - val_mse: 0.1468\n",
            "\n",
            "Epoch 00315: loss improved from 0.02323 to 0.02318, saving model to poids_train.hdf5\n",
            "Epoch 316/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0235 - mse: 0.0235 - val_loss: 0.1468 - val_mse: 0.1468\n",
            "\n",
            "Epoch 00316: loss improved from 0.02318 to 0.02313, saving model to poids_train.hdf5\n",
            "Epoch 317/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0234 - mse: 0.0234 - val_loss: 0.1469 - val_mse: 0.1469\n",
            "\n",
            "Epoch 00317: loss improved from 0.02313 to 0.02308, saving model to poids_train.hdf5\n",
            "Epoch 318/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0234 - mse: 0.0234 - val_loss: 0.1470 - val_mse: 0.1470\n",
            "\n",
            "Epoch 00318: loss improved from 0.02308 to 0.02304, saving model to poids_train.hdf5\n",
            "Epoch 319/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0233 - mse: 0.0233 - val_loss: 0.1471 - val_mse: 0.1471\n",
            "\n",
            "Epoch 00319: loss improved from 0.02304 to 0.02299, saving model to poids_train.hdf5\n",
            "Epoch 320/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0233 - mse: 0.0233 - val_loss: 0.1472 - val_mse: 0.1472\n",
            "\n",
            "Epoch 00320: loss improved from 0.02299 to 0.02294, saving model to poids_train.hdf5\n",
            "Epoch 321/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0232 - mse: 0.0232 - val_loss: 0.1472 - val_mse: 0.1472\n",
            "\n",
            "Epoch 00321: loss improved from 0.02294 to 0.02289, saving model to poids_train.hdf5\n",
            "Epoch 322/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0232 - mse: 0.0232 - val_loss: 0.1473 - val_mse: 0.1473\n",
            "\n",
            "Epoch 00322: loss improved from 0.02289 to 0.02285, saving model to poids_train.hdf5\n",
            "Epoch 323/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0231 - mse: 0.0231 - val_loss: 0.1474 - val_mse: 0.1474\n",
            "\n",
            "Epoch 00323: loss improved from 0.02285 to 0.02280, saving model to poids_train.hdf5\n",
            "Epoch 324/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0231 - mse: 0.0231 - val_loss: 0.1475 - val_mse: 0.1475\n",
            "\n",
            "Epoch 00324: loss improved from 0.02280 to 0.02275, saving model to poids_train.hdf5\n",
            "Epoch 325/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0230 - mse: 0.0230 - val_loss: 0.1476 - val_mse: 0.1476\n",
            "\n",
            "Epoch 00325: loss improved from 0.02275 to 0.02271, saving model to poids_train.hdf5\n",
            "Epoch 326/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0230 - mse: 0.0230 - val_loss: 0.1476 - val_mse: 0.1476\n",
            "\n",
            "Epoch 00326: loss improved from 0.02271 to 0.02266, saving model to poids_train.hdf5\n",
            "Epoch 327/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0229 - mse: 0.0229 - val_loss: 0.1477 - val_mse: 0.1477\n",
            "\n",
            "Epoch 00327: loss improved from 0.02266 to 0.02261, saving model to poids_train.hdf5\n",
            "Epoch 328/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0229 - mse: 0.0229 - val_loss: 0.1478 - val_mse: 0.1478\n",
            "\n",
            "Epoch 00328: loss improved from 0.02261 to 0.02257, saving model to poids_train.hdf5\n",
            "Epoch 329/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0228 - mse: 0.0228 - val_loss: 0.1479 - val_mse: 0.1479\n",
            "\n",
            "Epoch 00329: loss improved from 0.02257 to 0.02252, saving model to poids_train.hdf5\n",
            "Epoch 330/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0228 - mse: 0.0228 - val_loss: 0.1480 - val_mse: 0.1480\n",
            "\n",
            "Epoch 00330: loss improved from 0.02252 to 0.02248, saving model to poids_train.hdf5\n",
            "Epoch 331/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0227 - mse: 0.0227 - val_loss: 0.1481 - val_mse: 0.1481\n",
            "\n",
            "Epoch 00331: loss improved from 0.02248 to 0.02243, saving model to poids_train.hdf5\n",
            "Epoch 332/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0227 - mse: 0.0227 - val_loss: 0.1482 - val_mse: 0.1482\n",
            "\n",
            "Epoch 00332: loss improved from 0.02243 to 0.02239, saving model to poids_train.hdf5\n",
            "Epoch 333/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0226 - mse: 0.0226 - val_loss: 0.1483 - val_mse: 0.1483\n",
            "\n",
            "Epoch 00333: loss improved from 0.02239 to 0.02234, saving model to poids_train.hdf5\n",
            "Epoch 334/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0226 - mse: 0.0226 - val_loss: 0.1484 - val_mse: 0.1484\n",
            "\n",
            "Epoch 00334: loss improved from 0.02234 to 0.02230, saving model to poids_train.hdf5\n",
            "Epoch 335/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0225 - mse: 0.0225 - val_loss: 0.1485 - val_mse: 0.1485\n",
            "\n",
            "Epoch 00335: loss improved from 0.02230 to 0.02225, saving model to poids_train.hdf5\n",
            "Epoch 336/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0225 - mse: 0.0225 - val_loss: 0.1486 - val_mse: 0.1486\n",
            "\n",
            "Epoch 00336: loss improved from 0.02225 to 0.02221, saving model to poids_train.hdf5\n",
            "Epoch 337/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0224 - mse: 0.0224 - val_loss: 0.1487 - val_mse: 0.1487\n",
            "\n",
            "Epoch 00337: loss improved from 0.02221 to 0.02216, saving model to poids_train.hdf5\n",
            "Epoch 338/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0224 - mse: 0.0224 - val_loss: 0.1488 - val_mse: 0.1488\n",
            "\n",
            "Epoch 00338: loss improved from 0.02216 to 0.02212, saving model to poids_train.hdf5\n",
            "Epoch 339/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0223 - mse: 0.0223 - val_loss: 0.1489 - val_mse: 0.1489\n",
            "\n",
            "Epoch 00339: loss improved from 0.02212 to 0.02208, saving model to poids_train.hdf5\n",
            "Epoch 340/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0223 - mse: 0.0223 - val_loss: 0.1490 - val_mse: 0.1490\n",
            "\n",
            "Epoch 00340: loss improved from 0.02208 to 0.02203, saving model to poids_train.hdf5\n",
            "Epoch 341/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0222 - mse: 0.0222 - val_loss: 0.1491 - val_mse: 0.1491\n",
            "\n",
            "Epoch 00341: loss improved from 0.02203 to 0.02199, saving model to poids_train.hdf5\n",
            "Epoch 342/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0222 - mse: 0.0222 - val_loss: 0.1492 - val_mse: 0.1492\n",
            "\n",
            "Epoch 00342: loss improved from 0.02199 to 0.02194, saving model to poids_train.hdf5\n",
            "Epoch 343/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0221 - mse: 0.0221 - val_loss: 0.1493 - val_mse: 0.1493\n",
            "\n",
            "Epoch 00343: loss improved from 0.02194 to 0.02190, saving model to poids_train.hdf5\n",
            "Epoch 344/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0221 - mse: 0.0221 - val_loss: 0.1494 - val_mse: 0.1494\n",
            "\n",
            "Epoch 00344: loss improved from 0.02190 to 0.02186, saving model to poids_train.hdf5\n",
            "Epoch 345/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0220 - mse: 0.0220 - val_loss: 0.1495 - val_mse: 0.1495\n",
            "\n",
            "Epoch 00345: loss improved from 0.02186 to 0.02182, saving model to poids_train.hdf5\n",
            "Epoch 346/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0220 - mse: 0.0220 - val_loss: 0.1496 - val_mse: 0.1496\n",
            "\n",
            "Epoch 00346: loss improved from 0.02182 to 0.02177, saving model to poids_train.hdf5\n",
            "Epoch 347/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0219 - mse: 0.0219 - val_loss: 0.1498 - val_mse: 0.1498\n",
            "\n",
            "Epoch 00347: loss improved from 0.02177 to 0.02173, saving model to poids_train.hdf5\n",
            "Epoch 348/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0219 - mse: 0.0219 - val_loss: 0.1499 - val_mse: 0.1499\n",
            "\n",
            "Epoch 00348: loss improved from 0.02173 to 0.02169, saving model to poids_train.hdf5\n",
            "Epoch 349/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0218 - mse: 0.0218 - val_loss: 0.1500 - val_mse: 0.1500\n",
            "\n",
            "Epoch 00349: loss improved from 0.02169 to 0.02164, saving model to poids_train.hdf5\n",
            "Epoch 350/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0218 - mse: 0.0218 - val_loss: 0.1501 - val_mse: 0.1501\n",
            "\n",
            "Epoch 00350: loss improved from 0.02164 to 0.02160, saving model to poids_train.hdf5\n",
            "Epoch 351/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0217 - mse: 0.0217 - val_loss: 0.1502 - val_mse: 0.1502\n",
            "\n",
            "Epoch 00351: loss improved from 0.02160 to 0.02156, saving model to poids_train.hdf5\n",
            "Epoch 352/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0217 - mse: 0.0217 - val_loss: 0.1503 - val_mse: 0.1503\n",
            "\n",
            "Epoch 00352: loss improved from 0.02156 to 0.02152, saving model to poids_train.hdf5\n",
            "Epoch 353/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0216 - mse: 0.0216 - val_loss: 0.1505 - val_mse: 0.1505\n",
            "\n",
            "Epoch 00353: loss improved from 0.02152 to 0.02148, saving model to poids_train.hdf5\n",
            "Epoch 354/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0216 - mse: 0.0216 - val_loss: 0.1506 - val_mse: 0.1506\n",
            "\n",
            "Epoch 00354: loss improved from 0.02148 to 0.02144, saving model to poids_train.hdf5\n",
            "Epoch 355/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0216 - mse: 0.0216 - val_loss: 0.1507 - val_mse: 0.1507\n",
            "\n",
            "Epoch 00355: loss improved from 0.02144 to 0.02139, saving model to poids_train.hdf5\n",
            "Epoch 356/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0215 - mse: 0.0215 - val_loss: 0.1508 - val_mse: 0.1508\n",
            "\n",
            "Epoch 00356: loss improved from 0.02139 to 0.02135, saving model to poids_train.hdf5\n",
            "Epoch 357/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0215 - mse: 0.0215 - val_loss: 0.1510 - val_mse: 0.1510\n",
            "\n",
            "Epoch 00357: loss improved from 0.02135 to 0.02131, saving model to poids_train.hdf5\n",
            "Epoch 358/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0214 - mse: 0.0214 - val_loss: 0.1511 - val_mse: 0.1511\n",
            "\n",
            "Epoch 00358: loss improved from 0.02131 to 0.02127, saving model to poids_train.hdf5\n",
            "Epoch 359/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0214 - mse: 0.0214 - val_loss: 0.1512 - val_mse: 0.1512\n",
            "\n",
            "Epoch 00359: loss improved from 0.02127 to 0.02123, saving model to poids_train.hdf5\n",
            "Epoch 360/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0213 - mse: 0.0213 - val_loss: 0.1514 - val_mse: 0.1514\n",
            "\n",
            "Epoch 00360: loss improved from 0.02123 to 0.02119, saving model to poids_train.hdf5\n",
            "Epoch 361/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0213 - mse: 0.0213 - val_loss: 0.1515 - val_mse: 0.1515\n",
            "\n",
            "Epoch 00361: loss improved from 0.02119 to 0.02115, saving model to poids_train.hdf5\n",
            "Epoch 362/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0212 - mse: 0.0212 - val_loss: 0.1516 - val_mse: 0.1516\n",
            "\n",
            "Epoch 00362: loss improved from 0.02115 to 0.02111, saving model to poids_train.hdf5\n",
            "Epoch 363/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0212 - mse: 0.0212 - val_loss: 0.1518 - val_mse: 0.1518\n",
            "\n",
            "Epoch 00363: loss improved from 0.02111 to 0.02107, saving model to poids_train.hdf5\n",
            "Epoch 364/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0212 - mse: 0.0212 - val_loss: 0.1519 - val_mse: 0.1519\n",
            "\n",
            "Epoch 00364: loss improved from 0.02107 to 0.02103, saving model to poids_train.hdf5\n",
            "Epoch 365/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0211 - mse: 0.0211 - val_loss: 0.1520 - val_mse: 0.1520\n",
            "\n",
            "Epoch 00365: loss improved from 0.02103 to 0.02099, saving model to poids_train.hdf5\n",
            "Epoch 366/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0211 - mse: 0.0211 - val_loss: 0.1522 - val_mse: 0.1522\n",
            "\n",
            "Epoch 00366: loss improved from 0.02099 to 0.02095, saving model to poids_train.hdf5\n",
            "Epoch 367/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0210 - mse: 0.0210 - val_loss: 0.1523 - val_mse: 0.1523\n",
            "\n",
            "Epoch 00367: loss improved from 0.02095 to 0.02091, saving model to poids_train.hdf5\n",
            "Epoch 368/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0210 - mse: 0.0210 - val_loss: 0.1525 - val_mse: 0.1525\n",
            "\n",
            "Epoch 00368: loss improved from 0.02091 to 0.02087, saving model to poids_train.hdf5\n",
            "Epoch 369/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0209 - mse: 0.0209 - val_loss: 0.1526 - val_mse: 0.1526\n",
            "\n",
            "Epoch 00369: loss improved from 0.02087 to 0.02083, saving model to poids_train.hdf5\n",
            "Epoch 370/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0209 - mse: 0.0209 - val_loss: 0.1527 - val_mse: 0.1527\n",
            "\n",
            "Epoch 00370: loss improved from 0.02083 to 0.02079, saving model to poids_train.hdf5\n",
            "Epoch 371/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0208 - mse: 0.0208 - val_loss: 0.1529 - val_mse: 0.1529\n",
            "\n",
            "Epoch 00371: loss improved from 0.02079 to 0.02075, saving model to poids_train.hdf5\n",
            "Epoch 372/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0208 - mse: 0.0208 - val_loss: 0.1530 - val_mse: 0.1530\n",
            "\n",
            "Epoch 00372: loss improved from 0.02075 to 0.02071, saving model to poids_train.hdf5\n",
            "Epoch 373/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0208 - mse: 0.0208 - val_loss: 0.1532 - val_mse: 0.1532\n",
            "\n",
            "Epoch 00373: loss improved from 0.02071 to 0.02067, saving model to poids_train.hdf5\n",
            "Epoch 374/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0207 - mse: 0.0207 - val_loss: 0.1533 - val_mse: 0.1533\n",
            "\n",
            "Epoch 00374: loss improved from 0.02067 to 0.02063, saving model to poids_train.hdf5\n",
            "Epoch 375/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0207 - mse: 0.0207 - val_loss: 0.1535 - val_mse: 0.1535\n",
            "\n",
            "Epoch 00375: loss improved from 0.02063 to 0.02059, saving model to poids_train.hdf5\n",
            "Epoch 376/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0206 - mse: 0.0206 - val_loss: 0.1536 - val_mse: 0.1536\n",
            "\n",
            "Epoch 00376: loss improved from 0.02059 to 0.02055, saving model to poids_train.hdf5\n",
            "Epoch 377/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0206 - mse: 0.0206 - val_loss: 0.1538 - val_mse: 0.1538\n",
            "\n",
            "Epoch 00377: loss improved from 0.02055 to 0.02052, saving model to poids_train.hdf5\n",
            "Epoch 378/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0205 - mse: 0.0205 - val_loss: 0.1539 - val_mse: 0.1539\n",
            "\n",
            "Epoch 00378: loss improved from 0.02052 to 0.02048, saving model to poids_train.hdf5\n",
            "Epoch 379/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0205 - mse: 0.0205 - val_loss: 0.1541 - val_mse: 0.1541\n",
            "\n",
            "Epoch 00379: loss improved from 0.02048 to 0.02044, saving model to poids_train.hdf5\n",
            "Epoch 380/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0205 - mse: 0.0205 - val_loss: 0.1543 - val_mse: 0.1543\n",
            "\n",
            "Epoch 00380: loss improved from 0.02044 to 0.02040, saving model to poids_train.hdf5\n",
            "Epoch 381/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0204 - mse: 0.0204 - val_loss: 0.1544 - val_mse: 0.1544\n",
            "\n",
            "Epoch 00381: loss improved from 0.02040 to 0.02036, saving model to poids_train.hdf5\n",
            "Epoch 382/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0204 - mse: 0.0204 - val_loss: 0.1546 - val_mse: 0.1546\n",
            "\n",
            "Epoch 00382: loss improved from 0.02036 to 0.02033, saving model to poids_train.hdf5\n",
            "Epoch 383/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0203 - mse: 0.0203 - val_loss: 0.1547 - val_mse: 0.1547\n",
            "\n",
            "Epoch 00383: loss improved from 0.02033 to 0.02029, saving model to poids_train.hdf5\n",
            "Epoch 384/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0203 - mse: 0.0203 - val_loss: 0.1549 - val_mse: 0.1549\n",
            "\n",
            "Epoch 00384: loss improved from 0.02029 to 0.02025, saving model to poids_train.hdf5\n",
            "Epoch 385/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0203 - mse: 0.0203 - val_loss: 0.1551 - val_mse: 0.1551\n",
            "\n",
            "Epoch 00385: loss improved from 0.02025 to 0.02021, saving model to poids_train.hdf5\n",
            "Epoch 386/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0202 - mse: 0.0202 - val_loss: 0.1552 - val_mse: 0.1552\n",
            "\n",
            "Epoch 00386: loss improved from 0.02021 to 0.02018, saving model to poids_train.hdf5\n",
            "Epoch 387/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0202 - mse: 0.0202 - val_loss: 0.1554 - val_mse: 0.1554\n",
            "\n",
            "Epoch 00387: loss improved from 0.02018 to 0.02014, saving model to poids_train.hdf5\n",
            "Epoch 388/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0201 - mse: 0.0201 - val_loss: 0.1556 - val_mse: 0.1556\n",
            "\n",
            "Epoch 00388: loss improved from 0.02014 to 0.02010, saving model to poids_train.hdf5\n",
            "Epoch 389/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0201 - mse: 0.0201 - val_loss: 0.1557 - val_mse: 0.1557\n",
            "\n",
            "Epoch 00389: loss improved from 0.02010 to 0.02006, saving model to poids_train.hdf5\n",
            "Epoch 390/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0200 - mse: 0.0200 - val_loss: 0.1559 - val_mse: 0.1559\n",
            "\n",
            "Epoch 00390: loss improved from 0.02006 to 0.02003, saving model to poids_train.hdf5\n",
            "Epoch 391/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0200 - mse: 0.0200 - val_loss: 0.1561 - val_mse: 0.1561\n",
            "\n",
            "Epoch 00391: loss improved from 0.02003 to 0.01999, saving model to poids_train.hdf5\n",
            "Epoch 392/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0200 - mse: 0.0200 - val_loss: 0.1562 - val_mse: 0.1562\n",
            "\n",
            "Epoch 00392: loss improved from 0.01999 to 0.01995, saving model to poids_train.hdf5\n",
            "Epoch 393/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0199 - mse: 0.0199 - val_loss: 0.1564 - val_mse: 0.1564\n",
            "\n",
            "Epoch 00393: loss improved from 0.01995 to 0.01992, saving model to poids_train.hdf5\n",
            "Epoch 394/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0199 - mse: 0.0199 - val_loss: 0.1566 - val_mse: 0.1566\n",
            "\n",
            "Epoch 00394: loss improved from 0.01992 to 0.01988, saving model to poids_train.hdf5\n",
            "Epoch 395/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0198 - mse: 0.0198 - val_loss: 0.1568 - val_mse: 0.1568\n",
            "\n",
            "Epoch 00395: loss improved from 0.01988 to 0.01984, saving model to poids_train.hdf5\n",
            "Epoch 396/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0198 - mse: 0.0198 - val_loss: 0.1569 - val_mse: 0.1569\n",
            "\n",
            "Epoch 00396: loss improved from 0.01984 to 0.01981, saving model to poids_train.hdf5\n",
            "Epoch 397/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0198 - mse: 0.0198 - val_loss: 0.1571 - val_mse: 0.1571\n",
            "\n",
            "Epoch 00397: loss improved from 0.01981 to 0.01977, saving model to poids_train.hdf5\n",
            "Epoch 398/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0197 - mse: 0.0197 - val_loss: 0.1573 - val_mse: 0.1573\n",
            "\n",
            "Epoch 00398: loss improved from 0.01977 to 0.01974, saving model to poids_train.hdf5\n",
            "Epoch 399/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0197 - mse: 0.0197 - val_loss: 0.1575 - val_mse: 0.1575\n",
            "\n",
            "Epoch 00399: loss improved from 0.01974 to 0.01970, saving model to poids_train.hdf5\n",
            "Epoch 400/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0197 - mse: 0.0197 - val_loss: 0.1577 - val_mse: 0.1577\n",
            "\n",
            "Epoch 00400: loss improved from 0.01970 to 0.01966, saving model to poids_train.hdf5\n",
            "Epoch 401/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0196 - mse: 0.0196 - val_loss: 0.1578 - val_mse: 0.1578\n",
            "\n",
            "Epoch 00401: loss improved from 0.01966 to 0.01963, saving model to poids_train.hdf5\n",
            "Epoch 402/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0196 - mse: 0.0196 - val_loss: 0.1580 - val_mse: 0.1580\n",
            "\n",
            "Epoch 00402: loss improved from 0.01963 to 0.01959, saving model to poids_train.hdf5\n",
            "Epoch 403/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0195 - mse: 0.0195 - val_loss: 0.1582 - val_mse: 0.1582\n",
            "\n",
            "Epoch 00403: loss improved from 0.01959 to 0.01956, saving model to poids_train.hdf5\n",
            "Epoch 404/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0195 - mse: 0.0195 - val_loss: 0.1584 - val_mse: 0.1584\n",
            "\n",
            "Epoch 00404: loss improved from 0.01956 to 0.01952, saving model to poids_train.hdf5\n",
            "Epoch 405/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0195 - mse: 0.0195 - val_loss: 0.1586 - val_mse: 0.1586\n",
            "\n",
            "Epoch 00405: loss improved from 0.01952 to 0.01949, saving model to poids_train.hdf5\n",
            "Epoch 406/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0194 - mse: 0.0194 - val_loss: 0.1588 - val_mse: 0.1588\n",
            "\n",
            "Epoch 00406: loss improved from 0.01949 to 0.01945, saving model to poids_train.hdf5\n",
            "Epoch 407/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0194 - mse: 0.0194 - val_loss: 0.1590 - val_mse: 0.1590\n",
            "\n",
            "Epoch 00407: loss improved from 0.01945 to 0.01942, saving model to poids_train.hdf5\n",
            "Epoch 408/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0193 - mse: 0.0193 - val_loss: 0.1592 - val_mse: 0.1592\n",
            "\n",
            "Epoch 00408: loss improved from 0.01942 to 0.01938, saving model to poids_train.hdf5\n",
            "Epoch 409/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0193 - mse: 0.0193 - val_loss: 0.1594 - val_mse: 0.1594\n",
            "\n",
            "Epoch 00409: loss improved from 0.01938 to 0.01935, saving model to poids_train.hdf5\n",
            "Epoch 410/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0193 - mse: 0.0193 - val_loss: 0.1596 - val_mse: 0.1596\n",
            "\n",
            "Epoch 00410: loss improved from 0.01935 to 0.01931, saving model to poids_train.hdf5\n",
            "Epoch 411/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0192 - mse: 0.0192 - val_loss: 0.1597 - val_mse: 0.1597\n",
            "\n",
            "Epoch 00411: loss improved from 0.01931 to 0.01928, saving model to poids_train.hdf5\n",
            "Epoch 412/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0192 - mse: 0.0192 - val_loss: 0.1599 - val_mse: 0.1599\n",
            "\n",
            "Epoch 00412: loss improved from 0.01928 to 0.01924, saving model to poids_train.hdf5\n",
            "Epoch 413/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0192 - mse: 0.0192 - val_loss: 0.1601 - val_mse: 0.1601\n",
            "\n",
            "Epoch 00413: loss improved from 0.01924 to 0.01921, saving model to poids_train.hdf5\n",
            "Epoch 414/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0191 - mse: 0.0191 - val_loss: 0.1603 - val_mse: 0.1603\n",
            "\n",
            "Epoch 00414: loss improved from 0.01921 to 0.01918, saving model to poids_train.hdf5\n",
            "Epoch 415/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0191 - mse: 0.0191 - val_loss: 0.1605 - val_mse: 0.1605\n",
            "\n",
            "Epoch 00415: loss improved from 0.01918 to 0.01914, saving model to poids_train.hdf5\n",
            "Epoch 416/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0190 - mse: 0.0190 - val_loss: 0.1607 - val_mse: 0.1607\n",
            "\n",
            "Epoch 00416: loss improved from 0.01914 to 0.01911, saving model to poids_train.hdf5\n",
            "Epoch 417/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0190 - mse: 0.0190 - val_loss: 0.1609 - val_mse: 0.1609\n",
            "\n",
            "Epoch 00417: loss improved from 0.01911 to 0.01907, saving model to poids_train.hdf5\n",
            "Epoch 418/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0190 - mse: 0.0190 - val_loss: 0.1611 - val_mse: 0.1611\n",
            "\n",
            "Epoch 00418: loss improved from 0.01907 to 0.01904, saving model to poids_train.hdf5\n",
            "Epoch 419/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0189 - mse: 0.0189 - val_loss: 0.1614 - val_mse: 0.1614\n",
            "\n",
            "Epoch 00419: loss improved from 0.01904 to 0.01901, saving model to poids_train.hdf5\n",
            "Epoch 420/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0189 - mse: 0.0189 - val_loss: 0.1616 - val_mse: 0.1616\n",
            "\n",
            "Epoch 00420: loss improved from 0.01901 to 0.01897, saving model to poids_train.hdf5\n",
            "Epoch 421/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0189 - mse: 0.0189 - val_loss: 0.1618 - val_mse: 0.1618\n",
            "\n",
            "Epoch 00421: loss improved from 0.01897 to 0.01894, saving model to poids_train.hdf5\n",
            "Epoch 422/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0188 - mse: 0.0188 - val_loss: 0.1620 - val_mse: 0.1620\n",
            "\n",
            "Epoch 00422: loss improved from 0.01894 to 0.01891, saving model to poids_train.hdf5\n",
            "Epoch 423/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0188 - mse: 0.0188 - val_loss: 0.1622 - val_mse: 0.1622\n",
            "\n",
            "Epoch 00423: loss improved from 0.01891 to 0.01887, saving model to poids_train.hdf5\n",
            "Epoch 424/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0187 - mse: 0.0187 - val_loss: 0.1624 - val_mse: 0.1624\n",
            "\n",
            "Epoch 00424: loss improved from 0.01887 to 0.01884, saving model to poids_train.hdf5\n",
            "Epoch 425/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0187 - mse: 0.0187 - val_loss: 0.1626 - val_mse: 0.1626\n",
            "\n",
            "Epoch 00425: loss improved from 0.01884 to 0.01881, saving model to poids_train.hdf5\n",
            "Epoch 426/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0187 - mse: 0.0187 - val_loss: 0.1628 - val_mse: 0.1628\n",
            "\n",
            "Epoch 00426: loss improved from 0.01881 to 0.01877, saving model to poids_train.hdf5\n",
            "Epoch 427/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0186 - mse: 0.0186 - val_loss: 0.1630 - val_mse: 0.1630\n",
            "\n",
            "Epoch 00427: loss improved from 0.01877 to 0.01874, saving model to poids_train.hdf5\n",
            "Epoch 428/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0186 - mse: 0.0186 - val_loss: 0.1632 - val_mse: 0.1632\n",
            "\n",
            "Epoch 00428: loss improved from 0.01874 to 0.01871, saving model to poids_train.hdf5\n",
            "Epoch 429/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0186 - mse: 0.0186 - val_loss: 0.1635 - val_mse: 0.1635\n",
            "\n",
            "Epoch 00429: loss improved from 0.01871 to 0.01868, saving model to poids_train.hdf5\n",
            "Epoch 430/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0185 - mse: 0.0185 - val_loss: 0.1637 - val_mse: 0.1637\n",
            "\n",
            "Epoch 00430: loss improved from 0.01868 to 0.01864, saving model to poids_train.hdf5\n",
            "Epoch 431/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0185 - mse: 0.0185 - val_loss: 0.1639 - val_mse: 0.1639\n",
            "\n",
            "Epoch 00431: loss improved from 0.01864 to 0.01861, saving model to poids_train.hdf5\n",
            "Epoch 432/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0185 - mse: 0.0185 - val_loss: 0.1641 - val_mse: 0.1641\n",
            "\n",
            "Epoch 00432: loss improved from 0.01861 to 0.01858, saving model to poids_train.hdf5\n",
            "Epoch 433/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0184 - mse: 0.0184 - val_loss: 0.1643 - val_mse: 0.1643\n",
            "\n",
            "Epoch 00433: loss improved from 0.01858 to 0.01855, saving model to poids_train.hdf5\n",
            "Epoch 434/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0184 - mse: 0.0184 - val_loss: 0.1646 - val_mse: 0.1646\n",
            "\n",
            "Epoch 00434: loss improved from 0.01855 to 0.01852, saving model to poids_train.hdf5\n",
            "Epoch 435/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0184 - mse: 0.0184 - val_loss: 0.1648 - val_mse: 0.1648\n",
            "\n",
            "Epoch 00435: loss improved from 0.01852 to 0.01848, saving model to poids_train.hdf5\n",
            "Epoch 436/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0183 - mse: 0.0183 - val_loss: 0.1650 - val_mse: 0.1650\n",
            "\n",
            "Epoch 00436: loss improved from 0.01848 to 0.01845, saving model to poids_train.hdf5\n",
            "Epoch 437/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0183 - mse: 0.0183 - val_loss: 0.1652 - val_mse: 0.1652\n",
            "\n",
            "Epoch 00437: loss improved from 0.01845 to 0.01842, saving model to poids_train.hdf5\n",
            "Epoch 438/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0183 - mse: 0.0183 - val_loss: 0.1655 - val_mse: 0.1655\n",
            "\n",
            "Epoch 00438: loss improved from 0.01842 to 0.01839, saving model to poids_train.hdf5\n",
            "Epoch 439/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0182 - mse: 0.0182 - val_loss: 0.1657 - val_mse: 0.1657\n",
            "\n",
            "Epoch 00439: loss improved from 0.01839 to 0.01836, saving model to poids_train.hdf5\n",
            "Epoch 440/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0182 - mse: 0.0182 - val_loss: 0.1659 - val_mse: 0.1659\n",
            "\n",
            "Epoch 00440: loss improved from 0.01836 to 0.01833, saving model to poids_train.hdf5\n",
            "Epoch 441/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0182 - mse: 0.0182 - val_loss: 0.1661 - val_mse: 0.1661\n",
            "\n",
            "Epoch 00441: loss improved from 0.01833 to 0.01829, saving model to poids_train.hdf5\n",
            "Epoch 442/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0181 - mse: 0.0181 - val_loss: 0.1664 - val_mse: 0.1664\n",
            "\n",
            "Epoch 00442: loss improved from 0.01829 to 0.01826, saving model to poids_train.hdf5\n",
            "Epoch 443/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0181 - mse: 0.0181 - val_loss: 0.1666 - val_mse: 0.1666\n",
            "\n",
            "Epoch 00443: loss improved from 0.01826 to 0.01823, saving model to poids_train.hdf5\n",
            "Epoch 444/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0181 - mse: 0.0181 - val_loss: 0.1668 - val_mse: 0.1668\n",
            "\n",
            "Epoch 00444: loss improved from 0.01823 to 0.01820, saving model to poids_train.hdf5\n",
            "Epoch 445/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0180 - mse: 0.0180 - val_loss: 0.1671 - val_mse: 0.1671\n",
            "\n",
            "Epoch 00445: loss improved from 0.01820 to 0.01817, saving model to poids_train.hdf5\n",
            "Epoch 446/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0180 - mse: 0.0180 - val_loss: 0.1673 - val_mse: 0.1673\n",
            "\n",
            "Epoch 00446: loss improved from 0.01817 to 0.01814, saving model to poids_train.hdf5\n",
            "Epoch 447/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0180 - mse: 0.0180 - val_loss: 0.1675 - val_mse: 0.1675\n",
            "\n",
            "Epoch 00447: loss improved from 0.01814 to 0.01811, saving model to poids_train.hdf5\n",
            "Epoch 448/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0179 - mse: 0.0179 - val_loss: 0.1678 - val_mse: 0.1678\n",
            "\n",
            "Epoch 00448: loss improved from 0.01811 to 0.01808, saving model to poids_train.hdf5\n",
            "Epoch 449/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0179 - mse: 0.0179 - val_loss: 0.1680 - val_mse: 0.1680\n",
            "\n",
            "Epoch 00449: loss improved from 0.01808 to 0.01805, saving model to poids_train.hdf5\n",
            "Epoch 450/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0179 - mse: 0.0179 - val_loss: 0.1682 - val_mse: 0.1682\n",
            "\n",
            "Epoch 00450: loss improved from 0.01805 to 0.01802, saving model to poids_train.hdf5\n",
            "Epoch 451/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0178 - mse: 0.0178 - val_loss: 0.1685 - val_mse: 0.1685\n",
            "\n",
            "Epoch 00451: loss improved from 0.01802 to 0.01799, saving model to poids_train.hdf5\n",
            "Epoch 452/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0178 - mse: 0.0178 - val_loss: 0.1687 - val_mse: 0.1687\n",
            "\n",
            "Epoch 00452: loss improved from 0.01799 to 0.01796, saving model to poids_train.hdf5\n",
            "Epoch 453/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0178 - mse: 0.0178 - val_loss: 0.1690 - val_mse: 0.1690\n",
            "\n",
            "Epoch 00453: loss improved from 0.01796 to 0.01793, saving model to poids_train.hdf5\n",
            "Epoch 454/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0177 - mse: 0.0177 - val_loss: 0.1692 - val_mse: 0.1692\n",
            "\n",
            "Epoch 00454: loss improved from 0.01793 to 0.01790, saving model to poids_train.hdf5\n",
            "Epoch 455/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0177 - mse: 0.0177 - val_loss: 0.1694 - val_mse: 0.1694\n",
            "\n",
            "Epoch 00455: loss improved from 0.01790 to 0.01787, saving model to poids_train.hdf5\n",
            "Epoch 456/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0177 - mse: 0.0177 - val_loss: 0.1697 - val_mse: 0.1697\n",
            "\n",
            "Epoch 00456: loss improved from 0.01787 to 0.01784, saving model to poids_train.hdf5\n",
            "Epoch 457/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0176 - mse: 0.0176 - val_loss: 0.1699 - val_mse: 0.1699\n",
            "\n",
            "Epoch 00457: loss improved from 0.01784 to 0.01781, saving model to poids_train.hdf5\n",
            "Epoch 458/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0176 - mse: 0.0176 - val_loss: 0.1702 - val_mse: 0.1702\n",
            "\n",
            "Epoch 00458: loss improved from 0.01781 to 0.01778, saving model to poids_train.hdf5\n",
            "Epoch 459/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0176 - mse: 0.0176 - val_loss: 0.1704 - val_mse: 0.1704\n",
            "\n",
            "Epoch 00459: loss improved from 0.01778 to 0.01775, saving model to poids_train.hdf5\n",
            "Epoch 460/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.1707 - val_mse: 0.1707\n",
            "\n",
            "Epoch 00460: loss improved from 0.01775 to 0.01772, saving model to poids_train.hdf5\n",
            "Epoch 461/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.1709 - val_mse: 0.1709\n",
            "\n",
            "Epoch 00461: loss improved from 0.01772 to 0.01769, saving model to poids_train.hdf5\n",
            "Epoch 462/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.1712 - val_mse: 0.1712\n",
            "\n",
            "Epoch 00462: loss improved from 0.01769 to 0.01766, saving model to poids_train.hdf5\n",
            "Epoch 463/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.1714 - val_mse: 0.1714\n",
            "\n",
            "Epoch 00463: loss improved from 0.01766 to 0.01763, saving model to poids_train.hdf5\n",
            "Epoch 464/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.1717 - val_mse: 0.1717\n",
            "\n",
            "Epoch 00464: loss improved from 0.01763 to 0.01761, saving model to poids_train.hdf5\n",
            "Epoch 465/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.1719 - val_mse: 0.1719\n",
            "\n",
            "Epoch 00465: loss improved from 0.01761 to 0.01758, saving model to poids_train.hdf5\n",
            "Epoch 466/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.1722 - val_mse: 0.1722\n",
            "\n",
            "Epoch 00466: loss improved from 0.01758 to 0.01755, saving model to poids_train.hdf5\n",
            "Epoch 467/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.1724 - val_mse: 0.1724\n",
            "\n",
            "Epoch 00467: loss improved from 0.01755 to 0.01752, saving model to poids_train.hdf5\n",
            "Epoch 468/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.1727 - val_mse: 0.1727\n",
            "\n",
            "Epoch 00468: loss improved from 0.01752 to 0.01749, saving model to poids_train.hdf5\n",
            "Epoch 469/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0172 - mse: 0.0172 - val_loss: 0.1729 - val_mse: 0.1729\n",
            "\n",
            "Epoch 00469: loss improved from 0.01749 to 0.01746, saving model to poids_train.hdf5\n",
            "Epoch 470/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0172 - mse: 0.0172 - val_loss: 0.1732 - val_mse: 0.1732\n",
            "\n",
            "Epoch 00470: loss improved from 0.01746 to 0.01743, saving model to poids_train.hdf5\n",
            "Epoch 471/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0172 - mse: 0.0172 - val_loss: 0.1734 - val_mse: 0.1734\n",
            "\n",
            "Epoch 00471: loss improved from 0.01743 to 0.01741, saving model to poids_train.hdf5\n",
            "Epoch 472/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0172 - mse: 0.0172 - val_loss: 0.1737 - val_mse: 0.1737\n",
            "\n",
            "Epoch 00472: loss improved from 0.01741 to 0.01738, saving model to poids_train.hdf5\n",
            "Epoch 473/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0171 - mse: 0.0171 - val_loss: 0.1739 - val_mse: 0.1739\n",
            "\n",
            "Epoch 00473: loss improved from 0.01738 to 0.01735, saving model to poids_train.hdf5\n",
            "Epoch 474/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0171 - mse: 0.0171 - val_loss: 0.1742 - val_mse: 0.1742\n",
            "\n",
            "Epoch 00474: loss improved from 0.01735 to 0.01732, saving model to poids_train.hdf5\n",
            "Epoch 475/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0171 - mse: 0.0171 - val_loss: 0.1745 - val_mse: 0.1745\n",
            "\n",
            "Epoch 00475: loss improved from 0.01732 to 0.01730, saving model to poids_train.hdf5\n",
            "Epoch 476/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0170 - mse: 0.0170 - val_loss: 0.1747 - val_mse: 0.1747\n",
            "\n",
            "Epoch 00476: loss improved from 0.01730 to 0.01727, saving model to poids_train.hdf5\n",
            "Epoch 477/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0170 - mse: 0.0170 - val_loss: 0.1750 - val_mse: 0.1750\n",
            "\n",
            "Epoch 00477: loss improved from 0.01727 to 0.01724, saving model to poids_train.hdf5\n",
            "Epoch 478/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0170 - mse: 0.0170 - val_loss: 0.1752 - val_mse: 0.1752\n",
            "\n",
            "Epoch 00478: loss improved from 0.01724 to 0.01721, saving model to poids_train.hdf5\n",
            "Epoch 479/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0169 - mse: 0.0169 - val_loss: 0.1755 - val_mse: 0.1755\n",
            "\n",
            "Epoch 00479: loss improved from 0.01721 to 0.01719, saving model to poids_train.hdf5\n",
            "Epoch 480/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0169 - mse: 0.0169 - val_loss: 0.1758 - val_mse: 0.1758\n",
            "\n",
            "Epoch 00480: loss improved from 0.01719 to 0.01716, saving model to poids_train.hdf5\n",
            "Epoch 481/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0169 - mse: 0.0169 - val_loss: 0.1760 - val_mse: 0.1760\n",
            "\n",
            "Epoch 00481: loss improved from 0.01716 to 0.01713, saving model to poids_train.hdf5\n",
            "Epoch 482/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0169 - mse: 0.0169 - val_loss: 0.1763 - val_mse: 0.1763\n",
            "\n",
            "Epoch 00482: loss improved from 0.01713 to 0.01710, saving model to poids_train.hdf5\n",
            "Epoch 483/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0168 - mse: 0.0168 - val_loss: 0.1766 - val_mse: 0.1766\n",
            "\n",
            "Epoch 00483: loss improved from 0.01710 to 0.01708, saving model to poids_train.hdf5\n",
            "Epoch 484/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0168 - mse: 0.0168 - val_loss: 0.1768 - val_mse: 0.1768\n",
            "\n",
            "Epoch 00484: loss improved from 0.01708 to 0.01705, saving model to poids_train.hdf5\n",
            "Epoch 485/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0168 - mse: 0.0168 - val_loss: 0.1771 - val_mse: 0.1771\n",
            "\n",
            "Epoch 00485: loss improved from 0.01705 to 0.01702, saving model to poids_train.hdf5\n",
            "Epoch 486/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0167 - mse: 0.0167 - val_loss: 0.1774 - val_mse: 0.1774\n",
            "\n",
            "Epoch 00486: loss improved from 0.01702 to 0.01700, saving model to poids_train.hdf5\n",
            "Epoch 487/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0167 - mse: 0.0167 - val_loss: 0.1776 - val_mse: 0.1776\n",
            "\n",
            "Epoch 00487: loss improved from 0.01700 to 0.01697, saving model to poids_train.hdf5\n",
            "Epoch 488/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0167 - mse: 0.0167 - val_loss: 0.1779 - val_mse: 0.1779\n",
            "\n",
            "Epoch 00488: loss improved from 0.01697 to 0.01695, saving model to poids_train.hdf5\n",
            "Epoch 489/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0167 - mse: 0.0167 - val_loss: 0.1782 - val_mse: 0.1782\n",
            "\n",
            "Epoch 00489: loss improved from 0.01695 to 0.01692, saving model to poids_train.hdf5\n",
            "Epoch 490/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0166 - mse: 0.0166 - val_loss: 0.1784 - val_mse: 0.1784\n",
            "\n",
            "Epoch 00490: loss improved from 0.01692 to 0.01689, saving model to poids_train.hdf5\n",
            "Epoch 491/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0166 - mse: 0.0166 - val_loss: 0.1787 - val_mse: 0.1787\n",
            "\n",
            "Epoch 00491: loss improved from 0.01689 to 0.01687, saving model to poids_train.hdf5\n",
            "Epoch 492/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0166 - mse: 0.0166 - val_loss: 0.1790 - val_mse: 0.1790\n",
            "\n",
            "Epoch 00492: loss improved from 0.01687 to 0.01684, saving model to poids_train.hdf5\n",
            "Epoch 493/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0165 - mse: 0.0165 - val_loss: 0.1792 - val_mse: 0.1792\n",
            "\n",
            "Epoch 00493: loss improved from 0.01684 to 0.01682, saving model to poids_train.hdf5\n",
            "Epoch 494/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0165 - mse: 0.0165 - val_loss: 0.1795 - val_mse: 0.1795\n",
            "\n",
            "Epoch 00494: loss improved from 0.01682 to 0.01679, saving model to poids_train.hdf5\n",
            "Epoch 495/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0165 - mse: 0.0165 - val_loss: 0.1798 - val_mse: 0.1798\n",
            "\n",
            "Epoch 00495: loss improved from 0.01679 to 0.01676, saving model to poids_train.hdf5\n",
            "Epoch 496/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0165 - mse: 0.0165 - val_loss: 0.1800 - val_mse: 0.1800\n",
            "\n",
            "Epoch 00496: loss improved from 0.01676 to 0.01674, saving model to poids_train.hdf5\n",
            "Epoch 497/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0164 - mse: 0.0164 - val_loss: 0.1803 - val_mse: 0.1803\n",
            "\n",
            "Epoch 00497: loss improved from 0.01674 to 0.01671, saving model to poids_train.hdf5\n",
            "Epoch 498/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0164 - mse: 0.0164 - val_loss: 0.1806 - val_mse: 0.1806\n",
            "\n",
            "Epoch 00498: loss improved from 0.01671 to 0.01669, saving model to poids_train.hdf5\n",
            "Epoch 499/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0164 - mse: 0.0164 - val_loss: 0.1809 - val_mse: 0.1809\n",
            "\n",
            "Epoch 00499: loss improved from 0.01669 to 0.01666, saving model to poids_train.hdf5\n",
            "Epoch 500/500\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0164 - mse: 0.0164 - val_loss: 0.1811 - val_mse: 0.1811\n",
            "\n",
            "Epoch 00500: loss improved from 0.01666 to 0.01664, saving model to poids_train.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfbomV0LS9LD"
      },
      "source": [
        "model.load_weights(\"poids_train.hdf5\")"
      ],
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MDY8O1-l6kN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "9896e180-02e2-4599-cd5a-3eb0ed417a1a"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "#erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "#plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, \"Evolution de l'erreur en fonction de la période\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 192
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEuSDQ6vZnBm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eba908bd-1930-4fff-c804-57efe008fa53"
      },
      "source": [
        "# Evaluation du modèle\n",
        "\n",
        "model.evaluate(dataset)\n",
        "model.evaluate(dataset_val)\n"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "186/186 [==============================] - 0s 2ms/step - loss: 0.0153 - mse: 0.0153\n",
            "187/187 [==============================] - 0s 2ms/step - loss: 0.1811 - mse: 0.1811\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.18114088475704193, 0.18114088475704193]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbzro22hgt4b"
      },
      "source": [
        "**3. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMmVn1e5zEAm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8764d51d-6feb-44e1-b03d-e50e384898ed"
      },
      "source": [
        "# Création des instants d'entrainement et de validation\n",
        "y_train_timing = serie_entrainement.index[taille_fenetre + horizon-1:]\n",
        "y_val_timing = serie_test.index[taille_fenetre + horizon-1:]\n",
        "\n",
        "# Calcul des prédictions\n",
        "pred_ent = model.predict(x_train, verbose=1)\n",
        "pred_val = model.predict(x_val, verbose=1)"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6/6 [==============================] - 0s 2ms/step\n",
            "6/6 [==============================] - 0s 2ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "8ZklNPJ5NAQv",
        "outputId": "ff77eea3-f22a-4ae1-bf6b-953c28b2d422"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "tmax = len(np.asarray(predictions)[:,0])\n",
        "\n",
        "\n",
        "fig.add_trace(go.Scatter(x=serie_entrainement.index,y=serie_entrainement,line=dict(color='blue', width=1),name=\"true\"))\n",
        "fig.add_trace(go.Scatter(x=serie_test.index,y=serie_test,line=dict(color='green', width=1),name=\"true\"))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=y_train_timing,y=pred_ent[:,0],line=dict(color='red', width=1),name=\"Prédiction\"))\n",
        "fig.add_trace(go.Scatter(x=y_val_timing,y=pred_val[:,0],line=dict(color='red', width=1)))\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"aefbc7b2-e16c-4b3e-8952-f53a65c7afa6\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"aefbc7b2-e16c-4b3e-8952-f53a65c7afa6\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'aefbc7b2-e16c-4b3e-8952-f53a65c7afa6',\n",
              "                        [{\"line\": {\"color\": \"blue\", \"width\": 1}, \"name\": \"true\", \"type\": \"scatter\", \"x\": [\"2020-03-18T00:00:00\", \"2020-03-19T00:00:00\", \"2020-03-20T00:00:00\", \"2020-03-21T00:00:00\", \"2020-03-22T00:00:00\", \"2020-03-23T00:00:00\", \"2020-03-24T00:00:00\", \"2020-03-25T00:00:00\", \"2020-03-26T00:00:00\", \"2020-03-27T00:00:00\", \"2020-03-28T00:00:00\", \"2020-03-29T00:00:00\", \"2020-03-30T00:00:00\", \"2020-03-31T00:00:00\", \"2020-04-01T00:00:00\", \"2020-04-02T00:00:00\", \"2020-04-03T00:00:00\", \"2020-04-04T00:00:00\", \"2020-04-05T00:00:00\", \"2020-04-06T00:00:00\", \"2020-04-07T00:00:00\", \"2020-04-08T00:00:00\", \"2020-04-09T00:00:00\", \"2020-04-10T00:00:00\", \"2020-04-11T00:00:00\", \"2020-04-12T00:00:00\", \"2020-04-13T00:00:00\", \"2020-04-14T00:00:00\", \"2020-04-15T00:00:00\", \"2020-04-16T00:00:00\", \"2020-04-17T00:00:00\", \"2020-04-18T00:00:00\", \"2020-04-19T00:00:00\", \"2020-04-20T00:00:00\", \"2020-04-21T00:00:00\", \"2020-04-22T00:00:00\", \"2020-04-23T00:00:00\", \"2020-04-24T00:00:00\", \"2020-04-25T00:00:00\", \"2020-04-26T00:00:00\", \"2020-04-27T00:00:00\", \"2020-04-28T00:00:00\", \"2020-04-29T00:00:00\", \"2020-04-30T00:00:00\", \"2020-05-01T00:00:00\", \"2020-05-02T00:00:00\", \"2020-05-03T00:00:00\", \"2020-05-04T00:00:00\", \"2020-05-05T00:00:00\", \"2020-05-06T00:00:00\", \"2020-05-07T00:00:00\", \"2020-05-08T00:00:00\", \"2020-05-09T00:00:00\", \"2020-05-10T00:00:00\", \"2020-05-11T00:00:00\", \"2020-05-12T00:00:00\", \"2020-05-13T00:00:00\", \"2020-05-14T00:00:00\", \"2020-05-15T00:00:00\", \"2020-05-16T00:00:00\", \"2020-05-17T00:00:00\", \"2020-05-18T00:00:00\", \"2020-05-19T00:00:00\", \"2020-05-20T00:00:00\", \"2020-05-21T00:00:00\", \"2020-05-22T00:00:00\", \"2020-05-23T00:00:00\", \"2020-05-24T00:00:00\", \"2020-05-25T00:00:00\", \"2020-05-26T00:00:00\", \"2020-05-27T00:00:00\", \"2020-05-28T00:00:00\", \"2020-05-29T00:00:00\", \"2020-05-30T00:00:00\", \"2020-05-31T00:00:00\", \"2020-06-01T00:00:00\", \"2020-06-02T00:00:00\", \"2020-06-03T00:00:00\", \"2020-06-04T00:00:00\", \"2020-06-05T00:00:00\", \"2020-06-06T00:00:00\", \"2020-06-07T00:00:00\", \"2020-06-08T00:00:00\", \"2020-06-09T00:00:00\", \"2020-06-10T00:00:00\", \"2020-06-11T00:00:00\", \"2020-06-12T00:00:00\", \"2020-06-13T00:00:00\", \"2020-06-14T00:00:00\", \"2020-06-15T00:00:00\", \"2020-06-16T00:00:00\", \"2020-06-17T00:00:00\", \"2020-06-18T00:00:00\", \"2020-06-19T00:00:00\", \"2020-06-20T00:00:00\", \"2020-06-21T00:00:00\", \"2020-06-22T00:00:00\", \"2020-06-23T00:00:00\", \"2020-06-24T00:00:00\", \"2020-06-25T00:00:00\", \"2020-06-26T00:00:00\", \"2020-06-27T00:00:00\", \"2020-06-28T00:00:00\", \"2020-06-29T00:00:00\", \"2020-06-30T00:00:00\", \"2020-07-01T00:00:00\", \"2020-07-02T00:00:00\", \"2020-07-03T00:00:00\", \"2020-07-04T00:00:00\", \"2020-07-05T00:00:00\", \"2020-07-06T00:00:00\", \"2020-07-07T00:00:00\", \"2020-07-08T00:00:00\", \"2020-07-09T00:00:00\", \"2020-07-10T00:00:00\", \"2020-07-11T00:00:00\", \"2020-07-12T00:00:00\", \"2020-07-13T00:00:00\", \"2020-07-14T00:00:00\", \"2020-07-15T00:00:00\", \"2020-07-16T00:00:00\", \"2020-07-17T00:00:00\", \"2020-07-18T00:00:00\", \"2020-07-19T00:00:00\", \"2020-07-20T00:00:00\", \"2020-07-21T00:00:00\", \"2020-07-22T00:00:00\", \"2020-07-23T00:00:00\", \"2020-07-24T00:00:00\", \"2020-07-25T00:00:00\", \"2020-07-26T00:00:00\", \"2020-07-27T00:00:00\", \"2020-07-28T00:00:00\", \"2020-07-29T00:00:00\", \"2020-07-30T00:00:00\", \"2020-07-31T00:00:00\", \"2020-08-01T00:00:00\", \"2020-08-02T00:00:00\", \"2020-08-03T00:00:00\", \"2020-08-04T00:00:00\", \"2020-08-05T00:00:00\", \"2020-08-06T00:00:00\", \"2020-08-07T00:00:00\", \"2020-08-08T00:00:00\", \"2020-08-09T00:00:00\", \"2020-08-10T00:00:00\", \"2020-08-11T00:00:00\", \"2020-08-12T00:00:00\", \"2020-08-13T00:00:00\", \"2020-08-14T00:00:00\", \"2020-08-15T00:00:00\", \"2020-08-16T00:00:00\", \"2020-08-17T00:00:00\", \"2020-08-18T00:00:00\", \"2020-08-19T00:00:00\", \"2020-08-20T00:00:00\", \"2020-08-21T00:00:00\", \"2020-08-22T00:00:00\", \"2020-08-23T00:00:00\", \"2020-08-24T00:00:00\", \"2020-08-25T00:00:00\", \"2020-08-26T00:00:00\", \"2020-08-27T00:00:00\", \"2020-08-28T00:00:00\", \"2020-08-29T00:00:00\", \"2020-08-30T00:00:00\", \"2020-08-31T00:00:00\", \"2020-09-01T00:00:00\", \"2020-09-02T00:00:00\", \"2020-09-03T00:00:00\", \"2020-09-04T00:00:00\", \"2020-09-05T00:00:00\", \"2020-09-06T00:00:00\", \"2020-09-07T00:00:00\", \"2020-09-08T00:00:00\", \"2020-09-09T00:00:00\", \"2020-09-10T00:00:00\", \"2020-09-11T00:00:00\", \"2020-09-12T00:00:00\", \"2020-09-13T00:00:00\", \"2020-09-14T00:00:00\", \"2020-09-15T00:00:00\", \"2020-09-16T00:00:00\", \"2020-09-17T00:00:00\", \"2020-09-18T00:00:00\", \"2020-09-19T00:00:00\", \"2020-09-20T00:00:00\", \"2020-09-21T00:00:00\", \"2020-09-22T00:00:00\", \"2020-09-23T00:00:00\", \"2020-09-24T00:00:00\", \"2020-09-25T00:00:00\", \"2020-09-26T00:00:00\", \"2020-09-27T00:00:00\", \"2020-09-28T00:00:00\", \"2020-09-29T00:00:00\", \"2020-09-30T00:00:00\", \"2020-10-01T00:00:00\", \"2020-10-02T00:00:00\", \"2020-10-03T00:00:00\"], \"y\": [-0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.7101431396027159, -0.6496301051838075, -0.5549791147171378, -0.4453167764541238, -0.32929449021508855, -0.2994120875924591, -0.2972609287182167, -0.10650925376093122, 0.01807090365172089, 0.14120136704042696, 0.3405109131713292, 0.5106395215298984, 0.5825630508469627, 0.5925238517211724, 0.7875778444269429, 1.0929488759226706, 1.3733945230711975, 1.6275118561719297, 1.9699202317633109, 2.0830898942778076, 2.1165731497986253, 2.427649428743431, 2.701688363592583, 2.940841113220325, 3.1138223453034333, 3.1466041359739547, 3.173633914872045, 3.1771880034468802, 3.2796954002368706, 3.329499404607919, 3.1935555166204646, 3.0675724295072193, 2.8952926622744077, 2.8020446015083307, 2.776417752310833, 2.446869565641554, 2.0304332661822198, 1.396168748544156, 1.1427528802937197, 0.9292269798634756, 0.8573034505464114, 0.8494938085464443, 0.6402234615413323, 0.4173914607638217, 0.5910741576972264, 0.3654362975185302, 0.1846454234354548, 0.12057830044171103, 0.11061749956750125, -0.06591782109044254, -0.1833898013534239, -0.26667706124529067, -0.29585799901762366, -0.3464102325623223, -0.3342982728138699, -0.33715089653840896, -0.4090276615321201, -0.441061223028992, -0.4339530458793211, -0.5037254163221428, -0.5243484829208589, -0.5393130663938501, -0.5393130663938501, -0.5165388409208915, -0.48450527942401966, -0.5008727925976039, -0.486609673974909, -0.4830555854000736, -0.47454447854980986, -0.47309478452586384, -0.4773503379509957, -0.46313398365165404, -0.46028135992711505, -0.43325158102902456, -0.3620295165622569, -0.2873001278440069, -0.20901665055092136, -0.21967891627542763, -0.18479273105401672, -0.17342900047921397, -0.22608562857480202, -0.4076247318315271, -0.4702421608013248, -0.5428671549686854, -0.5257514126214518, -0.5528279558428953, -0.5214958591963199, -0.48590820912461263, -0.49091199172339406, -0.48165265569948074, -0.4759474082504027, -0.41898846240632986, -0.38410227718491896, -0.40046979035850316, -0.37128885258617017, -0.19405206707793016, -0.21041958025151433, -0.19760615565276554, -0.2360464294490118, -0.27163407952071905, -0.251011012922003, -0.25386363664654205, -0.26667706124529067, -0.2567162603710809, -0.28159488039492897, -0.24105021204779323, -0.21542336285029576, -0.21327220397605323, -0.19550176110187606, -0.19690469080246906, -0.17268077130556442, -0.16271997043135464, -0.1328375678087253, -0.10010254146155695, -0.10010254146155695, -0.10295516518609583, -0.0730259982401133, -0.06947190966527793, -0.07377422741376286, -0.06591782109044254, -0.08658765201251165, -0.04884884306656187, -0.030330171018735162, -0.04955030791685836, -0.07873124568919121, -0.08303356343767627, -0.1698749119043786, -0.1698749119043786, -0.20901665055092136, -0.251011012922003, -0.2823431095685785, -0.2830445744188749, -0.2823431095685785, -0.27238230869436875, -0.3115240473409114, -0.33785236138870545, -0.32288777791571416, -0.32503893678995666, -0.32003515419117523, -0.31362844189180084, -0.28159488039492897, -0.2709326146704227, -0.2360464294490118, -0.25386363664654205, -0.2396005180238472, -0.24105021204779323, -0.251011012922003, -0.30081501729305204, -0.33359680796357344, -0.3805481886100836, -0.4054735729572846, -0.42539517470570426, -0.42684486872965016, -0.4246937098554078, -0.41968992725662635, -0.38485050635856854]}, {\"line\": {\"color\": \"green\", \"width\": 1}, \"name\": \"true\", \"type\": \"scatter\", \"x\": [\"2020-10-04T00:00:00\", \"2020-10-05T00:00:00\", \"2020-10-06T00:00:00\", \"2020-10-07T00:00:00\", \"2020-10-08T00:00:00\", \"2020-10-09T00:00:00\", \"2020-10-10T00:00:00\", \"2020-10-11T00:00:00\", \"2020-10-12T00:00:00\", \"2020-10-13T00:00:00\", \"2020-10-14T00:00:00\", \"2020-10-15T00:00:00\", \"2020-10-16T00:00:00\", \"2020-10-17T00:00:00\", \"2020-10-18T00:00:00\", \"2020-10-19T00:00:00\", \"2020-10-20T00:00:00\", \"2020-10-21T00:00:00\", \"2020-10-22T00:00:00\", \"2020-10-23T00:00:00\", \"2020-10-24T00:00:00\", \"2020-10-25T00:00:00\", \"2020-10-26T00:00:00\", \"2020-10-27T00:00:00\", \"2020-10-28T00:00:00\", \"2020-10-29T00:00:00\", \"2020-10-30T00:00:00\", \"2020-10-31T00:00:00\", \"2020-11-01T00:00:00\", \"2020-11-02T00:00:00\", \"2020-11-03T00:00:00\", \"2020-11-04T00:00:00\", \"2020-11-05T00:00:00\", \"2020-11-06T00:00:00\", \"2020-11-07T00:00:00\", \"2020-11-08T00:00:00\", \"2020-11-09T00:00:00\", \"2020-11-10T00:00:00\", \"2020-11-11T00:00:00\", \"2020-11-12T00:00:00\", \"2020-11-13T00:00:00\", \"2020-11-14T00:00:00\", \"2020-11-15T00:00:00\", \"2020-11-16T00:00:00\", \"2020-11-17T00:00:00\", \"2020-11-18T00:00:00\", \"2020-11-19T00:00:00\", \"2020-11-20T00:00:00\", \"2020-11-21T00:00:00\", \"2020-11-22T00:00:00\", \"2020-11-23T00:00:00\", \"2020-11-24T00:00:00\", \"2020-11-25T00:00:00\", \"2020-11-26T00:00:00\", \"2020-11-27T00:00:00\", \"2020-11-28T00:00:00\", \"2020-11-29T00:00:00\", \"2020-11-30T00:00:00\", \"2020-12-01T00:00:00\", \"2020-12-02T00:00:00\", \"2020-12-03T00:00:00\", \"2020-12-04T00:00:00\", \"2020-12-05T00:00:00\", \"2020-12-06T00:00:00\", \"2020-12-07T00:00:00\", \"2020-12-08T00:00:00\", \"2020-12-09T00:00:00\", \"2020-12-10T00:00:00\", \"2020-12-11T00:00:00\", \"2020-12-12T00:00:00\", \"2020-12-13T00:00:00\", \"2020-12-14T00:00:00\", \"2020-12-15T00:00:00\", \"2020-12-16T00:00:00\", \"2020-12-17T00:00:00\", \"2020-12-18T00:00:00\", \"2020-12-19T00:00:00\", \"2020-12-20T00:00:00\", \"2020-12-21T00:00:00\", \"2020-12-22T00:00:00\", \"2020-12-23T00:00:00\", \"2020-12-24T00:00:00\", \"2020-12-25T00:00:00\", \"2020-12-26T00:00:00\", \"2020-12-27T00:00:00\", \"2020-12-28T00:00:00\", \"2020-12-29T00:00:00\", \"2020-12-30T00:00:00\", \"2020-12-31T00:00:00\", \"2021-01-01T00:00:00\", \"2021-01-02T00:00:00\", \"2021-01-03T00:00:00\", \"2021-01-04T00:00:00\", \"2021-01-05T00:00:00\", \"2021-01-06T00:00:00\", \"2021-01-07T00:00:00\", \"2021-01-08T00:00:00\", \"2021-01-09T00:00:00\", \"2021-01-10T00:00:00\", \"2021-01-11T00:00:00\", \"2021-01-12T00:00:00\", \"2021-01-13T00:00:00\", \"2021-01-14T00:00:00\", \"2021-01-15T00:00:00\", \"2021-01-16T00:00:00\", \"2021-01-17T00:00:00\", \"2021-01-18T00:00:00\", \"2021-01-19T00:00:00\", \"2021-01-20T00:00:00\", \"2021-01-21T00:00:00\", \"2021-01-22T00:00:00\", \"2021-01-23T00:00:00\", \"2021-01-24T00:00:00\", \"2021-01-25T00:00:00\", \"2021-01-26T00:00:00\", \"2021-01-27T00:00:00\", \"2021-01-28T00:00:00\", \"2021-01-29T00:00:00\", \"2021-01-30T00:00:00\", \"2021-01-31T00:00:00\", \"2021-02-01T00:00:00\", \"2021-02-02T00:00:00\", \"2021-02-03T00:00:00\", \"2021-02-04T00:00:00\", \"2021-02-05T00:00:00\", \"2021-02-06T00:00:00\", \"2021-02-07T00:00:00\", \"2021-02-08T00:00:00\", \"2021-02-09T00:00:00\", \"2021-02-10T00:00:00\", \"2021-02-11T00:00:00\", \"2021-02-12T00:00:00\", \"2021-02-13T00:00:00\", \"2021-02-14T00:00:00\", \"2021-02-15T00:00:00\", \"2021-02-16T00:00:00\", \"2021-02-17T00:00:00\", \"2021-02-18T00:00:00\", \"2021-02-19T00:00:00\", \"2021-02-20T00:00:00\", \"2021-02-21T00:00:00\", \"2021-02-22T00:00:00\", \"2021-02-23T00:00:00\", \"2021-02-24T00:00:00\", \"2021-02-25T00:00:00\", \"2021-02-26T00:00:00\", \"2021-02-27T00:00:00\", \"2021-02-28T00:00:00\", \"2021-03-01T00:00:00\", \"2021-03-02T00:00:00\", \"2021-03-03T00:00:00\", \"2021-03-04T00:00:00\", \"2021-03-05T00:00:00\", \"2021-03-06T00:00:00\", \"2021-03-07T00:00:00\", \"2021-03-08T00:00:00\", \"2021-03-09T00:00:00\", \"2021-03-10T00:00:00\", \"2021-03-11T00:00:00\", \"2021-03-12T00:00:00\", \"2021-03-13T00:00:00\", \"2021-03-14T00:00:00\", \"2021-03-15T00:00:00\", \"2021-03-16T00:00:00\", \"2021-03-17T00:00:00\", \"2021-03-18T00:00:00\", \"2021-03-19T00:00:00\", \"2021-03-20T00:00:00\", \"2021-03-21T00:00:00\", \"2021-03-22T00:00:00\", \"2021-03-23T00:00:00\", \"2021-03-24T00:00:00\", \"2021-03-25T00:00:00\", \"2021-03-26T00:00:00\", \"2021-03-27T00:00:00\", \"2021-03-28T00:00:00\", \"2021-03-29T00:00:00\", \"2021-03-30T00:00:00\", \"2021-03-31T00:00:00\", \"2021-04-01T00:00:00\", \"2021-04-02T00:00:00\", \"2021-04-03T00:00:00\", \"2021-04-04T00:00:00\", \"2021-04-05T00:00:00\", \"2021-04-06T00:00:00\", \"2021-04-07T00:00:00\", \"2021-04-08T00:00:00\", \"2021-04-09T00:00:00\", \"2021-04-10T00:00:00\", \"2021-04-11T00:00:00\", \"2021-04-12T00:00:00\", \"2021-04-13T00:00:00\", \"2021-04-14T00:00:00\", \"2021-04-15T00:00:00\", \"2021-04-16T00:00:00\", \"2021-04-17T00:00:00\", \"2021-04-18T00:00:00\", \"2021-04-19T00:00:00\", \"2021-04-20T00:00:00\", \"2021-04-21T00:00:00\", \"2021-04-22T00:00:00\"], \"y\": [-0.37769556488554457, -0.34000352026294783, -0.2951565341673272, -0.27804079182009345, -0.2702311498201262, -0.2175277574011851, -0.14920508098230947, -0.10936187748547023, -0.061662267665310674, -0.028927241318142314, -0.002552162946995296, 0.00025369645419066964, 0.05580971259767067, 0.06787490802276978, 0.1554644856631218, 0.22238423238140456, 0.31348113427323887, 0.35047171404553895, 0.351921408069485, 0.43876275653618735, 0.5184959278532189, 0.564044378799136, 0.6451804798167606, 0.6615479929903448, 0.6715555581879076, 0.676512576463336, 0.3248916291713947, 0.42094554933865713, 0.39962101788964455, 0.3006677096744901, 0.25441779387827657, 0.24726285240525248, 0.24871254642919852, 0.5562347367991687, 0.3732927038418506, 0.3462161606204072, 0.33340273602165843, 0.2707853070518607, 0.2515651701537375, 0.24300729898012075, 0.20882257860900633, -0.1155932235722704, -0.44000902575354717, -0.7644248279348239, -1.0888406301161007, -1.0710234229185704, -1.0603611571940643, -1.0397380905953482, -1.0005495876254522, -0.9927399456254851, -0.9877361630267035, -1.0049310449980733, -1.0221259269694432, -1.0393208089408128, -1.0565156909121827, -1.0737105728835523, -1.0909054548549222, -1.1081003368262918, -1.1252952187976615, -1.1424901007690313, -1.159684982740401, -1.1768798647117709, -1.1940747466831407, -1.2112696286545104, -1.2048629163551359, -1.2013088277803006, -1.2013088277803006, -1.2013088277803006, -1.1934524214569802, -1.1870457091576057, -1.1856427794570128, -1.1913480269060908, -1.1898983328821449, -1.1884954031815518, -1.1884954031815518, -1.1942006506306297, -1.1991576689060581, -1.2041614515048396, -1.2041614515048396, -1.2048629163551359, -1.2048629163551359, -1.2041614515048396, -1.203459986654543, -1.2027117574808937, -1.2041614515048396, -1.2048629163551359, -1.2055643812054326, -1.2048629163551359, -1.2048629163551359, -1.207715540079675, -1.209118469780268, -1.2098666989539173, -1.2041614515048396, -1.2048629163551359, -1.2070140752293785, -1.2070140752293785, -1.2070140752293785, -1.2055643812054326, -1.2027117574808937, -1.2055643812054326, -1.206265846055729, -1.203459986654543, -1.2041614515048396, -1.2006073629300043, -1.1956035803312226, -1.1977547392054653, -1.1970532743551687, -1.1934524214569802, -1.1920494917563873, -1.1920494917563873, -1.1913480269060908, -1.1927509566066836, -1.1920494917563873, -1.1956035803312226, -1.1991576689060581, -1.2027117574808937, -1.2027117574808937, -1.2055643812054326, -1.207715540079675, -1.207715540079675, -1.2055643812054326, -1.2055643812054326, -1.206265846055729, -1.2055643812054326, -1.2070140752293785, -1.210568163804214, -1.210568163804214, -1.2126725583551032, -1.210568163804214, -1.207715540079675, -1.2084170049299714, -1.206265846055729, -1.1991576689060581, -1.1977547392054653, -1.1934524214569802, -1.1827901557324738, -1.18423984975642, -1.1827901557324738, -1.1756819785828032, -1.1756819785828032, -1.174232284558857, -1.1664226425588897, -1.1692752662834287, -1.1671241074091863, -1.1671241074091863, -1.1699767311337252, -1.1671241074091863, -1.1650197128582969, -1.1664226425588897, -1.1721278900079677, -1.1699767311337252, -1.1706781959840216, -1.1614188599601083, -1.1635700188343507, -1.1600159302595152, -1.159314465409219, -1.1479039705110632, -1.1529077531098446, -1.153609217960141, -1.1486054353613595, -1.1400943285110958, -1.1315364573374789, -1.1194244975890266, -1.1016540547148495, -1.0902435598166937, -0.8211616432429696, -0.7955815583688254, -0.7898763109197473, -0.8809732128115816, -0.8816746776618781, -0.8809732128115816, -0.8653071644882939, -0.9877361630267035, -0.9557026015298317, -0.9308239815059837, -0.9222661103323667, -0.912305309458157, -0.9001933497097048, -0.8888296191349021, -0.8880813899612524, -0.8944881022606268, -0.7400255422253453, -0.7756131922970526, -0.8126505363927059, -0.8226113372669156, -0.8632027699374044, -0.8617530759134584, -0.8418314741650389, -0.7976859529197147, -0.7364714536505098, -0.7393240773750487, -0.7371729185008064, -0.747133719375016, -0.7478819485486657, -0.8375759207399069, -0.8403817801410928, -0.8425329390153353, -0.8660086293385905, -0.8646056996379974, -0.8688612530631293, -0.8539434339134911]}, {\"line\": {\"color\": \"red\", \"width\": 1}, \"name\": \"Pr\\u00e9diction\", \"type\": \"scatter\", \"x\": [\"2020-04-01T00:00:00\", \"2020-04-02T00:00:00\", \"2020-04-03T00:00:00\", \"2020-04-04T00:00:00\", \"2020-04-05T00:00:00\", \"2020-04-06T00:00:00\", \"2020-04-07T00:00:00\", \"2020-04-08T00:00:00\", \"2020-04-09T00:00:00\", \"2020-04-10T00:00:00\", \"2020-04-11T00:00:00\", \"2020-04-12T00:00:00\", \"2020-04-13T00:00:00\", \"2020-04-14T00:00:00\", \"2020-04-15T00:00:00\", \"2020-04-16T00:00:00\", \"2020-04-17T00:00:00\", \"2020-04-18T00:00:00\", \"2020-04-19T00:00:00\", \"2020-04-20T00:00:00\", \"2020-04-21T00:00:00\", \"2020-04-22T00:00:00\", \"2020-04-23T00:00:00\", \"2020-04-24T00:00:00\", \"2020-04-25T00:00:00\", \"2020-04-26T00:00:00\", \"2020-04-27T00:00:00\", \"2020-04-28T00:00:00\", \"2020-04-29T00:00:00\", \"2020-04-30T00:00:00\", \"2020-05-01T00:00:00\", \"2020-05-02T00:00:00\", \"2020-05-03T00:00:00\", \"2020-05-04T00:00:00\", \"2020-05-05T00:00:00\", \"2020-05-06T00:00:00\", \"2020-05-07T00:00:00\", \"2020-05-08T00:00:00\", \"2020-05-09T00:00:00\", \"2020-05-10T00:00:00\", \"2020-05-11T00:00:00\", \"2020-05-12T00:00:00\", \"2020-05-13T00:00:00\", \"2020-05-14T00:00:00\", \"2020-05-15T00:00:00\", \"2020-05-16T00:00:00\", \"2020-05-17T00:00:00\", \"2020-05-18T00:00:00\", \"2020-05-19T00:00:00\", \"2020-05-20T00:00:00\", \"2020-05-21T00:00:00\", \"2020-05-22T00:00:00\", \"2020-05-23T00:00:00\", \"2020-05-24T00:00:00\", \"2020-05-25T00:00:00\", \"2020-05-26T00:00:00\", \"2020-05-27T00:00:00\", \"2020-05-28T00:00:00\", \"2020-05-29T00:00:00\", \"2020-05-30T00:00:00\", \"2020-05-31T00:00:00\", \"2020-06-01T00:00:00\", \"2020-06-02T00:00:00\", \"2020-06-03T00:00:00\", \"2020-06-04T00:00:00\", \"2020-06-05T00:00:00\", \"2020-06-06T00:00:00\", \"2020-06-07T00:00:00\", \"2020-06-08T00:00:00\", \"2020-06-09T00:00:00\", \"2020-06-10T00:00:00\", \"2020-06-11T00:00:00\", \"2020-06-12T00:00:00\", \"2020-06-13T00:00:00\", \"2020-06-14T00:00:00\", \"2020-06-15T00:00:00\", \"2020-06-16T00:00:00\", \"2020-06-17T00:00:00\", \"2020-06-18T00:00:00\", \"2020-06-19T00:00:00\", \"2020-06-20T00:00:00\", \"2020-06-21T00:00:00\", \"2020-06-22T00:00:00\", \"2020-06-23T00:00:00\", \"2020-06-24T00:00:00\", \"2020-06-25T00:00:00\", \"2020-06-26T00:00:00\", \"2020-06-27T00:00:00\", \"2020-06-28T00:00:00\", \"2020-06-29T00:00:00\", \"2020-06-30T00:00:00\", \"2020-07-01T00:00:00\", \"2020-07-02T00:00:00\", \"2020-07-03T00:00:00\", \"2020-07-04T00:00:00\", \"2020-07-05T00:00:00\", \"2020-07-06T00:00:00\", \"2020-07-07T00:00:00\", \"2020-07-08T00:00:00\", \"2020-07-09T00:00:00\", \"2020-07-10T00:00:00\", \"2020-07-11T00:00:00\", \"2020-07-12T00:00:00\", \"2020-07-13T00:00:00\", \"2020-07-14T00:00:00\", \"2020-07-15T00:00:00\", \"2020-07-16T00:00:00\", \"2020-07-17T00:00:00\", \"2020-07-18T00:00:00\", \"2020-07-19T00:00:00\", \"2020-07-20T00:00:00\", \"2020-07-21T00:00:00\", \"2020-07-22T00:00:00\", \"2020-07-23T00:00:00\", \"2020-07-24T00:00:00\", \"2020-07-25T00:00:00\", \"2020-07-26T00:00:00\", \"2020-07-27T00:00:00\", \"2020-07-28T00:00:00\", \"2020-07-29T00:00:00\", \"2020-07-30T00:00:00\", \"2020-07-31T00:00:00\", \"2020-08-01T00:00:00\", \"2020-08-02T00:00:00\", \"2020-08-03T00:00:00\", \"2020-08-04T00:00:00\", \"2020-08-05T00:00:00\", \"2020-08-06T00:00:00\", \"2020-08-07T00:00:00\", \"2020-08-08T00:00:00\", \"2020-08-09T00:00:00\", \"2020-08-10T00:00:00\", \"2020-08-11T00:00:00\", \"2020-08-12T00:00:00\", \"2020-08-13T00:00:00\", \"2020-08-14T00:00:00\", \"2020-08-15T00:00:00\", \"2020-08-16T00:00:00\", \"2020-08-17T00:00:00\", \"2020-08-18T00:00:00\", \"2020-08-19T00:00:00\", \"2020-08-20T00:00:00\", \"2020-08-21T00:00:00\", \"2020-08-22T00:00:00\", \"2020-08-23T00:00:00\", \"2020-08-24T00:00:00\", \"2020-08-25T00:00:00\", \"2020-08-26T00:00:00\", \"2020-08-27T00:00:00\", \"2020-08-28T00:00:00\", \"2020-08-29T00:00:00\", \"2020-08-30T00:00:00\", \"2020-08-31T00:00:00\", \"2020-09-01T00:00:00\", \"2020-09-02T00:00:00\", \"2020-09-03T00:00:00\", \"2020-09-04T00:00:00\", \"2020-09-05T00:00:00\", \"2020-09-06T00:00:00\", \"2020-09-07T00:00:00\", \"2020-09-08T00:00:00\", \"2020-09-09T00:00:00\", \"2020-09-10T00:00:00\", \"2020-09-11T00:00:00\", \"2020-09-12T00:00:00\", \"2020-09-13T00:00:00\", \"2020-09-14T00:00:00\", \"2020-09-15T00:00:00\", \"2020-09-16T00:00:00\", \"2020-09-17T00:00:00\", \"2020-09-18T00:00:00\", \"2020-09-19T00:00:00\", \"2020-09-20T00:00:00\", \"2020-09-21T00:00:00\", \"2020-09-22T00:00:00\", \"2020-09-23T00:00:00\", \"2020-09-24T00:00:00\", \"2020-09-25T00:00:00\", \"2020-09-26T00:00:00\", \"2020-09-27T00:00:00\", \"2020-09-28T00:00:00\", \"2020-09-29T00:00:00\", \"2020-09-30T00:00:00\", \"2020-10-01T00:00:00\", \"2020-10-02T00:00:00\", \"2020-10-03T00:00:00\"], \"y\": [-0.606747031211853, -0.606747031211853, -0.606747031211853, -0.606747031211853, -0.606747031211853, -0.606747031211853, -0.606747031211853, -0.606747031211853, -0.606747031211853, -0.606747031211853, -0.606747031211853, -0.606747031211853, -0.606747031211853, -0.606747031211853, -0.606747031211853, -0.606747031211853, -0.606747031211853, -0.606747031211853, -0.606747031211853, -0.606747031211853, -0.606747031211853, -0.606747031211853, -0.606747031211853, -0.606747031211853, -0.606747031211853, -0.606747031211853, -0.606747031211853, -0.606747031211853, -0.606747031211853, -0.606747031211853, -0.606747031211853, -0.606747031211853, -0.606747031211853, -0.606747031211853, -0.606747031211853, -0.606747031211853, -0.4678882360458374, -0.2871415615081787, -0.07282698154449463, 0.14996230602264404, 0.16916656494140625, 0.16156184673309326, 0.5316027402877808, 0.600233793258667, 0.7006733417510986, 1.0395276546478271, 1.3204801082611084, 1.5373804569244385, 1.5915762186050415, 2.0218629837036133, 2.433117628097534, 2.4407310485839844, 2.6447532176971436, 2.911050319671631, 3.0625312328338623, 2.9757957458496094, 3.161986827850342, 3.381420135498047, 3.3061819076538086, 3.2345690727233887, 3.1971921920776367, 3.09963059425354, 2.889399528503418, 2.825885772705078, 2.779731273651123, 2.3942277431488037, 1.932735562324524, 1.3722805976867676, 1.0587791204452515, 1.022614598274231, 0.9340829849243164, 0.8560250997543335, 0.5656804442405701, 0.4677053689956665, 0.4060157537460327, 0.32967472076416016, 0.3685189485549927, 0.26182806491851807, -0.16453886032104492, -0.06834590435028076, -0.08344972133636475, -0.2297687530517578, -0.28379642963409424, -0.3055688142776489, -0.43773090839385986, -0.5240546464920044, -0.4957655668258667, -0.3260667324066162, -0.5310541391372681, -0.34787797927856445, -0.31599295139312744, -0.5163074731826782, -0.6272541284561157, -0.534403920173645, -0.5593594312667847, -0.5572251081466675, -0.5882819890975952, -0.5830949544906616, -0.5612508058547974, -0.4993194341659546, -0.4532686471939087, -0.3691059350967407, -0.35910582542419434, -0.3112248182296753, -0.3172088861465454, -0.3330475091934204, -0.33699989318847656, -0.3744645118713379, -0.3536897897720337, -0.2617912292480469, -0.1975313425064087, -0.11987769603729248, -0.18471288681030273, -0.13391757011413574, -0.13115477561950684, -0.20892798900604248, -0.45340263843536377, -0.48376619815826416, -0.5719226598739624, -0.528408408164978, -0.5955561399459839, -0.5440374612808228, -0.5365122556686401, -0.5584589242935181, -0.5345953702926636, -0.3647724390029907, -0.22344720363616943, -0.18655383586883545, -0.2692493200302124, -0.25756096839904785, -0.04364335536956787, -0.17217934131622314, -0.14977407455444336, -0.2305530309677124, -0.2654283046722412, -0.231980562210083, -0.23627865314483643, -0.24835872650146484, -0.23131632804870605, -0.30138278007507324, -0.23669016361236572, -0.21372652053833008, -0.19700777530670166, -0.16356289386749268, -0.17221879959106445, -0.13454890251159668, -0.1199573278427124, -0.076302170753479, -0.022498488426208496, -0.0353090763092041, -0.04846084117889404, -0.0037404298782348633, -0.007512092590332031, -0.015523314476013184, -0.019260644912719727, -0.06295382976531982, -0.036179542541503906, -0.042771220207214355, -0.07001423835754395, -0.11795604228973389, -0.1631031036376953, -0.2854684591293335, -0.28358376026153564, -0.3401683568954468, -0.3727363348007202, -0.4594184160232544, -0.4811364412307739, -0.44287705421447754, -0.38802921772003174, -0.4138451814651489, -0.3742983341217041, -0.3539083003997803, -0.3449052572250366, -0.32713305950164795, -0.31593191623687744, -0.2731074094772339, -0.26501762866973877, -0.2182009220123291, -0.24532878398895264, -0.21513819694519043, -0.22346651554107666, -0.23309540748596191, -0.30189430713653564, -0.3352227210998535, -0.3969595432281494, -0.41898977756500244]}, {\"line\": {\"color\": \"red\", \"width\": 1}, \"type\": \"scatter\", \"x\": [\"2020-10-18T00:00:00\", \"2020-10-19T00:00:00\", \"2020-10-20T00:00:00\", \"2020-10-21T00:00:00\", \"2020-10-22T00:00:00\", \"2020-10-23T00:00:00\", \"2020-10-24T00:00:00\", \"2020-10-25T00:00:00\", \"2020-10-26T00:00:00\", \"2020-10-27T00:00:00\", \"2020-10-28T00:00:00\", \"2020-10-29T00:00:00\", \"2020-10-30T00:00:00\", \"2020-10-31T00:00:00\", \"2020-11-01T00:00:00\", \"2020-11-02T00:00:00\", \"2020-11-03T00:00:00\", \"2020-11-04T00:00:00\", \"2020-11-05T00:00:00\", \"2020-11-06T00:00:00\", \"2020-11-07T00:00:00\", \"2020-11-08T00:00:00\", \"2020-11-09T00:00:00\", \"2020-11-10T00:00:00\", \"2020-11-11T00:00:00\", \"2020-11-12T00:00:00\", \"2020-11-13T00:00:00\", \"2020-11-14T00:00:00\", \"2020-11-15T00:00:00\", \"2020-11-16T00:00:00\", \"2020-11-17T00:00:00\", \"2020-11-18T00:00:00\", \"2020-11-19T00:00:00\", \"2020-11-20T00:00:00\", \"2020-11-21T00:00:00\", \"2020-11-22T00:00:00\", \"2020-11-23T00:00:00\", \"2020-11-24T00:00:00\", \"2020-11-25T00:00:00\", \"2020-11-26T00:00:00\", \"2020-11-27T00:00:00\", \"2020-11-28T00:00:00\", \"2020-11-29T00:00:00\", \"2020-11-30T00:00:00\", \"2020-12-01T00:00:00\", \"2020-12-02T00:00:00\", \"2020-12-03T00:00:00\", \"2020-12-04T00:00:00\", \"2020-12-05T00:00:00\", \"2020-12-06T00:00:00\", \"2020-12-07T00:00:00\", \"2020-12-08T00:00:00\", \"2020-12-09T00:00:00\", \"2020-12-10T00:00:00\", \"2020-12-11T00:00:00\", \"2020-12-12T00:00:00\", \"2020-12-13T00:00:00\", \"2020-12-14T00:00:00\", \"2020-12-15T00:00:00\", \"2020-12-16T00:00:00\", \"2020-12-17T00:00:00\", \"2020-12-18T00:00:00\", \"2020-12-19T00:00:00\", \"2020-12-20T00:00:00\", \"2020-12-21T00:00:00\", \"2020-12-22T00:00:00\", \"2020-12-23T00:00:00\", \"2020-12-24T00:00:00\", \"2020-12-25T00:00:00\", \"2020-12-26T00:00:00\", \"2020-12-27T00:00:00\", \"2020-12-28T00:00:00\", \"2020-12-29T00:00:00\", \"2020-12-30T00:00:00\", \"2020-12-31T00:00:00\", \"2021-01-01T00:00:00\", \"2021-01-02T00:00:00\", \"2021-01-03T00:00:00\", \"2021-01-04T00:00:00\", \"2021-01-05T00:00:00\", \"2021-01-06T00:00:00\", \"2021-01-07T00:00:00\", \"2021-01-08T00:00:00\", \"2021-01-09T00:00:00\", \"2021-01-10T00:00:00\", \"2021-01-11T00:00:00\", \"2021-01-12T00:00:00\", \"2021-01-13T00:00:00\", \"2021-01-14T00:00:00\", \"2021-01-15T00:00:00\", \"2021-01-16T00:00:00\", \"2021-01-17T00:00:00\", \"2021-01-18T00:00:00\", \"2021-01-19T00:00:00\", \"2021-01-20T00:00:00\", \"2021-01-21T00:00:00\", \"2021-01-22T00:00:00\", \"2021-01-23T00:00:00\", \"2021-01-24T00:00:00\", \"2021-01-25T00:00:00\", \"2021-01-26T00:00:00\", \"2021-01-27T00:00:00\", \"2021-01-28T00:00:00\", \"2021-01-29T00:00:00\", \"2021-01-30T00:00:00\", \"2021-01-31T00:00:00\", \"2021-02-01T00:00:00\", \"2021-02-02T00:00:00\", \"2021-02-03T00:00:00\", \"2021-02-04T00:00:00\", \"2021-02-05T00:00:00\", \"2021-02-06T00:00:00\", \"2021-02-07T00:00:00\", \"2021-02-08T00:00:00\", \"2021-02-09T00:00:00\", \"2021-02-10T00:00:00\", \"2021-02-11T00:00:00\", \"2021-02-12T00:00:00\", \"2021-02-13T00:00:00\", \"2021-02-14T00:00:00\", \"2021-02-15T00:00:00\", \"2021-02-16T00:00:00\", \"2021-02-17T00:00:00\", \"2021-02-18T00:00:00\", \"2021-02-19T00:00:00\", \"2021-02-20T00:00:00\", \"2021-02-21T00:00:00\", \"2021-02-22T00:00:00\", \"2021-02-23T00:00:00\", \"2021-02-24T00:00:00\", \"2021-02-25T00:00:00\", \"2021-02-26T00:00:00\", \"2021-02-27T00:00:00\", \"2021-02-28T00:00:00\", \"2021-03-01T00:00:00\", \"2021-03-02T00:00:00\", \"2021-03-03T00:00:00\", \"2021-03-04T00:00:00\", \"2021-03-05T00:00:00\", \"2021-03-06T00:00:00\", \"2021-03-07T00:00:00\", \"2021-03-08T00:00:00\", \"2021-03-09T00:00:00\", \"2021-03-10T00:00:00\", \"2021-03-11T00:00:00\", \"2021-03-12T00:00:00\", \"2021-03-13T00:00:00\", \"2021-03-14T00:00:00\", \"2021-03-15T00:00:00\", \"2021-03-16T00:00:00\", \"2021-03-17T00:00:00\", \"2021-03-18T00:00:00\", \"2021-03-19T00:00:00\", \"2021-03-20T00:00:00\", \"2021-03-21T00:00:00\", \"2021-03-22T00:00:00\", \"2021-03-23T00:00:00\", \"2021-03-24T00:00:00\", \"2021-03-25T00:00:00\", \"2021-03-26T00:00:00\", \"2021-03-27T00:00:00\", \"2021-03-28T00:00:00\", \"2021-03-29T00:00:00\", \"2021-03-30T00:00:00\", \"2021-03-31T00:00:00\", \"2021-04-01T00:00:00\", \"2021-04-02T00:00:00\", \"2021-04-03T00:00:00\", \"2021-04-04T00:00:00\", \"2021-04-05T00:00:00\", \"2021-04-06T00:00:00\", \"2021-04-07T00:00:00\", \"2021-04-08T00:00:00\", \"2021-04-09T00:00:00\", \"2021-04-10T00:00:00\", \"2021-04-11T00:00:00\", \"2021-04-12T00:00:00\", \"2021-04-13T00:00:00\", \"2021-04-14T00:00:00\", \"2021-04-15T00:00:00\", \"2021-04-16T00:00:00\", \"2021-04-17T00:00:00\", \"2021-04-18T00:00:00\", \"2021-04-19T00:00:00\", \"2021-04-20T00:00:00\", \"2021-04-21T00:00:00\", \"2021-04-22T00:00:00\"], \"y\": [0.108254075050354, 0.1543804407119751, 0.15444254875183105, 0.2523660659790039, 0.27305173873901367, 0.4145258665084839, 0.5078312158584595, 0.6593626141548157, 0.7013025879859924, 0.6973281502723694, 0.9031915664672852, 1.1485157012939453, 1.3970314264297485, 1.7059073448181152, 1.828538179397583, 1.6703206300735474, 1.2949217557907104, 0.6417319774627686, 0.7398509383201599, 0.41432368755340576, 0.21696138381958008, 0.10050618648529053, 0.02302730083465576, 0.03325450420379639, 0.39370739459991455, 0.16939163208007812, 0.755024254322052, 0.42610132694244385, 0.415665864944458, 0.6611331701278687, 0.7617199420928955, 0.7257242202758789, 0.23631465435028076, -0.5292567014694214, -0.5550211668014526, -0.7404426336288452, -0.8608061075210571, -1.061414122581482, -1.5403238534927368, -1.6440168619155884, -1.5804401636123657, -1.3052984476089478, -1.293047308921814, -1.225045084953308, -1.2447398900985718, -1.2681235074996948, -1.3015278577804565, -1.3301647901535034, -1.363181471824646, -1.3929251432418823, -1.4228869676589966, -1.451346755027771, -1.4795573949813843, -1.5075355768203735, -1.5352951288223267, -1.513100266456604, -1.5181928873062134, -1.5137044191360474, -1.5167030096054077, -1.4969960451126099, -1.490899920463562, -1.4867464303970337, -1.497780680656433, -1.487308144569397, -1.4896169900894165, -1.48776113986969, -1.501046061515808, -1.505508542060852, -1.516363263130188, -1.512985348701477, -1.5174576044082642, -1.5149790048599243, -1.51545250415802, -1.5139962434768677, -1.51333749294281, -1.5161608457565308, -1.5156453847885132, -1.5171154737472534, -1.5146726369857788, -1.515919804573059, -1.5211509466171265, -1.5221611261367798, -1.5239778757095337, -1.511236548423767, -1.5184723138809204, -1.5184940099716187, -1.5194579362869263, -1.5187803506851196, -1.5161319971084595, -1.5109177827835083, -1.5185374021530151, -1.516339898109436, -1.512588620185852, -1.515316128730774, -1.5061060190200806, -1.4996484518051147, -1.5058358907699585, -1.501418948173523, -1.4963449239730835, -1.4944511651992798, -1.4946893453598022, -1.4928926229476929, -1.496484398841858, -1.4935389757156372, -1.5031136274337769, -1.5060189962387085, -1.513405203819275, -1.510741114616394, -1.5191477537155151, -1.520039677619934, -1.520889401435852, -1.5161346197128296, -1.5185457468032837, -1.5184482336044312, -1.5168946981430054, -1.5202358961105347, -1.5260413885116577, -1.5236676931381226, -1.5293382406234741, -1.522215723991394, -1.5202916860580444, -1.5218943357467651, -1.5167156457901, -1.5040589570999146, -1.5056477785110474, -1.4942492246627808, -1.4763177633285522, -1.4852267503738403, -1.4765483140945435, -1.4660457372665405, -1.4686182737350464, -1.4635800123214722, -1.4496618509292603, -1.4607797861099243, -1.4505833387374878, -1.4568625688552856, -1.4586302042007446, -1.452797532081604, -1.4511417150497437, -1.4542893171310425, -1.465433955192566, -1.4568003416061401, -1.4631417989730835, -1.4397464990615845, -1.4552966356277466, -1.4382785558700562, -1.4462379217147827, -1.4162975549697876, -1.4405180215835571, -1.4276939630508423, -1.4249576330184937, -1.4057430028915405, -1.3960050344467163, -1.372092604637146, -1.3424845933914185, -1.3276218175888062, -0.7341800928115845, -0.889917254447937, -0.760809063911438, -1.034042239189148, -0.8963309526443481, -0.9733465909957886, -0.8830744028091431, -1.2022160291671753, -1.0045782327651978, -1.073874831199646, -1.0125535726547241, -1.0323396921157837, -0.9854708909988403, -0.9799612760543823, -0.9750458002090454, -0.9885190725326538, -0.6264561414718628, -0.8271936178207397, -0.8052722215652466, -0.8599139451980591, -0.9130243062973022, -0.9032179117202759, -0.8657981157302856, -0.7775989770889282, -0.6666849851608276, -0.7027827501296997, -0.6754473447799683, -0.7040494680404663, -0.6849771738052368, -0.8945175409317017, -0.8258706331253052]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"rangeslider\": {\"visible\": true}}, \"yaxis\": {\"autorange\": true, \"fixedrange\": false}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('aefbc7b2-e16c-4b3e-8952-f53a65c7afa6');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}