{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reseau_GRU_Avec_Attention.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNkv/f2NNlGixYmFFV1NTLi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/Reseau_GRU_Avec_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubCeIvtF6R4W"
      },
      "source": [
        "Dans ce carnet nous allons mettre en place un modèle à réseau de neurones récurrent de type GRU associé à une couche d'attention pour réaliser des prédictions sur notre série temporelle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRhtHsNn5fc3"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFeah3y_6kif"
      },
      "source": [
        "# Création de la série temporelle et du dataset pour l'entrainement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJfSLtub6sdc"
      },
      "source": [
        "# Fonction permettant d'afficher une série temporelle\n",
        "def affiche_serie(temps, serie, format=\"-\", debut=0, fin=None, label=None):\n",
        "    plt.plot(temps[debut:fin], serie[debut:fin], format, label=label)\n",
        "    plt.xlabel(\"Temps\")\n",
        "    plt.ylabel(\"Valeur\")\n",
        "    if label:\n",
        "        plt.legend(fontsize=14)\n",
        "    plt.grid(True)\n",
        "\n",
        "# Fonction permettant de créer une tendance\n",
        "def tendance(temps, pente=0):\n",
        "    return pente * temps\n",
        "\n",
        "# Fonction permettant de créer un motif\n",
        "def motif_periodique(instants):\n",
        "    return (np.where(instants < 0.4,                            # Si les instants sont < 0.4\n",
        "                    np.cos(instants * 2 * np.pi),               # Alors on retourne la fonction cos(2*pi*t)\n",
        "                    1 / np.exp(3 * instants)))                  # Sinon, on retourne la fonction exp(-3t)\n",
        "\n",
        "# Fonction permettant de créer une saisonnalité avec un motif\n",
        "def saisonnalite(temps, periode, amplitude=1, phase=0):\n",
        "    \"\"\"Répétition du motif sur la même période\"\"\"\n",
        "    instants = ((temps + phase) % periode) / periode            # Mapping du temps =[0 1 2 ... 1460] => instants = [0.0 ... 1.0]\n",
        "    return amplitude * motif_periodique(instants)\n",
        "\n",
        "# Fonction permettant de générer du bruit gaussien N(0,1)\n",
        "def bruit_blanc(temps, niveau_bruit=1, graine=None):\n",
        "    rnd = np.random.RandomState(graine)\n",
        "    return rnd.randn(len(temps)) * niveau_bruit\n",
        "\n",
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "# au format X(X1,X2,...Xn) / Y(Y1,Y2,...,Yn)\n",
        "# X sont les données d'entrées du réseau\n",
        "# Y sont les labels\n",
        "\n",
        "def prepare_dataset_XY(serie, taille_fenetre, batch_size, buffer_melange):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie)\n",
        "  dataset = dataset.window(taille_fenetre+1, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre + 1))\n",
        "  dataset = dataset.shuffle(buffer_melange).map(lambda x: (x[:-1], x[-1:]))\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "# Création de la série temporelle\n",
        "temps = np.arange(4 * 365)                # temps = [0 1 2 .... 4*365] = [0 1 2 .... 1460]\n",
        "amplitude = 40                            # Amplitude de la la saisonnalité\n",
        "niveau_bruit = 5                          # Niveau du bruit\n",
        "offset = 10                               # Offset de la série\n",
        "\n",
        "serie = offset + tendance(temps, 0.1) + saisonnalite(temps, periode=365, amplitude=amplitude) + bruit_blanc(temps,niveau_bruit,graine=40)\n",
        "\n",
        "temps_separation = 1000\n",
        "\n",
        "# Extraction des temps et des données d'entrainement\n",
        "temps_entrainement = temps[:temps_separation]\n",
        "x_entrainement = serie[:temps_separation]\n",
        "\n",
        "# Exctraction des temps et des données de valiadation\n",
        "temps_validation = temps[temps_separation:]\n",
        "x_validation = serie[temps_separation:]\n",
        "\n",
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "taille_fenetre = 20\n",
        "batch_size = 32\n",
        "buffer_melange = 1000\n",
        "\n",
        "# Création du dataset X,Y\n",
        "dataset = prepare_dataset_XY(x_entrainement,taille_fenetre,batch_size,buffer_melange)\n",
        "\n",
        "# Création du dataset X,Y de validation\n",
        "dataset_Val = prepare_dataset_XY(x_validation,taille_fenetre,batch_size,buffer_melange)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyndQsxw0wue"
      },
      "source": [
        "# Calcul de la moyenne et de l'écart type de la série\n",
        "mean = tf.math.reduce_mean(serie)\n",
        "std = tf.math.reduce_std(serie)\n",
        "\n",
        "# Normalise les données\n",
        "Serie_Normalisee = (serie-mean)/std\n",
        "min = tf.math.reduce_min(serie)\n",
        "max = tf.math.reduce_max(serie)\n",
        "\n",
        "# Création des données pour l'entrainement et le test\n",
        "x_entrainement_norm = Serie_Normalisee[:temps_separation]\n",
        "x_validation_norm = Serie_Normalisee[temps_separation:]\n",
        "\n",
        "# Création du dataset X,Y\n",
        "dataset_norm = prepare_dataset_XY(x_entrainement_norm,taille_fenetre,batch_size,buffer_melange)\n",
        "\n",
        "# Création du dataset X,Y de validation\n",
        "dataset_Val_norm = prepare_dataset_XY(x_validation_norm,taille_fenetre,batch_size,buffer_melange)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Yt-EgZ3sgPY"
      },
      "source": [
        "# Création du modèle GRU avec couche d'attention personnalisée simple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyrcfKcgCsZ7"
      },
      "source": [
        "**1. Création du réseau et adaptation des formats d'entrée et de sortie**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeJyix8HK7Kt"
      },
      "source": [
        "Sous forme de shéma, notre réseau est donc le suivant :\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OZkfsmnBNHY"
      },
      "source": [
        "<img src=\"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/images/attention_11_Attention.png?raw=true\" width=\"1200\"> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kgTrJOQ5DUo"
      },
      "source": [
        "# Remise à zéro de tous les états générés par Keras\n",
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLNIAGDlBizT"
      },
      "source": [
        "On créé une classe dérivée de la classe [Layer](https://keras.io/api/layers/base_layer/#layer-class) de Keras. Les méthodes utilisées sont les suivantes :  \n",
        " - [build](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#build) : Permet de créer les variables utilisées par la couche (commes les poids et les offsets)\n",
        " - [call](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#call) : Permet d'implanter la logique de la couche"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3COaR59t5WzJ"
      },
      "source": [
        "<img src=\"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/images/attention_11_Attention_2.png?raw=true\" width=\"1200\"> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hhk0kmPSgqva"
      },
      "source": [
        "# Classe d'attention simple\n",
        "# Applique les poids d'attention sur les vecteurs de la couche récurrente\n",
        "\n",
        "# Importe le Backend de Keras\n",
        "from keras import backend as K\n",
        "\n",
        "# Définit une nouvelle classe Couche_Attention\n",
        "# Héritée de la classe Layer de Keras\n",
        "\n",
        "class Couche_Attention(tf.keras.layers.Layer):\n",
        "  # Fonction d'initialisation de la classe d'attention\n",
        "  def __init__(self):\n",
        "    super().__init__()          # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.w = self.add_weight(shape=(input_shape[2],1),initializer=\"normal\",name=\"w\")\n",
        "    self.b = self.add_weight(shape=(input_shape[1],1),initializer=\"zeros\",name=\"b\")\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Définit la logique de la couche d'attention\n",
        "  # Arguments :   x : Tenseur d'entrée de dimension (None, nbr_v,dim)\n",
        "  def call(self,x):\n",
        "    e = K.tanh(K.dot(x,self.w)+self.b)\n",
        "    a = tf.keras.activations.softmax(e,axis=1)\n",
        "    xa = tf.multiply(x,a)\n",
        "    sortie = K.sum(xa,axis=1)\n",
        "    return sortie"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTqdYAsF_ici"
      },
      "source": [
        "dim_GRU = 40\n",
        "\n",
        "# Fonction de la couche lambda d'entrée\n",
        "def Traitement_Entrees(x):\n",
        "  return tf.expand_dims(x,axis=-1)\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.Input(shape=(taille_fenetre,)))\n",
        "model.add(tf.keras.layers.Lambda(Traitement_Entrees))\n",
        "model.add(tf.keras.layers.GRU(dim_GRU,return_sequences=True))\n",
        "model.add(Couche_Attention())\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "model.save_weights('model_initial.h5')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUM0-SSXGLIQ"
      },
      "source": [
        "**2. Optimisation du taux d'apprentissage**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jejCBhXVuNQ4"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(lr=1e-8)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur, metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset_norm,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1_WMNlzu2B4"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 1])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XdXh_b0GP_F"
      },
      "source": [
        "**3. Entrainement du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80pEtJ10wIfY"
      },
      "source": [
        "# Charge les meilleurs poids\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16ujUiELwR33"
      },
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "class TimingCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, logs={}):\n",
        "        self.logs=[]\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.starttime = timer()\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.logs.append(timer()-self.starttime)\n",
        "\n",
        "cb = TimingCallback()\n",
        "\n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.1,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0.8)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(learning_rate=lr_schedule,momentum=0.9)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur,metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle\n",
        "historique = model.fit(dataset_norm,validation_data=dataset_Val_norm, epochs=500,verbose=1, callbacks=[CheckPoint,cb])\n",
        "\n",
        "print(cb.logs)\n",
        "print(sum(cb.logs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COP9u4yitvmw"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o2zh6N9t1b7"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement[400:500])),erreur_entrainement[400:500], label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement[400:500])),erreur_validation[400:500], label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6Gq2CkeGR_1"
      },
      "source": [
        "**4. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRxXtHRXGXfF"
      },
      "source": [
        "taille_fenetre = 20\n",
        "\n",
        "# Création d'une liste vide pour recevoir les prédictions\n",
        "predictions = []\n",
        "\n",
        "# Calcul des prédiction pour chaque groupe de 20 valeurs consécutives de la série\n",
        "# dans l'intervalle de validation\n",
        "for t in temps[temps_separation:-taille_fenetre]:\n",
        "    X = np.reshape(Serie_Normalisee[t:t+taille_fenetre],(1,taille_fenetre))\n",
        "    predictions.append(model.predict(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd2nfDcgG8EO"
      },
      "source": [
        "# Affiche la série et les prédictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps,serie,label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title('Prédictions avec le modèle GRU + Attention')\n",
        "plt.show()\n",
        "\n",
        "# Zoom sur l'intervalle de validation\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps[temps_separation:],serie[temps_separation:],label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title(\"Prédictions avec le modèle GRU + Attention (zoom sur l'intervalle de validation)\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDS7BJvZG_e1"
      },
      "source": [
        "# Calcule de l'erreur quadratique moyenne et de l'erreur absolue moyenne \n",
        "\n",
        "mae = tf.keras.metrics.mean_absolute_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "mse = tf.keras.metrics.mean_squared_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "\n",
        "print(mae)\n",
        "print(mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7MYRUhhcUGq"
      },
      "source": [
        "# Création du modèle LSTM avec couche d'attention personnalisée simple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-J0Du8hceO7"
      },
      "source": [
        "**1. Création du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dk-OVhlxcYoN"
      },
      "source": [
        "dim_GRU = 40\n",
        "\n",
        "# Fonction de la couche lambda d'entrée\n",
        "def Traitement_Entrees(x):\n",
        "  return tf.expand_dims(x,axis=-1)\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.Input(shape=(taille_fenetre,)))\n",
        "model.add(tf.keras.layers.Lambda(Traitement_Entrees))\n",
        "model.add(tf.keras.layers.LSTM(dim_GRU,return_sequences=True))\n",
        "model.add(Couche_Attention())\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "model.save_weights('model_initial.h5')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RV7DMcPcg9p"
      },
      "source": [
        "**2. Optimisation du taux d'apprentissage**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUC-JS47cdKL"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(lr=1e-8)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur, metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset_norm,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "No3DCTltd1Hl"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 1])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKmlwKcHcnBK"
      },
      "source": [
        "**3. Entrainement**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4TH6_RLcoxK"
      },
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "class TimingCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, logs={}):\n",
        "        self.logs=[]\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.starttime = timer()\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.logs.append(timer()-self.starttime)\n",
        "\n",
        "cb = TimingCallback()\n",
        "\n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.1,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0.8)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(learning_rate=lr_schedule,momentum=0.9)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur,metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle\n",
        "historique = model.fit(dataset_norm,validation_data=dataset_Val_norm, epochs=500,verbose=1, callbacks=[CheckPoint,cb])\n",
        "\n",
        "print(cb.logs)\n",
        "print(sum(cb.logs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qdJe8PYk0x8"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wela4RRGcraC"
      },
      "source": [
        "**4. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiC511R5cqtc"
      },
      "source": [
        "taille_fenetre = 20\n",
        "\n",
        "# Création d'une liste vide pour recevoir les prédictions\n",
        "predictions = []\n",
        "\n",
        "# Calcul des prédiction pour chaque groupe de 20 valeurs consécutives de la série\n",
        "# dans l'intervalle de validation\n",
        "for t in temps[temps_separation:-taille_fenetre]:\n",
        "    X = np.reshape(Serie_Normalisee[t:t+taille_fenetre],(1,taille_fenetre))\n",
        "    predictions.append(model.predict(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1knHYB7-lBrV"
      },
      "source": [
        "# Affiche la série et les prédictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps,serie,label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title('Prédictions avec le modèle GRU + Attention')\n",
        "plt.show()\n",
        "\n",
        "# Zoom sur l'intervalle de validation\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps[temps_separation:],serie[temps_separation:],label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title(\"Prédictions avec le modèle GRU + Attention (zoom sur l'intervalle de validation)\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0WO5Bp7cuqs"
      },
      "source": [
        "# Calcule de l'erreur quadratique moyenne et de l'erreur absolue moyenne \n",
        "\n",
        "mae = tf.keras.metrics.mean_absolute_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "mse = tf.keras.metrics.mean_squared_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "\n",
        "print(mae)\n",
        "print(mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXXKV_rQuWbf"
      },
      "source": [
        "# Création du modèle LSTM avec couche d'attention personnalisée simple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfiypQrDuWbh"
      },
      "source": [
        "**1. Création du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LygZm1-4uWbi",
        "outputId": "8092bcdb-43a3-420a-bbd0-69b215872e3a"
      },
      "source": [
        "dim_GRU = 40\n",
        "\n",
        "# Fonction de la couche lambda d'entrée\n",
        "def Traitement_Entrees(x):\n",
        "  return tf.expand_dims(x,axis=-1)\n",
        "\n",
        "# Encodeur\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.Input(shape=(taille_fenetre,)))\n",
        "model.add(tf.keras.layers.Lambda(Traitement_Entrees))\n",
        "model.add(tf.keras.layers.GRU(dim_GRU,return_sequences=True))\n",
        "\n",
        "# Décodeur\n",
        "model.add(Couche_Attention())\n",
        "model.add(tf.keras.layers.Lambda(Traitement_Entrees))\n",
        "model.add(tf.keras.layers.GRU(dim_GRU))\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "model.save_weights('model_initial.h5')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda (Lambda)              (None, 20, 1)             0         \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, 20, 40)            5160      \n",
            "_________________________________________________________________\n",
            "couche__attention (Couche_At (None, 40)                60        \n",
            "_________________________________________________________________\n",
            "lambda_1 (Lambda)            (None, 40, 1)             0         \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 40)                5160      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 41        \n",
            "=================================================================\n",
            "Total params: 10,421\n",
            "Trainable params: 10,421\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j19xcCoKuWbk"
      },
      "source": [
        "**2. Optimisation du taux d'apprentissage**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fg50B9YiuWbk",
        "outputId": "904ecba3-d4e7-4a76-defb-fe2bdb14d019"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(lr=1e-8)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur, metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset_norm,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "30/30 [==============================] - 3s 16ms/step - loss: 0.3647 - mae: 0.7381\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.36642, saving model to poids.hdf5\n",
            "Epoch 2/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.3692 - mae: 0.7393\n",
            "\n",
            "Epoch 00002: loss did not improve from 0.36642\n",
            "Epoch 3/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.3647 - mae: 0.7362\n",
            "\n",
            "Epoch 00003: loss did not improve from 0.36642\n",
            "Epoch 4/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3761 - mae: 0.7512\n",
            "\n",
            "Epoch 00004: loss did not improve from 0.36642\n",
            "Epoch 5/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.3682 - mae: 0.7425\n",
            "\n",
            "Epoch 00005: loss did not improve from 0.36642\n",
            "Epoch 6/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.3721 - mae: 0.7436\n",
            "\n",
            "Epoch 00006: loss improved from 0.36642 to 0.36589, saving model to poids.hdf5\n",
            "Epoch 7/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.3661 - mae: 0.7372\n",
            "\n",
            "Epoch 00007: loss did not improve from 0.36589\n",
            "Epoch 8/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.3940 - mae: 0.7718\n",
            "\n",
            "Epoch 00008: loss improved from 0.36589 to 0.36528, saving model to poids.hdf5\n",
            "Epoch 9/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3683 - mae: 0.7410\n",
            "\n",
            "Epoch 00009: loss did not improve from 0.36528\n",
            "Epoch 10/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.3604 - mae: 0.7331\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.36528\n",
            "Epoch 11/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3689 - mae: 0.7427\n",
            "\n",
            "Epoch 00011: loss did not improve from 0.36528\n",
            "Epoch 12/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3499 - mae: 0.7160\n",
            "\n",
            "Epoch 00012: loss did not improve from 0.36528\n",
            "Epoch 13/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3491 - mae: 0.7182\n",
            "\n",
            "Epoch 00013: loss did not improve from 0.36528\n",
            "Epoch 14/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.3756 - mae: 0.7463\n",
            "\n",
            "Epoch 00014: loss improved from 0.36528 to 0.36310, saving model to poids.hdf5\n",
            "Epoch 15/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3717 - mae: 0.7473\n",
            "\n",
            "Epoch 00015: loss did not improve from 0.36310\n",
            "Epoch 16/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3773 - mae: 0.7568\n",
            "\n",
            "Epoch 00016: loss did not improve from 0.36310\n",
            "Epoch 17/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.3887 - mae: 0.7666\n",
            "\n",
            "Epoch 00017: loss did not improve from 0.36310\n",
            "Epoch 18/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3720 - mae: 0.7506\n",
            "\n",
            "Epoch 00018: loss did not improve from 0.36310\n",
            "Epoch 19/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.3835 - mae: 0.7611\n",
            "\n",
            "Epoch 00019: loss did not improve from 0.36310\n",
            "Epoch 20/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.3657 - mae: 0.7419\n",
            "\n",
            "Epoch 00020: loss did not improve from 0.36310\n",
            "Epoch 21/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.3725 - mae: 0.7491\n",
            "\n",
            "Epoch 00021: loss did not improve from 0.36310\n",
            "Epoch 22/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.3695 - mae: 0.7439\n",
            "\n",
            "Epoch 00022: loss did not improve from 0.36310\n",
            "Epoch 23/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3719 - mae: 0.7415\n",
            "\n",
            "Epoch 00023: loss did not improve from 0.36310\n",
            "Epoch 24/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.3689 - mae: 0.7413\n",
            "\n",
            "Epoch 00024: loss did not improve from 0.36310\n",
            "Epoch 25/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3563 - mae: 0.7255\n",
            "\n",
            "Epoch 00025: loss did not improve from 0.36310\n",
            "Epoch 26/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3592 - mae: 0.7320\n",
            "\n",
            "Epoch 00026: loss improved from 0.36310 to 0.36197, saving model to poids.hdf5\n",
            "Epoch 27/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3647 - mae: 0.7351\n",
            "\n",
            "Epoch 00027: loss did not improve from 0.36197\n",
            "Epoch 28/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3612 - mae: 0.7364\n",
            "\n",
            "Epoch 00028: loss did not improve from 0.36197\n",
            "Epoch 29/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3552 - mae: 0.7280\n",
            "\n",
            "Epoch 00029: loss did not improve from 0.36197\n",
            "Epoch 30/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.3675 - mae: 0.7440\n",
            "\n",
            "Epoch 00030: loss did not improve from 0.36197\n",
            "Epoch 31/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3767 - mae: 0.7537\n",
            "\n",
            "Epoch 00031: loss did not improve from 0.36197\n",
            "Epoch 32/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3618 - mae: 0.7328\n",
            "\n",
            "Epoch 00032: loss did not improve from 0.36197\n",
            "Epoch 33/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.3384 - mae: 0.7026\n",
            "\n",
            "Epoch 00033: loss did not improve from 0.36197\n",
            "Epoch 34/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3518 - mae: 0.7208\n",
            "\n",
            "Epoch 00034: loss did not improve from 0.36197\n",
            "Epoch 35/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.3683 - mae: 0.7373\n",
            "\n",
            "Epoch 00035: loss did not improve from 0.36197\n",
            "Epoch 36/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3784 - mae: 0.7518\n",
            "\n",
            "Epoch 00036: loss did not improve from 0.36197\n",
            "Epoch 37/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3591 - mae: 0.7259\n",
            "\n",
            "Epoch 00037: loss improved from 0.36197 to 0.36014, saving model to poids.hdf5\n",
            "Epoch 38/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.3665 - mae: 0.7365\n",
            "\n",
            "Epoch 00038: loss did not improve from 0.36014\n",
            "Epoch 39/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3738 - mae: 0.7455\n",
            "\n",
            "Epoch 00039: loss improved from 0.36014 to 0.35834, saving model to poids.hdf5\n",
            "Epoch 40/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3745 - mae: 0.7488\n",
            "\n",
            "Epoch 00040: loss did not improve from 0.35834\n",
            "Epoch 41/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.3417 - mae: 0.7008\n",
            "\n",
            "Epoch 00041: loss did not improve from 0.35834\n",
            "Epoch 42/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.3411 - mae: 0.7025\n",
            "\n",
            "Epoch 00042: loss improved from 0.35834 to 0.35357, saving model to poids.hdf5\n",
            "Epoch 43/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3518 - mae: 0.7235\n",
            "\n",
            "Epoch 00043: loss did not improve from 0.35357\n",
            "Epoch 44/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3739 - mae: 0.7456\n",
            "\n",
            "Epoch 00044: loss improved from 0.35357 to 0.34976, saving model to poids.hdf5\n",
            "Epoch 45/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3377 - mae: 0.6959\n",
            "\n",
            "Epoch 00045: loss improved from 0.34976 to 0.34702, saving model to poids.hdf5\n",
            "Epoch 46/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3313 - mae: 0.6968\n",
            "\n",
            "Epoch 00046: loss improved from 0.34702 to 0.34081, saving model to poids.hdf5\n",
            "Epoch 47/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.3307 - mae: 0.6969\n",
            "\n",
            "Epoch 00047: loss improved from 0.34081 to 0.33411, saving model to poids.hdf5\n",
            "Epoch 48/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3373 - mae: 0.6985\n",
            "\n",
            "Epoch 00048: loss improved from 0.33411 to 0.32804, saving model to poids.hdf5\n",
            "Epoch 49/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.3133 - mae: 0.6631\n",
            "\n",
            "Epoch 00049: loss improved from 0.32804 to 0.32244, saving model to poids.hdf5\n",
            "Epoch 50/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3144 - mae: 0.6675\n",
            "\n",
            "Epoch 00050: loss improved from 0.32244 to 0.31471, saving model to poids.hdf5\n",
            "Epoch 51/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3179 - mae: 0.6725\n",
            "\n",
            "Epoch 00051: loss improved from 0.31471 to 0.30283, saving model to poids.hdf5\n",
            "Epoch 52/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3062 - mae: 0.6594\n",
            "\n",
            "Epoch 00052: loss improved from 0.30283 to 0.29546, saving model to poids.hdf5\n",
            "Epoch 53/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.2976 - mae: 0.6510\n",
            "\n",
            "Epoch 00053: loss improved from 0.29546 to 0.28973, saving model to poids.hdf5\n",
            "Epoch 54/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.2800 - mae: 0.6252\n",
            "\n",
            "Epoch 00054: loss improved from 0.28973 to 0.27999, saving model to poids.hdf5\n",
            "Epoch 55/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.2778 - mae: 0.6313\n",
            "\n",
            "Epoch 00055: loss improved from 0.27999 to 0.27346, saving model to poids.hdf5\n",
            "Epoch 56/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.2671 - mae: 0.6121\n",
            "\n",
            "Epoch 00056: loss improved from 0.27346 to 0.26699, saving model to poids.hdf5\n",
            "Epoch 57/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.2584 - mae: 0.6090\n",
            "\n",
            "Epoch 00057: loss improved from 0.26699 to 0.26492, saving model to poids.hdf5\n",
            "Epoch 58/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.2778 - mae: 0.6400\n",
            "\n",
            "Epoch 00058: loss improved from 0.26492 to 0.26419, saving model to poids.hdf5\n",
            "Epoch 59/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.2584 - mae: 0.6117\n",
            "\n",
            "Epoch 00059: loss improved from 0.26419 to 0.26173, saving model to poids.hdf5\n",
            "Epoch 60/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.2534 - mae: 0.6028\n",
            "\n",
            "Epoch 00060: loss improved from 0.26173 to 0.25970, saving model to poids.hdf5\n",
            "Epoch 61/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.2542 - mae: 0.6133\n",
            "\n",
            "Epoch 00061: loss did not improve from 0.25970\n",
            "Epoch 62/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.2659 - mae: 0.6240\n",
            "\n",
            "Epoch 00062: loss improved from 0.25970 to 0.25703, saving model to poids.hdf5\n",
            "Epoch 63/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.2507 - mae: 0.6086\n",
            "\n",
            "Epoch 00063: loss did not improve from 0.25703\n",
            "Epoch 64/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.2699 - mae: 0.6341\n",
            "\n",
            "Epoch 00064: loss improved from 0.25703 to 0.25454, saving model to poids.hdf5\n",
            "Epoch 65/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.2490 - mae: 0.6057\n",
            "\n",
            "Epoch 00065: loss improved from 0.25454 to 0.24651, saving model to poids.hdf5\n",
            "Epoch 66/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.2248 - mae: 0.5716\n",
            "\n",
            "Epoch 00066: loss improved from 0.24651 to 0.22932, saving model to poids.hdf5\n",
            "Epoch 67/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.1970 - mae: 0.5348\n",
            "\n",
            "Epoch 00067: loss improved from 0.22932 to 0.18935, saving model to poids.hdf5\n",
            "Epoch 68/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.1203 - mae: 0.4004\n",
            "\n",
            "Epoch 00068: loss improved from 0.18935 to 0.10412, saving model to poids.hdf5\n",
            "Epoch 69/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.0488 - mae: 0.2317\n",
            "\n",
            "Epoch 00069: loss improved from 0.10412 to 0.03974, saving model to poids.hdf5\n",
            "Epoch 70/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.0283 - mae: 0.1675\n",
            "\n",
            "Epoch 00070: loss improved from 0.03974 to 0.03060, saving model to poids.hdf5\n",
            "Epoch 71/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.0297 - mae: 0.1708\n",
            "\n",
            "Epoch 00071: loss improved from 0.03060 to 0.02983, saving model to poids.hdf5\n",
            "Epoch 72/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.0275 - mae: 0.1641\n",
            "\n",
            "Epoch 00072: loss improved from 0.02983 to 0.02812, saving model to poids.hdf5\n",
            "Epoch 73/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.0284 - mae: 0.1714\n",
            "\n",
            "Epoch 00073: loss did not improve from 0.02812\n",
            "Epoch 74/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.0402 - mae: 0.2009\n",
            "\n",
            "Epoch 00074: loss did not improve from 0.02812\n",
            "Epoch 75/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.0557 - mae: 0.2626\n",
            "\n",
            "Epoch 00075: loss did not improve from 0.02812\n",
            "Epoch 76/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.0815 - mae: 0.3105\n",
            "\n",
            "Epoch 00076: loss did not improve from 0.02812\n",
            "Epoch 77/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.0337 - mae: 0.1962\n",
            "\n",
            "Epoch 00077: loss did not improve from 0.02812\n",
            "Epoch 78/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.0479 - mae: 0.2309\n",
            "\n",
            "Epoch 00078: loss did not improve from 0.02812\n",
            "Epoch 79/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.0429 - mae: 0.2241\n",
            "\n",
            "Epoch 00079: loss did not improve from 0.02812\n",
            "Epoch 80/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.0774 - mae: 0.2994\n",
            "\n",
            "Epoch 00080: loss did not improve from 0.02812\n",
            "Epoch 81/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.0633 - mae: 0.2767\n",
            "\n",
            "Epoch 00081: loss did not improve from 0.02812\n",
            "Epoch 82/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.1217 - mae: 0.4028\n",
            "\n",
            "Epoch 00082: loss did not improve from 0.02812\n",
            "Epoch 83/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.2056 - mae: 0.5463\n",
            "\n",
            "Epoch 00083: loss did not improve from 0.02812\n",
            "Epoch 84/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 2.8759 - mae: 3.2915\n",
            "\n",
            "Epoch 00084: loss did not improve from 0.02812\n",
            "Epoch 85/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 19.6201 - mae: 20.1201\n",
            "\n",
            "Epoch 00085: loss did not improve from 0.02812\n",
            "Epoch 86/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 24.4323 - mae: 24.9323\n",
            "\n",
            "Epoch 00086: loss did not improve from 0.02812\n",
            "Epoch 87/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 30.5165 - mae: 31.0165\n",
            "\n",
            "Epoch 00087: loss did not improve from 0.02812\n",
            "Epoch 88/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 38.0669 - mae: 38.5669\n",
            "\n",
            "Epoch 00088: loss did not improve from 0.02812\n",
            "Epoch 89/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 47.6879 - mae: 48.1879\n",
            "\n",
            "Epoch 00089: loss did not improve from 0.02812\n",
            "Epoch 90/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 59.7928 - mae: 60.2928\n",
            "\n",
            "Epoch 00090: loss did not improve from 0.02812\n",
            "Epoch 91/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 74.9247 - mae: 75.4247\n",
            "\n",
            "Epoch 00091: loss did not improve from 0.02812\n",
            "Epoch 92/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 78.1780 - mae: 78.6780\n",
            "\n",
            "Epoch 00092: loss did not improve from 0.02812\n",
            "Epoch 93/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 103.1962 - mae: 103.6962\n",
            "\n",
            "Epoch 00093: loss did not improve from 0.02812\n",
            "Epoch 94/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 129.7518 - mae: 130.2518\n",
            "\n",
            "Epoch 00094: loss did not improve from 0.02812\n",
            "Epoch 95/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 163.1996 - mae: 163.6996\n",
            "\n",
            "Epoch 00095: loss did not improve from 0.02812\n",
            "Epoch 96/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 205.2862 - mae: 205.7862\n",
            "\n",
            "Epoch 00096: loss did not improve from 0.02812\n",
            "Epoch 97/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 258.2919 - mae: 258.7919\n",
            "\n",
            "Epoch 00097: loss did not improve from 0.02812\n",
            "Epoch 98/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 324.9594 - mae: 325.4594\n",
            "\n",
            "Epoch 00098: loss did not improve from 0.02812\n",
            "Epoch 99/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 409.0139 - mae: 409.5139\n",
            "\n",
            "Epoch 00099: loss did not improve from 0.02812\n",
            "Epoch 100/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 514.7180 - mae: 515.2180\n",
            "\n",
            "Epoch 00100: loss did not improve from 0.02812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "eEaAVFisuWbl",
        "outputId": "d7904b4e-8d92-42fd-96a8-654e9b2f26ba"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 1])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, \"Evolution de l'erreur en fonction du taux d'apprentissage\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAF5CAYAAAC7nq8lAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxcdb3/8fdnZrKvbZO0adN9pbQFSmWRsojIplLEFRUVUdyvXJXf5V4VuSjuu+JV3MANRFCsgIILyA4tUAq0lIauSdM2TZpm37+/P85JmIakSZpJ5zvJ6/l4zCOZOWfOfGbOSeY93+93vseccwIAAMDhiSS7AAAAgFRGmAIAABgBwhQAAMAIEKYAAABGgDAFAAAwAoQpAACAESBMIanMzJnZvMO876lmtinRNQ3wWNvM7KzDuN8ZZlYxGjWlGjM7xcw2m1mjmV14BB/3x2b2+SPwOGNiX4+V59GXmb3LzO5Ndh0YmwhTGJIwTLSEb4Q9lx8e4RoOCl7OuQedcwuPZA0jFb6Os5JdR5JcK+mHzrlc59wdo/EAZvY+M3so/jbn3Iedc18cjcdLlP7q9kUqHrNmNiv8fxHruc0591vn3NnJrAtjV2zwVYBeb3TO/SPZRYxHZhZzznUOdtsItm+SzDnXnYjtDWCmpOdHcfsYIxJ5bANHAi1TGBEzyzCzOjNbEndbcdiKVRJe/6CZlZtZrZmtNrOpA2zrfjP7QNz13k/rZvZAePMzYavY2/t2R5jZUeE26szseTO7IG7ZjWZ2vZndZWYNZva4mc09xPO6xMy2m1mNmX22z7KImV1lZi+Fy281s4nDfOl6XrtvmtkOM9sTdkdlhcvOMLMKM/svM9st6Zdmdo2Z3WZmvzGzeknvM7MCM/u5mVWZWaWZfcnMouE2rjGz38Q93kGf1sPX6joze1hSs6Q5/dQ41cxuN7NqM9tqZv8Rt+ya8Ln/KnxNnzezFQM815fC7f8l3H8Z4bZXh8dFuZl9cKjbNrPpZvbHsK4aM/uhmR0l6ceSTg4foy5c90Yz+1LcfQc8HsPX58MWdEfWhceMDfCcssJt7zezDZJe1Wf5QS2pfeuIu32gul9vZk+bWb2Z7TSza+Lu84quOIvrijazu83sW3HLbjGzXxzO8+iz7qFq6jm+LjezXeEx+Zm45T3H7+/DffqUmR3Tp/7/MrP1kprMLGZmJ5nZI+G+eMbMzohb/34z+6KZPRxu714zKwoX9/y/qAtf05Pt4P8nZmbfMbO94XN51sL/YWZ2vpltCLdZ2fMczGyCmd0ZHnP7w9/L4uqZbWYPhPf7R3jsxP/9DfhcMAY457hwGfQiaZukswZY9gtJ18Vd/5ikv4W/nylpn6TlkjIk/UDSA3HrOknzwt/vl/SBuGXvk/RQf+uG18+QVBH+niapXNL/SEoPH7dB0sJw+Y2SaiSdoKBF9reSbhng+SyW1CjptLDmb0vq7Hn+kj4p6TFJZeHyn0i6eYBt9dbYz7LvSFotaaKkPEl/kfSVuPt1Svpa+BhZkq6R1CHpQgUfhLIk/Sl8/BxJJZKekPShcBvXSPpN3OPNCl/DWNzrvUPS0eFrktanvoikJyVdHb6mcyRtkXRO3PZbJZ0vKSrpK5IeG+oxpOAN70eSMiUdK6la0pmDbTu8/kz4+uWE91/Z3zETt++/NIzj8U5JhZJmhDWdO8Dz+aqkB8P9N13Sc/H7Wq88Xnvr6Gdb/dV9hqSl4X5YJmmPpAsHOq7iX19JUyTtDZ/vu8L9lnc4z2MYNc0Kn/PN4X5ZGr5+PTVdo+D4fYuCv9fPSNqq8LgL618X1pAlaZqCv9nzw8d7XXi9OO74fUnSgnD9+yV9tb9jve9rLOkcBcd2oSSTdJSk0nBZlaRTw98nSFoe/j5J0pslZSv4e/2DpDvitv+opG8q+FtZKale4d/fYM+FS+pfkl4Al9S4hP/oGiXVxV0+GC47S9JLces+LOk94e8/l/T1uGW54T/UWeH1RIWpUyXtlhSJW36zpGvC32+U9LO4ZedLemGA53q14oKWgjeGdr38prBR0mvjlpeGzynWz7Z6a+xzu0lqkjQ37raTJW2Nu1+7pMy45dfo4Df+yZLaJGXF3XaxpPvi1h8sTF17iH1+oqQdfW77b0m/jNv+P+KWLZbUMsgx1PMaTpfUpbg3eAWB6cbBth2+TtUDvN4HHTNx+74nTA3leFwZt/xWSVcN8Hy2KC5oSbpcCQxT/azzXUnfGei40ivD6psl7VQQHlceYruHfB7DqKnn+FoUt/zrkn4et08fi1sW0cHBZZuk98ct/y9Jv+7zePdIem/c8fu5uGUf1csf4npqGShMnSnpRUknKe5/Rrhsh6QPScof5LkfK2l/+PsMBR9+suOW/0Yvh6lDPhcuqX+hmw/DcaFzrjDu8tPw9vskZZvZiRYMVD1WQYuJJE2VtL1nA865RgWfyKYluLapkna6g8f8bO/zOLvjfm9W8EY64LZ6rjjnmhTU3GOmpD+FzfV1CsJVl4JwM1TFCj7hPhm3nb+Ft/eods619rnfzrjfZyr4hF8Vt42fKGihGqqdh1g2U9LUnm2H2/8fHfw8+76mmRY36PcQpkqqdc41xN022P7q2fZ0Sdvd4Y2pGcrxeFjHSfx2EyH8e7ov7FY6IOnDkooGu1+cvyhoxdvknDvU4PYhP48h1tR3W1P7Wxb+rVYMtFzB8ffWPsffSgUfXnoMdV8dxDn3L0k/lHS9pL1mdoOZ5YeL36zgw9Z2M/u3mZ0cPvdsM/uJBd3/9QpaVgst6FbvOZ6bR/BckMIIUxgx51yXgk/wF4eXO+PeJHcp+EciSTKzHAXN5ZX9bKpJQcDoMWUYZeySNN3M4o/pGQM8zmCqFLxhSwr+iSqoucdOSef1CZaZzrnhPNY+SS2Sjo7bRoFzLv7NwPVzv/jbdipomSqK20a+c+7ocPlQXs/+HiN++1v7PM8859z5gz67we2SNNHM8uJuG+r+2ilpxgCh7VDPp+dxh3o8Duag40RB/fGaNfTjub+6f6egG3i6c65AwbiqnvFbB+3b8A29uM/9r1MQ9EvN7OJDPPZgz2OoNfXou61d/S0L/1bL+izve3z/us/xl+Oc++oh6utvO/2v4Nz3nXPHK2j1XCDpyvD2Nc65VQo+lNyh4H+bJH1a0kJJJzrn8hUMA5CC51+l4HiO39/xr8NIngtSAGEKifI7SW9XMD7jd3G33yzpUjM71swyJH1Z0uPOuW39bGOdpIvCT4DzJF3WZ/ke9TNIOvS4gjev/2dmaeHgzjdKuuUwnsttkt5gZivNLF3BV/rj/1Z+LOk6M5sp9Q64XzWcBwg/lf9U0nfs5YH608zsnGFso0rSvZK+ZWb5FgyMn2tmp4errJN0mpnNMLMCBV10w/GEpIZwUHCWmUXNbImZDThAeRi175T0iKSvmFmmmS1TsL9/c+h79tZVJemrZpYT3v+UcNkeSWXhfuvPcI7Hwdwq6b/Dgcllkj7RZ/k6Se8MX7dzJZ3+ii28rL+68xS0drSa2QmS3hm37EUFLXWvN7M0SZ9TMAZMkmRmp0m6VNJ7JL1X0g/MbKDW4MGeR7xD1dTj8+Hf8NFhDb+PW3a8mV0UBuErFHwYeGyAx/qNpDea2Tnha5hpwcD7sgHWj1ctqVsD/L8ws1eFrWxpCoJpq6RuM0u3YD6qAudch4JxTz2t3XkKPgDVWfCFky/0bM85t13SWknXhNs4WcH/n0Q8F6QAwhSGo+ebWD2Xnq48OeceV/BPaaqkv8bd/g9Jn5d0u4I3wLmS3jHA9r+jYJzQHkk3KRgkHu8aSTeFzeRvi1/gnGtX8M/rPAWtPj9SMG7rheE+Sefc8woG0f8urHm/gu6IHt9T8On8XjNrUPBmcOJwH0fBOIpySY+F3Qb/UPDJdzjeo2DA64awztsUdh045/6u4I1svYLBtncOZ8Nhi+MbFHTbblXwuv5MUsEwaxzIxQrGtuxS0C38BTeEqTfCut4oaZ6C8S0VCoK8JP1LwfQLu81sXz/3Hc7xOJj/VdCNtVVBqP11n+WfDOusU/Ah41Bza/VX90clXRseY1fr5RYSOecOhMt/pqBVrUnhMRp2V/1K0sedc5XOuQcVjBX7pVm/30wc7HnEG7CmOP9WcFz/U9I3nXPxE2X+WcG+2i/pEkkXhaHlFcLAvUpB13K1gtadKzWE962wu+06SQ+H/y9O6rNKvoIPM/sVPPcaSd8Il10iaVv4N/lhBftOCsaHZSn4O3hMQbd8vHcpGM9XI+lLCv722kb6XJAazLlBW0MBADgkC8ZL9nw77xXj2SyYRmGec+7dR7ay5DCz3yv4kssXBl0ZKY9UDADACIVdh3PD7vZzFbREjcpM//DPoGHKzH5hwcRmzw2w3Mzs+xZMgrfezJYnvkwAALw2RcF0DY2Svi/pI865p5NaEY6YQbv5woGMjZJ+5Zxb0s/y8xUMWDxfwbiR7znnDmf8CAAAQMoZykC+ByTVHmKVVQqClnPOPaZg3g3mzgAAAONCIsZMTdPBk5NVKPETMgIAAHhpKDMVJ4yZXa7gVAXKyck5ftGiRUfy4QEASFkbq+qVn5mmaROykl3KuPTkk0/uc871nRxXUmLCVKUOnum1TAPMJuycu0HSDZK0YsUKt3bt2gQ8PAAAY9+rrvuHzjqqRF+5aFmySxmXzGzAUy0loptvtaT3hN/qO0nSgXBmZgAAkFD9zbuKZBu0ZcrMblZwhvIiM6tQMIV+miQ5534s6W4F3+QrV3A6j0tHq1gAAMYr5tj216Bhyjl3qBNkygVzK3wsYRUBAIB+9XtCICQdM6ADAJASaJryFWEKAIAU4BwjpnxFmAIAIEXQzecnwhQAACmATj5/EaYAAEgRRkeflwhTAACkAMfcCN4iTAEAkCIYM+UnwhQAACmAdil/EaYAAEgBTI3gL8IUAAApwujn8xJhCgCAFMAAdH8RpgAAAEaAMAUAQAqgXcpfhCkAAFIEQ6b8RJgCACAV0DTlLcIUAAApwInTyfiKMAUAQIqgm89PhCkAAFIAUyP4izAFAECKoGHKT4QpAABSAO1S/iJMAQCQIhgz5SfCFAAAKYAhU/4iTAEAkAKcHCc69hRhCgCAFEGU8hNhCgCAFEA3n78IUwAApAqaprxEmAIAIAXQMOUvwhQAACmCc/P5iTAFAEAqoGnKW4QpAABSQDA1QrKrQH8IUwAApAiylJ8IUwAApACmRvAXYQoAgBRBN5+fCFMAAKQAGqb8RZgCACAFOOeYGsFThCkAAFIE3Xx+IkwBAJAC6ObzF2EKAIAUQcOUnwhTAACkAKZG8BdhCgCAVMGgKS8RpgAAAEaAMAUAgOdc2MdHu5SfCFMAAKQIevn8RJgCAMBzDD73G2EKAIAUwQzofiJMAQDgORqm/EaYAgAgRTBmyk+EKQAAPOcYNOU1whQAAJ7riVI0TPmJMAUAQIqgm89PhCkAADxHL5/fCFMAAKQIo2nKS4QpAAA855gcwWuEKQAAgBEgTAEA4DnGTPmNMAUAQIpgyJSfhhSmzOxcM9tkZuVmdlU/y2eY2X1m9rSZrTez8xNfKgAA4xvn5vPToGHKzKKSrpd0nqTFki42s8V9VvucpFudc8dJeoekHyW6UAAAxiu6+fw2lJapEySVO+e2OOfaJd0iaVWfdZyk/PD3Akm7ElciAACQ6ObzVWwI60yTtDPueoWkE/usc42ke83sE5JyJJ2VkOoAAABTI3guUQPQL5Z0o3OuTNL5kn5tZq/YtpldbmZrzWxtdXV1gh4aAIDxgYYpPw0lTFVKmh53vSy8Ld5lkm6VJOfco5IyJRX13ZBz7gbn3Arn3Iri4uLDqxgAgHGGMVN+G0qYWiNpvpnNNrN0BQPMV/dZZ4ek10qSmR2lIEzR9AQAQAL0ZCnGTPlp0DDlnOuU9HFJ90jaqOBbe8+b2bVmdkG42qclfdDMnpF0s6T3OUeOBgAgkZgawU9DGYAu59zdku7uc9vVcb9vkHRKYksDAACSRPuE35gBHQCAFEE3n58IUwAAeI52Kb8RpgAA8By9fH4jTAEAkCKMfj4vEaYAAPAdLVNeI0wBAJAiaJfyE2EKAADPcW4+vxGmAABIEQyZ8hNhCgAAz/FtPr8RpgAA8FzvufmSWgUGQpgCACBFMDWCnwhTAAB4jnPz+Y0wBQBAiqBhyk+EKQAAPEe7lN8IUwAApAgapvxEmAIAwHMMmfIbYQoAAM/1zoDOoCkvEaYAAEgRRCk/EaYAAPAd3XxeI0wBAJAi6OXzE2EKAADP0TDlN8IUAAApwhg15SXCFAAAnmNqBL8RpgAA8FzP1AiMmfITYQoAgBRBlvITYQoAAM/Rzec3whQAACmCbj4/EaYAAPAcDVN+I0wBAJAimBrBT4QpAAA85xg05TXCFAAAnuvNUjRMeYkwBQBAiiBL+YkwBQAAMAKEKQAAUoQxN4KXCFMAAHiO8ed+I0wBAOC53nPzJbkO9I8wBQAAMAKEKQAAPNfTzceQKT8RpgAASBGEKT8RpgAA8Bzjz/1GmAIAIEVwbj4/EaYAAPAc5+bzG2EKAADP9Z6aj4YpLxGmAAAARoAwBQCA5+jl8xthCgCAFMG5+fxEmAIAwHs0TfmMMAUAQIqgXcpPhCkAADzHmCm/EaYAAPAcUyP4jTAFAECKYAZ0PxGmAADwHN18fiNMAQCQIujm8xNhCgAAzzmmRvAaYQoAgBRBw5SfCFMAAHiOMVN+I0wBAOC5njDFmCk/DSlMmdm5ZrbJzMrN7KoB1nmbmW0ws+fN7HeJLRMAANDR56fYYCuYWVTS9ZJeJ6lC0hozW+2c2xC3znxJ/y3pFOfcfjMrGa2CAQAYbxiA7rehtEydIKncObfFOdcu6RZJq/qs80FJ1zvn9kuSc25vYssEAAB08/lpKGFqmqSdcdcrwtviLZC0wMweNrPHzOzc/jZkZpeb2VozW1tdXX14FQMAMM4wAN1viRqAHpM0X9IZki6W9FMzK+y7knPuBufcCufciuLi4gQ9NAAA4wMNU34aSpiqlDQ97npZeFu8CkmrnXMdzrmtkl5UEK4AAADGtKGEqTWS5pvZbDNLl/QOSav7rHOHglYpmVmRgm6/LQmsEwCAcevlqRFom/LRoGHKOdcp6eOS7pG0UdKtzrnnzexaM7sgXO0eSTVmtkHSfZKudM7VjFbRAACMR0QpPw06NYIkOefulnR3n9uujvvdSfpUeAEAAAnE1Ah+YwZ0AABSBL18fiJMAQDgOaZG8BthCgAAz/VkKVqm/ESYAgAAGAHCFAAAnnNhP5/xfT4vEaYAAEgVZCkvEaYAAPAc48/9RpgCACBF0DDlJ8IUAACeY2oEvxGmAADwXjgAnbkRvESYAgAgRRCl/ESYAgDAc3Tz+Y0wBQBAiqCXz0+EKQAAPEfDlN8IUwAApAhmQPcTYQoAAM8xZspvhCkAADzXe24+Gqa8RJgCACBFkKX8RJgCAMBz9PL5jTAFAECqoGnKS4QpAAA8xwB0vxGmAABIEUyN4CfCFAAAnnOMmvIaYQoAAN+FWYqpEfxEmAIAIEWQpfxEmAIAwHN08vmNMAUAQIow+vm8RJgCAMBzTI3gN8IUAAApgoYpPxGmAADwHFMj+I0wBQCA53q6+WiY8hNhCgCAFEE3n58IUwAAeI5OPr8RpgAASBk0TfmIMAUAgOcccyN4jTAFAECKYMyUnwhTAAB4jnYpvxGmAADwHVMjeI0wBQBAiuDcfH4iTAEA4DlmQPcbYQoAgBRBu5SfCFMAAHiOmRH8RpgCAMBzvefmo2nKS4QpAACAESBMAQDguZ5ePmPUlJcIUwAApAi6+fxEmAIAwHOcm89vhCkAAIARIEwBAOA52qX8RpgCAMBzTI3gN8IUAAApgm/z+YkwBQCA9+jo8xlhCgCAFEE3n58IUwAAeI6ZEfxGmAIAIEXQMuUnwhQAAJ6jYcpvQwpTZnaumW0ys3Izu+oQ673ZzJyZrUhciQAAjG+9UyPwbT4vDRqmzCwq6XpJ50laLOliM1vcz3p5kj4p6fFEFwkAAOjm89VQWqZOkFTunNvinGuXdIukVf2s90VJX5PUmsD6AAAY9xwdfV4bSpiaJmln3PWK8LZeZrZc0nTn3F0JrA0AAMShYcpPIx6AbmYRSd+W9OkhrHu5ma01s7XV1dUjfWgAAMYFpkbw21DCVKWk6XHXy8LbeuRJWiLpfjPbJukkSav7G4TunLvBObfCObeiuLj48KsGAGAcYsyUn4YSptZImm9ms80sXdI7JK3uWeicO+CcK3LOzXLOzZL0mKQLnHNrR6ViAADGGRqm/DZomHLOdUr6uKR7JG2UdKtz7nkzu9bMLhjtAgEAGO9cbz8fTVM+ig1lJefc3ZLu7nPb1QOse8bIywIAAH3RzecnZkAHAAAYAcIUAAApgoYpPxGmAADwHFMj+G1IY6YwdA2tHXpsS602VtVrcn6GZkzM0cxJ2ZqSn6lIZOx/pnDOyejUB4BRwf9XPyUtTDW0duj2JytU19KhA83twc+WDnV2O5mkiJnM1Pt7JGKKRV7+GY2Y0qIRpUVN6dGo0mKm9GhEGbGIcjJiys2IKS8zTXmZMeVlxmQy7aht1vbaJu2oadb2mmbtqG1WdnpUC6bkaeHkPC2YnKcFk3M1KTdDktTW2aUDLR2qD2tr6+xWTnpMORlRZafHlJMeU0ZaRM/vqtdDm/fpofJqPb2jTp3dr/wIkR6LaPqELJUWZCknI6rcjDTlZkSVE9a5YHKuls+YoAk56SN6XZ1z2lbTrKe279eTO/Zre01T+LpElZEWCX5PiyhiJueCUxQEP6WISZNyMjQ5P1OT84OfJfkZyk6P6UBLh+qa23WguUN1LR3a39yuvfVt2lPfqqoDrdpT36rd9a1qbO3U8hkTdPrCYp02v1hHT81/RYh0zqmuuUM79zcrFomoMDtNhdlpykqLHrF/FM457aht1sPlNXpmZ51yM2MqyctQSX6GSvIyVZKXoYKsNDkFnwi7nVO3C16r4rwMZaZFB9x2dUObHtxcrQc379O+xjaV5AWv55SCzN7fs9Kj4XEcUdRM0agpNyOmgqy0I/L8AaQWTifjt6SFqW01zfr0H56RFHw7oSArTQVZaYpGTArf3HvevLqdU3e3U5dz6up26ux26uoKfrZ3daurn/ByKOnRiMomZmnGxGw1tXXqzmd26Xetnb3LC7LS1NbZpdaO7iFv00xaOq1Al582RyvnF+m46RO0r7FN22sODnB7G1pV3dCmxrbO3kt8/XOKc3T8jAlaPnOCFkzOU31Lh6ob27SvsU37GtpV09Smjq5uZcaiykyPBj/TIopGTBur6vXUjjrVNrVLkvIyYppbkqv67k61d3arrbNLbZ3dauvsVrcLQquZhT+lrm6nupaOITcnWxi+SgsyNX1itl41a6IyYhE9trVG37hnk75xzyZNyknXqfOLNLUwS9trm7WjplnbaprUEPd6x++Xguw0FeVmaOHkXC0qzddRpfk6qjRPxbkZMjM1tnVq274mbQ0v22ua1d7VHR4rTt3dwfGSFo2oKDddxXkZvZdJORnaVtOkR8pr9FD5PlXWtUiSJmSnqbWjWy0dXUN+3tMnZGt+Sa7mhZdJuel6fGutHnxxnzZU1UuSJuaka/qELL20t1F7G9r6Ddl9t/uqmRP1+mWlOm/JFJXkZw64bne3GxctnQACPf+X+av3k7kkdcQuXnac++t9D6kwK115mbERvTF0dTt1dAUhob2zW01tnWpo7VRDW4caWjvV2NqpLuc0fUK2Zk7K1uT8zCC0hZxz2tvQpk27G/TingZtr2lWVnpUBVlpyg9DXkFWmtKjEbV0dKqprUvN7S//nFWUo1PmFh1Wq5JzTk3tXXq+8oCe3LE/aFHavl/7mztesW52elRFuRlKj0XU2hGEvdaOLrV0dKmr2x0UxJbPmKD5JbnDfl07urq1r7FNe8JWp731rWpu71JhdpoKstJ7W5EKstI0KSeopT/VDW16qLxaD7y4Tw9urlZdc4fKJmRpxqQczZqUrRkTszV9YnZvK1VdS4fqmjt0oKVduw+0atPuBu068PI5syflpCsaMe1taDvocabkZyo7PSqzsAUzbNEMnke7DrS88nXMz4zp5LmTtHJekV49r0hzinIkSY1tndrb0Ka99W3a29CqhtbOuO1KFv4b23WgRZv3NuqlvY3aUt2k9q4gdMcipuNnTtBpC4p1+oJiLS59uVWuu9uppqk9eE0bWtXW0a3O7qC1q7Mr+JBQWdeivz23W5v2NATBatZEvWFZqaYWZGlbTRAet9U0adu+Zu060KK5xbk6ac5EnTh7kk6cM1EleQeHr86ubu1paNPuAz2hMV0Tc9KVn5lGEANSzB+fqtCnbn1G/77yDM2clJPscsYlM3vSOfeKs7tISQxTK1ascGvXMkl6f5xzvS0vhdnpKs7NUFFeurLTB25I7Op2BwVEnzjn1O007Prqmtv1wu4Gbayq1wtVDepyQWCcPSlHs4tzNHNijrLSB+5uk6TWji7ta2xTdUNwmZyfqSXTChL2WnV2dWtHbbP2NrRpybQC5WaMvLF3854G3fVsle5aX6XNext7b8/PjGl2ca5mT8rWlIIsvbC7Xmu21qqpPWhRm1uco3kludrb0KaquiC09dcYFrEgWBXlZujkuZN09tGTdcKsiYpF+T4K4Kvbn6zQp/9AmEomwhSQosr3NuhAS6dmF+VoQnbaK8aUdXZ167ld9Xp8S40e21KjHbXNmlKQqdKCLE0tyFRpYZZKCzJlZtrf1K7apnbtbw5+Vuxv0aNbatTe2a3C7DSduahEZy+eomOnF6q5PeiCbmgNLk1tnZpdnKNl0woIXUAS9ISpB658jWZMyk52OePSocIU3+YDPDavJO+Qy2PRiI6dXqhjpxfqQ6fPHfb2m9o69eDmat37/B79c+Ne/fGpykOun58Z08r5RTp1frFOW1CsaYVZw35MAMPH8HO/EaaAcSwnI6Zzl5Tq3CWl6ujq1pqttXppX5Pyer8RG1NuZkxZaVFtqKrXAy8G4+Dufna3JGl2UY4WTcnT3OKXB+PPKc45ZJc0gOHr6UViZgQ/8WsdzOsAABiFSURBVB8PgCQpLRrRq8NB+f2ZU5yrNyybKuecyvc26t8vVuvxrbXatLtB927Yc9C3UhdOztMFx07VhcdNo/UKwJhHmAIwLGam+ZPzNH9ynj5w6hxJwZxsO2qaVb63UeV7G/XA5ure6TFOnD1Rbzpums5bWso8WsBhopvPb4QpACOWEYv2BixJ+sRr52tHTbP+vK5Sf3q6Ulf98Vldvfp5nb14st7+quk6ZW4R0zMAh4FuPj8RpgCMihmTsvWJ187Xx8+cp2crD+iPT1XqjnWVunN9laYVZumtK8r01hXT6QYEhoKmKa8RpgCMKjPTsrJCLSsr1FXnLdLfN+zR79fs1Hf/sVnf++dmrZxXpDcdN01nHz0lIfN0AWMZ5+bzE/+5ABwxmWlRvfGYqXrjMVO1s7ZZf3iyondm54zYszpr8WStOmaqTl9YrIzYoSdkBcYTzs3nN8IUgKSYPjFbn3rdAv3nWfP11I79+vO6XbprfTDze35mTOccPUXnLZ2iU+YVEaww7nFuPr8RpgAklZnp+JkTdfzMifr8Gxbr4fJ9Wr1ul/723G794ckK5WXEdOZRJTpvyRSdvqBk0FMIAWMZvXx+IkwB8EZaNKIzFpbojIUlauvs0iPlNfrbc7t174bd+vO6XcpKi+rC46bpspWzNa8kN9nlAkcMnXx+I0wB8FJGLKrXLCrRaxaV6LquJXpia63+vG6Xbn+qQjc/sUNnLirRB06drZPnTGJQLsYNo6PPS4QpAN6Lxc3OfuW5C/Wbx7br149u1zt/+rgWl+brQ6fP0QXHTCVUYcxyNE15jdO/A0gpRbkZuuKsBXr4qjP11YuWqr2rW5+8ZZ0uu2mt9ja0Jrs8YFTxecFPhCkAKSkzLap3nDBD915xmr7wxmDg+jnfeUB3P1uV7NKAhGNqBL8RpgCktEjEdOkps3XXf6zU9InZ+uhvn9IVtzytA80dyS4NSBimRvAbYQrAmDCvJE+3f+TVuuKs+frL+iqd890HdOf6Xers6k52aUDikKa8RJgCMGakRSO64qwF+tNHX628zJg+/runderX79P195WrprEt2eUBh41OPr8RpgCMOcvKCvW3K07TDZccr7nFufrGPZt08lf/pU/f+oyerTggx1ejkKKYGsFPTI0AYEyKRkxnHz1FZx89ReV7G3TTI9t1+1MVuv2pCs0tztHrl5bq/GWlWjg5jykV4D8+AHiNMAVgzJtXkqcvXrhEV567MDwH4C798L5yff9f5ZpTnKPzl5TqwuOmal5JXrJLBQ6J3O8nwhSAcSM/M02XnDRTl5w0U9UNbfrb87t19/oq/ej+cl1/f7nedvx0ffrsBSrJz0x2qcBBaJfyG2EKwLhUnJdxULD6yb9f0k2PbtNf1u/SR06fqw+eNkeZaZxUGX5gagS/MQAdwLhXnJehz71hse79z9N16vwifevvL+rMb96vO56uVHc3bQLwB+P7/ESYAoDQ7KIc/eSSFbrl8pM0MTddV/x+nV7zrfv1g39uVmVdS7LLwzjGN1D9RpgCgD5OmjNJqz+2Ut+/+DhNLcjSt/7+olZ+7V96988e15/XVaq1oyvZJWKcol3KT4yZAoB+RCKmC46ZqguOmaqdtc26/akK3fZkhT55yzrlZcT0hmOm6i3Hl2n5jEK6XjDqaJfyG2EKAAYxfWK2rjhrgf7jzPl6bGuNbltboTuertTNT+zQnKIcvfn4Ml20fJpKC7KSXSrGOHK7nwhTADBEkYjp1XOL9Oq5Rbr2wk7d/WyVbnuyQt+4Z5O+ee8mLZlaoLIJWZpWmKVp4c+phVkqyEpTdnpUWelRZcaiikR4R8TwMGTKb4QpADgMuRkxvW3FdL1txXRtr2nS7U9V6ukd+7VpT4P+9cJetXUOfILlrLSo8jJjmjExW7OKcjRrUs/PHE2fmK38zBhdhzhIT5bidDJ+IkwBwAjNnJSjT71uQe9155xqmtq1q65Fu+paVN/aqZb2LjW3d6mlvVMtHV2qa+7Q9tpmPfBitW5rOPgkzLkZMU0tzOxt2ZqSn6lIxNTe2a2Orp6LU1rUNH9ynhZNydOCyXnMizUekKW8RJgCgAQzMxXlZqgoN0PLygoHXb+5vVPba5q1bV+TKva3qDIMYbsOtOiZigOqbWrvXTctakqLRpQWjai1o6u3BSxi0qxJOVpUmqf8zDS1dXarrbNLbR3dauvsVmd3t2ZNytHCKXlaOCVPi6bka2JO+qi9BkgspkbwG2EKAJIsOz2mo0rzdVRpfr/L2zq7ZDKlRe2g7r+ubqcdtc16oapeL+xu0KbdDdqwq14tHV3KiEWVEYsoIy2izFjQYnXvhj26Zc3O3vsX52VoxcwJumzlbK2YNXF0nyQSgt5fPxGmAMBzGbH+u++iEdPsohzNLsrReUtLB92Oc07VjW3aFAavjVUN+tcLe/TX53ZrxcwJ+vDpc3XmohIGyAPDRJgCgHHCzFSSl6mSvEydOr9YUtDFeOuanfrpg1v1gV+t1YLJubr8tLladexUpUWZ19kXnJvPb/ylAMA4lp0e0/tOma37rzxD3337sYqY6TN/eEbv++UTamlnpndgKAhTAAClRSO68Lhp+usnT9VXL1qqR16q0WU3rSFQecKFkyMwZYafCFMAgF5mpnecMEPfeusxenRLjd5/4xo1t3cmuyyEiFJ+IkwBAF7houVl+vbbjtHjWwlUPmBmBL8RpgAA/XrTcWX6ztuP1RNba3XpLwlUPqCXz0+EKQDAgFYdO03fefuxWrOtVu/75Rq1dTKGKhlomPIbYQoAcEirjp2mb7zlGD2xtVZ/fnpXsssZl16eGoGmKR8RpgAAg7po+TQtmpKnXzy8lVObAH0QpgAAgzIzXXrKLL2wu0GPbalNdjnjzstTIyS5EPSLMAUAGJJVx07ThOw03fjI1mSXAniFMAUAGJLMtKguPmGG/r5hj3bWNie7nHGFnlW/EaYAAEN2yckzZWb61aPbkl3KuEQ3n5+GFKbM7Fwz22Rm5WZ2VT/LP2VmG8xsvZn908xmJr5UAECylRZk6dwlU3TLmp1qamPeKUAaQpgys6ik6yWdJ2mxpIvNbHGf1Z6WtMI5t0zSbZK+nuhCAQB+eP8ps9TQ2qk/Pl2Z7FLGjZ5vUDI1gp+G0jJ1gqRy59wW51y7pFskrYpfwTl3n3OupwP9MUlliS0TAOCL5TMmaFlZgW5kmoQjjm4+Pw0lTE2TtDPuekV420Auk/TXkRQFAPCXmel9r56ll6qb9ODmfckuZ1wgs/otoQPQzezdklZI+sYAyy83s7Vmtra6ujqRDw0AOIJev6xURbkZ+uXDTJNwJNEw5aehhKlKSdPjrpeFtx3EzM6S9FlJFzjn2vrbkHPuBufcCufciuLi4sOpFwDggYxYVO8+aYbu21Strfuakl3OmEfDlN+GEqbWSJpvZrPNLF3SOyStjl/BzI6T9BMFQWpv4ssEAPjmnSfOUFrUdNMj25JdyrhhDJry0qBhyjnXKenjku6RtFHSrc65583sWjO7IFztG5JyJf3BzNaZ2eoBNgcAGCNK8jJ19uIp+utzVckuZcxjzJTfYkNZyTl3t6S7+9x2ddzvZyW4LgBAClg+c4LuerZKe+tbVZKfmexyxqzec/MluQ70jxnQAQCHbVlZgSTp2coDSa5kbKvc36Ki3HRFIsQpHxGmAACHbXFpviImra8gTI2mDVX1Wjy1INllYACEKQDAYcvJiGleSa6eo2Vq1LR3dmvznkYtLs1PdikYAGEKADAiS6cVan3lAWZDHyUvVTeqvatbR5XmJbsUDIAwBQAYkWVlBapuaNOe+n6nGMQIbayqlyQdPZWWKV8RpgAAI7I0HIS+vqIuyZWMTRt21SszLaLZRbnJLgUDIEwBAEZkcWm+ohHjG32jZENVvRZOzlOUb/J5izAFABiRzLSo5pfk8o2+UeCc08aqei2mi89rhCkAwIgtKyvQswxCT7jd9a3a39zBN/k8R5gCAIzY0rJC1Ta1q7KuJdmljCkbdgWDz48iTHmNMAUAGLFl08KZ0OnqS6ieMLWIMOU1whQAYMQWleYpLWpazyD0hNq4u16zJmUrN2NIp9JFkhCmAAAjlhGLauGUPFqmEmzDLgafpwLCFAAgIZZOK2QQegI1tnVqW02zjppCmPIdYQoAkBDLygp0oKVDO2sZhJ4IL4Qzn9My5T/CFAAgIZaGg9DXVzITeiJsJEylDMIUACAhFkzOU3o0wripBNlQVa/C7DRNyc9MdikYBGEKAJAQ6bGIjirNYyb0BNmwq16LS/NlxmlkfEeYAgAkzNKyAj1XeUDd3QxCH4nOrm69sLuBmc9TBGEKAJAwy6YVqqGtU9tqmpJdSkrbVtOkts5uZj5PEYQpAEDCLC0LZ0Jn8s4ReX4Xg89TCWEKAJAw80tylRGLMG5qhDZU1Ss9GtHc4txkl4IhIEwBABImFo3o6Kn5fKNvhDZWNWj+5Fylx3ibTgXsJQBAQi0rK9Rzuw6oi0Hoh23DrnrGS6UQwhQAIKGWTitQc3uXtlQ3JruUlLS3oVX7Gtv4Jl8KIUwBABJqWTgInXFTh2cDg89TDmEKAJBQc4pzlZ8Z0xNba5NdSkraWNUgSZzgOIUQpgAACRWNmF49t0gPle+Tc4ybGq4NVfWaVpilguy0ZJeCISJMAQAS7pT5Raqsa9HWfUzeOVwbdh2giy/FEKYAAAl36rwiSdJD5fuSXElqaW7v1NZ9TXyTL8UQpgAACTdzUrbKJmTpoc2EqeG44YEt6nbSyjCMIjUQpgAACWdmOnV+kR59qUadXd3JLicllO9t1I/ue0mrjp2qE2ZPTHY5GAbCFABgVJwyr0gNbZ16hikSBtXd7fQ/f3xWWelRff4Ni5NdDoaJMAUAGBWnzC2SmfQw46YGdevanXpiW60+e/5RKsrNSHY5GCbCFABgVEzISdeSqQWMmxpEdUObvnz3Rp04e6LeuqIs2eXgMBCmAACjZuX8Ij21Y78a2zqTXYq3rr1zg1o7uvXli5bKzJJdDg4DYQoAMGpWzitSZ7fT41tqkl2Kl+7ftFd/eWaXPvaaeZpbnJvscnCYCFMAgFFz/MwJyohFmG+qH83tnfrcHc9pbnGOPnzGnGSXgxGIJbsAAMDYlZkW1QmzJ465cVPf/+dmbd3XpK+/ZZnSoodul9hZ26wbH9mm5vYudXZ1q6vbqaPbqWJ/syr2t+jWD52sjFj0CFWO0UCYAgCMqlPnF+nLd7+g3QdaNaUgM9nlDGhXXYsm5aYPGmzWbKvVt//+oiQpNyOmL164ZMB1axrb9O6fP66qulYVZKcpFjFFI6a0aETRiOnKcxYyp9QYQJgCAIyqU8LZvB8u36c3H+/ft9Uq9jfr239/UX96ulIr5xXpxktPUDTS/0Dw1o4uXXX7epVNyNJrF5Xopke3a/7kXL3n5Fn9rvuBX63V7gOtuvnyk3T8zAmj/EyQLIyZAgCMqqOm5GtSTrp346Zqm9p17V826Mxv/lt3ra/S646arAc379M379004H1+dF+5Xqpu0nVvWqqr33i0XruoRP/7lw16cHP1Qet1dTt98pantW5nnb73jmMJUmMcYQoAMKoiEdMp84r0UPk+OeeSXY6a2zv1/X9u1mlfv083PrJVbzpumu6/8gzd8J4VuviEGfq/+1/SX5+tesX9Xthdrx/d/5IuOm6aTl9QrGjE9L2Lj9O84lx99LdP6aXqxt51r7tro+55fo8+9/rFOndJ6ZF8ekgCwhQAYNStnFek6oY2bdrTkNQ6Wtq7dMnPn9C3//6iTpk3Sff+52n62luWqbQgS5J0zQWLdez0Qn3mD89oc1ytXd1OV93+rAqy0g463UtuRkw/e+8KpUcj+sBNa1XX3K5fPLRVv3h4qy49ZZYuWzn7iD9HHHmEKQDAqFs5Pxg3lcxv9XV0detjv3tKT+3Yr+vfuVw/uWSF5pXkHbRORiyq/3v3cmWlR/WhXz+p+tYOSdJNj2zTup11uvqNizUhJ/2g+0yfmK0fX3K8KvY36+0/eUxfvGuDzjl6sj73es6xN14QpgAAo25qYZbmFOckbdyUc07//cdn9a8X9uqLq5bo9csG7norLcjSD9+5XNtrm/XpW5/RztpmffPeTXrNwmJdcMzUfu/zqlkT9eU3LdWmPQ06pqxQ3337cQMOYsfYw7f5AABHxKnzinTr2go9s7NOx0wvPKKP/fV7Num2Jyt0xVnz9e6TZg66/klzJumz5x+la+/coKd31EmSvvSmQ5/u5a0rpmtWUY4WTslTVjrzRo0ntEwBAI6It71qurLSo1p1/cP6yG+eVPnexsHvlAA/f2ir/u/+l/SuE2fok6+dP+T7XXrKLK06dqr2Nbbp/52zUNMKswa9z6tmTVR+ZtpIykUKsmR9s2LFihVu7dq1SXlsAEByNLR26GcPbtXPHtyilo4uvXl5ma543YIhBZXD8ed1lfrkLet07tFTdP27lg+76621o0tPbK3VynlFitBtN66Z2ZPOuRX9LiNMAQCOtJrGNv3o/pf068e2S046ae4k5WZElZ0eU3Z68DMnPapJuRkqycvQ5PxMleRnaFJOuqIR04GWDlXWtaiqrlVVB1q060Cr9jW0aX9zh+qa21Xb3K665g7VNrXrxNkTddP7T1BmGl1vOHyEKQCAl3bVteiH95Xr+coDamrvUkt7l5raO9Xc3qX2zu5XrB8xKT0WUWvHwcvSoqZJORmakJOuCdlpmpCdrsLsNJUWZOo9r55F1xtG7FBhigHoAICkmVqYpS+/aWm/y9o7u1XT1KY99W3aW9+qvQ3Bz5aOLk3Oz9TUwqzgUpCpotwMuuGQNIQpAICX0mMRlRZk9U6oCfiKb/MBAACMAGEKAABgBAhTAAAAIzCkMGVm55rZJjMrN7Or+lmeYWa/D5c/bmazEl0oAACAjwYNU2YWlXS9pPMkLZZ0sZn1PXvjZZL2O+fmSfqOpK8lulAAAAAfDaVl6gRJ5c65Lc65dkm3SFrVZ51Vkm4Kf79N0mvtUCcwAgAAGCOGEqamSdoZd70ivK3fdZxznZIOSJqUiAIBAAB8dkTnmTKzyyVdHl5tM7PnjuTjI+GKJO1LdhEYEfZhamP/pT72YeqYOdCCoYSpSknT466Xhbf1t06FmcUkFUiq6bsh59wNkm6QJDNbO9C07EgN7MPUxz5Mbey/1Mc+HBuG0s23RtJ8M5ttZumS3iFpdZ91Vkt6b/j7WyT9yyXrpH8AAABH0KAtU865TjP7uKR7JEUl/cI597yZXStprXNutaSfS/q1mZVLqlUQuAAAAMa8IY2Zcs7dLenuPrddHfd7q6S3DvOxbxjm+vAP+zD1sQ9TG/sv9bEPxwCjNw4AAODwcToZAACAESBMAQAAjABhCgAAYAS8DFNmNsPM7jCzX/R3YmX4zcwiZnadmf3AzN47+D3gIzPLMbO1ZvaGZNeC4TOzC83sp+FJ6M9Odj0YmvDv7qZw370r2fVgaBIepsIAtLfv7OZmdq6ZbTKz8iEEpKWSbnPOvV/ScYmuEQNL0P5bpWBy1w4Fpx/CEZSgfShJ/yXp1tGpEoeSiH3onLvDOfdBSR+W9PbRrBeHNsz9eZGC978PSrrgiBeLw5Lwb/OZ2WmSGiX9yjm3JLwtKulFSa9T8Oa6RtLFCuat+kqfTbxfUpeCEyY7Sb92zv0yoUViQAnaf++XtN859xMzu80595YjVT8Stg+PUXB+zUxJ+5xzdx6Z6iElZh865/aG9/uWpN865546QuWjj2Huz1WS/uqcW2dmv3POvTNJZWMYEn5uPufcA2Y2q8/NJ0gqd85tkSQzu0XSKufcVyS9ogvBzD4j6Qvhtm6TRJg6QhK0/yoktYdXu0avWvQnQfvwDEk5khZLajGzu51z3aNZN16WoH1okr6q4I2ZIJVEw9mfCoJVmaR18nQoDl7pSJ3oeJqknXHXKySdeIj1/ybpGjN7p6Rto1gXhma4+++Pkn5gZqdKemA0C8OQDWsfOuc+K0lm9j4FLVMEqeQb7t/hJySdJanAzOY55348msVh2Aban9+X9EMze72kvySjMAzfkQpTw+Kce07BOf6QgpxzzZIuS3YdGDnn3I3JrgGHxzn3fQVvzEghzrkmSZcmuw4Mz5FqQqyUND3uell4G1ID+y/1sQ9TH/twbGF/jiFHKkytkTTfzGabWbqCEyGvPkKPjZFj/6U+9mHqYx+OLezPMWQ0pka4WdKjkhaaWYWZXeac65T0cUn3SNoo6Vbn3POJfmyMHPsv9bEPUx/7cGxhf459nOgYAABgBPjaJQAAwAgQpgAAAEaAMAUAADAChCkAAIARIEwBAACMAGEKAABgBAhTAAAAI0CYAgAAGAHCFAAAwAj8f3LB5Q8Vmbj7AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPcIR0yMuWbm"
      },
      "source": [
        "**3. Entrainement**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PD2YaeKpzCS-"
      },
      "source": [
        "# Charge les meilleurs poids\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DstONd8uWbm",
        "outputId": "801cda2e-8c15-4012-a4f6-faae586fd8d6"
      },
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "class TimingCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, logs={}):\n",
        "        self.logs=[]\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.starttime = timer()\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.logs.append(timer()-self.starttime)\n",
        "\n",
        "cb = TimingCallback()\n",
        "\n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.2,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0.05)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(learning_rate=lr_schedule,momentum=0.9)\n",
        "#optimiseur=tf.keras.optimizers.SGD(0.2,momentum=0.9)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur,metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle\n",
        "historique = model.fit(dataset_norm,validation_data=dataset_Val_norm, epochs=500,verbose=1, callbacks=[CheckPoint,cb])\n",
        "\n",
        "print(cb.logs)\n",
        "print(sum(cb.logs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "30/30 [==============================] - 4s 53ms/step - loss: 0.0311 - mae: 0.1797 - val_loss: 0.0708 - val_mae: 0.3127\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.03384, saving model to poids.hdf5\n",
            "Epoch 2/500\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.0347 - mae: 0.1921 - val_loss: 0.0962 - val_mae: 0.3728\n",
            "\n",
            "Epoch 00002: loss did not improve from 0.03384\n",
            "Epoch 3/500\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.0303 - mae: 0.1823 - val_loss: 0.1353 - val_mae: 0.4516\n",
            "\n",
            "Epoch 00003: loss improved from 0.03384 to 0.02887, saving model to poids.hdf5\n",
            "Epoch 4/500\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.0243 - mae: 0.1566 - val_loss: 0.1258 - val_mae: 0.4372\n",
            "\n",
            "Epoch 00004: loss improved from 0.02887 to 0.02580, saving model to poids.hdf5\n",
            "Epoch 5/500\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.0234 - mae: 0.1526 - val_loss: 0.0560 - val_mae: 0.2785\n",
            "\n",
            "Epoch 00005: loss improved from 0.02580 to 0.02351, saving model to poids.hdf5\n",
            "Epoch 6/500\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.0197 - mae: 0.1493 - val_loss: 0.1015 - val_mae: 0.3864\n",
            "\n",
            "Epoch 00006: loss improved from 0.02351 to 0.02220, saving model to poids.hdf5\n",
            "Epoch 7/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0201 - mae: 0.1452 - val_loss: 0.0423 - val_mae: 0.2372\n",
            "\n",
            "Epoch 00007: loss improved from 0.02220 to 0.02141, saving model to poids.hdf5\n",
            "Epoch 8/500\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.0250 - mae: 0.1703 - val_loss: 0.0793 - val_mae: 0.3335\n",
            "\n",
            "Epoch 00008: loss did not improve from 0.02141\n",
            "Epoch 9/500\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.0210 - mae: 0.1501 - val_loss: 0.0534 - val_mae: 0.2659\n",
            "\n",
            "Epoch 00009: loss improved from 0.02141 to 0.02099, saving model to poids.hdf5\n",
            "Epoch 10/500\n",
            "30/30 [==============================] - 1s 20ms/step - loss: 0.0233 - mae: 0.1544 - val_loss: 0.0444 - val_mae: 0.2396\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.02099\n",
            "Epoch 11/500\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.0181 - mae: 0.1367 - val_loss: 0.0426 - val_mae: 0.2323\n",
            "\n",
            "Epoch 00011: loss improved from 0.02099 to 0.01882, saving model to poids.hdf5\n",
            "Epoch 12/500\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.0205 - mae: 0.1442 - val_loss: 0.0374 - val_mae: 0.2144\n",
            "\n",
            "Epoch 00012: loss improved from 0.01882 to 0.01863, saving model to poids.hdf5\n",
            "Epoch 13/500\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.0191 - mae: 0.1396 - val_loss: 0.0373 - val_mae: 0.2121\n",
            "\n",
            "Epoch 00013: loss improved from 0.01863 to 0.01829, saving model to poids.hdf5\n",
            "Epoch 14/500\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.0156 - mae: 0.1311 - val_loss: 0.0312 - val_mae: 0.1887\n",
            "\n",
            "Epoch 00014: loss improved from 0.01829 to 0.01678, saving model to poids.hdf5\n",
            "Epoch 15/500\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.0189 - mae: 0.1399 - val_loss: 0.0309 - val_mae: 0.1859\n",
            "\n",
            "Epoch 00015: loss improved from 0.01678 to 0.01663, saving model to poids.hdf5\n",
            "Epoch 16/500\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.0162 - mae: 0.1302 - val_loss: 0.0264 - val_mae: 0.1685\n",
            "\n",
            "Epoch 00016: loss improved from 0.01663 to 0.01646, saving model to poids.hdf5\n",
            "Epoch 17/500\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.0144 - mae: 0.1243 - val_loss: 0.0197 - val_mae: 0.1388\n",
            "\n",
            "Epoch 00017: loss improved from 0.01646 to 0.01549, saving model to poids.hdf5\n",
            "Epoch 18/500\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.0162 - mae: 0.1282 - val_loss: 0.0233 - val_mae: 0.1497\n",
            "\n",
            "Epoch 00018: loss did not improve from 0.01549\n",
            "Epoch 19/500\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.0159 - mae: 0.1293 - val_loss: 0.0193 - val_mae: 0.1340\n",
            "\n",
            "Epoch 00019: loss did not improve from 0.01549\n",
            "Epoch 20/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0136 - mae: 0.1178 - val_loss: 0.0178 - val_mae: 0.1282\n",
            "\n",
            "Epoch 00020: loss improved from 0.01549 to 0.01414, saving model to poids.hdf5\n",
            "Epoch 21/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0142 - mae: 0.1206 - val_loss: 0.0197 - val_mae: 0.1394\n",
            "\n",
            "Epoch 00021: loss did not improve from 0.01414\n",
            "Epoch 22/500\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.0146 - mae: 0.1211 - val_loss: 0.0177 - val_mae: 0.1352\n",
            "\n",
            "Epoch 00022: loss improved from 0.01414 to 0.01386, saving model to poids.hdf5\n",
            "Epoch 23/500\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.0154 - mae: 0.1201 - val_loss: 0.0216 - val_mae: 0.1510\n",
            "\n",
            "Epoch 00023: loss did not improve from 0.01386\n",
            "Epoch 24/500\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.0129 - mae: 0.1186 - val_loss: 0.0189 - val_mae: 0.1331\n",
            "\n",
            "Epoch 00024: loss did not improve from 0.01386\n",
            "Epoch 25/500\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.0135 - mae: 0.1211 - val_loss: 0.0230 - val_mae: 0.1638\n",
            "\n",
            "Epoch 00025: loss did not improve from 0.01386\n",
            "Epoch 26/500\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.0158 - mae: 0.1248 - val_loss: 0.0187 - val_mae: 0.1319\n",
            "\n",
            "Epoch 00026: loss improved from 0.01386 to 0.01375, saving model to poids.hdf5\n",
            "Epoch 27/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0129 - mae: 0.1148 - val_loss: 0.0274 - val_mae: 0.1869\n",
            "\n",
            "Epoch 00027: loss improved from 0.01375 to 0.01352, saving model to poids.hdf5\n",
            "Epoch 28/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0151 - mae: 0.1215 - val_loss: 0.0191 - val_mae: 0.1315\n",
            "\n",
            "Epoch 00028: loss improved from 0.01352 to 0.01346, saving model to poids.hdf5\n",
            "Epoch 29/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0140 - mae: 0.1214 - val_loss: 0.0189 - val_mae: 0.1312\n",
            "\n",
            "Epoch 00029: loss improved from 0.01346 to 0.01318, saving model to poids.hdf5\n",
            "Epoch 30/500\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.0155 - mae: 0.1247 - val_loss: 0.0268 - val_mae: 0.1840\n",
            "\n",
            "Epoch 00030: loss did not improve from 0.01318\n",
            "Epoch 31/500\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.0169 - mae: 0.1305 - val_loss: 0.0204 - val_mae: 0.1477\n",
            "\n",
            "Epoch 00031: loss did not improve from 0.01318\n",
            "Epoch 32/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0146 - mae: 0.1187 - val_loss: 0.0189 - val_mae: 0.1406\n",
            "\n",
            "Epoch 00032: loss did not improve from 0.01318\n",
            "Epoch 33/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0119 - mae: 0.1174 - val_loss: 0.0185 - val_mae: 0.1383\n",
            "\n",
            "Epoch 00033: loss improved from 0.01318 to 0.01291, saving model to poids.hdf5\n",
            "Epoch 34/500\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.0135 - mae: 0.1187 - val_loss: 0.0199 - val_mae: 0.1460\n",
            "\n",
            "Epoch 00034: loss did not improve from 0.01291\n",
            "Epoch 35/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0139 - mae: 0.1197 - val_loss: 0.0173 - val_mae: 0.1291\n",
            "\n",
            "Epoch 00035: loss did not improve from 0.01291\n",
            "Epoch 36/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0112 - mae: 0.1133 - val_loss: 0.0227 - val_mae: 0.1625\n",
            "\n",
            "Epoch 00036: loss improved from 0.01291 to 0.01202, saving model to poids.hdf5\n",
            "Epoch 37/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0116 - mae: 0.1120 - val_loss: 0.0192 - val_mae: 0.1397\n",
            "\n",
            "Epoch 00037: loss did not improve from 0.01202\n",
            "Epoch 38/500\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.0121 - mae: 0.1128 - val_loss: 0.0177 - val_mae: 0.1361\n",
            "\n",
            "Epoch 00038: loss did not improve from 0.01202\n",
            "Epoch 39/500\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.0116 - mae: 0.1129 - val_loss: 0.0212 - val_mae: 0.1547\n",
            "\n",
            "Epoch 00039: loss did not improve from 0.01202\n",
            "Epoch 40/500\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.0133 - mae: 0.1175 - val_loss: 0.0184 - val_mae: 0.1347\n",
            "\n",
            "Epoch 00040: loss did not improve from 0.01202\n",
            "Epoch 41/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0145 - mae: 0.1210 - val_loss: 0.0187 - val_mae: 0.1437\n",
            "\n",
            "Epoch 00041: loss did not improve from 0.01202\n",
            "Epoch 42/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0129 - mae: 0.1161 - val_loss: 0.0191 - val_mae: 0.1414\n",
            "\n",
            "Epoch 00042: loss did not improve from 0.01202\n",
            "Epoch 43/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0123 - mae: 0.1171 - val_loss: 0.0176 - val_mae: 0.1307\n",
            "\n",
            "Epoch 00043: loss did not improve from 0.01202\n",
            "Epoch 44/500\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.0109 - mae: 0.1086 - val_loss: 0.0181 - val_mae: 0.1307\n",
            "\n",
            "Epoch 00044: loss did not improve from 0.01202\n",
            "Epoch 45/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0124 - mae: 0.1119 - val_loss: 0.0183 - val_mae: 0.1429\n",
            "\n",
            "Epoch 00045: loss did not improve from 0.01202\n",
            "Epoch 46/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0130 - mae: 0.1164 - val_loss: 0.0172 - val_mae: 0.1345\n",
            "\n",
            "Epoch 00046: loss did not improve from 0.01202\n",
            "Epoch 47/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0124 - mae: 0.1132 - val_loss: 0.0176 - val_mae: 0.1344\n",
            "\n",
            "Epoch 00047: loss did not improve from 0.01202\n",
            "Epoch 48/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0107 - mae: 0.1071 - val_loss: 0.0195 - val_mae: 0.1456\n",
            "\n",
            "Epoch 00048: loss did not improve from 0.01202\n",
            "Epoch 49/500\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.0112 - mae: 0.1101 - val_loss: 0.0200 - val_mae: 0.1471\n",
            "\n",
            "Epoch 00049: loss did not improve from 0.01202\n",
            "Epoch 50/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0134 - mae: 0.1174 - val_loss: 0.0181 - val_mae: 0.1376\n",
            "\n",
            "Epoch 00050: loss did not improve from 0.01202\n",
            "Epoch 51/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0117 - mae: 0.1120 - val_loss: 0.0197 - val_mae: 0.1463\n",
            "\n",
            "Epoch 00051: loss did not improve from 0.01202\n",
            "Epoch 52/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0115 - mae: 0.1097 - val_loss: 0.0180 - val_mae: 0.1371\n",
            "\n",
            "Epoch 00052: loss did not improve from 0.01202\n",
            "Epoch 53/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0089 - mae: 0.1026 - val_loss: 0.0184 - val_mae: 0.1394\n",
            "\n",
            "Epoch 00053: loss improved from 0.01202 to 0.01165, saving model to poids.hdf5\n",
            "Epoch 54/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0111 - mae: 0.1118 - val_loss: 0.0177 - val_mae: 0.1344\n",
            "\n",
            "Epoch 00054: loss improved from 0.01165 to 0.01149, saving model to poids.hdf5\n",
            "Epoch 55/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0114 - mae: 0.1113 - val_loss: 0.0184 - val_mae: 0.1396\n",
            "\n",
            "Epoch 00055: loss did not improve from 0.01149\n",
            "Epoch 56/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0119 - mae: 0.1178 - val_loss: 0.0194 - val_mae: 0.1458\n",
            "\n",
            "Epoch 00056: loss did not improve from 0.01149\n",
            "Epoch 57/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0113 - mae: 0.1112 - val_loss: 0.0191 - val_mae: 0.1428\n",
            "\n",
            "Epoch 00057: loss did not improve from 0.01149\n",
            "Epoch 58/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0121 - mae: 0.1139 - val_loss: 0.0182 - val_mae: 0.1372\n",
            "\n",
            "Epoch 00058: loss did not improve from 0.01149\n",
            "Epoch 59/500\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.0101 - mae: 0.1074 - val_loss: 0.0188 - val_mae: 0.1454\n",
            "\n",
            "Epoch 00059: loss did not improve from 0.01149\n",
            "Epoch 60/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0119 - mae: 0.1114 - val_loss: 0.0178 - val_mae: 0.1364\n",
            "\n",
            "Epoch 00060: loss did not improve from 0.01149\n",
            "Epoch 61/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0115 - mae: 0.1124 - val_loss: 0.0202 - val_mae: 0.1501\n",
            "\n",
            "Epoch 00061: loss did not improve from 0.01149\n",
            "Epoch 62/500\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.0129 - mae: 0.1121 - val_loss: 0.0180 - val_mae: 0.1369\n",
            "\n",
            "Epoch 00062: loss did not improve from 0.01149\n",
            "Epoch 63/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0147 - mae: 0.1200 - val_loss: 0.0174 - val_mae: 0.1314\n",
            "\n",
            "Epoch 00063: loss did not improve from 0.01149\n",
            "Epoch 64/500\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.0131 - mae: 0.1128 - val_loss: 0.0176 - val_mae: 0.1359\n",
            "\n",
            "Epoch 00064: loss did not improve from 0.01149\n",
            "Epoch 65/500\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.0133 - mae: 0.1161 - val_loss: 0.0180 - val_mae: 0.1366\n",
            "\n",
            "Epoch 00065: loss did not improve from 0.01149\n",
            "Epoch 66/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0115 - mae: 0.1126 - val_loss: 0.0168 - val_mae: 0.1343\n",
            "\n",
            "Epoch 00066: loss did not improve from 0.01149\n",
            "Epoch 67/500\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.0131 - mae: 0.1136 - val_loss: 0.0173 - val_mae: 0.1324\n",
            "\n",
            "Epoch 00067: loss did not improve from 0.01149\n",
            "Epoch 68/500\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.0108 - mae: 0.1124 - val_loss: 0.0171 - val_mae: 0.1320\n",
            "\n",
            "Epoch 00068: loss did not improve from 0.01149\n",
            "Epoch 69/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0122 - mae: 0.1139 - val_loss: 0.0193 - val_mae: 0.1459\n",
            "\n",
            "Epoch 00069: loss did not improve from 0.01149\n",
            "Epoch 70/500\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.0112 - mae: 0.1119 - val_loss: 0.0178 - val_mae: 0.1404\n",
            "\n",
            "Epoch 00070: loss did not improve from 0.01149\n",
            "Epoch 71/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0127 - mae: 0.1158 - val_loss: 0.0189 - val_mae: 0.1428\n",
            "\n",
            "Epoch 00071: loss did not improve from 0.01149\n",
            "Epoch 72/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0124 - mae: 0.1130 - val_loss: 0.0174 - val_mae: 0.1346\n",
            "\n",
            "Epoch 00072: loss did not improve from 0.01149\n",
            "Epoch 73/500\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.0107 - mae: 0.1081 - val_loss: 0.0165 - val_mae: 0.1321\n",
            "\n",
            "Epoch 00073: loss did not improve from 0.01149\n",
            "Epoch 74/500\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.0127 - mae: 0.1137 - val_loss: 0.0180 - val_mae: 0.1366\n",
            "\n",
            "Epoch 00074: loss did not improve from 0.01149\n",
            "Epoch 75/500\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.0122 - mae: 0.1151 - val_loss: 0.0190 - val_mae: 0.1447\n",
            "\n",
            "Epoch 00075: loss did not improve from 0.01149\n",
            "Epoch 76/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0108 - mae: 0.1074 - val_loss: 0.0172 - val_mae: 0.1367\n",
            "\n",
            "Epoch 00076: loss did not improve from 0.01149\n",
            "Epoch 77/500\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.0119 - mae: 0.1107 - val_loss: 0.0195 - val_mae: 0.1466\n",
            "\n",
            "Epoch 00077: loss improved from 0.01149 to 0.01145, saving model to poids.hdf5\n",
            "Epoch 78/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0131 - mae: 0.1156 - val_loss: 0.0195 - val_mae: 0.1464\n",
            "\n",
            "Epoch 00078: loss did not improve from 0.01145\n",
            "Epoch 79/500\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.0146 - mae: 0.1166 - val_loss: 0.0192 - val_mae: 0.1460\n",
            "\n",
            "Epoch 00079: loss did not improve from 0.01145\n",
            "Epoch 80/500\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.0117 - mae: 0.1112 - val_loss: 0.0177 - val_mae: 0.1377\n",
            "\n",
            "Epoch 00080: loss did not improve from 0.01145\n",
            "Epoch 81/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0100 - mae: 0.1072 - val_loss: 0.0163 - val_mae: 0.1316\n",
            "\n",
            "Epoch 00081: loss did not improve from 0.01145\n",
            "Epoch 82/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0152 - mae: 0.1203 - val_loss: 0.0174 - val_mae: 0.1347\n",
            "\n",
            "Epoch 00082: loss did not improve from 0.01145\n",
            "Epoch 83/500\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.0106 - mae: 0.1082 - val_loss: 0.0169 - val_mae: 0.1356\n",
            "\n",
            "Epoch 00083: loss did not improve from 0.01145\n",
            "Epoch 84/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0124 - mae: 0.1114 - val_loss: 0.0215 - val_mae: 0.1579\n",
            "\n",
            "Epoch 00084: loss improved from 0.01145 to 0.01088, saving model to poids.hdf5\n",
            "Epoch 85/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0123 - mae: 0.1122 - val_loss: 0.0174 - val_mae: 0.1352\n",
            "\n",
            "Epoch 00085: loss did not improve from 0.01088\n",
            "Epoch 86/500\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.0091 - mae: 0.1054 - val_loss: 0.0185 - val_mae: 0.1412\n",
            "\n",
            "Epoch 00086: loss improved from 0.01088 to 0.01067, saving model to poids.hdf5\n",
            "Epoch 87/500\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.0120 - mae: 0.1112 - val_loss: 0.0183 - val_mae: 0.1412\n",
            "\n",
            "Epoch 00087: loss did not improve from 0.01067\n",
            "Epoch 88/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0103 - mae: 0.1070 - val_loss: 0.0160 - val_mae: 0.1310\n",
            "\n",
            "Epoch 00088: loss did not improve from 0.01067\n",
            "Epoch 89/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0103 - mae: 0.1061 - val_loss: 0.0154 - val_mae: 0.1259\n",
            "\n",
            "Epoch 00089: loss did not improve from 0.01067\n",
            "Epoch 90/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0145 - mae: 0.1201 - val_loss: 0.0164 - val_mae: 0.1300\n",
            "\n",
            "Epoch 00090: loss did not improve from 0.01067\n",
            "Epoch 91/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0133 - mae: 0.1150 - val_loss: 0.0187 - val_mae: 0.1421\n",
            "\n",
            "Epoch 00091: loss did not improve from 0.01067\n",
            "Epoch 92/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0128 - mae: 0.1150 - val_loss: 0.0159 - val_mae: 0.1359\n",
            "\n",
            "Epoch 00092: loss did not improve from 0.01067\n",
            "Epoch 93/500\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.0120 - mae: 0.1121 - val_loss: 0.0203 - val_mae: 0.1524\n",
            "\n",
            "Epoch 00093: loss did not improve from 0.01067\n",
            "Epoch 94/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0107 - mae: 0.1096 - val_loss: 0.0187 - val_mae: 0.1431\n",
            "\n",
            "Epoch 00094: loss did not improve from 0.01067\n",
            "Epoch 95/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0115 - mae: 0.1090 - val_loss: 0.0176 - val_mae: 0.1406\n",
            "\n",
            "Epoch 00095: loss did not improve from 0.01067\n",
            "Epoch 96/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0112 - mae: 0.1114 - val_loss: 0.0178 - val_mae: 0.1378\n",
            "\n",
            "Epoch 00096: loss did not improve from 0.01067\n",
            "Epoch 97/500\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.0126 - mae: 0.1167 - val_loss: 0.0175 - val_mae: 0.1355\n",
            "\n",
            "Epoch 00097: loss did not improve from 0.01067\n",
            "Epoch 98/500\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0109 - mae: 0.1066 - val_loss: 0.0211 - val_mae: 0.1574\n",
            "\n",
            "Epoch 00098: loss did not improve from 0.01067\n",
            "Epoch 99/500\n",
            "13/30 [============>.................] - ETA: 0s - loss: 0.0088 - mae: 0.1033"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCTzYW31uWbn"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doz2h-t1uWbo"
      },
      "source": [
        "**4. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "By0U8eN8uWbo"
      },
      "source": [
        "taille_fenetre = 20\n",
        "\n",
        "# Création d'une liste vide pour recevoir les prédictions\n",
        "predictions = []\n",
        "\n",
        "# Calcul des prédiction pour chaque groupe de 20 valeurs consécutives de la série\n",
        "# dans l'intervalle de validation\n",
        "for t in temps[temps_separation:-taille_fenetre]:\n",
        "    X = np.reshape(Serie_Normalisee[t:t+taille_fenetre],(1,taille_fenetre))\n",
        "    predictions.append(model.predict(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ0EFFIcuWbp"
      },
      "source": [
        "# Affiche la série et les prédictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps,serie,label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title('Prédictions avec le modèle GRU + Attention')\n",
        "plt.show()\n",
        "\n",
        "# Zoom sur l'intervalle de validation\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps[temps_separation:],serie[temps_separation:],label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title(\"Prédictions avec le modèle GRU + Attention (zoom sur l'intervalle de validation)\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1gFt22CuWbq"
      },
      "source": [
        "# Calcule de l'erreur quadratique moyenne et de l'erreur absolue moyenne \n",
        "\n",
        "mae = tf.keras.metrics.mean_absolute_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "mse = tf.keras.metrics.mean_squared_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "\n",
        "print(mae)\n",
        "print(mse)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}