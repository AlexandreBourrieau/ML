{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reseau_GRU_Avec_Attention.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/Reseau_GRU_Avec_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubCeIvtF6R4W"
      },
      "source": [
        "Dans ce carnet nous allons mettre en place un modèle à réseau de neurones récurrent de type GRU associé à une couche d'attention pour réaliser des prédictions sur notre série temporelle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRhtHsNn5fc3"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFeah3y_6kif"
      },
      "source": [
        "# Création de la série temporelle et du dataset pour l'entrainement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJfSLtub6sdc"
      },
      "source": [
        "# Fonction permettant d'afficher une série temporelle\n",
        "def affiche_serie(temps, serie, format=\"-\", debut=0, fin=None, label=None):\n",
        "    plt.plot(temps[debut:fin], serie[debut:fin], format, label=label)\n",
        "    plt.xlabel(\"Temps\")\n",
        "    plt.ylabel(\"Valeur\")\n",
        "    if label:\n",
        "        plt.legend(fontsize=14)\n",
        "    plt.grid(True)\n",
        "\n",
        "# Fonction permettant de créer une tendance\n",
        "def tendance(temps, pente=0):\n",
        "    return pente * temps\n",
        "\n",
        "# Fonction permettant de créer un motif\n",
        "def motif_periodique(instants):\n",
        "    return (np.where(instants < 0.4,                            # Si les instants sont < 0.4\n",
        "                    np.cos(instants * 2 * np.pi),               # Alors on retourne la fonction cos(2*pi*t)\n",
        "                    1 / np.exp(3 * instants)))                  # Sinon, on retourne la fonction exp(-3t)\n",
        "\n",
        "# Fonction permettant de créer une saisonnalité avec un motif\n",
        "def saisonnalite(temps, periode, amplitude=1, phase=0):\n",
        "    \"\"\"Répétition du motif sur la même période\"\"\"\n",
        "    instants = ((temps + phase) % periode) / periode            # Mapping du temps =[0 1 2 ... 1460] => instants = [0.0 ... 1.0]\n",
        "    return amplitude * motif_periodique(instants)\n",
        "\n",
        "# Fonction permettant de générer du bruit gaussien N(0,1)\n",
        "def bruit_blanc(temps, niveau_bruit=1, graine=None):\n",
        "    rnd = np.random.RandomState(graine)\n",
        "    return rnd.randn(len(temps)) * niveau_bruit\n",
        "\n",
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "# au format X(X1,X2,...Xn) / Y(Y1,Y2,...,Yn)\n",
        "# X sont les données d'entrées du réseau\n",
        "# Y sont les labels\n",
        "\n",
        "def prepare_dataset_XY(serie, taille_fenetre, batch_size, buffer_melange):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie)\n",
        "  dataset = dataset.window(taille_fenetre+1, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre + 1))\n",
        "  dataset = dataset.shuffle(buffer_melange).map(lambda x: (x[:-1], x[-1:]))\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "# Création de la série temporelle\n",
        "temps = np.arange(4 * 365)                # temps = [0 1 2 .... 4*365] = [0 1 2 .... 1460]\n",
        "amplitude = 40                            # Amplitude de la la saisonnalité\n",
        "niveau_bruit = 5                          # Niveau du bruit\n",
        "offset = 10                               # Offset de la série\n",
        "\n",
        "serie = offset + tendance(temps, 0.1) + saisonnalite(temps, periode=365, amplitude=amplitude) + bruit_blanc(temps,niveau_bruit,graine=40)\n",
        "\n",
        "temps_separation = 1000\n",
        "\n",
        "# Extraction des temps et des données d'entrainement\n",
        "temps_entrainement = temps[:temps_separation]\n",
        "x_entrainement = serie[:temps_separation]\n",
        "\n",
        "# Exctraction des temps et des données de valiadation\n",
        "temps_validation = temps[temps_separation:]\n",
        "x_validation = serie[temps_separation:]\n",
        "\n",
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "taille_fenetre = 20\n",
        "batch_size = 32\n",
        "buffer_melange = 1000\n",
        "\n",
        "# Création du dataset X,Y\n",
        "dataset = prepare_dataset_XY(x_entrainement,taille_fenetre,batch_size,buffer_melange)\n",
        "\n",
        "# Création du dataset X,Y de validation\n",
        "dataset_Val = prepare_dataset_XY(x_validation,taille_fenetre,batch_size,buffer_melange)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyndQsxw0wue"
      },
      "source": [
        "# Calcul de la moyenne et de l'écart type de la série\n",
        "mean = tf.math.reduce_mean(serie)\n",
        "std = tf.math.reduce_std(serie)\n",
        "\n",
        "# Normalise les données\n",
        "Serie_Normalisee = (serie-mean)/std\n",
        "min = tf.math.reduce_min(serie)\n",
        "max = tf.math.reduce_max(serie)\n",
        "\n",
        "# Création des données pour l'entrainement et le test\n",
        "x_entrainement_norm = Serie_Normalisee[:temps_separation]\n",
        "x_validation_norm = Serie_Normalisee[temps_separation:]\n",
        "\n",
        "# Création du dataset X,Y\n",
        "dataset_norm = prepare_dataset_XY(x_entrainement_norm,taille_fenetre,batch_size,buffer_melange)\n",
        "\n",
        "# Création du dataset X,Y de validation\n",
        "dataset_Val_norm = prepare_dataset_XY(x_validation_norm,taille_fenetre,batch_size,buffer_melange)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Yt-EgZ3sgPY"
      },
      "source": [
        "# Création du modèle GRU avec couche d'attention personnalisée simple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyrcfKcgCsZ7"
      },
      "source": [
        "**1. Création du réseau et adaptation des formats d'entrée et de sortie**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeJyix8HK7Kt"
      },
      "source": [
        "Sous forme de shéma, notre réseau est donc le suivant :\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OZkfsmnBNHY"
      },
      "source": [
        "<img src=\"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/images/attention_11_Attention.png?raw=true\" width=\"1200\"> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kgTrJOQ5DUo"
      },
      "source": [
        "# Remise à zéro de tous les états générés par Keras\n",
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLNIAGDlBizT"
      },
      "source": [
        "On créé une classe dérivée de la classe [Layer](https://keras.io/api/layers/base_layer/#layer-class) de Keras. Les méthodes utilisées sont les suivantes :  \n",
        " - [build](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#build) : Permet de créer les variables utilisées par la couche (commes les poids et les offsets)\n",
        " - [call](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#call) : Permet d'implanter la logique de la couche"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3COaR59t5WzJ"
      },
      "source": [
        "<img src=\"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/images/attention_11_Attention_2.png?raw=true\" width=\"1200\"> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hhk0kmPSgqva"
      },
      "source": [
        "# Classe d'attention simple\n",
        "# Applique les poids d'attention sur les vecteurs de la couche récurrente\n",
        "\n",
        "# Importe le Backend de Keras\n",
        "from keras import backend as K\n",
        "\n",
        "# Définit une nouvelle classe Couche_Attention\n",
        "# Héritée de la classe Layer de Keras\n",
        "\n",
        "class Couche_Attention(tf.keras.layers.Layer):\n",
        "  # Fonction d'initialisation de la classe d'attention\n",
        "  def __init__(self):\n",
        "    super().__init__()          # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.w = self.add_weight(shape=(input_shape[2],1),initializer=\"normal\",name=\"w\")\n",
        "    self.b = self.add_weight(shape=(input_shape[1],1),initializer=\"zeros\",name=\"b\")\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Définit la logique de la couche d'attention\n",
        "  # Arguments :   x : Tenseur d'entrée de dimension (None, nbr_v,dim)\n",
        "  def call(self,x):\n",
        "    e = K.tanh(K.dot(x,self.w)+self.b)\n",
        "    a = tf.keras.activations.softmax(e,axis=1)\n",
        "    xa = tf.multiply(x,a)\n",
        "    sortie = K.sum(xa,axis=1)\n",
        "    return sortie"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTqdYAsF_ici",
        "outputId": "1ab48038-7ef8-4b9a-c0f4-fd58c528d905",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dim_GRU = 40\n",
        "\n",
        "# Fonction de la couche lambda d'entrée\n",
        "def Traitement_Entrees(x):\n",
        "  return tf.expand_dims(x,axis=-1)\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.Input(shape=(taille_fenetre,)))\n",
        "model.add(tf.keras.layers.Lambda(Traitement_Entrees))\n",
        "model.add(tf.keras.layers.GRU(dim_GRU,return_sequences=True))\n",
        "model.add(Couche_Attention())\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "model.save_weights('model_initial.h5')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda_2 (Lambda)            (None, 20, 1)             0         \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, 20, 40)            5160      \n",
            "_________________________________________________________________\n",
            "couche__attention_1 (Couche_ (None, 40)                60        \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 41        \n",
            "=================================================================\n",
            "Total params: 5,261\n",
            "Trainable params: 5,261\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUM0-SSXGLIQ"
      },
      "source": [
        "**2. Optimisation du taux d'apprentissage**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jejCBhXVuNQ4",
        "outputId": "af07b09a-9c64-4587-819f-e6812bc37262",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(lr=1e-8)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur, metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset_norm,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "30/30 [==============================] - 2s 7ms/step - loss: 0.3353 - mae: 0.7086\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.32265, saving model to poids.hdf5\n",
            "Epoch 2/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.3191 - mae: 0.6841\n",
            "\n",
            "Epoch 00002: loss improved from 0.32265 to 0.32247, saving model to poids.hdf5\n",
            "Epoch 3/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.3290 - mae: 0.6959\n",
            "\n",
            "Epoch 00003: loss improved from 0.32247 to 0.32147, saving model to poids.hdf5\n",
            "Epoch 4/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.3208 - mae: 0.6850\n",
            "\n",
            "Epoch 00004: loss improved from 0.32147 to 0.31804, saving model to poids.hdf5\n",
            "Epoch 5/100\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.2996 - mae: 0.6576\n",
            "\n",
            "Epoch 00005: loss did not improve from 0.31804\n",
            "Epoch 6/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.3134 - mae: 0.6742\n",
            "\n",
            "Epoch 00006: loss did not improve from 0.31804\n",
            "Epoch 7/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.3041 - mae: 0.6623\n",
            "\n",
            "Epoch 00007: loss did not improve from 0.31804\n",
            "Epoch 8/100\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.3271 - mae: 0.6952\n",
            "\n",
            "Epoch 00008: loss did not improve from 0.31804\n",
            "Epoch 9/100\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.3212 - mae: 0.6831\n",
            "\n",
            "Epoch 00009: loss did not improve from 0.31804\n",
            "Epoch 10/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.3251 - mae: 0.6881\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.31804\n",
            "Epoch 11/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.3116 - mae: 0.6700\n",
            "\n",
            "Epoch 00011: loss did not improve from 0.31804\n",
            "Epoch 12/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.3153 - mae: 0.6772\n",
            "\n",
            "Epoch 00012: loss did not improve from 0.31804\n",
            "Epoch 13/100\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.3065 - mae: 0.6637\n",
            "\n",
            "Epoch 00013: loss did not improve from 0.31804\n",
            "Epoch 14/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.3285 - mae: 0.6950\n",
            "\n",
            "Epoch 00014: loss did not improve from 0.31804\n",
            "Epoch 15/100\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.3354 - mae: 0.7047\n",
            "\n",
            "Epoch 00015: loss did not improve from 0.31804\n",
            "Epoch 16/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.3322 - mae: 0.6943\n",
            "\n",
            "Epoch 00016: loss did not improve from 0.31804\n",
            "Epoch 17/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.3224 - mae: 0.6869\n",
            "\n",
            "Epoch 00017: loss did not improve from 0.31804\n",
            "Epoch 18/100\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.3439 - mae: 0.7127\n",
            "\n",
            "Epoch 00018: loss did not improve from 0.31804\n",
            "Epoch 19/100\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.3271 - mae: 0.6886\n",
            "\n",
            "Epoch 00019: loss did not improve from 0.31804\n",
            "Epoch 20/100\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.3184 - mae: 0.6766\n",
            "\n",
            "Epoch 00020: loss did not improve from 0.31804\n",
            "Epoch 21/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.3259 - mae: 0.6901\n",
            "\n",
            "Epoch 00021: loss did not improve from 0.31804\n",
            "Epoch 22/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.3302 - mae: 0.6959\n",
            "\n",
            "Epoch 00022: loss did not improve from 0.31804\n",
            "Epoch 23/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.3037 - mae: 0.6606\n",
            "\n",
            "Epoch 00023: loss did not improve from 0.31804\n",
            "Epoch 24/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.3368 - mae: 0.7050\n",
            "\n",
            "Epoch 00024: loss did not improve from 0.31804\n",
            "Epoch 25/100\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.3084 - mae: 0.6585\n",
            "\n",
            "Epoch 00025: loss did not improve from 0.31804\n",
            "Epoch 26/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.3402 - mae: 0.7123\n",
            "\n",
            "Epoch 00026: loss did not improve from 0.31804\n",
            "Epoch 27/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.3168 - mae: 0.6802\n",
            "\n",
            "Epoch 00027: loss did not improve from 0.31804\n",
            "Epoch 28/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.3152 - mae: 0.6745\n",
            "\n",
            "Epoch 00028: loss did not improve from 0.31804\n",
            "Epoch 29/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.3264 - mae: 0.6871\n",
            "\n",
            "Epoch 00029: loss did not improve from 0.31804\n",
            "Epoch 30/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.3156 - mae: 0.6765\n",
            "\n",
            "Epoch 00030: loss improved from 0.31804 to 0.31793, saving model to poids.hdf5\n",
            "Epoch 31/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.3092 - mae: 0.6710\n",
            "\n",
            "Epoch 00031: loss did not improve from 0.31793\n",
            "Epoch 32/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.3290 - mae: 0.6911\n",
            "\n",
            "Epoch 00032: loss did not improve from 0.31793\n",
            "Epoch 33/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.2929 - mae: 0.6507\n",
            "\n",
            "Epoch 00033: loss did not improve from 0.31793\n",
            "Epoch 34/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.3234 - mae: 0.6912\n",
            "\n",
            "Epoch 00034: loss improved from 0.31793 to 0.31663, saving model to poids.hdf5\n",
            "Epoch 35/100\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.3253 - mae: 0.6916\n",
            "\n",
            "Epoch 00035: loss improved from 0.31663 to 0.31452, saving model to poids.hdf5\n",
            "Epoch 36/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.2948 - mae: 0.6458\n",
            "\n",
            "Epoch 00036: loss improved from 0.31452 to 0.31362, saving model to poids.hdf5\n",
            "Epoch 37/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.2956 - mae: 0.6532\n",
            "\n",
            "Epoch 00037: loss improved from 0.31362 to 0.31094, saving model to poids.hdf5\n",
            "Epoch 38/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.3264 - mae: 0.6913\n",
            "\n",
            "Epoch 00038: loss did not improve from 0.31094\n",
            "Epoch 39/100\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.2883 - mae: 0.6433\n",
            "\n",
            "Epoch 00039: loss improved from 0.31094 to 0.30768, saving model to poids.hdf5\n",
            "Epoch 40/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.3139 - mae: 0.6775\n",
            "\n",
            "Epoch 00040: loss improved from 0.30768 to 0.30641, saving model to poids.hdf5\n",
            "Epoch 41/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.2825 - mae: 0.6301\n",
            "\n",
            "Epoch 00041: loss improved from 0.30641 to 0.30046, saving model to poids.hdf5\n",
            "Epoch 42/100\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.3229 - mae: 0.6886\n",
            "\n",
            "Epoch 00042: loss improved from 0.30046 to 0.29993, saving model to poids.hdf5\n",
            "Epoch 43/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.2916 - mae: 0.6466\n",
            "\n",
            "Epoch 00043: loss improved from 0.29993 to 0.29070, saving model to poids.hdf5\n",
            "Epoch 44/100\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.2765 - mae: 0.6285\n",
            "\n",
            "Epoch 00044: loss improved from 0.29070 to 0.28235, saving model to poids.hdf5\n",
            "Epoch 45/100\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.2895 - mae: 0.6437\n",
            "\n",
            "Epoch 00045: loss improved from 0.28235 to 0.27364, saving model to poids.hdf5\n",
            "Epoch 46/100\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.2661 - mae: 0.6099\n",
            "\n",
            "Epoch 00046: loss improved from 0.27364 to 0.26413, saving model to poids.hdf5\n",
            "Epoch 47/100\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.2512 - mae: 0.5929\n",
            "\n",
            "Epoch 00047: loss improved from 0.26413 to 0.25049, saving model to poids.hdf5\n",
            "Epoch 48/100\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.2420 - mae: 0.5794\n",
            "\n",
            "Epoch 00048: loss improved from 0.25049 to 0.23844, saving model to poids.hdf5\n",
            "Epoch 49/100\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.2081 - mae: 0.5299\n",
            "\n",
            "Epoch 00049: loss improved from 0.23844 to 0.21874, saving model to poids.hdf5\n",
            "Epoch 50/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.2099 - mae: 0.5340\n",
            "\n",
            "Epoch 00050: loss improved from 0.21874 to 0.19931, saving model to poids.hdf5\n",
            "Epoch 51/100\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.1915 - mae: 0.5064\n",
            "\n",
            "Epoch 00051: loss improved from 0.19931 to 0.17798, saving model to poids.hdf5\n",
            "Epoch 52/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.1705 - mae: 0.4743\n",
            "\n",
            "Epoch 00052: loss improved from 0.17798 to 0.15723, saving model to poids.hdf5\n",
            "Epoch 53/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.1418 - mae: 0.4407\n",
            "\n",
            "Epoch 00053: loss improved from 0.15723 to 0.13344, saving model to poids.hdf5\n",
            "Epoch 54/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.1125 - mae: 0.3973\n",
            "\n",
            "Epoch 00054: loss improved from 0.13344 to 0.11332, saving model to poids.hdf5\n",
            "Epoch 55/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.0958 - mae: 0.3624\n",
            "\n",
            "Epoch 00055: loss improved from 0.11332 to 0.09414, saving model to poids.hdf5\n",
            "Epoch 56/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.0798 - mae: 0.3279\n",
            "\n",
            "Epoch 00056: loss improved from 0.09414 to 0.07777, saving model to poids.hdf5\n",
            "Epoch 57/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.0673 - mae: 0.2980\n",
            "\n",
            "Epoch 00057: loss improved from 0.07777 to 0.06404, saving model to poids.hdf5\n",
            "Epoch 58/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.0560 - mae: 0.2642\n",
            "\n",
            "Epoch 00058: loss improved from 0.06404 to 0.05234, saving model to poids.hdf5\n",
            "Epoch 59/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.0473 - mae: 0.2340\n",
            "\n",
            "Epoch 00059: loss improved from 0.05234 to 0.04312, saving model to poids.hdf5\n",
            "Epoch 60/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.0413 - mae: 0.2137\n",
            "\n",
            "Epoch 00060: loss improved from 0.04312 to 0.03560, saving model to poids.hdf5\n",
            "Epoch 61/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.0312 - mae: 0.1801\n",
            "\n",
            "Epoch 00061: loss improved from 0.03560 to 0.03112, saving model to poids.hdf5\n",
            "Epoch 62/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.0336 - mae: 0.1796\n",
            "\n",
            "Epoch 00062: loss improved from 0.03112 to 0.02783, saving model to poids.hdf5\n",
            "Epoch 63/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0271 - mae: 0.1578\n",
            "\n",
            "Epoch 00063: loss improved from 0.02783 to 0.02750, saving model to poids.hdf5\n",
            "Epoch 64/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0302 - mae: 0.1684\n",
            "\n",
            "Epoch 00064: loss improved from 0.02750 to 0.02666, saving model to poids.hdf5\n",
            "Epoch 65/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.0231 - mae: 0.1509\n",
            "\n",
            "Epoch 00065: loss improved from 0.02666 to 0.02640, saving model to poids.hdf5\n",
            "Epoch 66/100\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.0255 - mae: 0.1556\n",
            "\n",
            "Epoch 00066: loss did not improve from 0.02640\n",
            "Epoch 67/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.0248 - mae: 0.1559\n",
            "\n",
            "Epoch 00067: loss did not improve from 0.02640\n",
            "Epoch 68/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.0265 - mae: 0.1535\n",
            "\n",
            "Epoch 00068: loss improved from 0.02640 to 0.02582, saving model to poids.hdf5\n",
            "Epoch 69/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.0244 - mae: 0.1520\n",
            "\n",
            "Epoch 00069: loss did not improve from 0.02582\n",
            "Epoch 70/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.0296 - mae: 0.1670\n",
            "\n",
            "Epoch 00070: loss did not improve from 0.02582\n",
            "Epoch 71/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.0268 - mae: 0.1603\n",
            "\n",
            "Epoch 00071: loss did not improve from 0.02582\n",
            "Epoch 72/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.0296 - mae: 0.1691\n",
            "\n",
            "Epoch 00072: loss did not improve from 0.02582\n",
            "Epoch 73/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.0230 - mae: 0.1496\n",
            "\n",
            "Epoch 00073: loss did not improve from 0.02582\n",
            "Epoch 74/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.0240 - mae: 0.1512\n",
            "\n",
            "Epoch 00074: loss did not improve from 0.02582\n",
            "Epoch 75/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.0263 - mae: 0.1591\n",
            "\n",
            "Epoch 00075: loss did not improve from 0.02582\n",
            "Epoch 76/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.0273 - mae: 0.1600\n",
            "\n",
            "Epoch 00076: loss did not improve from 0.02582\n",
            "Epoch 77/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0396 - mae: 0.2128\n",
            "\n",
            "Epoch 00077: loss did not improve from 0.02582\n",
            "Epoch 78/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.0281 - mae: 0.1684\n",
            "\n",
            "Epoch 00078: loss did not improve from 0.02582\n",
            "Epoch 79/100\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.0607 - mae: 0.2603\n",
            "\n",
            "Epoch 00079: loss did not improve from 0.02582\n",
            "Epoch 80/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.0303 - mae: 0.1792\n",
            "\n",
            "Epoch 00080: loss did not improve from 0.02582\n",
            "Epoch 81/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.0386 - mae: 0.2031\n",
            "\n",
            "Epoch 00081: loss did not improve from 0.02582\n",
            "Epoch 82/100\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.0487 - mae: 0.2401\n",
            "\n",
            "Epoch 00082: loss did not improve from 0.02582\n",
            "Epoch 83/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.0769 - mae: 0.3243\n",
            "\n",
            "Epoch 00083: loss did not improve from 0.02582\n",
            "Epoch 84/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 3.2195 - mae: 3.6074\n",
            "\n",
            "Epoch 00084: loss did not improve from 0.02582\n",
            "Epoch 85/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 8.2429 - mae: 8.7408\n",
            "\n",
            "Epoch 00085: loss did not improve from 0.02582\n",
            "Epoch 86/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 2.3816 - mae: 2.8641\n",
            "\n",
            "Epoch 00086: loss did not improve from 0.02582\n",
            "Epoch 87/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 1.4706 - mae: 1.9520\n",
            "\n",
            "Epoch 00087: loss did not improve from 0.02582\n",
            "Epoch 88/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 1.7766 - mae: 2.2438\n",
            "\n",
            "Epoch 00088: loss did not improve from 0.02582\n",
            "Epoch 89/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 2.8414 - mae: 3.3313\n",
            "\n",
            "Epoch 00089: loss did not improve from 0.02582\n",
            "Epoch 90/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 3.5645 - mae: 4.0620\n",
            "\n",
            "Epoch 00090: loss did not improve from 0.02582\n",
            "Epoch 91/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 19.3845 - mae: 19.8683\n",
            "\n",
            "Epoch 00091: loss did not improve from 0.02582\n",
            "Epoch 92/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 33.2215 - mae: 33.7215\n",
            "\n",
            "Epoch 00092: loss did not improve from 0.02582\n",
            "Epoch 93/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 23.0141 - mae: 23.5141\n",
            "\n",
            "Epoch 00093: loss did not improve from 0.02582\n",
            "Epoch 94/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 29.0516 - mae: 29.5506\n",
            "\n",
            "Epoch 00094: loss did not improve from 0.02582\n",
            "Epoch 95/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 34.8404 - mae: 35.3404\n",
            "\n",
            "Epoch 00095: loss did not improve from 0.02582\n",
            "Epoch 96/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 59.3613 - mae: 59.8613\n",
            "\n",
            "Epoch 00096: loss did not improve from 0.02582\n",
            "Epoch 97/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 74.4886 - mae: 74.9886\n",
            "\n",
            "Epoch 00097: loss did not improve from 0.02582\n",
            "Epoch 98/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 93.5165 - mae: 94.0165\n",
            "\n",
            "Epoch 00098: loss did not improve from 0.02582\n",
            "Epoch 99/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 117.5671 - mae: 118.0671\n",
            "\n",
            "Epoch 00099: loss did not improve from 0.02582\n",
            "Epoch 100/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 147.7972 - mae: 148.2972\n",
            "\n",
            "Epoch 00100: loss did not improve from 0.02582\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1_WMNlzu2B4",
        "outputId": "bad11dba-4124-4a10-f63a-de16ec1a04cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 1])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, \"Evolution de l'erreur en fonction du taux d'apprentissage\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XdXh_b0GP_F"
      },
      "source": [
        "**3. Entrainement du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80pEtJ10wIfY"
      },
      "source": [
        "# Charge les meilleurs poids\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16ujUiELwR33",
        "outputId": "5309af68-4d4d-46bd-e4f6-c5eb0c82a399",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "class TimingCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, logs={}):\n",
        "        self.logs=[]\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.starttime = timer()\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.logs.append(timer()-self.starttime)\n",
        "\n",
        "cb = TimingCallback()\n",
        "\n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.1,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0.05)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(learning_rate=lr_schedule,momentum=0.9)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur,metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle\n",
        "historique = model.fit(dataset_norm,validation_data=dataset_Val_norm, epochs=500,verbose=1, callbacks=[CheckPoint,cb])\n",
        "\n",
        "print(cb.logs)\n",
        "print(sum(cb.logs))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "30/30 [==============================] - 3s 25ms/step - loss: 0.0353 - mae: 0.1826 - val_loss: 0.0232 - val_mae: 0.1454\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.03005, saving model to poids.hdf5\n",
            "Epoch 2/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0221 - mae: 0.1527 - val_loss: 0.0220 - val_mae: 0.1499\n",
            "\n",
            "Epoch 00002: loss improved from 0.03005 to 0.02520, saving model to poids.hdf5\n",
            "Epoch 3/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0310 - mae: 0.1833 - val_loss: 0.0278 - val_mae: 0.1631\n",
            "\n",
            "Epoch 00003: loss did not improve from 0.02520\n",
            "Epoch 4/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0248 - mae: 0.1566 - val_loss: 0.0223 - val_mae: 0.1416\n",
            "\n",
            "Epoch 00004: loss improved from 0.02520 to 0.02499, saving model to poids.hdf5\n",
            "Epoch 5/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0267 - mae: 0.1533 - val_loss: 0.0213 - val_mae: 0.1400\n",
            "\n",
            "Epoch 00005: loss improved from 0.02499 to 0.02467, saving model to poids.hdf5\n",
            "Epoch 6/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0206 - mae: 0.1417 - val_loss: 0.0202 - val_mae: 0.1384\n",
            "\n",
            "Epoch 00006: loss improved from 0.02467 to 0.02288, saving model to poids.hdf5\n",
            "Epoch 7/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0215 - mae: 0.1497 - val_loss: 0.0199 - val_mae: 0.1366\n",
            "\n",
            "Epoch 00007: loss improved from 0.02288 to 0.02255, saving model to poids.hdf5\n",
            "Epoch 8/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0209 - mae: 0.1456 - val_loss: 0.0203 - val_mae: 0.1377\n",
            "\n",
            "Epoch 00008: loss did not improve from 0.02255\n",
            "Epoch 9/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0250 - mae: 0.1559 - val_loss: 0.0217 - val_mae: 0.1437\n",
            "\n",
            "Epoch 00009: loss improved from 0.02255 to 0.02182, saving model to poids.hdf5\n",
            "Epoch 10/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0209 - mae: 0.1426 - val_loss: 0.0198 - val_mae: 0.1372\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.02182\n",
            "Epoch 11/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0236 - mae: 0.1500 - val_loss: 0.0224 - val_mae: 0.1462\n",
            "\n",
            "Epoch 00011: loss improved from 0.02182 to 0.02083, saving model to poids.hdf5\n",
            "Epoch 12/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0218 - mae: 0.1477 - val_loss: 0.0198 - val_mae: 0.1354\n",
            "\n",
            "Epoch 00012: loss did not improve from 0.02083\n",
            "Epoch 13/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0224 - mae: 0.1434 - val_loss: 0.0242 - val_mae: 0.1581\n",
            "\n",
            "Epoch 00013: loss did not improve from 0.02083\n",
            "Epoch 14/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0219 - mae: 0.1473 - val_loss: 0.0197 - val_mae: 0.1369\n",
            "\n",
            "Epoch 00014: loss did not improve from 0.02083\n",
            "Epoch 15/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0229 - mae: 0.1533 - val_loss: 0.0187 - val_mae: 0.1354\n",
            "\n",
            "Epoch 00015: loss did not improve from 0.02083\n",
            "Epoch 16/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0185 - mae: 0.1349 - val_loss: 0.0194 - val_mae: 0.1362\n",
            "\n",
            "Epoch 00016: loss improved from 0.02083 to 0.01966, saving model to poids.hdf5\n",
            "Epoch 17/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0213 - mae: 0.1456 - val_loss: 0.0195 - val_mae: 0.1361\n",
            "\n",
            "Epoch 00017: loss did not improve from 0.01966\n",
            "Epoch 18/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0188 - mae: 0.1398 - val_loss: 0.0198 - val_mae: 0.1361\n",
            "\n",
            "Epoch 00018: loss did not improve from 0.01966\n",
            "Epoch 19/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0228 - mae: 0.1466 - val_loss: 0.0195 - val_mae: 0.1344\n",
            "\n",
            "Epoch 00019: loss did not improve from 0.01966\n",
            "Epoch 20/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0206 - mae: 0.1401 - val_loss: 0.0190 - val_mae: 0.1361\n",
            "\n",
            "Epoch 00020: loss improved from 0.01966 to 0.01890, saving model to poids.hdf5\n",
            "Epoch 21/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0190 - mae: 0.1419 - val_loss: 0.0197 - val_mae: 0.1386\n",
            "\n",
            "Epoch 00021: loss improved from 0.01890 to 0.01887, saving model to poids.hdf5\n",
            "Epoch 22/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0174 - mae: 0.1328 - val_loss: 0.0180 - val_mae: 0.1341\n",
            "\n",
            "Epoch 00022: loss did not improve from 0.01887\n",
            "Epoch 23/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0186 - mae: 0.1380 - val_loss: 0.0179 - val_mae: 0.1330\n",
            "\n",
            "Epoch 00023: loss did not improve from 0.01887\n",
            "Epoch 24/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0203 - mae: 0.1402 - val_loss: 0.0172 - val_mae: 0.1313\n",
            "\n",
            "Epoch 00024: loss improved from 0.01887 to 0.01874, saving model to poids.hdf5\n",
            "Epoch 25/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0165 - mae: 0.1299 - val_loss: 0.0198 - val_mae: 0.1354\n",
            "\n",
            "Epoch 00025: loss improved from 0.01874 to 0.01856, saving model to poids.hdf5\n",
            "Epoch 26/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0210 - mae: 0.1461 - val_loss: 0.0198 - val_mae: 0.1406\n",
            "\n",
            "Epoch 00026: loss did not improve from 0.01856\n",
            "Epoch 27/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0187 - mae: 0.1378 - val_loss: 0.0188 - val_mae: 0.1329\n",
            "\n",
            "Epoch 00027: loss improved from 0.01856 to 0.01834, saving model to poids.hdf5\n",
            "Epoch 28/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0169 - mae: 0.1335 - val_loss: 0.0186 - val_mae: 0.1330\n",
            "\n",
            "Epoch 00028: loss improved from 0.01834 to 0.01829, saving model to poids.hdf5\n",
            "Epoch 29/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0155 - mae: 0.1269 - val_loss: 0.0192 - val_mae: 0.1355\n",
            "\n",
            "Epoch 00029: loss improved from 0.01829 to 0.01716, saving model to poids.hdf5\n",
            "Epoch 30/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0180 - mae: 0.1338 - val_loss: 0.0197 - val_mae: 0.1392\n",
            "\n",
            "Epoch 00030: loss did not improve from 0.01716\n",
            "Epoch 31/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0171 - mae: 0.1302 - val_loss: 0.0178 - val_mae: 0.1347\n",
            "\n",
            "Epoch 00031: loss did not improve from 0.01716\n",
            "Epoch 32/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0164 - mae: 0.1329 - val_loss: 0.0192 - val_mae: 0.1319\n",
            "\n",
            "Epoch 00032: loss did not improve from 0.01716\n",
            "Epoch 33/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0166 - mae: 0.1302 - val_loss: 0.0209 - val_mae: 0.1472\n",
            "\n",
            "Epoch 00033: loss did not improve from 0.01716\n",
            "Epoch 34/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0185 - mae: 0.1373 - val_loss: 0.0198 - val_mae: 0.1379\n",
            "\n",
            "Epoch 00034: loss did not improve from 0.01716\n",
            "Epoch 35/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0196 - mae: 0.1360 - val_loss: 0.0189 - val_mae: 0.1310\n",
            "\n",
            "Epoch 00035: loss did not improve from 0.01716\n",
            "Epoch 36/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0156 - mae: 0.1300 - val_loss: 0.0202 - val_mae: 0.1414\n",
            "\n",
            "Epoch 00036: loss improved from 0.01716 to 0.01661, saving model to poids.hdf5\n",
            "Epoch 37/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0161 - mae: 0.1258 - val_loss: 0.0194 - val_mae: 0.1372\n",
            "\n",
            "Epoch 00037: loss did not improve from 0.01661\n",
            "Epoch 38/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0160 - mae: 0.1335 - val_loss: 0.0203 - val_mae: 0.1444\n",
            "\n",
            "Epoch 00038: loss did not improve from 0.01661\n",
            "Epoch 39/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0169 - mae: 0.1274 - val_loss: 0.0198 - val_mae: 0.1457\n",
            "\n",
            "Epoch 00039: loss did not improve from 0.01661\n",
            "Epoch 40/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0140 - mae: 0.1201 - val_loss: 0.0197 - val_mae: 0.1435\n",
            "\n",
            "Epoch 00040: loss did not improve from 0.01661\n",
            "Epoch 41/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0153 - mae: 0.1282 - val_loss: 0.0193 - val_mae: 0.1390\n",
            "\n",
            "Epoch 00041: loss improved from 0.01661 to 0.01630, saving model to poids.hdf5\n",
            "Epoch 42/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0171 - mae: 0.1320 - val_loss: 0.0216 - val_mae: 0.1504\n",
            "\n",
            "Epoch 00042: loss did not improve from 0.01630\n",
            "Epoch 43/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0153 - mae: 0.1245 - val_loss: 0.0232 - val_mae: 0.1596\n",
            "\n",
            "Epoch 00043: loss improved from 0.01630 to 0.01622, saving model to poids.hdf5\n",
            "Epoch 44/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0144 - mae: 0.1214 - val_loss: 0.0226 - val_mae: 0.1558\n",
            "\n",
            "Epoch 00044: loss improved from 0.01622 to 0.01600, saving model to poids.hdf5\n",
            "Epoch 45/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0156 - mae: 0.1266 - val_loss: 0.0200 - val_mae: 0.1397\n",
            "\n",
            "Epoch 00045: loss did not improve from 0.01600\n",
            "Epoch 46/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0150 - mae: 0.1245 - val_loss: 0.0218 - val_mae: 0.1547\n",
            "\n",
            "Epoch 00046: loss did not improve from 0.01600\n",
            "Epoch 47/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0147 - mae: 0.1245 - val_loss: 0.0229 - val_mae: 0.1575\n",
            "\n",
            "Epoch 00047: loss improved from 0.01600 to 0.01575, saving model to poids.hdf5\n",
            "Epoch 48/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0143 - mae: 0.1180 - val_loss: 0.0220 - val_mae: 0.1514\n",
            "\n",
            "Epoch 00048: loss improved from 0.01575 to 0.01573, saving model to poids.hdf5\n",
            "Epoch 49/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0153 - mae: 0.1279 - val_loss: 0.0227 - val_mae: 0.1582\n",
            "\n",
            "Epoch 00049: loss improved from 0.01573 to 0.01556, saving model to poids.hdf5\n",
            "Epoch 50/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0128 - mae: 0.1180 - val_loss: 0.0226 - val_mae: 0.1549\n",
            "\n",
            "Epoch 00050: loss improved from 0.01556 to 0.01535, saving model to poids.hdf5\n",
            "Epoch 51/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0146 - mae: 0.1198 - val_loss: 0.0290 - val_mae: 0.1927\n",
            "\n",
            "Epoch 00051: loss improved from 0.01535 to 0.01523, saving model to poids.hdf5\n",
            "Epoch 52/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0140 - mae: 0.1198 - val_loss: 0.0245 - val_mae: 0.1642\n",
            "\n",
            "Epoch 00052: loss improved from 0.01523 to 0.01518, saving model to poids.hdf5\n",
            "Epoch 53/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0153 - mae: 0.1214 - val_loss: 0.0303 - val_mae: 0.1934\n",
            "\n",
            "Epoch 00053: loss did not improve from 0.01518\n",
            "Epoch 54/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0144 - mae: 0.1234 - val_loss: 0.0314 - val_mae: 0.1995\n",
            "\n",
            "Epoch 00054: loss improved from 0.01518 to 0.01500, saving model to poids.hdf5\n",
            "Epoch 55/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0154 - mae: 0.1243 - val_loss: 0.0280 - val_mae: 0.1826\n",
            "\n",
            "Epoch 00055: loss did not improve from 0.01500\n",
            "Epoch 56/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0149 - mae: 0.1220 - val_loss: 0.0262 - val_mae: 0.1739\n",
            "\n",
            "Epoch 00056: loss improved from 0.01500 to 0.01488, saving model to poids.hdf5\n",
            "Epoch 57/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0140 - mae: 0.1204 - val_loss: 0.0338 - val_mae: 0.2096\n",
            "\n",
            "Epoch 00057: loss improved from 0.01488 to 0.01471, saving model to poids.hdf5\n",
            "Epoch 58/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0148 - mae: 0.1237 - val_loss: 0.0276 - val_mae: 0.1797\n",
            "\n",
            "Epoch 00058: loss improved from 0.01471 to 0.01454, saving model to poids.hdf5\n",
            "Epoch 59/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0131 - mae: 0.1167 - val_loss: 0.0317 - val_mae: 0.2038\n",
            "\n",
            "Epoch 00059: loss improved from 0.01454 to 0.01410, saving model to poids.hdf5\n",
            "Epoch 60/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0167 - mae: 0.1275 - val_loss: 0.0254 - val_mae: 0.1682\n",
            "\n",
            "Epoch 00060: loss did not improve from 0.01410\n",
            "Epoch 61/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0121 - mae: 0.1137 - val_loss: 0.0345 - val_mae: 0.2130\n",
            "\n",
            "Epoch 00061: loss did not improve from 0.01410\n",
            "Epoch 62/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0167 - mae: 0.1248 - val_loss: 0.0276 - val_mae: 0.1828\n",
            "\n",
            "Epoch 00062: loss did not improve from 0.01410\n",
            "Epoch 63/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0131 - mae: 0.1160 - val_loss: 0.0347 - val_mae: 0.2121\n",
            "\n",
            "Epoch 00063: loss improved from 0.01410 to 0.01407, saving model to poids.hdf5\n",
            "Epoch 64/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0170 - mae: 0.1243 - val_loss: 0.0328 - val_mae: 0.2054\n",
            "\n",
            "Epoch 00064: loss did not improve from 0.01407\n",
            "Epoch 65/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0158 - mae: 0.1254 - val_loss: 0.0294 - val_mae: 0.1931\n",
            "\n",
            "Epoch 00065: loss did not improve from 0.01407\n",
            "Epoch 66/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0137 - mae: 0.1186 - val_loss: 0.0320 - val_mae: 0.2031\n",
            "\n",
            "Epoch 00066: loss did not improve from 0.01407\n",
            "Epoch 67/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0124 - mae: 0.1131 - val_loss: 0.0342 - val_mae: 0.2116\n",
            "\n",
            "Epoch 00067: loss did not improve from 0.01407\n",
            "Epoch 68/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0134 - mae: 0.1207 - val_loss: 0.0301 - val_mae: 0.1911\n",
            "\n",
            "Epoch 00068: loss improved from 0.01407 to 0.01363, saving model to poids.hdf5\n",
            "Epoch 69/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0121 - mae: 0.1111 - val_loss: 0.0396 - val_mae: 0.2316\n",
            "\n",
            "Epoch 00069: loss did not improve from 0.01363\n",
            "Epoch 70/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0128 - mae: 0.1143 - val_loss: 0.0323 - val_mae: 0.2058\n",
            "\n",
            "Epoch 00070: loss improved from 0.01363 to 0.01356, saving model to poids.hdf5\n",
            "Epoch 71/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0142 - mae: 0.1164 - val_loss: 0.0372 - val_mae: 0.2220\n",
            "\n",
            "Epoch 00071: loss did not improve from 0.01356\n",
            "Epoch 72/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0116 - mae: 0.1131 - val_loss: 0.0363 - val_mae: 0.2190\n",
            "\n",
            "Epoch 00072: loss improved from 0.01356 to 0.01335, saving model to poids.hdf5\n",
            "Epoch 73/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0161 - mae: 0.1237 - val_loss: 0.0384 - val_mae: 0.2275\n",
            "\n",
            "Epoch 00073: loss did not improve from 0.01335\n",
            "Epoch 74/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0139 - mae: 0.1195 - val_loss: 0.0311 - val_mae: 0.1981\n",
            "\n",
            "Epoch 00074: loss did not improve from 0.01335\n",
            "Epoch 75/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0144 - mae: 0.1167 - val_loss: 0.0344 - val_mae: 0.2098\n",
            "\n",
            "Epoch 00075: loss did not improve from 0.01335\n",
            "Epoch 76/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0128 - mae: 0.1153 - val_loss: 0.0363 - val_mae: 0.2213\n",
            "\n",
            "Epoch 00076: loss improved from 0.01335 to 0.01333, saving model to poids.hdf5\n",
            "Epoch 77/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0163 - mae: 0.1235 - val_loss: 0.0389 - val_mae: 0.2314\n",
            "\n",
            "Epoch 00077: loss did not improve from 0.01333\n",
            "Epoch 78/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0153 - mae: 0.1179 - val_loss: 0.0341 - val_mae: 0.2097\n",
            "\n",
            "Epoch 00078: loss did not improve from 0.01333\n",
            "Epoch 79/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0130 - mae: 0.1142 - val_loss: 0.0403 - val_mae: 0.2338\n",
            "\n",
            "Epoch 00079: loss did not improve from 0.01333\n",
            "Epoch 80/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0124 - mae: 0.1092 - val_loss: 0.0385 - val_mae: 0.2262\n",
            "\n",
            "Epoch 00080: loss did not improve from 0.01333\n",
            "Epoch 81/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0135 - mae: 0.1147 - val_loss: 0.0340 - val_mae: 0.2095\n",
            "\n",
            "Epoch 00081: loss did not improve from 0.01333\n",
            "Epoch 82/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0130 - mae: 0.1163 - val_loss: 0.0379 - val_mae: 0.2256\n",
            "\n",
            "Epoch 00082: loss did not improve from 0.01333\n",
            "Epoch 83/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0147 - mae: 0.1202 - val_loss: 0.0333 - val_mae: 0.2066\n",
            "\n",
            "Epoch 00083: loss did not improve from 0.01333\n",
            "Epoch 84/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0135 - mae: 0.1190 - val_loss: 0.0353 - val_mae: 0.2186\n",
            "\n",
            "Epoch 00084: loss did not improve from 0.01333\n",
            "Epoch 85/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0130 - mae: 0.1160 - val_loss: 0.0377 - val_mae: 0.2238\n",
            "\n",
            "Epoch 00085: loss did not improve from 0.01333\n",
            "Epoch 86/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0142 - mae: 0.1130 - val_loss: 0.0361 - val_mae: 0.2186\n",
            "\n",
            "Epoch 00086: loss did not improve from 0.01333\n",
            "Epoch 87/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0120 - mae: 0.1119 - val_loss: 0.0363 - val_mae: 0.2208\n",
            "\n",
            "Epoch 00087: loss did not improve from 0.01333\n",
            "Epoch 88/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0127 - mae: 0.1117 - val_loss: 0.0386 - val_mae: 0.2266\n",
            "\n",
            "Epoch 00088: loss improved from 0.01333 to 0.01331, saving model to poids.hdf5\n",
            "Epoch 89/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0133 - mae: 0.1162 - val_loss: 0.0358 - val_mae: 0.2187\n",
            "\n",
            "Epoch 00089: loss did not improve from 0.01331\n",
            "Epoch 90/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0133 - mae: 0.1179 - val_loss: 0.0399 - val_mae: 0.2342\n",
            "\n",
            "Epoch 00090: loss did not improve from 0.01331\n",
            "Epoch 91/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0127 - mae: 0.1135 - val_loss: 0.0356 - val_mae: 0.2143\n",
            "\n",
            "Epoch 00091: loss did not improve from 0.01331\n",
            "Epoch 92/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0120 - mae: 0.1108 - val_loss: 0.0378 - val_mae: 0.2244\n",
            "\n",
            "Epoch 00092: loss did not improve from 0.01331\n",
            "Epoch 93/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0133 - mae: 0.1126 - val_loss: 0.0366 - val_mae: 0.2195\n",
            "\n",
            "Epoch 00093: loss did not improve from 0.01331\n",
            "Epoch 94/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0140 - mae: 0.1180 - val_loss: 0.0394 - val_mae: 0.2310\n",
            "\n",
            "Epoch 00094: loss did not improve from 0.01331\n",
            "Epoch 95/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0135 - mae: 0.1177 - val_loss: 0.0329 - val_mae: 0.2060\n",
            "\n",
            "Epoch 00095: loss did not improve from 0.01331\n",
            "Epoch 96/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0135 - mae: 0.1117 - val_loss: 0.0360 - val_mae: 0.2182\n",
            "\n",
            "Epoch 00096: loss did not improve from 0.01331\n",
            "Epoch 97/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0130 - mae: 0.1136 - val_loss: 0.0359 - val_mae: 0.2166\n",
            "\n",
            "Epoch 00097: loss improved from 0.01331 to 0.01325, saving model to poids.hdf5\n",
            "Epoch 98/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0119 - mae: 0.1095 - val_loss: 0.0380 - val_mae: 0.2262\n",
            "\n",
            "Epoch 00098: loss improved from 0.01325 to 0.01318, saving model to poids.hdf5\n",
            "Epoch 99/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0142 - mae: 0.1172 - val_loss: 0.0353 - val_mae: 0.2143\n",
            "\n",
            "Epoch 00099: loss did not improve from 0.01318\n",
            "Epoch 100/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0129 - mae: 0.1151 - val_loss: 0.0396 - val_mae: 0.2320\n",
            "\n",
            "Epoch 00100: loss did not improve from 0.01318\n",
            "Epoch 101/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0134 - mae: 0.1126 - val_loss: 0.0359 - val_mae: 0.2177\n",
            "\n",
            "Epoch 00101: loss improved from 0.01318 to 0.01309, saving model to poids.hdf5\n",
            "Epoch 102/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0121 - mae: 0.1104 - val_loss: 0.0372 - val_mae: 0.2226\n",
            "\n",
            "Epoch 00102: loss improved from 0.01309 to 0.01291, saving model to poids.hdf5\n",
            "Epoch 103/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0119 - mae: 0.1103 - val_loss: 0.0340 - val_mae: 0.2101\n",
            "\n",
            "Epoch 00103: loss did not improve from 0.01291\n",
            "Epoch 104/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0161 - mae: 0.1180 - val_loss: 0.0375 - val_mae: 0.2238\n",
            "\n",
            "Epoch 00104: loss did not improve from 0.01291\n",
            "Epoch 105/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0132 - mae: 0.1134 - val_loss: 0.0335 - val_mae: 0.2075\n",
            "\n",
            "Epoch 00105: loss did not improve from 0.01291\n",
            "Epoch 106/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0131 - mae: 0.1154 - val_loss: 0.0383 - val_mae: 0.2263\n",
            "\n",
            "Epoch 00106: loss did not improve from 0.01291\n",
            "Epoch 107/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0149 - mae: 0.1177 - val_loss: 0.0363 - val_mae: 0.2187\n",
            "\n",
            "Epoch 00107: loss did not improve from 0.01291\n",
            "Epoch 108/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0128 - mae: 0.1146 - val_loss: 0.0374 - val_mae: 0.2236\n",
            "\n",
            "Epoch 00108: loss did not improve from 0.01291\n",
            "Epoch 109/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0131 - mae: 0.1144 - val_loss: 0.0355 - val_mae: 0.2155\n",
            "\n",
            "Epoch 00109: loss did not improve from 0.01291\n",
            "Epoch 110/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0113 - mae: 0.1114 - val_loss: 0.0386 - val_mae: 0.2287\n",
            "\n",
            "Epoch 00110: loss did not improve from 0.01291\n",
            "Epoch 111/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0110 - mae: 0.1069 - val_loss: 0.0328 - val_mae: 0.2038\n",
            "\n",
            "Epoch 00111: loss did not improve from 0.01291\n",
            "Epoch 112/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0107 - mae: 0.1083 - val_loss: 0.0342 - val_mae: 0.2137\n",
            "\n",
            "Epoch 00112: loss did not improve from 0.01291\n",
            "Epoch 113/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0133 - mae: 0.1146 - val_loss: 0.0334 - val_mae: 0.2075\n",
            "\n",
            "Epoch 00113: loss improved from 0.01291 to 0.01280, saving model to poids.hdf5\n",
            "Epoch 114/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0144 - mae: 0.1173 - val_loss: 0.0340 - val_mae: 0.2094\n",
            "\n",
            "Epoch 00114: loss did not improve from 0.01280\n",
            "Epoch 115/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0122 - mae: 0.1089 - val_loss: 0.0301 - val_mae: 0.1970\n",
            "\n",
            "Epoch 00115: loss did not improve from 0.01280\n",
            "Epoch 116/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0133 - mae: 0.1142 - val_loss: 0.0345 - val_mae: 0.2120\n",
            "\n",
            "Epoch 00116: loss improved from 0.01280 to 0.01265, saving model to poids.hdf5\n",
            "Epoch 117/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0130 - mae: 0.1140 - val_loss: 0.0323 - val_mae: 0.2027\n",
            "\n",
            "Epoch 00117: loss did not improve from 0.01265\n",
            "Epoch 118/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0129 - mae: 0.1147 - val_loss: 0.0332 - val_mae: 0.2092\n",
            "\n",
            "Epoch 00118: loss did not improve from 0.01265\n",
            "Epoch 119/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0143 - mae: 0.1152 - val_loss: 0.0365 - val_mae: 0.2198\n",
            "\n",
            "Epoch 00119: loss did not improve from 0.01265\n",
            "Epoch 120/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0124 - mae: 0.1111 - val_loss: 0.0338 - val_mae: 0.2088\n",
            "\n",
            "Epoch 00120: loss did not improve from 0.01265\n",
            "Epoch 121/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0125 - mae: 0.1141 - val_loss: 0.0334 - val_mae: 0.2071\n",
            "\n",
            "Epoch 00121: loss did not improve from 0.01265\n",
            "Epoch 122/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0136 - mae: 0.1175 - val_loss: 0.0351 - val_mae: 0.2149\n",
            "\n",
            "Epoch 00122: loss did not improve from 0.01265\n",
            "Epoch 123/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0137 - mae: 0.1159 - val_loss: 0.0339 - val_mae: 0.2092\n",
            "\n",
            "Epoch 00123: loss did not improve from 0.01265\n",
            "Epoch 124/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0122 - mae: 0.1131 - val_loss: 0.0331 - val_mae: 0.2063\n",
            "\n",
            "Epoch 00124: loss did not improve from 0.01265\n",
            "Epoch 125/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0109 - mae: 0.1066 - val_loss: 0.0324 - val_mae: 0.2032\n",
            "\n",
            "Epoch 00125: loss did not improve from 0.01265\n",
            "Epoch 126/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0117 - mae: 0.1095 - val_loss: 0.0339 - val_mae: 0.2093\n",
            "\n",
            "Epoch 00126: loss improved from 0.01265 to 0.01263, saving model to poids.hdf5\n",
            "Epoch 127/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0124 - mae: 0.1124 - val_loss: 0.0347 - val_mae: 0.2130\n",
            "\n",
            "Epoch 00127: loss did not improve from 0.01263\n",
            "Epoch 128/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0127 - mae: 0.1143 - val_loss: 0.0308 - val_mae: 0.1969\n",
            "\n",
            "Epoch 00128: loss did not improve from 0.01263\n",
            "Epoch 129/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0142 - mae: 0.1185 - val_loss: 0.0340 - val_mae: 0.2111\n",
            "\n",
            "Epoch 00129: loss did not improve from 0.01263\n",
            "Epoch 130/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0151 - mae: 0.1161 - val_loss: 0.0316 - val_mae: 0.1993\n",
            "\n",
            "Epoch 00130: loss did not improve from 0.01263\n",
            "Epoch 131/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0130 - mae: 0.1134 - val_loss: 0.0344 - val_mae: 0.2108\n",
            "\n",
            "Epoch 00131: loss did not improve from 0.01263\n",
            "Epoch 132/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0139 - mae: 0.1142 - val_loss: 0.0357 - val_mae: 0.2158\n",
            "\n",
            "Epoch 00132: loss did not improve from 0.01263\n",
            "Epoch 133/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0127 - mae: 0.1092 - val_loss: 0.0349 - val_mae: 0.2129\n",
            "\n",
            "Epoch 00133: loss did not improve from 0.01263\n",
            "Epoch 134/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0136 - mae: 0.1162 - val_loss: 0.0325 - val_mae: 0.2040\n",
            "\n",
            "Epoch 00134: loss did not improve from 0.01263\n",
            "Epoch 135/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0129 - mae: 0.1127 - val_loss: 0.0369 - val_mae: 0.2215\n",
            "\n",
            "Epoch 00135: loss improved from 0.01263 to 0.01205, saving model to poids.hdf5\n",
            "Epoch 136/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0141 - mae: 0.1163 - val_loss: 0.0346 - val_mae: 0.2125\n",
            "\n",
            "Epoch 00136: loss did not improve from 0.01205\n",
            "Epoch 137/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0145 - mae: 0.1160 - val_loss: 0.0333 - val_mae: 0.2074\n",
            "\n",
            "Epoch 00137: loss did not improve from 0.01205\n",
            "Epoch 138/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0116 - mae: 0.1102 - val_loss: 0.0311 - val_mae: 0.1993\n",
            "\n",
            "Epoch 00138: loss did not improve from 0.01205\n",
            "Epoch 139/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0101 - mae: 0.1061 - val_loss: 0.0312 - val_mae: 0.1985\n",
            "\n",
            "Epoch 00139: loss did not improve from 0.01205\n",
            "Epoch 140/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0128 - mae: 0.1112 - val_loss: 0.0312 - val_mae: 0.1987\n",
            "\n",
            "Epoch 00140: loss did not improve from 0.01205\n",
            "Epoch 141/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0133 - mae: 0.1146 - val_loss: 0.0317 - val_mae: 0.2027\n",
            "\n",
            "Epoch 00141: loss did not improve from 0.01205\n",
            "Epoch 142/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0128 - mae: 0.1131 - val_loss: 0.0364 - val_mae: 0.2206\n",
            "\n",
            "Epoch 00142: loss did not improve from 0.01205\n",
            "Epoch 143/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0130 - mae: 0.1120 - val_loss: 0.0314 - val_mae: 0.2001\n",
            "\n",
            "Epoch 00143: loss did not improve from 0.01205\n",
            "Epoch 144/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0125 - mae: 0.1142 - val_loss: 0.0292 - val_mae: 0.1925\n",
            "\n",
            "Epoch 00144: loss did not improve from 0.01205\n",
            "Epoch 145/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0134 - mae: 0.1140 - val_loss: 0.0331 - val_mae: 0.2061\n",
            "\n",
            "Epoch 00145: loss did not improve from 0.01205\n",
            "Epoch 146/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0130 - mae: 0.1114 - val_loss: 0.0326 - val_mae: 0.2038\n",
            "\n",
            "Epoch 00146: loss did not improve from 0.01205\n",
            "Epoch 147/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0139 - mae: 0.1176 - val_loss: 0.0311 - val_mae: 0.1985\n",
            "\n",
            "Epoch 00147: loss did not improve from 0.01205\n",
            "Epoch 148/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0126 - mae: 0.1109 - val_loss: 0.0339 - val_mae: 0.2101\n",
            "\n",
            "Epoch 00148: loss did not improve from 0.01205\n",
            "Epoch 149/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0115 - mae: 0.1090 - val_loss: 0.0324 - val_mae: 0.2042\n",
            "\n",
            "Epoch 00149: loss did not improve from 0.01205\n",
            "Epoch 150/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0114 - mae: 0.1097 - val_loss: 0.0311 - val_mae: 0.1988\n",
            "\n",
            "Epoch 00150: loss did not improve from 0.01205\n",
            "Epoch 151/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0111 - mae: 0.1059 - val_loss: 0.0339 - val_mae: 0.2104\n",
            "\n",
            "Epoch 00151: loss did not improve from 0.01205\n",
            "Epoch 152/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0130 - mae: 0.1117 - val_loss: 0.0332 - val_mae: 0.2058\n",
            "\n",
            "Epoch 00152: loss did not improve from 0.01205\n",
            "Epoch 153/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0115 - mae: 0.1127 - val_loss: 0.0298 - val_mae: 0.1943\n",
            "\n",
            "Epoch 00153: loss did not improve from 0.01205\n",
            "Epoch 154/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0138 - mae: 0.1169 - val_loss: 0.0302 - val_mae: 0.1960\n",
            "\n",
            "Epoch 00154: loss did not improve from 0.01205\n",
            "Epoch 155/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0115 - mae: 0.1072 - val_loss: 0.0301 - val_mae: 0.1937\n",
            "\n",
            "Epoch 00155: loss did not improve from 0.01205\n",
            "Epoch 156/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0120 - mae: 0.1107 - val_loss: 0.0304 - val_mae: 0.1982\n",
            "\n",
            "Epoch 00156: loss did not improve from 0.01205\n",
            "Epoch 157/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0132 - mae: 0.1149 - val_loss: 0.0297 - val_mae: 0.1947\n",
            "\n",
            "Epoch 00157: loss did not improve from 0.01205\n",
            "Epoch 158/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0101 - mae: 0.1057 - val_loss: 0.0293 - val_mae: 0.1934\n",
            "\n",
            "Epoch 00158: loss did not improve from 0.01205\n",
            "Epoch 159/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0131 - mae: 0.1141 - val_loss: 0.0309 - val_mae: 0.1970\n",
            "\n",
            "Epoch 00159: loss did not improve from 0.01205\n",
            "Epoch 160/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0134 - mae: 0.1147 - val_loss: 0.0300 - val_mae: 0.1938\n",
            "\n",
            "Epoch 00160: loss did not improve from 0.01205\n",
            "Epoch 161/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0125 - mae: 0.1099 - val_loss: 0.0280 - val_mae: 0.1849\n",
            "\n",
            "Epoch 00161: loss did not improve from 0.01205\n",
            "Epoch 162/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0147 - mae: 0.1177 - val_loss: 0.0309 - val_mae: 0.1979\n",
            "\n",
            "Epoch 00162: loss did not improve from 0.01205\n",
            "Epoch 163/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0130 - mae: 0.1103 - val_loss: 0.0296 - val_mae: 0.1948\n",
            "\n",
            "Epoch 00163: loss did not improve from 0.01205\n",
            "Epoch 164/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0114 - mae: 0.1084 - val_loss: 0.0295 - val_mae: 0.1928\n",
            "\n",
            "Epoch 00164: loss did not improve from 0.01205\n",
            "Epoch 165/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0125 - mae: 0.1124 - val_loss: 0.0313 - val_mae: 0.1993\n",
            "\n",
            "Epoch 00165: loss did not improve from 0.01205\n",
            "Epoch 166/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0121 - mae: 0.1087 - val_loss: 0.0302 - val_mae: 0.1953\n",
            "\n",
            "Epoch 00166: loss did not improve from 0.01205\n",
            "Epoch 167/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0124 - mae: 0.1124 - val_loss: 0.0288 - val_mae: 0.1876\n",
            "\n",
            "Epoch 00167: loss did not improve from 0.01205\n",
            "Epoch 168/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0116 - mae: 0.1074 - val_loss: 0.0277 - val_mae: 0.1863\n",
            "\n",
            "Epoch 00168: loss did not improve from 0.01205\n",
            "Epoch 169/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0121 - mae: 0.1104 - val_loss: 0.0299 - val_mae: 0.1929\n",
            "\n",
            "Epoch 00169: loss did not improve from 0.01205\n",
            "Epoch 170/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0113 - mae: 0.1065 - val_loss: 0.0298 - val_mae: 0.1936\n",
            "\n",
            "Epoch 00170: loss did not improve from 0.01205\n",
            "Epoch 171/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0108 - mae: 0.1074 - val_loss: 0.0312 - val_mae: 0.1992\n",
            "\n",
            "Epoch 00171: loss did not improve from 0.01205\n",
            "Epoch 172/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0122 - mae: 0.1090 - val_loss: 0.0304 - val_mae: 0.1955\n",
            "\n",
            "Epoch 00172: loss did not improve from 0.01205\n",
            "Epoch 173/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0123 - mae: 0.1095 - val_loss: 0.0293 - val_mae: 0.1945\n",
            "\n",
            "Epoch 00173: loss did not improve from 0.01205\n",
            "Epoch 174/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0130 - mae: 0.1097 - val_loss: 0.0320 - val_mae: 0.2026\n",
            "\n",
            "Epoch 00174: loss did not improve from 0.01205\n",
            "Epoch 175/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0142 - mae: 0.1168 - val_loss: 0.0303 - val_mae: 0.1942\n",
            "\n",
            "Epoch 00175: loss did not improve from 0.01205\n",
            "Epoch 176/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0110 - mae: 0.1058 - val_loss: 0.0305 - val_mae: 0.1967\n",
            "\n",
            "Epoch 00176: loss did not improve from 0.01205\n",
            "Epoch 177/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0138 - mae: 0.1141 - val_loss: 0.0288 - val_mae: 0.1887\n",
            "\n",
            "Epoch 00177: loss did not improve from 0.01205\n",
            "Epoch 178/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0125 - mae: 0.1121 - val_loss: 0.0285 - val_mae: 0.1872\n",
            "\n",
            "Epoch 00178: loss did not improve from 0.01205\n",
            "Epoch 179/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0128 - mae: 0.1124 - val_loss: 0.0291 - val_mae: 0.1903\n",
            "\n",
            "Epoch 00179: loss did not improve from 0.01205\n",
            "Epoch 180/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0120 - mae: 0.1113 - val_loss: 0.0286 - val_mae: 0.1899\n",
            "\n",
            "Epoch 00180: loss did not improve from 0.01205\n",
            "Epoch 181/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0134 - mae: 0.1133 - val_loss: 0.0311 - val_mae: 0.1978\n",
            "\n",
            "Epoch 00181: loss did not improve from 0.01205\n",
            "Epoch 182/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0138 - mae: 0.1123 - val_loss: 0.0295 - val_mae: 0.1911\n",
            "\n",
            "Epoch 00182: loss did not improve from 0.01205\n",
            "Epoch 183/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0110 - mae: 0.1066 - val_loss: 0.0292 - val_mae: 0.1924\n",
            "\n",
            "Epoch 00183: loss did not improve from 0.01205\n",
            "Epoch 184/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0113 - mae: 0.1097 - val_loss: 0.0270 - val_mae: 0.1837\n",
            "\n",
            "Epoch 00184: loss did not improve from 0.01205\n",
            "Epoch 185/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0108 - mae: 0.1088 - val_loss: 0.0289 - val_mae: 0.1889\n",
            "\n",
            "Epoch 00185: loss did not improve from 0.01205\n",
            "Epoch 186/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0118 - mae: 0.1110 - val_loss: 0.0285 - val_mae: 0.1876\n",
            "\n",
            "Epoch 00186: loss did not improve from 0.01205\n",
            "Epoch 187/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0107 - mae: 0.1051 - val_loss: 0.0288 - val_mae: 0.1893\n",
            "\n",
            "Epoch 00187: loss did not improve from 0.01205\n",
            "Epoch 188/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0110 - mae: 0.1085 - val_loss: 0.0288 - val_mae: 0.1888\n",
            "\n",
            "Epoch 00188: loss did not improve from 0.01205\n",
            "Epoch 189/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0122 - mae: 0.1085 - val_loss: 0.0294 - val_mae: 0.1901\n",
            "\n",
            "Epoch 00189: loss did not improve from 0.01205\n",
            "Epoch 190/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0143 - mae: 0.1155 - val_loss: 0.0290 - val_mae: 0.1889\n",
            "\n",
            "Epoch 00190: loss did not improve from 0.01205\n",
            "Epoch 191/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0140 - mae: 0.1152 - val_loss: 0.0286 - val_mae: 0.1903\n",
            "\n",
            "Epoch 00191: loss did not improve from 0.01205\n",
            "Epoch 192/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0141 - mae: 0.1138 - val_loss: 0.0284 - val_mae: 0.1886\n",
            "\n",
            "Epoch 00192: loss did not improve from 0.01205\n",
            "Epoch 193/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0114 - mae: 0.1080 - val_loss: 0.0289 - val_mae: 0.1888\n",
            "\n",
            "Epoch 00193: loss did not improve from 0.01205\n",
            "Epoch 194/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0117 - mae: 0.1126 - val_loss: 0.0290 - val_mae: 0.1916\n",
            "\n",
            "Epoch 00194: loss did not improve from 0.01205\n",
            "Epoch 195/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0107 - mae: 0.1040 - val_loss: 0.0267 - val_mae: 0.1828\n",
            "\n",
            "Epoch 00195: loss did not improve from 0.01205\n",
            "Epoch 196/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0139 - mae: 0.1129 - val_loss: 0.0275 - val_mae: 0.1828\n",
            "\n",
            "Epoch 00196: loss did not improve from 0.01205\n",
            "Epoch 197/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0119 - mae: 0.1100 - val_loss: 0.0290 - val_mae: 0.1894\n",
            "\n",
            "Epoch 00197: loss did not improve from 0.01205\n",
            "Epoch 198/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0147 - mae: 0.1170 - val_loss: 0.0295 - val_mae: 0.1917\n",
            "\n",
            "Epoch 00198: loss did not improve from 0.01205\n",
            "Epoch 199/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0168 - mae: 0.1197 - val_loss: 0.0284 - val_mae: 0.1860\n",
            "\n",
            "Epoch 00199: loss did not improve from 0.01205\n",
            "Epoch 200/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0125 - mae: 0.1129 - val_loss: 0.0278 - val_mae: 0.1872\n",
            "\n",
            "Epoch 00200: loss did not improve from 0.01205\n",
            "Epoch 201/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0149 - mae: 0.1167 - val_loss: 0.0282 - val_mae: 0.1871\n",
            "\n",
            "Epoch 00201: loss did not improve from 0.01205\n",
            "Epoch 202/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0116 - mae: 0.1079 - val_loss: 0.0271 - val_mae: 0.1822\n",
            "\n",
            "Epoch 00202: loss did not improve from 0.01205\n",
            "Epoch 203/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0130 - mae: 0.1141 - val_loss: 0.0297 - val_mae: 0.1926\n",
            "\n",
            "Epoch 00203: loss did not improve from 0.01205\n",
            "Epoch 204/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0130 - mae: 0.1101 - val_loss: 0.0271 - val_mae: 0.1851\n",
            "\n",
            "Epoch 00204: loss did not improve from 0.01205\n",
            "Epoch 205/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0110 - mae: 0.1064 - val_loss: 0.0274 - val_mae: 0.1832\n",
            "\n",
            "Epoch 00205: loss did not improve from 0.01205\n",
            "Epoch 206/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0130 - mae: 0.1118 - val_loss: 0.0271 - val_mae: 0.1856\n",
            "\n",
            "Epoch 00206: loss did not improve from 0.01205\n",
            "Epoch 207/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0135 - mae: 0.1130 - val_loss: 0.0269 - val_mae: 0.1801\n",
            "\n",
            "Epoch 00207: loss did not improve from 0.01205\n",
            "Epoch 208/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0116 - mae: 0.1075 - val_loss: 0.0270 - val_mae: 0.1828\n",
            "\n",
            "Epoch 00208: loss did not improve from 0.01205\n",
            "Epoch 209/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0114 - mae: 0.1103 - val_loss: 0.0266 - val_mae: 0.1829\n",
            "\n",
            "Epoch 00209: loss did not improve from 0.01205\n",
            "Epoch 210/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0117 - mae: 0.1085 - val_loss: 0.0269 - val_mae: 0.1817\n",
            "\n",
            "Epoch 00210: loss did not improve from 0.01205\n",
            "Epoch 211/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0109 - mae: 0.1077 - val_loss: 0.0269 - val_mae: 0.1813\n",
            "\n",
            "Epoch 00211: loss did not improve from 0.01205\n",
            "Epoch 212/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0123 - mae: 0.1119 - val_loss: 0.0279 - val_mae: 0.1850\n",
            "\n",
            "Epoch 00212: loss did not improve from 0.01205\n",
            "Epoch 213/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0141 - mae: 0.1171 - val_loss: 0.0283 - val_mae: 0.1868\n",
            "\n",
            "Epoch 00213: loss did not improve from 0.01205\n",
            "Epoch 214/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0109 - mae: 0.1075 - val_loss: 0.0278 - val_mae: 0.1853\n",
            "\n",
            "Epoch 00214: loss did not improve from 0.01205\n",
            "Epoch 215/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0136 - mae: 0.1130 - val_loss: 0.0263 - val_mae: 0.1784\n",
            "\n",
            "Epoch 00215: loss did not improve from 0.01205\n",
            "Epoch 216/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0127 - mae: 0.1143 - val_loss: 0.0277 - val_mae: 0.1840\n",
            "\n",
            "Epoch 00216: loss did not improve from 0.01205\n",
            "Epoch 217/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0111 - mae: 0.1068 - val_loss: 0.0266 - val_mae: 0.1808\n",
            "\n",
            "Epoch 00217: loss did not improve from 0.01205\n",
            "Epoch 218/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0101 - mae: 0.1054 - val_loss: 0.0274 - val_mae: 0.1826\n",
            "\n",
            "Epoch 00218: loss improved from 0.01205 to 0.01193, saving model to poids.hdf5\n",
            "Epoch 219/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0115 - mae: 0.1074 - val_loss: 0.0276 - val_mae: 0.1830\n",
            "\n",
            "Epoch 00219: loss did not improve from 0.01193\n",
            "Epoch 220/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0158 - mae: 0.1191 - val_loss: 0.0274 - val_mae: 0.1817\n",
            "\n",
            "Epoch 00220: loss did not improve from 0.01193\n",
            "Epoch 221/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0124 - mae: 0.1106 - val_loss: 0.0265 - val_mae: 0.1815\n",
            "\n",
            "Epoch 00221: loss did not improve from 0.01193\n",
            "Epoch 222/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0119 - mae: 0.1101 - val_loss: 0.0284 - val_mae: 0.1872\n",
            "\n",
            "Epoch 00222: loss improved from 0.01193 to 0.01185, saving model to poids.hdf5\n",
            "Epoch 223/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0139 - mae: 0.1151 - val_loss: 0.0281 - val_mae: 0.1857\n",
            "\n",
            "Epoch 00223: loss did not improve from 0.01185\n",
            "Epoch 224/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0121 - mae: 0.1081 - val_loss: 0.0266 - val_mae: 0.1812\n",
            "\n",
            "Epoch 00224: loss did not improve from 0.01185\n",
            "Epoch 225/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0111 - mae: 0.1072 - val_loss: 0.0276 - val_mae: 0.1836\n",
            "\n",
            "Epoch 00225: loss did not improve from 0.01185\n",
            "Epoch 226/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0142 - mae: 0.1121 - val_loss: 0.0271 - val_mae: 0.1811\n",
            "\n",
            "Epoch 00226: loss did not improve from 0.01185\n",
            "Epoch 227/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0124 - mae: 0.1137 - val_loss: 0.0253 - val_mae: 0.1765\n",
            "\n",
            "Epoch 00227: loss did not improve from 0.01185\n",
            "Epoch 228/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0120 - mae: 0.1062 - val_loss: 0.0250 - val_mae: 0.1716\n",
            "\n",
            "Epoch 00228: loss did not improve from 0.01185\n",
            "Epoch 229/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0119 - mae: 0.1100 - val_loss: 0.0254 - val_mae: 0.1772\n",
            "\n",
            "Epoch 00229: loss did not improve from 0.01185\n",
            "Epoch 230/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0103 - mae: 0.1051 - val_loss: 0.0279 - val_mae: 0.1852\n",
            "\n",
            "Epoch 00230: loss did not improve from 0.01185\n",
            "Epoch 231/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0134 - mae: 0.1150 - val_loss: 0.0266 - val_mae: 0.1795\n",
            "\n",
            "Epoch 00231: loss did not improve from 0.01185\n",
            "Epoch 232/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0135 - mae: 0.1182 - val_loss: 0.0269 - val_mae: 0.1799\n",
            "\n",
            "Epoch 00232: loss did not improve from 0.01185\n",
            "Epoch 233/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0116 - mae: 0.1069 - val_loss: 0.0273 - val_mae: 0.1821\n",
            "\n",
            "Epoch 00233: loss did not improve from 0.01185\n",
            "Epoch 234/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0134 - mae: 0.1131 - val_loss: 0.0275 - val_mae: 0.1833\n",
            "\n",
            "Epoch 00234: loss did not improve from 0.01185\n",
            "Epoch 235/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0110 - mae: 0.1089 - val_loss: 0.0262 - val_mae: 0.1797\n",
            "\n",
            "Epoch 00235: loss did not improve from 0.01185\n",
            "Epoch 236/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0142 - mae: 0.1163 - val_loss: 0.0269 - val_mae: 0.1806\n",
            "\n",
            "Epoch 00236: loss did not improve from 0.01185\n",
            "Epoch 237/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0120 - mae: 0.1103 - val_loss: 0.0270 - val_mae: 0.1815\n",
            "\n",
            "Epoch 00237: loss did not improve from 0.01185\n",
            "Epoch 238/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0119 - mae: 0.1090 - val_loss: 0.0264 - val_mae: 0.1783\n",
            "\n",
            "Epoch 00238: loss did not improve from 0.01185\n",
            "Epoch 239/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0139 - mae: 0.1138 - val_loss: 0.0272 - val_mae: 0.1818\n",
            "\n",
            "Epoch 00239: loss did not improve from 0.01185\n",
            "Epoch 240/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0119 - mae: 0.1091 - val_loss: 0.0266 - val_mae: 0.1795\n",
            "\n",
            "Epoch 00240: loss did not improve from 0.01185\n",
            "Epoch 241/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0121 - mae: 0.1131 - val_loss: 0.0279 - val_mae: 0.1850\n",
            "\n",
            "Epoch 00241: loss did not improve from 0.01185\n",
            "Epoch 242/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0120 - mae: 0.1122 - val_loss: 0.0271 - val_mae: 0.1816\n",
            "\n",
            "Epoch 00242: loss did not improve from 0.01185\n",
            "Epoch 243/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0118 - mae: 0.1071 - val_loss: 0.0253 - val_mae: 0.1763\n",
            "\n",
            "Epoch 00243: loss did not improve from 0.01185\n",
            "Epoch 244/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0126 - mae: 0.1104 - val_loss: 0.0257 - val_mae: 0.1754\n",
            "\n",
            "Epoch 00244: loss did not improve from 0.01185\n",
            "Epoch 245/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0110 - mae: 0.1078 - val_loss: 0.0268 - val_mae: 0.1807\n",
            "\n",
            "Epoch 00245: loss did not improve from 0.01185\n",
            "Epoch 246/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0121 - mae: 0.1120 - val_loss: 0.0265 - val_mae: 0.1788\n",
            "\n",
            "Epoch 00246: loss did not improve from 0.01185\n",
            "Epoch 247/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0131 - mae: 0.1129 - val_loss: 0.0250 - val_mae: 0.1752\n",
            "\n",
            "Epoch 00247: loss did not improve from 0.01185\n",
            "Epoch 248/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0119 - mae: 0.1105 - val_loss: 0.0250 - val_mae: 0.1706\n",
            "\n",
            "Epoch 00248: loss did not improve from 0.01185\n",
            "Epoch 249/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0130 - mae: 0.1124 - val_loss: 0.0264 - val_mae: 0.1809\n",
            "\n",
            "Epoch 00249: loss did not improve from 0.01185\n",
            "Epoch 250/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0120 - mae: 0.1071 - val_loss: 0.0262 - val_mae: 0.1809\n",
            "\n",
            "Epoch 00250: loss did not improve from 0.01185\n",
            "Epoch 251/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0123 - mae: 0.1129 - val_loss: 0.0270 - val_mae: 0.1815\n",
            "\n",
            "Epoch 00251: loss did not improve from 0.01185\n",
            "Epoch 252/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0110 - mae: 0.1069 - val_loss: 0.0267 - val_mae: 0.1798\n",
            "\n",
            "Epoch 00252: loss did not improve from 0.01185\n",
            "Epoch 253/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0139 - mae: 0.1136 - val_loss: 0.0260 - val_mae: 0.1771\n",
            "\n",
            "Epoch 00253: loss did not improve from 0.01185\n",
            "Epoch 254/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0103 - mae: 0.1053 - val_loss: 0.0266 - val_mae: 0.1803\n",
            "\n",
            "Epoch 00254: loss did not improve from 0.01185\n",
            "Epoch 255/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0133 - mae: 0.1161 - val_loss: 0.0258 - val_mae: 0.1782\n",
            "\n",
            "Epoch 00255: loss did not improve from 0.01185\n",
            "Epoch 256/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0108 - mae: 0.1070 - val_loss: 0.0266 - val_mae: 0.1794\n",
            "\n",
            "Epoch 00256: loss did not improve from 0.01185\n",
            "Epoch 257/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0112 - mae: 0.1120 - val_loss: 0.0254 - val_mae: 0.1741\n",
            "\n",
            "Epoch 00257: loss did not improve from 0.01185\n",
            "Epoch 258/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0127 - mae: 0.1149 - val_loss: 0.0241 - val_mae: 0.1721\n",
            "\n",
            "Epoch 00258: loss improved from 0.01185 to 0.01181, saving model to poids.hdf5\n",
            "Epoch 259/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0112 - mae: 0.1074 - val_loss: 0.0258 - val_mae: 0.1786\n",
            "\n",
            "Epoch 00259: loss did not improve from 0.01181\n",
            "Epoch 260/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0112 - mae: 0.1088 - val_loss: 0.0266 - val_mae: 0.1785\n",
            "\n",
            "Epoch 00260: loss did not improve from 0.01181\n",
            "Epoch 261/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0114 - mae: 0.1089 - val_loss: 0.0274 - val_mae: 0.1834\n",
            "\n",
            "Epoch 00261: loss did not improve from 0.01181\n",
            "Epoch 262/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0127 - mae: 0.1109 - val_loss: 0.0262 - val_mae: 0.1777\n",
            "\n",
            "Epoch 00262: loss did not improve from 0.01181\n",
            "Epoch 263/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0117 - mae: 0.1088 - val_loss: 0.0248 - val_mae: 0.1713\n",
            "\n",
            "Epoch 00263: loss did not improve from 0.01181\n",
            "Epoch 264/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0105 - mae: 0.1056 - val_loss: 0.0263 - val_mae: 0.1785\n",
            "\n",
            "Epoch 00264: loss did not improve from 0.01181\n",
            "Epoch 265/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0113 - mae: 0.1097 - val_loss: 0.0273 - val_mae: 0.1829\n",
            "\n",
            "Epoch 00265: loss did not improve from 0.01181\n",
            "Epoch 266/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0112 - mae: 0.1055 - val_loss: 0.0248 - val_mae: 0.1708\n",
            "\n",
            "Epoch 00266: loss did not improve from 0.01181\n",
            "Epoch 267/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0116 - mae: 0.1073 - val_loss: 0.0263 - val_mae: 0.1775\n",
            "\n",
            "Epoch 00267: loss did not improve from 0.01181\n",
            "Epoch 268/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0108 - mae: 0.1100 - val_loss: 0.0259 - val_mae: 0.1754\n",
            "\n",
            "Epoch 00268: loss did not improve from 0.01181\n",
            "Epoch 269/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0112 - mae: 0.1098 - val_loss: 0.0259 - val_mae: 0.1761\n",
            "\n",
            "Epoch 00269: loss did not improve from 0.01181\n",
            "Epoch 270/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0112 - mae: 0.1065 - val_loss: 0.0253 - val_mae: 0.1768\n",
            "\n",
            "Epoch 00270: loss did not improve from 0.01181\n",
            "Epoch 271/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0128 - mae: 0.1111 - val_loss: 0.0263 - val_mae: 0.1773\n",
            "\n",
            "Epoch 00271: loss did not improve from 0.01181\n",
            "Epoch 272/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0105 - mae: 0.1068 - val_loss: 0.0261 - val_mae: 0.1766\n",
            "\n",
            "Epoch 00272: loss did not improve from 0.01181\n",
            "Epoch 273/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0124 - mae: 0.1117 - val_loss: 0.0258 - val_mae: 0.1764\n",
            "\n",
            "Epoch 00273: loss did not improve from 0.01181\n",
            "Epoch 274/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0116 - mae: 0.1093 - val_loss: 0.0254 - val_mae: 0.1741\n",
            "\n",
            "Epoch 00274: loss did not improve from 0.01181\n",
            "Epoch 275/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0120 - mae: 0.1113 - val_loss: 0.0246 - val_mae: 0.1713\n",
            "\n",
            "Epoch 00275: loss did not improve from 0.01181\n",
            "Epoch 276/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0103 - mae: 0.1055 - val_loss: 0.0250 - val_mae: 0.1732\n",
            "\n",
            "Epoch 00276: loss did not improve from 0.01181\n",
            "Epoch 277/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0112 - mae: 0.1100 - val_loss: 0.0254 - val_mae: 0.1743\n",
            "\n",
            "Epoch 00277: loss did not improve from 0.01181\n",
            "Epoch 278/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0130 - mae: 0.1124 - val_loss: 0.0236 - val_mae: 0.1708\n",
            "\n",
            "Epoch 00278: loss did not improve from 0.01181\n",
            "Epoch 279/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0130 - mae: 0.1111 - val_loss: 0.0248 - val_mae: 0.1733\n",
            "\n",
            "Epoch 00279: loss did not improve from 0.01181\n",
            "Epoch 280/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0112 - mae: 0.1100 - val_loss: 0.0256 - val_mae: 0.1758\n",
            "\n",
            "Epoch 00280: loss did not improve from 0.01181\n",
            "Epoch 281/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0123 - mae: 0.1112 - val_loss: 0.0250 - val_mae: 0.1727\n",
            "\n",
            "Epoch 00281: loss did not improve from 0.01181\n",
            "Epoch 282/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0136 - mae: 0.1139 - val_loss: 0.0258 - val_mae: 0.1762\n",
            "\n",
            "Epoch 00282: loss did not improve from 0.01181\n",
            "Epoch 283/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0107 - mae: 0.1078 - val_loss: 0.0253 - val_mae: 0.1746\n",
            "\n",
            "Epoch 00283: loss did not improve from 0.01181\n",
            "Epoch 284/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0106 - mae: 0.1070 - val_loss: 0.0252 - val_mae: 0.1725\n",
            "\n",
            "Epoch 00284: loss improved from 0.01181 to 0.01167, saving model to poids.hdf5\n",
            "Epoch 285/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0107 - mae: 0.1072 - val_loss: 0.0250 - val_mae: 0.1714\n",
            "\n",
            "Epoch 00285: loss did not improve from 0.01167\n",
            "Epoch 286/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0120 - mae: 0.1094 - val_loss: 0.0256 - val_mae: 0.1750\n",
            "\n",
            "Epoch 00286: loss did not improve from 0.01167\n",
            "Epoch 287/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0123 - mae: 0.1099 - val_loss: 0.0238 - val_mae: 0.1676\n",
            "\n",
            "Epoch 00287: loss did not improve from 0.01167\n",
            "Epoch 288/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0141 - mae: 0.1139 - val_loss: 0.0262 - val_mae: 0.1781\n",
            "\n",
            "Epoch 00288: loss did not improve from 0.01167\n",
            "Epoch 289/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0125 - mae: 0.1107 - val_loss: 0.0250 - val_mae: 0.1723\n",
            "\n",
            "Epoch 00289: loss did not improve from 0.01167\n",
            "Epoch 290/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0122 - mae: 0.1082 - val_loss: 0.0252 - val_mae: 0.1718\n",
            "\n",
            "Epoch 00290: loss did not improve from 0.01167\n",
            "Epoch 291/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0113 - mae: 0.1064 - val_loss: 0.0247 - val_mae: 0.1702\n",
            "\n",
            "Epoch 00291: loss did not improve from 0.01167\n",
            "Epoch 292/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0127 - mae: 0.1158 - val_loss: 0.0248 - val_mae: 0.1741\n",
            "\n",
            "Epoch 00292: loss did not improve from 0.01167\n",
            "Epoch 293/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0103 - mae: 0.1027 - val_loss: 0.0260 - val_mae: 0.1773\n",
            "\n",
            "Epoch 00293: loss did not improve from 0.01167\n",
            "Epoch 294/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0127 - mae: 0.1146 - val_loss: 0.0250 - val_mae: 0.1727\n",
            "\n",
            "Epoch 00294: loss did not improve from 0.01167\n",
            "Epoch 295/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0108 - mae: 0.1076 - val_loss: 0.0250 - val_mae: 0.1725\n",
            "\n",
            "Epoch 00295: loss did not improve from 0.01167\n",
            "Epoch 296/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0120 - mae: 0.1122 - val_loss: 0.0248 - val_mae: 0.1709\n",
            "\n",
            "Epoch 00296: loss did not improve from 0.01167\n",
            "Epoch 297/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0118 - mae: 0.1121 - val_loss: 0.0242 - val_mae: 0.1739\n",
            "\n",
            "Epoch 00297: loss did not improve from 0.01167\n",
            "Epoch 298/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0129 - mae: 0.1137 - val_loss: 0.0249 - val_mae: 0.1722\n",
            "\n",
            "Epoch 00298: loss did not improve from 0.01167\n",
            "Epoch 299/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0114 - mae: 0.1053 - val_loss: 0.0250 - val_mae: 0.1726\n",
            "\n",
            "Epoch 00299: loss did not improve from 0.01167\n",
            "Epoch 300/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0113 - mae: 0.1080 - val_loss: 0.0256 - val_mae: 0.1750\n",
            "\n",
            "Epoch 00300: loss did not improve from 0.01167\n",
            "Epoch 301/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0115 - mae: 0.1103 - val_loss: 0.0252 - val_mae: 0.1733\n",
            "\n",
            "Epoch 00301: loss did not improve from 0.01167\n",
            "Epoch 302/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0110 - mae: 0.1069 - val_loss: 0.0253 - val_mae: 0.1736\n",
            "\n",
            "Epoch 00302: loss did not improve from 0.01167\n",
            "Epoch 303/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0134 - mae: 0.1142 - val_loss: 0.0258 - val_mae: 0.1759\n",
            "\n",
            "Epoch 00303: loss did not improve from 0.01167\n",
            "Epoch 304/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0111 - mae: 0.1071 - val_loss: 0.0250 - val_mae: 0.1718\n",
            "\n",
            "Epoch 00304: loss did not improve from 0.01167\n",
            "Epoch 305/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0123 - mae: 0.1125 - val_loss: 0.0246 - val_mae: 0.1737\n",
            "\n",
            "Epoch 00305: loss did not improve from 0.01167\n",
            "Epoch 306/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0121 - mae: 0.1094 - val_loss: 0.0253 - val_mae: 0.1741\n",
            "\n",
            "Epoch 00306: loss did not improve from 0.01167\n",
            "Epoch 307/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0114 - mae: 0.1106 - val_loss: 0.0237 - val_mae: 0.1692\n",
            "\n",
            "Epoch 00307: loss did not improve from 0.01167\n",
            "Epoch 308/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0122 - mae: 0.1107 - val_loss: 0.0257 - val_mae: 0.1761\n",
            "\n",
            "Epoch 00308: loss did not improve from 0.01167\n",
            "Epoch 309/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0127 - mae: 0.1104 - val_loss: 0.0259 - val_mae: 0.1758\n",
            "\n",
            "Epoch 00309: loss did not improve from 0.01167\n",
            "Epoch 310/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0122 - mae: 0.1108 - val_loss: 0.0244 - val_mae: 0.1694\n",
            "\n",
            "Epoch 00310: loss did not improve from 0.01167\n",
            "Epoch 311/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0116 - mae: 0.1079 - val_loss: 0.0239 - val_mae: 0.1672\n",
            "\n",
            "Epoch 00311: loss did not improve from 0.01167\n",
            "Epoch 312/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0118 - mae: 0.1108 - val_loss: 0.0240 - val_mae: 0.1667\n",
            "\n",
            "Epoch 00312: loss did not improve from 0.01167\n",
            "Epoch 313/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0135 - mae: 0.1140 - val_loss: 0.0240 - val_mae: 0.1681\n",
            "\n",
            "Epoch 00313: loss did not improve from 0.01167\n",
            "Epoch 314/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0123 - mae: 0.1101 - val_loss: 0.0244 - val_mae: 0.1699\n",
            "\n",
            "Epoch 00314: loss did not improve from 0.01167\n",
            "Epoch 315/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0119 - mae: 0.1108 - val_loss: 0.0246 - val_mae: 0.1721\n",
            "\n",
            "Epoch 00315: loss did not improve from 0.01167\n",
            "Epoch 316/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0115 - mae: 0.1109 - val_loss: 0.0245 - val_mae: 0.1696\n",
            "\n",
            "Epoch 00316: loss did not improve from 0.01167\n",
            "Epoch 317/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0126 - mae: 0.1114 - val_loss: 0.0262 - val_mae: 0.1787\n",
            "\n",
            "Epoch 00317: loss did not improve from 0.01167\n",
            "Epoch 318/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0109 - mae: 0.1087 - val_loss: 0.0250 - val_mae: 0.1725\n",
            "\n",
            "Epoch 00318: loss did not improve from 0.01167\n",
            "Epoch 319/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0128 - mae: 0.1138 - val_loss: 0.0236 - val_mae: 0.1683\n",
            "\n",
            "Epoch 00319: loss did not improve from 0.01167\n",
            "Epoch 320/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0124 - mae: 0.1081 - val_loss: 0.0251 - val_mae: 0.1729\n",
            "\n",
            "Epoch 00320: loss did not improve from 0.01167\n",
            "Epoch 321/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0112 - mae: 0.1084 - val_loss: 0.0226 - val_mae: 0.1637\n",
            "\n",
            "Epoch 00321: loss did not improve from 0.01167\n",
            "Epoch 322/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0104 - mae: 0.1048 - val_loss: 0.0243 - val_mae: 0.1688\n",
            "\n",
            "Epoch 00322: loss did not improve from 0.01167\n",
            "Epoch 323/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0118 - mae: 0.1085 - val_loss: 0.0235 - val_mae: 0.1661\n",
            "\n",
            "Epoch 00323: loss did not improve from 0.01167\n",
            "Epoch 324/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0141 - mae: 0.1144 - val_loss: 0.0239 - val_mae: 0.1676\n",
            "\n",
            "Epoch 00324: loss did not improve from 0.01167\n",
            "Epoch 325/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0139 - mae: 0.1154 - val_loss: 0.0256 - val_mae: 0.1757\n",
            "\n",
            "Epoch 00325: loss did not improve from 0.01167\n",
            "Epoch 326/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0119 - mae: 0.1092 - val_loss: 0.0244 - val_mae: 0.1690\n",
            "\n",
            "Epoch 00326: loss did not improve from 0.01167\n",
            "Epoch 327/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0122 - mae: 0.1079 - val_loss: 0.0231 - val_mae: 0.1668\n",
            "\n",
            "Epoch 00327: loss did not improve from 0.01167\n",
            "Epoch 328/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0115 - mae: 0.1075 - val_loss: 0.0235 - val_mae: 0.1661\n",
            "\n",
            "Epoch 00328: loss did not improve from 0.01167\n",
            "Epoch 329/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0113 - mae: 0.1103 - val_loss: 0.0242 - val_mae: 0.1710\n",
            "\n",
            "Epoch 00329: loss did not improve from 0.01167\n",
            "Epoch 330/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0121 - mae: 0.1116 - val_loss: 0.0242 - val_mae: 0.1691\n",
            "\n",
            "Epoch 00330: loss improved from 0.01167 to 0.01148, saving model to poids.hdf5\n",
            "Epoch 331/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0122 - mae: 0.1112 - val_loss: 0.0227 - val_mae: 0.1658\n",
            "\n",
            "Epoch 00331: loss did not improve from 0.01148\n",
            "Epoch 332/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0110 - mae: 0.1094 - val_loss: 0.0246 - val_mae: 0.1721\n",
            "\n",
            "Epoch 00332: loss did not improve from 0.01148\n",
            "Epoch 333/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0115 - mae: 0.1110 - val_loss: 0.0244 - val_mae: 0.1703\n",
            "\n",
            "Epoch 00333: loss did not improve from 0.01148\n",
            "Epoch 334/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0119 - mae: 0.1100 - val_loss: 0.0244 - val_mae: 0.1696\n",
            "\n",
            "Epoch 00334: loss did not improve from 0.01148\n",
            "Epoch 335/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0118 - mae: 0.1113 - val_loss: 0.0249 - val_mae: 0.1720\n",
            "\n",
            "Epoch 00335: loss did not improve from 0.01148\n",
            "Epoch 336/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0110 - mae: 0.1077 - val_loss: 0.0241 - val_mae: 0.1682\n",
            "\n",
            "Epoch 00336: loss did not improve from 0.01148\n",
            "Epoch 337/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0111 - mae: 0.1097 - val_loss: 0.0223 - val_mae: 0.1654\n",
            "\n",
            "Epoch 00337: loss did not improve from 0.01148\n",
            "Epoch 338/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0130 - mae: 0.1155 - val_loss: 0.0245 - val_mae: 0.1695\n",
            "\n",
            "Epoch 00338: loss did not improve from 0.01148\n",
            "Epoch 339/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0116 - mae: 0.1103 - val_loss: 0.0244 - val_mae: 0.1700\n",
            "\n",
            "Epoch 00339: loss did not improve from 0.01148\n",
            "Epoch 340/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0110 - mae: 0.1092 - val_loss: 0.0244 - val_mae: 0.1705\n",
            "\n",
            "Epoch 00340: loss did not improve from 0.01148\n",
            "Epoch 341/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0122 - mae: 0.1107 - val_loss: 0.0232 - val_mae: 0.1676\n",
            "\n",
            "Epoch 00341: loss did not improve from 0.01148\n",
            "Epoch 342/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0117 - mae: 0.1119 - val_loss: 0.0247 - val_mae: 0.1700\n",
            "\n",
            "Epoch 00342: loss did not improve from 0.01148\n",
            "Epoch 343/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0140 - mae: 0.1160 - val_loss: 0.0238 - val_mae: 0.1682\n",
            "\n",
            "Epoch 00343: loss did not improve from 0.01148\n",
            "Epoch 344/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0103 - mae: 0.1061 - val_loss: 0.0244 - val_mae: 0.1703\n",
            "\n",
            "Epoch 00344: loss did not improve from 0.01148\n",
            "Epoch 345/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0139 - mae: 0.1156 - val_loss: 0.0246 - val_mae: 0.1703\n",
            "\n",
            "Epoch 00345: loss did not improve from 0.01148\n",
            "Epoch 346/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0101 - mae: 0.1067 - val_loss: 0.0240 - val_mae: 0.1681\n",
            "\n",
            "Epoch 00346: loss did not improve from 0.01148\n",
            "Epoch 347/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0115 - mae: 0.1060 - val_loss: 0.0244 - val_mae: 0.1699\n",
            "\n",
            "Epoch 00347: loss did not improve from 0.01148\n",
            "Epoch 348/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0114 - mae: 0.1098 - val_loss: 0.0243 - val_mae: 0.1687\n",
            "\n",
            "Epoch 00348: loss did not improve from 0.01148\n",
            "Epoch 349/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0120 - mae: 0.1100 - val_loss: 0.0243 - val_mae: 0.1691\n",
            "\n",
            "Epoch 00349: loss did not improve from 0.01148\n",
            "Epoch 350/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0108 - mae: 0.1073 - val_loss: 0.0231 - val_mae: 0.1648\n",
            "\n",
            "Epoch 00350: loss did not improve from 0.01148\n",
            "Epoch 351/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0134 - mae: 0.1133 - val_loss: 0.0235 - val_mae: 0.1686\n",
            "\n",
            "Epoch 00351: loss did not improve from 0.01148\n",
            "Epoch 352/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0133 - mae: 0.1143 - val_loss: 0.0246 - val_mae: 0.1710\n",
            "\n",
            "Epoch 00352: loss did not improve from 0.01148\n",
            "Epoch 353/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0111 - mae: 0.1090 - val_loss: 0.0241 - val_mae: 0.1675\n",
            "\n",
            "Epoch 00353: loss did not improve from 0.01148\n",
            "Epoch 354/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0129 - mae: 0.1131 - val_loss: 0.0225 - val_mae: 0.1605\n",
            "\n",
            "Epoch 00354: loss did not improve from 0.01148\n",
            "Epoch 355/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0113 - mae: 0.1067 - val_loss: 0.0238 - val_mae: 0.1674\n",
            "\n",
            "Epoch 00355: loss did not improve from 0.01148\n",
            "Epoch 356/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0112 - mae: 0.1128 - val_loss: 0.0242 - val_mae: 0.1683\n",
            "\n",
            "Epoch 00356: loss did not improve from 0.01148\n",
            "Epoch 357/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0119 - mae: 0.1095 - val_loss: 0.0243 - val_mae: 0.1701\n",
            "\n",
            "Epoch 00357: loss did not improve from 0.01148\n",
            "Epoch 358/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0128 - mae: 0.1105 - val_loss: 0.0247 - val_mae: 0.1713\n",
            "\n",
            "Epoch 00358: loss did not improve from 0.01148\n",
            "Epoch 359/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0113 - mae: 0.1104 - val_loss: 0.0225 - val_mae: 0.1652\n",
            "\n",
            "Epoch 00359: loss did not improve from 0.01148\n",
            "Epoch 360/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0121 - mae: 0.1080 - val_loss: 0.0237 - val_mae: 0.1669\n",
            "\n",
            "Epoch 00360: loss did not improve from 0.01148\n",
            "Epoch 361/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0111 - mae: 0.1068 - val_loss: 0.0246 - val_mae: 0.1706\n",
            "\n",
            "Epoch 00361: loss did not improve from 0.01148\n",
            "Epoch 362/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0121 - mae: 0.1108 - val_loss: 0.0237 - val_mae: 0.1660\n",
            "\n",
            "Epoch 00362: loss did not improve from 0.01148\n",
            "Epoch 363/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0129 - mae: 0.1094 - val_loss: 0.0227 - val_mae: 0.1626\n",
            "\n",
            "Epoch 00363: loss did not improve from 0.01148\n",
            "Epoch 364/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0108 - mae: 0.1087 - val_loss: 0.0236 - val_mae: 0.1660\n",
            "\n",
            "Epoch 00364: loss did not improve from 0.01148\n",
            "Epoch 365/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0119 - mae: 0.1099 - val_loss: 0.0226 - val_mae: 0.1637\n",
            "\n",
            "Epoch 00365: loss did not improve from 0.01148\n",
            "Epoch 366/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0103 - mae: 0.1070 - val_loss: 0.0229 - val_mae: 0.1648\n",
            "\n",
            "Epoch 00366: loss did not improve from 0.01148\n",
            "Epoch 367/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0137 - mae: 0.1126 - val_loss: 0.0250 - val_mae: 0.1721\n",
            "\n",
            "Epoch 00367: loss did not improve from 0.01148\n",
            "Epoch 368/500\n",
            "30/30 [==============================] - 1s 13ms/step - loss: 0.0126 - mae: 0.1087 - val_loss: 0.0239 - val_mae: 0.1673\n",
            "\n",
            "Epoch 00368: loss did not improve from 0.01148\n",
            "Epoch 369/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0114 - mae: 0.1093 - val_loss: 0.0246 - val_mae: 0.1709\n",
            "\n",
            "Epoch 00369: loss did not improve from 0.01148\n",
            "Epoch 370/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0113 - mae: 0.1073 - val_loss: 0.0244 - val_mae: 0.1700\n",
            "\n",
            "Epoch 00370: loss did not improve from 0.01148\n",
            "Epoch 371/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0105 - mae: 0.1049 - val_loss: 0.0236 - val_mae: 0.1652\n",
            "\n",
            "Epoch 00371: loss did not improve from 0.01148\n",
            "Epoch 372/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0135 - mae: 0.1150 - val_loss: 0.0244 - val_mae: 0.1699\n",
            "\n",
            "Epoch 00372: loss did not improve from 0.01148\n",
            "Epoch 373/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0109 - mae: 0.1060 - val_loss: 0.0230 - val_mae: 0.1639\n",
            "\n",
            "Epoch 00373: loss did not improve from 0.01148\n",
            "Epoch 374/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0113 - mae: 0.1078 - val_loss: 0.0241 - val_mae: 0.1682\n",
            "\n",
            "Epoch 00374: loss improved from 0.01148 to 0.01135, saving model to poids.hdf5\n",
            "Epoch 375/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0140 - mae: 0.1134 - val_loss: 0.0250 - val_mae: 0.1724\n",
            "\n",
            "Epoch 00375: loss did not improve from 0.01135\n",
            "Epoch 376/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0117 - mae: 0.1092 - val_loss: 0.0235 - val_mae: 0.1660\n",
            "\n",
            "Epoch 00376: loss did not improve from 0.01135\n",
            "Epoch 377/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0133 - mae: 0.1126 - val_loss: 0.0233 - val_mae: 0.1649\n",
            "\n",
            "Epoch 00377: loss did not improve from 0.01135\n",
            "Epoch 378/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0126 - mae: 0.1132 - val_loss: 0.0239 - val_mae: 0.1674\n",
            "\n",
            "Epoch 00378: loss did not improve from 0.01135\n",
            "Epoch 379/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0128 - mae: 0.1126 - val_loss: 0.0239 - val_mae: 0.1678\n",
            "\n",
            "Epoch 00379: loss did not improve from 0.01135\n",
            "Epoch 380/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0120 - mae: 0.1096 - val_loss: 0.0242 - val_mae: 0.1692\n",
            "\n",
            "Epoch 00380: loss did not improve from 0.01135\n",
            "Epoch 381/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0105 - mae: 0.1055 - val_loss: 0.0235 - val_mae: 0.1651\n",
            "\n",
            "Epoch 00381: loss did not improve from 0.01135\n",
            "Epoch 382/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0115 - mae: 0.1090 - val_loss: 0.0246 - val_mae: 0.1706\n",
            "\n",
            "Epoch 00382: loss did not improve from 0.01135\n",
            "Epoch 383/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0118 - mae: 0.1091 - val_loss: 0.0216 - val_mae: 0.1606\n",
            "\n",
            "Epoch 00383: loss did not improve from 0.01135\n",
            "Epoch 384/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0129 - mae: 0.1159 - val_loss: 0.0233 - val_mae: 0.1640\n",
            "\n",
            "Epoch 00384: loss did not improve from 0.01135\n",
            "Epoch 385/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0123 - mae: 0.1104 - val_loss: 0.0238 - val_mae: 0.1669\n",
            "\n",
            "Epoch 00385: loss did not improve from 0.01135\n",
            "Epoch 386/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0118 - mae: 0.1096 - val_loss: 0.0232 - val_mae: 0.1630\n",
            "\n",
            "Epoch 00386: loss did not improve from 0.01135\n",
            "Epoch 387/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0114 - mae: 0.1106 - val_loss: 0.0238 - val_mae: 0.1664\n",
            "\n",
            "Epoch 00387: loss did not improve from 0.01135\n",
            "Epoch 388/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0106 - mae: 0.1046 - val_loss: 0.0236 - val_mae: 0.1658\n",
            "\n",
            "Epoch 00388: loss did not improve from 0.01135\n",
            "Epoch 389/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0125 - mae: 0.1141 - val_loss: 0.0227 - val_mae: 0.1620\n",
            "\n",
            "Epoch 00389: loss did not improve from 0.01135\n",
            "Epoch 390/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0115 - mae: 0.1101 - val_loss: 0.0231 - val_mae: 0.1630\n",
            "\n",
            "Epoch 00390: loss improved from 0.01135 to 0.01133, saving model to poids.hdf5\n",
            "Epoch 391/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0112 - mae: 0.1114 - val_loss: 0.0234 - val_mae: 0.1649\n",
            "\n",
            "Epoch 00391: loss did not improve from 0.01133\n",
            "Epoch 392/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0123 - mae: 0.1109 - val_loss: 0.0240 - val_mae: 0.1676\n",
            "\n",
            "Epoch 00392: loss did not improve from 0.01133\n",
            "Epoch 393/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0109 - mae: 0.1062 - val_loss: 0.0232 - val_mae: 0.1635\n",
            "\n",
            "Epoch 00393: loss improved from 0.01133 to 0.01127, saving model to poids.hdf5\n",
            "Epoch 394/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0121 - mae: 0.1125 - val_loss: 0.0227 - val_mae: 0.1637\n",
            "\n",
            "Epoch 00394: loss did not improve from 0.01127\n",
            "Epoch 395/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0110 - mae: 0.1083 - val_loss: 0.0233 - val_mae: 0.1655\n",
            "\n",
            "Epoch 00395: loss did not improve from 0.01127\n",
            "Epoch 396/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0112 - mae: 0.1090 - val_loss: 0.0229 - val_mae: 0.1625\n",
            "\n",
            "Epoch 00396: loss did not improve from 0.01127\n",
            "Epoch 397/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0157 - mae: 0.1191 - val_loss: 0.0245 - val_mae: 0.1704\n",
            "\n",
            "Epoch 00397: loss did not improve from 0.01127\n",
            "Epoch 398/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0104 - mae: 0.1055 - val_loss: 0.0233 - val_mae: 0.1670\n",
            "\n",
            "Epoch 00398: loss did not improve from 0.01127\n",
            "Epoch 399/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0128 - mae: 0.1102 - val_loss: 0.0230 - val_mae: 0.1666\n",
            "\n",
            "Epoch 00399: loss did not improve from 0.01127\n",
            "Epoch 400/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0103 - mae: 0.1022 - val_loss: 0.0239 - val_mae: 0.1665\n",
            "\n",
            "Epoch 00400: loss did not improve from 0.01127\n",
            "Epoch 401/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0121 - mae: 0.1112 - val_loss: 0.0236 - val_mae: 0.1664\n",
            "\n",
            "Epoch 00401: loss did not improve from 0.01127\n",
            "Epoch 402/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0119 - mae: 0.1117 - val_loss: 0.0226 - val_mae: 0.1612\n",
            "\n",
            "Epoch 00402: loss did not improve from 0.01127\n",
            "Epoch 403/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0130 - mae: 0.1116 - val_loss: 0.0230 - val_mae: 0.1630\n",
            "\n",
            "Epoch 00403: loss did not improve from 0.01127\n",
            "Epoch 404/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0111 - mae: 0.1092 - val_loss: 0.0234 - val_mae: 0.1647\n",
            "\n",
            "Epoch 00404: loss did not improve from 0.01127\n",
            "Epoch 405/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0119 - mae: 0.1112 - val_loss: 0.0237 - val_mae: 0.1661\n",
            "\n",
            "Epoch 00405: loss did not improve from 0.01127\n",
            "Epoch 406/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0131 - mae: 0.1132 - val_loss: 0.0240 - val_mae: 0.1671\n",
            "\n",
            "Epoch 00406: loss did not improve from 0.01127\n",
            "Epoch 407/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0103 - mae: 0.1054 - val_loss: 0.0235 - val_mae: 0.1648\n",
            "\n",
            "Epoch 00407: loss did not improve from 0.01127\n",
            "Epoch 408/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0150 - mae: 0.1170 - val_loss: 0.0229 - val_mae: 0.1657\n",
            "\n",
            "Epoch 00408: loss did not improve from 0.01127\n",
            "Epoch 409/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0107 - mae: 0.1090 - val_loss: 0.0228 - val_mae: 0.1620\n",
            "\n",
            "Epoch 00409: loss did not improve from 0.01127\n",
            "Epoch 410/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0127 - mae: 0.1141 - val_loss: 0.0237 - val_mae: 0.1666\n",
            "\n",
            "Epoch 00410: loss did not improve from 0.01127\n",
            "Epoch 411/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0109 - mae: 0.1082 - val_loss: 0.0229 - val_mae: 0.1627\n",
            "\n",
            "Epoch 00411: loss did not improve from 0.01127\n",
            "Epoch 412/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0120 - mae: 0.1071 - val_loss: 0.0233 - val_mae: 0.1642\n",
            "\n",
            "Epoch 00412: loss did not improve from 0.01127\n",
            "Epoch 413/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0127 - mae: 0.1125 - val_loss: 0.0236 - val_mae: 0.1672\n",
            "\n",
            "Epoch 00413: loss did not improve from 0.01127\n",
            "Epoch 414/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0129 - mae: 0.1092 - val_loss: 0.0231 - val_mae: 0.1628\n",
            "\n",
            "Epoch 00414: loss did not improve from 0.01127\n",
            "Epoch 415/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0117 - mae: 0.1112 - val_loss: 0.0225 - val_mae: 0.1642\n",
            "\n",
            "Epoch 00415: loss did not improve from 0.01127\n",
            "Epoch 416/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0132 - mae: 0.1119 - val_loss: 0.0227 - val_mae: 0.1620\n",
            "\n",
            "Epoch 00416: loss did not improve from 0.01127\n",
            "Epoch 417/500\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.0115 - mae: 0.1084 - val_loss: 0.0230 - val_mae: 0.1630\n",
            "\n",
            "Epoch 00417: loss did not improve from 0.01127\n",
            "Epoch 418/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0104 - mae: 0.1078 - val_loss: 0.0231 - val_mae: 0.1631\n",
            "\n",
            "Epoch 00418: loss did not improve from 0.01127\n",
            "Epoch 419/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0106 - mae: 0.1047 - val_loss: 0.0233 - val_mae: 0.1642\n",
            "\n",
            "Epoch 00419: loss did not improve from 0.01127\n",
            "Epoch 420/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0111 - mae: 0.1082 - val_loss: 0.0232 - val_mae: 0.1636\n",
            "\n",
            "Epoch 00420: loss did not improve from 0.01127\n",
            "Epoch 421/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0106 - mae: 0.1081 - val_loss: 0.0233 - val_mae: 0.1636\n",
            "\n",
            "Epoch 00421: loss did not improve from 0.01127\n",
            "Epoch 422/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0131 - mae: 0.1114 - val_loss: 0.0234 - val_mae: 0.1644\n",
            "\n",
            "Epoch 00422: loss did not improve from 0.01127\n",
            "Epoch 423/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0117 - mae: 0.1094 - val_loss: 0.0231 - val_mae: 0.1641\n",
            "\n",
            "Epoch 00423: loss did not improve from 0.01127\n",
            "Epoch 424/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0097 - mae: 0.1011 - val_loss: 0.0229 - val_mae: 0.1658\n",
            "\n",
            "Epoch 00424: loss did not improve from 0.01127\n",
            "Epoch 425/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0105 - mae: 0.1028 - val_loss: 0.0216 - val_mae: 0.1592\n",
            "\n",
            "Epoch 00425: loss did not improve from 0.01127\n",
            "Epoch 426/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0113 - mae: 0.1064 - val_loss: 0.0235 - val_mae: 0.1666\n",
            "\n",
            "Epoch 00426: loss did not improve from 0.01127\n",
            "Epoch 427/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0125 - mae: 0.1109 - val_loss: 0.0238 - val_mae: 0.1667\n",
            "\n",
            "Epoch 00427: loss did not improve from 0.01127\n",
            "Epoch 428/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0105 - mae: 0.1073 - val_loss: 0.0224 - val_mae: 0.1627\n",
            "\n",
            "Epoch 00428: loss did not improve from 0.01127\n",
            "Epoch 429/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0103 - mae: 0.1037 - val_loss: 0.0222 - val_mae: 0.1595\n",
            "\n",
            "Epoch 00429: loss did not improve from 0.01127\n",
            "Epoch 430/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0120 - mae: 0.1123 - val_loss: 0.0229 - val_mae: 0.1622\n",
            "\n",
            "Epoch 00430: loss did not improve from 0.01127\n",
            "Epoch 431/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0125 - mae: 0.1117 - val_loss: 0.0225 - val_mae: 0.1638\n",
            "\n",
            "Epoch 00431: loss did not improve from 0.01127\n",
            "Epoch 432/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0122 - mae: 0.1097 - val_loss: 0.0233 - val_mae: 0.1650\n",
            "\n",
            "Epoch 00432: loss did not improve from 0.01127\n",
            "Epoch 433/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0121 - mae: 0.1135 - val_loss: 0.0225 - val_mae: 0.1631\n",
            "\n",
            "Epoch 00433: loss did not improve from 0.01127\n",
            "Epoch 434/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0128 - mae: 0.1133 - val_loss: 0.0224 - val_mae: 0.1608\n",
            "\n",
            "Epoch 00434: loss did not improve from 0.01127\n",
            "Epoch 435/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0121 - mae: 0.1108 - val_loss: 0.0240 - val_mae: 0.1679\n",
            "\n",
            "Epoch 00435: loss did not improve from 0.01127\n",
            "Epoch 436/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0129 - mae: 0.1117 - val_loss: 0.0236 - val_mae: 0.1657\n",
            "\n",
            "Epoch 00436: loss did not improve from 0.01127\n",
            "Epoch 437/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0100 - mae: 0.1019 - val_loss: 0.0233 - val_mae: 0.1672\n",
            "\n",
            "Epoch 00437: loss did not improve from 0.01127\n",
            "Epoch 438/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0133 - mae: 0.1128 - val_loss: 0.0210 - val_mae: 0.1589\n",
            "\n",
            "Epoch 00438: loss did not improve from 0.01127\n",
            "Epoch 439/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0113 - mae: 0.1095 - val_loss: 0.0218 - val_mae: 0.1606\n",
            "\n",
            "Epoch 00439: loss did not improve from 0.01127\n",
            "Epoch 440/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0129 - mae: 0.1116 - val_loss: 0.0218 - val_mae: 0.1600\n",
            "\n",
            "Epoch 00440: loss did not improve from 0.01127\n",
            "Epoch 441/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0108 - mae: 0.1066 - val_loss: 0.0232 - val_mae: 0.1645\n",
            "\n",
            "Epoch 00441: loss did not improve from 0.01127\n",
            "Epoch 442/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0129 - mae: 0.1134 - val_loss: 0.0234 - val_mae: 0.1651\n",
            "\n",
            "Epoch 00442: loss did not improve from 0.01127\n",
            "Epoch 443/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0128 - mae: 0.1108 - val_loss: 0.0234 - val_mae: 0.1653\n",
            "\n",
            "Epoch 00443: loss did not improve from 0.01127\n",
            "Epoch 444/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0105 - mae: 0.1054 - val_loss: 0.0229 - val_mae: 0.1620\n",
            "\n",
            "Epoch 00444: loss did not improve from 0.01127\n",
            "Epoch 445/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0133 - mae: 0.1117 - val_loss: 0.0234 - val_mae: 0.1645\n",
            "\n",
            "Epoch 00445: loss did not improve from 0.01127\n",
            "Epoch 446/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0110 - mae: 0.1085 - val_loss: 0.0212 - val_mae: 0.1576\n",
            "\n",
            "Epoch 00446: loss did not improve from 0.01127\n",
            "Epoch 447/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0100 - mae: 0.1078 - val_loss: 0.0234 - val_mae: 0.1648\n",
            "\n",
            "Epoch 00447: loss did not improve from 0.01127\n",
            "Epoch 448/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0117 - mae: 0.1070 - val_loss: 0.0232 - val_mae: 0.1645\n",
            "\n",
            "Epoch 00448: loss did not improve from 0.01127\n",
            "Epoch 449/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0094 - mae: 0.1033 - val_loss: 0.0227 - val_mae: 0.1611\n",
            "\n",
            "Epoch 00449: loss did not improve from 0.01127\n",
            "Epoch 450/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0108 - mae: 0.1066 - val_loss: 0.0225 - val_mae: 0.1604\n",
            "\n",
            "Epoch 00450: loss did not improve from 0.01127\n",
            "Epoch 451/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0128 - mae: 0.1126 - val_loss: 0.0232 - val_mae: 0.1638\n",
            "\n",
            "Epoch 00451: loss did not improve from 0.01127\n",
            "Epoch 452/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0100 - mae: 0.1055 - val_loss: 0.0238 - val_mae: 0.1669\n",
            "\n",
            "Epoch 00452: loss did not improve from 0.01127\n",
            "Epoch 453/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0128 - mae: 0.1114 - val_loss: 0.0231 - val_mae: 0.1637\n",
            "\n",
            "Epoch 00453: loss did not improve from 0.01127\n",
            "Epoch 454/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0117 - mae: 0.1089 - val_loss: 0.0215 - val_mae: 0.1573\n",
            "\n",
            "Epoch 00454: loss did not improve from 0.01127\n",
            "Epoch 455/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0109 - mae: 0.1090 - val_loss: 0.0232 - val_mae: 0.1643\n",
            "\n",
            "Epoch 00455: loss did not improve from 0.01127\n",
            "Epoch 456/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0121 - mae: 0.1066 - val_loss: 0.0229 - val_mae: 0.1633\n",
            "\n",
            "Epoch 00456: loss did not improve from 0.01127\n",
            "Epoch 457/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0121 - mae: 0.1090 - val_loss: 0.0236 - val_mae: 0.1658\n",
            "\n",
            "Epoch 00457: loss did not improve from 0.01127\n",
            "Epoch 458/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0149 - mae: 0.1142 - val_loss: 0.0234 - val_mae: 0.1646\n",
            "\n",
            "Epoch 00458: loss did not improve from 0.01127\n",
            "Epoch 459/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0120 - mae: 0.1086 - val_loss: 0.0227 - val_mae: 0.1615\n",
            "\n",
            "Epoch 00459: loss did not improve from 0.01127\n",
            "Epoch 460/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0111 - mae: 0.1098 - val_loss: 0.0220 - val_mae: 0.1615\n",
            "\n",
            "Epoch 00460: loss did not improve from 0.01127\n",
            "Epoch 461/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0126 - mae: 0.1133 - val_loss: 0.0236 - val_mae: 0.1661\n",
            "\n",
            "Epoch 00461: loss did not improve from 0.01127\n",
            "Epoch 462/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0124 - mae: 0.1082 - val_loss: 0.0229 - val_mae: 0.1628\n",
            "\n",
            "Epoch 00462: loss did not improve from 0.01127\n",
            "Epoch 463/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0117 - mae: 0.1059 - val_loss: 0.0231 - val_mae: 0.1630\n",
            "\n",
            "Epoch 00463: loss did not improve from 0.01127\n",
            "Epoch 464/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0099 - mae: 0.1023 - val_loss: 0.0216 - val_mae: 0.1599\n",
            "\n",
            "Epoch 00464: loss did not improve from 0.01127\n",
            "Epoch 465/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0112 - mae: 0.1082 - val_loss: 0.0228 - val_mae: 0.1627\n",
            "\n",
            "Epoch 00465: loss did not improve from 0.01127\n",
            "Epoch 466/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0106 - mae: 0.1034 - val_loss: 0.0230 - val_mae: 0.1622\n",
            "\n",
            "Epoch 00466: loss did not improve from 0.01127\n",
            "Epoch 467/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0114 - mae: 0.1074 - val_loss: 0.0225 - val_mae: 0.1603\n",
            "\n",
            "Epoch 00467: loss did not improve from 0.01127\n",
            "Epoch 468/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0120 - mae: 0.1073 - val_loss: 0.0230 - val_mae: 0.1633\n",
            "\n",
            "Epoch 00468: loss did not improve from 0.01127\n",
            "Epoch 469/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0141 - mae: 0.1125 - val_loss: 0.0226 - val_mae: 0.1607\n",
            "\n",
            "Epoch 00469: loss did not improve from 0.01127\n",
            "Epoch 470/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0110 - mae: 0.1081 - val_loss: 0.0230 - val_mae: 0.1635\n",
            "\n",
            "Epoch 00470: loss did not improve from 0.01127\n",
            "Epoch 471/500\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.0112 - mae: 0.1084 - val_loss: 0.0236 - val_mae: 0.1664\n",
            "\n",
            "Epoch 00471: loss did not improve from 0.01127\n",
            "Epoch 472/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0113 - mae: 0.1085 - val_loss: 0.0232 - val_mae: 0.1644\n",
            "\n",
            "Epoch 00472: loss did not improve from 0.01127\n",
            "Epoch 473/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0115 - mae: 0.1090 - val_loss: 0.0226 - val_mae: 0.1615\n",
            "\n",
            "Epoch 00473: loss did not improve from 0.01127\n",
            "Epoch 474/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0114 - mae: 0.1080 - val_loss: 0.0227 - val_mae: 0.1623\n",
            "\n",
            "Epoch 00474: loss did not improve from 0.01127\n",
            "Epoch 475/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0108 - mae: 0.1091 - val_loss: 0.0224 - val_mae: 0.1599\n",
            "\n",
            "Epoch 00475: loss did not improve from 0.01127\n",
            "Epoch 476/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0130 - mae: 0.1102 - val_loss: 0.0223 - val_mae: 0.1589\n",
            "\n",
            "Epoch 00476: loss did not improve from 0.01127\n",
            "Epoch 477/500\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.0134 - mae: 0.1143 - val_loss: 0.0225 - val_mae: 0.1604\n",
            "\n",
            "Epoch 00477: loss did not improve from 0.01127\n",
            "Epoch 478/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0104 - mae: 0.1056 - val_loss: 0.0222 - val_mae: 0.1599\n",
            "\n",
            "Epoch 00478: loss did not improve from 0.01127\n",
            "Epoch 479/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0108 - mae: 0.1073 - val_loss: 0.0229 - val_mae: 0.1618\n",
            "\n",
            "Epoch 00479: loss did not improve from 0.01127\n",
            "Epoch 480/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0117 - mae: 0.1114 - val_loss: 0.0225 - val_mae: 0.1613\n",
            "\n",
            "Epoch 00480: loss did not improve from 0.01127\n",
            "Epoch 481/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0110 - mae: 0.1085 - val_loss: 0.0234 - val_mae: 0.1651\n",
            "\n",
            "Epoch 00481: loss did not improve from 0.01127\n",
            "Epoch 482/500\n",
            "30/30 [==============================] - 1s 13ms/step - loss: 0.0112 - mae: 0.1088 - val_loss: 0.0226 - val_mae: 0.1615\n",
            "\n",
            "Epoch 00482: loss did not improve from 0.01127\n",
            "Epoch 483/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0101 - mae: 0.1051 - val_loss: 0.0216 - val_mae: 0.1602\n",
            "\n",
            "Epoch 00483: loss did not improve from 0.01127\n",
            "Epoch 484/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0114 - mae: 0.1105 - val_loss: 0.0227 - val_mae: 0.1614\n",
            "\n",
            "Epoch 00484: loss did not improve from 0.01127\n",
            "Epoch 485/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0114 - mae: 0.1079 - val_loss: 0.0216 - val_mae: 0.1593\n",
            "\n",
            "Epoch 00485: loss did not improve from 0.01127\n",
            "Epoch 486/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0131 - mae: 0.1085 - val_loss: 0.0227 - val_mae: 0.1610\n",
            "\n",
            "Epoch 00486: loss did not improve from 0.01127\n",
            "Epoch 487/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0129 - mae: 0.1109 - val_loss: 0.0225 - val_mae: 0.1602\n",
            "\n",
            "Epoch 00487: loss did not improve from 0.01127\n",
            "Epoch 488/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0112 - mae: 0.1065 - val_loss: 0.0227 - val_mae: 0.1612\n",
            "\n",
            "Epoch 00488: loss did not improve from 0.01127\n",
            "Epoch 489/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0107 - mae: 0.1065 - val_loss: 0.0227 - val_mae: 0.1614\n",
            "\n",
            "Epoch 00489: loss did not improve from 0.01127\n",
            "Epoch 490/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0132 - mae: 0.1147 - val_loss: 0.0229 - val_mae: 0.1628\n",
            "\n",
            "Epoch 00490: loss did not improve from 0.01127\n",
            "Epoch 491/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0116 - mae: 0.1062 - val_loss: 0.0223 - val_mae: 0.1605\n",
            "\n",
            "Epoch 00491: loss did not improve from 0.01127\n",
            "Epoch 492/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0115 - mae: 0.1051 - val_loss: 0.0218 - val_mae: 0.1596\n",
            "\n",
            "Epoch 00492: loss did not improve from 0.01127\n",
            "Epoch 493/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0116 - mae: 0.1106 - val_loss: 0.0226 - val_mae: 0.1613\n",
            "\n",
            "Epoch 00493: loss did not improve from 0.01127\n",
            "Epoch 494/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0121 - mae: 0.1117 - val_loss: 0.0227 - val_mae: 0.1623\n",
            "\n",
            "Epoch 00494: loss did not improve from 0.01127\n",
            "Epoch 495/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0136 - mae: 0.1129 - val_loss: 0.0231 - val_mae: 0.1637\n",
            "\n",
            "Epoch 00495: loss did not improve from 0.01127\n",
            "Epoch 496/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0096 - mae: 0.1004 - val_loss: 0.0221 - val_mae: 0.1618\n",
            "\n",
            "Epoch 00496: loss did not improve from 0.01127\n",
            "Epoch 497/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0112 - mae: 0.1069 - val_loss: 0.0229 - val_mae: 0.1630\n",
            "\n",
            "Epoch 00497: loss did not improve from 0.01127\n",
            "Epoch 498/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0116 - mae: 0.1106 - val_loss: 0.0224 - val_mae: 0.1602\n",
            "\n",
            "Epoch 00498: loss did not improve from 0.01127\n",
            "Epoch 499/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0108 - mae: 0.1065 - val_loss: 0.0223 - val_mae: 0.1604\n",
            "\n",
            "Epoch 00499: loss did not improve from 0.01127\n",
            "Epoch 500/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0096 - mae: 0.1057 - val_loss: 0.0225 - val_mae: 0.1620\n",
            "\n",
            "Epoch 00500: loss did not improve from 0.01127\n",
            "[2.8345178350000424, 0.4259317400001237, 0.4174838659998841, 0.4409250850001172, 0.4193839500001104, 0.44122066399995674, 0.4338482389998717, 0.4126019750001433, 0.4436740189999, 0.4233576520000497, 0.42988046100003885, 0.4154068070001813, 0.4108059850000245, 0.44108638800003064, 0.44582263699999203, 0.44856506499991156, 0.42397384300011254, 0.445162732999961, 0.4228423690001364, 0.4178390719998788, 0.4335559439998633, 0.41641478100018503, 0.43514636300005805, 0.4541359940001257, 0.4478333310000835, 0.43075911100004305, 0.42727527500005635, 0.42747057299993685, 0.42472732100009125, 0.4118026070000269, 0.42863312099984796, 0.42873364699994454, 0.4275837740001407, 0.47045009499993284, 0.41847560400015027, 0.458510040000192, 0.4368991899998491, 0.462936376000016, 0.42354345500007184, 0.39366630799986524, 0.43616672400003154, 0.45774792099996375, 0.4469942710002215, 0.42930010500003846, 0.40492865499982145, 0.41949726000007104, 0.4369439429999602, 0.4448358219999591, 0.45836043399981463, 0.42497637699989355, 0.4403868440001588, 0.4294331459998375, 0.4101389960001143, 0.4478151830001025, 0.4235051499999827, 0.41793109600007483, 0.44105029799993645, 0.42696661099989797, 0.41765576800003146, 0.43862542600004417, 0.40436249500021404, 0.44594653600006495, 0.4229837129998941, 0.4269851720000588, 0.4229446049998842, 0.4209754849998717, 0.40508495099993524, 0.43021599899998364, 0.43163614800005234, 0.43665375799992034, 0.4588928109999415, 0.4983078539999042, 0.4671474780000153, 0.44812164199993276, 0.421312295000007, 0.42493866199993136, 0.4327822489999562, 0.42315621099987766, 0.43584449900004074, 0.4190486709999277, 0.4457173660000535, 0.4281930419999753, 0.41261471400002847, 0.45444939499998327, 0.4114722150000034, 0.4456442440000501, 0.4184868889999507, 0.4164533460000257, 0.4285339069999736, 0.434418069000003, 0.47724760799997057, 0.43936353899994174, 0.474027494999973, 0.40464186300005167, 0.4462717279998287, 0.41483322099998077, 0.43036787999994885, 0.44069459299998925, 0.4548724780001976, 0.416226603000041, 0.42638367899985496, 0.48006634800003667, 0.41587001900006726, 0.44929501900014657, 0.4459590430001299, 0.4652830750001158, 0.4551279950001117, 0.42794051300006686, 0.4624139979998745, 0.4155573970001569, 0.44468033399994056, 0.4467137139999977, 0.46120350099999996, 0.4337904340000023, 0.4514052060001177, 0.4495996959999502, 0.4521451790001265, 0.4524854859998868, 0.45789817300010327, 0.45086947699996927, 0.4772048679999443, 0.4488239570000587, 0.47533745499981706, 0.4512179470000319, 0.43814100400004463, 0.47076523700002326, 0.42056351599990194, 0.43736725000007937, 0.4409231510001064, 0.4637755629998992, 0.4483109660000082, 0.4591002469999239, 0.4349011110000447, 0.46063310299996374, 0.4423449610001171, 0.42050471900006414, 0.44117559899996195, 0.45243063899988556, 0.450181344999919, 0.4560788840001351, 0.474196830999972, 0.43650322200005576, 0.46189826499994524, 0.435746368999844, 0.4273124229998757, 0.43356285599998046, 0.46624843099993996, 0.4372256420001577, 0.4660440469999685, 0.45987891999993735, 0.47310941299997467, 0.4662927019999188, 0.43661043799988875, 0.41168991499989716, 0.43382301900010134, 0.4549209530000553, 0.4612007589998939, 0.436235809999971, 0.4225282410000091, 0.43067208999991635, 0.42304721499999687, 0.4541805520000253, 0.4535879320001186, 0.4225970589998269, 0.4299275219998435, 0.44256302799999503, 0.44746977699992385, 0.4447198870000193, 0.4538015059999907, 0.4309259760000259, 0.4516645910000534, 0.4410380570000143, 0.43104043199991793, 0.46378664999997454, 0.4366143560000637, 0.4533236979998492, 0.45044720900000357, 0.4231929059999402, 0.43556241600003887, 0.4584384049999244, 0.43344192700010353, 0.4607178029998522, 0.4945746319999671, 0.44742407800004, 0.44065006199980417, 0.44613460100003977, 0.44923064099998555, 0.43910151200020664, 0.4466899059998468, 0.4496089579999989, 0.4409155409998675, 0.46668215899990173, 0.45818046899989895, 0.43716933199993946, 0.4624330149999878, 0.4377293390000432, 0.45824197499996444, 0.4730679740000596, 0.4573838149999574, 0.4703793680000672, 0.4494835240000157, 0.4751056170000538, 0.45871539299992037, 0.4724955690001025, 0.43880082200007564, 0.4438114019999375, 0.4591552340000362, 0.4607926710000356, 0.4625015980000171, 0.43441108000001805, 0.45940803500002403, 0.483053105999943, 0.48604790400008824, 0.4641916509999646, 0.4804560979998769, 0.5120284950000951, 0.4688485710000805, 0.45852681099995607, 0.45781342300006145, 0.447914406000109, 0.47004171799994765, 0.4707339869999032, 0.44594271999994817, 0.42853982900010124, 0.43013158799999474, 0.4555156320000151, 0.4804760729998634, 0.4583315739998852, 0.46720302099993205, 0.4667004910002106, 0.46386994499994216, 0.4465381590000561, 0.434263706000138, 0.45214523600020584, 0.42750374100000954, 0.49731244200006586, 0.4742867780000779, 0.4669740260001163, 0.4551904800000557, 0.451880231999894, 0.42141750800010414, 0.44322506200001044, 0.46982277899996916, 0.47228835399982927, 0.46163536499989277, 0.4595639490000849, 0.4526626770000348, 0.47360424899989084, 0.45434568099994976, 0.47275427199997466, 0.4837885900001311, 0.4435367189998942, 0.4463235700000041, 0.4348715149999407, 0.4296438760000001, 0.4570294169998306, 0.4598060280000027, 0.4785298070000863, 0.4580774140001722, 0.44139471299990873, 0.4625032869998904, 0.45551451800020004, 0.4805024960000992, 0.44641495700011546, 0.5120367419999639, 0.46338872899991657, 0.49020192400007545, 0.45380667700010235, 0.45968050599981325, 0.426311491999968, 0.4702338449999388, 0.4687769130000561, 0.44608969400019305, 0.4588646450001761, 0.48126127199998336, 0.4728048269998908, 0.4621333489999415, 0.4894831650001379, 0.43135213400000794, 0.4623556720000579, 0.4441209899998739, 0.4682779509998909, 0.4424691659999098, 0.5005671439998878, 0.46863376200008133, 0.4632880369999839, 0.4483097130000715, 0.4618364699999802, 0.46731562900004064, 0.466883978000169, 0.46228322000001754, 0.4554979019999337, 0.44991436600003, 0.4825344490000134, 0.4464071730001251, 0.4672353399998883, 0.4735117889999856, 0.47172615399995266, 0.4810305779999453, 0.4553462449998733, 0.4509292990001086, 0.4569035070001064, 0.46510546400008934, 0.4559112740000728, 0.4562926309999966, 0.4664644550000503, 0.46769083399999545, 0.4679220619998432, 0.4612747369999397, 0.4638634660000207, 0.4731147150000652, 0.4860379850001664, 0.4513452999999572, 0.45912985699987985, 0.4655565089999527, 0.48124347399993894, 0.45448484300004566, 0.46231101499984106, 0.49503860899994834, 0.4652305100000831, 0.46200291700006346, 0.4567079470000408, 0.4516548620001686, 0.4597554039999068, 0.4721185669998249, 0.469307869999966, 0.5024568930000441, 0.45175591199995324, 0.44597659700002623, 0.4841810699999769, 0.4595621739999842, 0.4611580739999681, 0.457531777999975, 0.4741024469999502, 0.444857834000004, 0.4735372229999939, 0.46027948500000093, 0.46100747000014053, 0.47286126099993453, 0.4410617739999907, 0.4955564619999677, 0.4651151289999689, 0.4879942260001826, 0.457481800000096, 0.47256124000000455, 0.4550379380000322, 0.48204084899998634, 0.46895612799994524, 0.4930175720000989, 0.47516869100013537, 0.46368340399999397, 0.47661397499996383, 0.48256143800017526, 0.4819902180001918, 0.47140905800006294, 0.48369917200011514, 0.47414824700013014, 0.45751474899998357, 0.47691894800004775, 0.47931769499996335, 0.45212149199983287, 0.47266957400006504, 0.45475528700012546, 0.46224352400008684, 0.4674539509999249, 0.4492768430000069, 0.4447754899999836, 0.5053081290000137, 0.483754172999852, 0.4608273880000979, 0.4557888910001111, 0.4658128730000044, 0.4629788549998466, 0.48928101399997104, 0.46949823499994636, 0.4858693169999242, 0.46575550700003987, 0.4832483980001143, 0.48096316499982095, 0.48171242900002653, 0.5248071260000415, 0.48732794100010324, 0.4588860700000623, 0.4646122750000359, 0.4483452050001233, 0.4868419939998603, 0.4572555260001536, 0.4893566279999959, 0.48040280200007146, 0.4666731500001333, 0.5116071399997963, 0.4647840789998554, 0.4626165699999092, 0.48569530100007796, 0.44821809600011875, 0.4745899909999025, 0.4614519750000454, 0.47869709099995816, 0.44692508299999645, 0.48102566299985483, 0.4544092679998357, 0.47831690499992874, 0.47849873299992396, 0.4668247839999822, 0.4622373699999116, 0.477065335999896, 0.47559612099985316, 0.46586654799989446, 0.4913924600000428, 0.4556859409999561, 0.4864967369999249, 0.4714781689999654, 0.4761662539999634, 0.48128020700005436, 0.48141471799999636, 0.45367450000003373, 0.5334292169998207, 0.4724953180000284, 0.4788937870000609, 0.48158689399997456, 0.45800403800012646, 0.4705787930001861, 0.46804645399993206, 0.4611538870001368, 0.4856707379999534, 0.48912921999999526, 0.46822214200005874, 0.45999536899989835, 0.48925880599995253, 0.47698191599988604, 0.4399888099999316, 0.5033170409999457, 0.49140669600001274, 0.4910160029999133, 0.47099459199989724, 0.46210961499991754, 0.4899016599999868, 0.48102314399989154, 0.498302515999967, 0.47125532500012923, 0.4914120600001297, 0.49145400299994435, 0.4692117919998964, 0.466184038999927, 0.4821598730000005, 0.4813971850001053, 0.48327812200000153, 0.47311762699996507, 0.49813101799986725, 0.4595941230002154, 0.48521532899985687, 0.4688879539999107, 0.47366932299996733, 0.470305124999868, 0.46277931200006606, 0.47234576500000003, 0.5013804249999794, 0.46861861399997906, 0.5240299229999437, 0.46458108199999515, 0.46805908700002874, 0.47551424300013423, 0.48625819699987005, 0.4734131079999315, 0.4878609739998865, 0.49028774199996406, 0.4829238299998906, 0.481141487000059, 0.5127282749999722, 0.4822953589998633, 0.5113524129999405, 0.47051621800005705, 0.48058640299996114, 0.49792412600004354, 0.49217827799998304, 0.4865095510001538, 0.5481693310000537, 0.4932294720001664, 0.471458302000201, 0.48458915900005195, 0.46845418899988545, 0.5025589439999294, 0.46184927400008746, 0.4860526730001311, 0.4529521179999847, 0.49564911399988887, 0.47530612599985034, 0.4904195520000485, 0.47842456900002617, 0.4804957110000032, 0.46374476799996955, 0.48378237599990825, 0.46795932700001686, 0.483573184000079, 0.48652554699992834, 0.46774239299998044, 0.45537865000005695, 0.46661311200000455, 0.4747782910001206, 0.4617714570001681]\n",
            "231.41891736299954\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COP9u4yitvmw",
        "outputId": "592d60c4-f7ce-4932-f0e7-5ff60b5cc1de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, \"Evolution de l'erreur en fonction de la période\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl8AAAF1CAYAAADWYI/QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd5hV1bn/P+uU6QNDFRGRIqh0EAUsVxRbgqIxIrGgJibqz6um6VWjUUw0ajSaeiWJsRuD5dpNRKMGSyygoIJGEFA6DDDAMH3O+v2x9jp7nX3qDDMw5f08zzy7rb32Pns2nO9833e9S2mtEQRBEARBEHYPoT19A4IgCIIgCJ0JEV+CIAiCIAi7ERFfgiAIgiAIuxERX4IgCIIgCLsREV+CIAiCIAi7ERFfgiAIgiAIuxERX4LQDJRSWim1fzPPPVIp9Z+Wvqc011qplDq2GedNVkqtbo17am8opQ5XSi1VSlUqpU7djdedrZT66W64TrN/10qp+5VSN7X0PQWucbhS6n2lVPcs7RYrpSY38xrN/vcsCM1BxJfQofHER7X3xWl/fr+b7yHhP3at9Rta6wN25z3sKt5zHLCn72MP8TPg91rrEq31061xAaXU+UqpN919WuuLtdY/b43rtReUUvsCvwCmaq23ZGqrtR6utX59t9yYIOwikT19A4KwGzhZa/3Knr6JzohSKqK1bsi2bxf6V4DSWsdaor807AcsbsX+hTRorVcBR2Vq05LvkyDsLsT5EjolSql8pVSFUmqEs6+X55L19ra/p5RappTaopR6VinVN01fryulvutsx10MpdQ8b/ciz3WbEQzzKKUO8vqo8EIn05xj9yul/qCUekEptUMp9a5SanCGzzVTKfWlUmqzUurawLGQUupqpdQX3vHHsoVy0lwjXyl1h1LqK6XUBi88Vugdm6yUWq2UukoptR64Tyk1Syn1hFLqYaXUduB8pVRXpdRflFLrlFJrlFI3KaXCXh+zlFIPO9cb4LmHEed536yUeguoAgaluMe+SqknlVKblFIrlFKXO8dmeZ/9Qe+ZLlZKjU/zWb/w+n/O+/3le30/670Xy5RS38u1b6XUvkqp//Pua7NS6vdKqYOA2cAk7xoVXtuEkF6m99F7PhcrEx6t8N4ZleYzFXp9b1VKLQEOyfXZZUIp1U0p9bx33lZvvV+G9iuVUtcopZZ47e9TShU4x09SSi30Ps/bSqlRgXOvUkp9BOxUSkWUE2L3fk+/Vkqt9X5+rZTKd86/0nv31iqlvhO4r7TvtyC0FCK+hE6J1roW+D/gTGf3GcC/tNYblVLHALd4+/YGvgT+1ozr/Je3OtoLW81xjyulosBzwFygN3AZ8IhSyg1Lfgu4EegGLANuTnUtpdQw4G5gJtAX6AG4X36XAadinIS+wFbgDzl+jgFa65Xe5q3AUGAMsD+wD3C907wP0B3jGF3o7TsFeAIoAx4B7gcavPPHAscD3yV3Znp9l2J+N3GUUiHMM13k3dsU4AdKqROcZtMwv88y4FkgZShaaz0Y+ArjnpZ4783fgNWYZ3g68AvvfcnYtycun/fud4B3b3/TWn8KXAz827tGWfA+cnwfT8IIqVFeuxNIzQ3AYO/nBOA85zq5PLt0hID7ML/3/kA1aZ6rw9nePQzGvFPXefcxFrgXuAjzHv8ReNYVUJh/u1OBshTO17XARMw7Oho41On7ROAK4DhgCBDMicz2fgvCrqO1lh/56bA/wEqgEqhwfr7nHTsW+MJp+xZwrrf+F+CXzrESoB4Y4G1rYH9v/XXgu07b84E3ne14W297MrDaWz8SWA+EnOOPArO89fuBe5xjXwc+S/NZr8d8mdvtYqAOONbb/hSY4hzf2/tMkRR9xe8xsF8BO4HBzr5JwArnvDqgwDk+C5jnbO8F1AKFzr4zgdec9g87xwZ4zzDiPO+fZfidTwC+Cuy7BrjP6f8V59gwoDrLO2Sf4b5AI1DqHL8FuD9b395z2pTmeSe8M87v/qYmvI9HOMcfA65O83mWAyc62xfiv48Zn12KvuL3mOLYGGBrlud6ceDd/sJbvxv4eaD9f4CjnHO/k+H39AXwdefYCcBKb/1e4Fbn2FDv+e1PlvdbfuSnpX4k50voDJyqU+d8vQYUKaUmABswXxZPecf6Ah/YhlrrSqXUZsxfwStb8N76Aqt0Ys7Sl951LOud9SrMF2/avuyG1nqnd8+W/YCnlFLutRoxYmhNjvfbCygCFjhRLQWEnTabtNY1gfNWOev7AVFgndNHKNAmG5na7gf0teE7jzDwhrMdfKYFKrfcob7AFq31Dmffl4AbtkzZN0a4fZnDNdJdN9v72Kz3hETnMJdnlxKlVBFwF3AixqUFKFVKhbXWjWlOC96HDaXuB5ynlLrMOZ7nHA+eG6QviZ/L7bsvsCBwzJLL+y0Iu4yIL6HTorVuVEo9hnFdNgDPO1+qazFfAAAopYox4Y9UImUn5j9sS58m3MZaYF+lVMgRYP2Bz5vQh2UdcJDd8L4MezjHV2Hcgrea0belHBNOGq61TifYdJZ9qzDOV880QiSX55nqGm7/K7TWQzK0aS5rge5KqVLnXelPbuJ1FdA/jcjL9HnsdXN9H7OxDiME7SCC/oF7bO6z+zFwADBBa71eKTUG+BAjXtKxr7PeH/M57X3crLVOGWL3yPTM7PNyP6Pt235+97qWXN5vQdhlJOdL6Oz8FZiByT35q7P/UeDbSqkxXp7JL4B3tZ/35LIQOE0pVaRMSYkLAsc3kCIp3ONdjEvxP0qpqDJ1ik6mGfllmJyqk5RSRyil8jAlEtx/47OBm5VS+0F8gMEpTbmAJxD/DNyl/IEJ++SYE2T7WIfJcfuVUqqLMgMBBiul7Ki2hcB/KaX6K6W6YsJeTeE9YIeXkF2olAorpUYopQ7Jemb2e18FvA3copQq8JLALwAeznxm/L7WAbcqpYq98w/3jm0A+nm/t1Q05X3MxmPANV6CfD9MLqB7j819dqUY4VKhzECOG3I457+VUv289tcCNifyz8DFSqkJylCslJqqlCrN8TM+ClznveM9MSF5+zt6DDPoY5j3B0r8Plvi/RaEXBDxJXQG7Eg1+2NDi2it38U4LX2Bvzv7XwF+CjyJ+cIcjEl8T8VdmDynDcADmIRyl1nAA96orTPcA1rrOozY+hrmr+7/xeSdfdbUD6m1Xgz8N0ZErsMk1LvFM3+DSQCfq5TaAbyDyfFpKldhEv/fUWb04isYx6MpnIsJIy3x7vMJTA4aWuuXMV/CH2HCQ883pWMvxHUSJoy8AvNc7wG6NvEe03EmJg9tLSZMfUOasHaq+zoZk1v0FeZ3M8M7/CrGpVmvlCpPcW5T3sds3IgJta3AiOCHAvfY3Gf3a6DQO+cd4B85nPNX7x6WY/K0bvLuYz7wPUzC/lbM+3Z+Dv1ZbgLmY96hjzEhW9v33717fdXr99XAuS3xfgtCRpTW2dxuQRAEQWhZlFIrMQNVpAaf0OkQ50sQBEEQBGE3IuJLEARBEARhNyJhR0EQBEEQhN2IOF+CIAiCIAi7ERFfgiAIgiAIu5F2VWS1Z8+eesCAAXv6NgRBEARBELKyYMGCcq11r+D+diW+BgwYwPz58/f0bQiCIAiCIGRFKfVlqv0SdhQEQRAEQdiNiPgSBEEQBEHYjYj4EgRBEARB2I20q5wvQRAEoX1SX1/P6tWrqamp2dO3IggtTkFBAf369SMajebUXsSXIAiC0OqsXr2a0tJSBgwYgFJqT9+OILQYWms2b97M6tWrGThwYE7nSNhREARBaHVqamro0aOHCC+hw6GUokePHk1ydUV8CYIgCLsFEV5CR6Wp77aIL0EQBKFTEA6HGTNmTPzn1ltv3dO31CqUlJTs9muuXLmSv/71r80697DDDmvhu9l1nn76aZYsWdJq/UvOlyAIgtApKCwsZOHChRnbNDY2Eg6H0243lYaGBiKR1vuqbe3+c8WKr7POOivpWLZ7fPvtt1vz1prF008/zUknncSwYcNapX9xvgRBEIROzYABA7jqqqsYN24cjz/+eNL23LlzmTRpEuPGjWP69OlUVlbGzysvLwdg/vz5TJ48GYBZs2Yxc+ZMDj/8cGbOnMnixYs59NBDGTNmDKNGjWLp0qUJ129sbOT8889nxIgRjBw5krvuuguAyZMnx2d1KS8vx06vd//99zNt2jSOOeYYpkyZkvGz3X777RxyyCGMGjWKG264AYCdO3cydepURo8ezYgRI5gzZ07SeV988QUnnngiBx98MEceeSSfffYZAOeffz6XX345hx12GIMGDeKJJ54A4Oqrr+aNN95gzJgx3HXXXUn3WFlZyZQpUxg3bhwjR47kmWeeiV/LOnWvv/46kydP5vTTT+fAAw/k7LPPRmsNwIIFCzjqqKM4+OCDOeGEE1i3bl38Gf3whz9k/PjxHHTQQbz//vucdtppDBkyhOuuuy5+jYcffjj+O7joootobGyMX/vaa69l9OjRTJw4kQ0bNvD222/z7LPPcuWVVzJmzBi++OILfvvb3zJs2DBGjRrFt771rYzPPBf2vFwWBEEQOhU3PreYJWu3t2ifw/p24YaTh2dsU11dzZgxY+Lb11xzDTNmzACgR48efPDBB4AREna7vLyc0047jVdeeYXi4mJuu+027rzzTq6//vqM11qyZAlvvvkmhYWFXHbZZXz/+9/n7LPPpq6uLv7Fb1m4cCFr1qzhk08+AaCioiLr5/3ggw/46KOP6N69e9o2c+fOZenSpbz33ntorZk2bRrz5s1j06ZN9O3blxdeeAGAbdu2JZ174YUXMnv2bIYMGcK7777LJZdcwquvvgrAunXrePPNN/nss8+YNm0ap59+Orfeeit33HEHzz//PGAEonuPDQ0NPPXUU3Tp0oXy8nImTpzItGnTknKlPvzwQxYvXkzfvn05/PDDeeutt5gwYQKXXXYZzzzzDL169WLOnDlce+213HvvvQDk5eUxf/58fvOb33DKKaewYMECunfvzuDBg/nhD3/Ixo0bmTNnDm+99RbRaJRLLrmERx55hHPPPZedO3cyceJEbr75Zv7nf/6HP//5z1x33XVMmzaNk046idNPPx2AW2+9lRUrVpCfn5/T7ycbIr6ElqGxAbauhJ777+k7EQRBSEmmsKMVYcHtd955hyVLlnD44YcDUFdXx6RJk7Jea9q0aRQWFgIwadIkbr75ZlavXh13ZVwGDRrE8uXLueyyy5g6dSrHH3981v6PO+64jMILjPiaO3cuY8eOBaCyspKlS5dy5JFH8uMf/5irrrqKk046iSOPPDLhvMrKSt5++22mT58e31dbWxtfP/XUUwmFQgwbNowNGzbkdI9aa37yk58wb948QqEQa9asYcOGDfTp0yfhnEMPPZR+/foBMGbMGFauXElZWRmffPIJxx13HGCcwr333jt+zrRp0wAYOXIkw4cPjx8bNGgQq1at4s0332TBggUccsghgBHhvXv3BoxwO+mkkwA4+OCDefnll1N+llGjRnH22Wdz6qmncuqpp6b9zLmSk/hSSp0I/AYIA/dorW8NHM8HHgQOBjYDM7TWK53j/YElwCyt9R259Cm0M167Cd68Cy5fCN1zq3MiCELnJJtDtScoLi5Oua215rjjjuPRRx9NOicSiRCLxQCSygy4/Z111llMmDCBF154ga9//ev88Y9/5Jhjjokf79atG4sWLeKll15i9uzZPPbYY9x77705958OrTXXXHMNF110UdKxDz74gBdffJHrrruOKVOmJDh5sViMsrKytEI1Pz8/4RrpcO/xkUceYdOmTSxYsIBoNMqAAQNSlmZw+w6HwzQ0NKC1Zvjw4fz73//OeD+hUCjh/FAoFD//vPPO45Zbbkk6NxqNxt03e71UvPDCC8ybN4/nnnuOm2++mY8//niXcu2y5nwppcLAH4CvAcOAM5VSwQy0C4CtWuv9gbuA2wLH7wT+3sQ+hfbEqvfNctvqPXsfgiAILcjEiRN56623WLZsGWDypT7//HPA5HwtWLAAgCeffDJtH8uXL2fQoEFcfvnlnHLKKXz00UcJx8vLy4nFYnzzm9/kpptuioc/3f5tblVTOOGEE7j33nvjOWpr1qxh48aNrF27lqKiIs455xyuvPLK+PUsXbp0YeDAgTz++OOAEViLFi3KeK3S0lJ27NiR9vi2bdvo3bs30WiU1157jS+//DLnz3HAAQewadOmuPiqr69n8eLFOZ8/ZcoUnnjiCTZu3AjAli1bsl7f/TyxWIxVq1Zx9NFHc9ttt7Ft27b4M20uuSTcHwos01ov11rXAX8DTgm0OQV4wFt/ApiiPCmplDoVWAG4TyqXPoX2RNTY69RX7dn7EARBSIPN+bI/V199ddZzevXqxf3338+ZZ57JqFGjmDRpUjz5/IYbbuD73/8+48ePzzgi8rHHHmPEiBGMGTOGTz75hHPPPTfh+Jo1a5g8eTJjxozhnHPOiTs0V1xxBXfffTdjx46NJ/Y3heOPP56zzjqLSZMmMXLkSE4//XR27NjBxx9/HE8+v/HGGxMS0y2PPPIIf/nLXxg9ejTDhw9PSJBPxahRowiHw4wePTo+YMDl7LPPZv78+YwcOZIHH3yQAw88MOfPkZeXxxNPPMFVV13F6NGjGTNmTJNGSA4bNoybbrqJ448/nlGjRnHcccfFE/bT8a1vfYvbb7+dsWPHsnTpUs455xxGjhzJ2LFjufzyyykrK8v5+qlQmSxDAKXU6cCJWuvvetszgQla60udNp94bVZ7218AE4Aa4GXgOOAKoFJrfUcufTp9XwhcCNC/f/+Dm6KWhd3InJnw6bMw/X4Y/o3MbRvqYN7tcPj3IX/316MRBGH38+mnn3LQQQft6dsQhFYj1TuulFqgtR4fbNvapSZmAXdprZvtz2mt/6S1Hq+1Ht+rV6+WuzOhZcnzYvv11dnbrlsI834JK99s3XsSBEEQhDZILtlia4B9ne1+3r5UbVYrpSJAV0zi/QTgdKXUL4EyIKaUqgEW5NCn0J6wYce6ndnbNtYlLgVBEAShE5GL+HofGKKUGogRSN8CgiVsnwXOA/4NnA68qk08Mz5+VSk1CxN2/L0n0LL1KbQF1n8Caz+EcTMzt4sWmWUuzleDN2RZxJcgCILQCckadtRaNwCXAi8BnwKPaa0XK6V+ppSa5jX7C9BDKbUM+BGQMYsxXZ/N/xhCq/Hhw/CPa7K3ixSYZV0OEebG+sRlOrSGl66FDa03v5YgCIIg7G5yKlKhtX4ReDGw73pnvQaYHjwv0H5Wtj6FNkiswfxkQ3sVm2vTDzWOYx2vmCO+tq2GLvuAW+24djv8+/dQ1AP2kkokgiAIQsdA5nYUMqNjvrDKhBVotTlMGRLM+dr4Gdw1HN7530C7+sR2giAIgtABEPElZEY35uZ8xTyBVpOL+AqEHTd7k8yufCvQzhNdDbUIgiDsKuFwOKHO1623dsyJVexE1buT888/P14I9rvf/S5LliSni9x///1cemlSRakEXn/99YQaXrNnz+bBBx9s2ZttA8jcjkJmYo2e+6UTQ4JJ7azzlUvYMZBwb0VYOPA6yqhIQRBakExzO1oaGxsTCqYGt5tKQ0PDLk1Ds6f7bw733HNPs899/fXXKSkp4bDDDgPg4osvbqnbalOI8yVkxhbhjWUJPTYp7BgIJ9pzQ9HU7cT5EgShFRkwYABXXXUV48aN4/HHH0/anjt3LpMmTWLcuHFMnz49PrXMgAED4pXn58+fz+TJkwGYNWsWM2fO5PDDD2fmzJksXrw4XlF+1KhRLF26NOH6jY2NnH/++YwYMYKRI0fGK8RPnjyZ+fPnA2YKogEDBgDGQZo2bRrHHHMMU6ZMyfjZbr/9dg455BBGjRrFDTfcAJgpkqZOncro0aMZMWIEc+bMSTjns88+49BDD41vr1y5kpEjRwLws5/9jEMOOYQRI0Zw4YUXppzb0b3v++67j6FDh3LooYfy1lt+dOO5555jwoQJjB07lmOPPZYNGzawcuVKZs+ezV133cWYMWN44403mDVrFnfccQcACxcuZOLEiYwaNYpvfOMbbN26NX69q666ikMPPZShQ4fyxhtvAGR97nuStiWXhbaHzffSjaR8XRrqoOLLJjpfAccr7nwFxZdtJ+JLEDoUf78a1n/csn32GQlfyxxGtNMLWa655hpmzJgBQI8ePeJzHF599dXx7fLyck477TReeeUViouLue2227jzzjsTJqJOxZIlS3jzzTcpLCzksssu4/vf/z5nn302dXV1NDYm/jG7cOFC1qxZwyeffAJARUVF1o/7wQcf8NFHH9G9e/e0bebOncvSpUt577330Fozbdo05s2bx6ZNm+jbty8vvPACYOZddDnwwAOpq6tjxYoVDBw4kDlz5sSf06WXXhr/7DNnzuT555/n5JNPTnn9devWccMNN7BgwQK6du3K0UcfzdixYwE44ogjeOedd1BKcc899/DLX/6SX/3qV1x88cWUlJRwxRVXAPDPf/4z3t+5557L7373O4466iiuv/56brzxRn79618DxgF87733ePHFF7nxxht55ZVXmD17dsbnvicR8SVkxjpe6Zyvjx+D538IQ080200SX4FRj6E0YccGCTsKgrDrZAo7WnER3H7nnXdYsmQJhx9+OAB1dXVMmjQp67WmTZtGYaEpPj1p0iRuvvlmVq9ezWmnncaQIUMS2g4aNIjly5dz2WWXMXXqVI4//vis/R933HEZhRcY8TV37ty44KmsrGTp0qUceeSR/PjHP+aqq67ipJNO4sgjj0w694wzzmDOnDlcffXVzJkzJ+6Ovfbaa/zyl7+kqqqKLVu2MHz48LTi691332Xy5MnY2WlmzJgRn5R89erVzJgxg3Xr1lFXV8fAgQMzfpZt27ZRUVHBUUcdBcB5553H9Ol+kYXTTjsNgIMPPpiVK1cC2Z/7nkTEl5AZHTPLdEn3lRuNSLLhxuYk3Kd1vuxxcb4EoUORxaHaExQXF6fc1lpz3HHH8eijjyadE4lEiMXM/5E1NTVp+zvrrLOYMGECL7zwAl//+tf54x//yDHHHBM/3q1bNxYtWsRLL73E7Nmzeeyxx7j33ntz7j8dWmuuueYaLrrooqRjH3zwAS+++CLXXXcdU6ZMSXLyZsyYwfTp0znttNNQSjFkyBBqamq45JJLmD9/Pvvuuy+zZs1Kuq9cueyyy/jRj37EtGnTeP3115k1a1az+rHk5+cDZlBFQ4P5vsr23PckkvMlZCYh7JgCW9E+vtyZPT8sXuHeE1dpc76a4Hxt/RKqtmRvJwiC0AQmTpzIW2+9xbJlywCTL2XdmwEDBrBgwQIAnnzyybR9LF++nEGDBnH55Zdzyimn8NFHHyUcLy8vJxaL8c1vfpObbropHv50+7cjCZvCCSecwL333hvPUVuzZg0bN25k7dq1FBUVcc4553DllVfGr+cyePBgwuEwP//5z+MuoBVaPXv2pLKyMus9TZgwgX/9619s3ryZ+vp6Hn/88fixbdu2sc8++wDwwAMPxPeXlpayY0dyBKVr165069Ytns/10EMPxV2wdGR77nsScb6EzGQLOzYExBeY0GNhWXLbxnp470/+/I9Jox13Iefrkekw6Cj4+u3Z2wqC0CkJ5nydeOKJWctN9OrVi/vvv58zzzyT2lrzf9FNN93E0KFDueGGG7jgggv46U9/Gk+2T8Vjjz3GQw89RDQapU+fPvzkJz9JOL5mzRq+/e1vx12uW265BYArrriCM844gz/96U9MnTq1yZ/3+OOP59NPP42HSUtKSnj44YdZtmwZV155JaFQiGg0yt13353y/BkzZnDllVeyYsUKAMrKyvje977HiBEj6NOnD4ccckjG6++9997MmjWLSZMmUVZWlvDsZ82axfTp0+nWrRvHHHNM/Bonn3wyp59+Os888wy/+93vEvp74IEHuPjii6mqqmLQoEHcd999Ga+f7bnvSVSqkQptlfHjx2s7gkLYTfztbPjsefjx51C6V/LxF34M798DPYdCuflrkB98AmX7JrddMQ8eOBl6D4eNi2HMOXDqH+CNX8E/fwaH/wCOu9Fv//lL8NczYMCRcP7zme/zl4Ng/2PhtD81/7MKgtBqfPrppxx00EF7+jYEodVI9Y4rpRZorccH20rYUchMMOdr8dNwz7F+CYp4uNGJ+6ery1XrzftYU5HYzoYV0yXcp+rvq3dgx3p/u6E2+1yRgiAIgtAGkLCjkJlYIOfr8fP8/eEI1FeZbbuE9CLIhhtrvGHNcfHlCTgr9OL92OMpEjrvPQGKe8GVy/w2uVTiFwRBEIQ9jDhfQmbSjXa0Ysw6Xm7OVzrnq94TX3WeA2ZFmu0jeI14kdU0/e3c5LXzJv/OlugvCIIgCG0AEV9CZqzIigVcKSuUrOPV4IivWDrnqypxO+h8BcVTuoT7YJ6idcbSXVcQhDZBe8oxFoSm0NR3W8SXkJn4aMeG1PvrU4QMs4Ud4+08cRV0vrSG134Bm21IMeB8BUWaLV0hYUdBaLMUFBSwefNmEWBCh0NrzebNmykoKMj5HMn5EjJjRVWwzpcVOq7jFSkwLtTq+fDURXDh61DYzT9eHxRf9UZ4xXO+vGvsWA//ug269vfaBZyvoMPVkCZsKQhCm6Ffv36sXr2aTZs27elbEYQWp6CggH79+uXcXsSXkJm0OV/e/voU4mvdIti6EravSxRfwbBjxVdwSz+IFiVew+aE1XqJ+UHnK+isWfHVKOJLENoq0Wg06xQygtBZkLCjkJl0RVbjOV+O+LIiyi2iWrMN5v7UhAaDYcftq42LZUVWkvjyqhwnOV8BkSVhR0EQBKEdIeJLyEzc+QqKL5vz5bhZUS/ebcVTrAFWvAFv/xbWfJAcdgxi+7QizV67oTYxyT44mjKesC/iSxAEQWj7SNhRyEy6uR3jzpdTgyuV82XFWe32ZOcriO3TFmP1b8IcC0fh71fDPuMSD8edLxntKAiCILR9RHwJmUk32lE3mvITwYR7cMRXvS++arYl53wlXSsQdnRpqDUV8N+9G4ad4u9f8gxsX5t4r4IgCILQhhHxJWTGDTu64ibWmFx5PlpolmEF3gQAACAASURBVK74soKruiKHsKMVXynaNdZBY5533BFxT1wART0SzxcEQRCENoyILyEzVnzNvRbWf+zvTym+bNjR5nwFna9cc75SOF9PXgATL/GOO/3E6p1K9xJ2FARBENo+Ir6EzFhB5AovMC5TfSCMmOR8OTlfNRVNCDumEGlfvGp+IFmc6TQjMgVBEAShDSKjHYXMBCe7ju9vTCwzAb7zZUtDNNb7bWq2pXa0XDLlfLkERV/8fHG+BEEQhLaPiC8hM8FRjpaUzldgaoXGet/FqtmWXjS5fUKK0Y4WZRbpwpetnfNVsQrm39e61xAEQRA6PCK+hMykC+XFYollJgAihYE2jvNVtTm5Ple6a6UTV+Fo5uOtLb4+fhye/4Ff/FUQBEEQmoGILyEzNuyoAq9KKucrkgcq7G+7OV+2HIQlnJd8rWxhx1AW8dXa0wtZ8Zgtd00QBEEQMiDiS8hMfGLtQO5XqpyvUMR3pyCxzpcVX/ldzTKvOPlamUY72v7ttVPR2s6XFV/ZSmYIgiAIQgZEfAmZSRt2bEgssAqe+HIcLbfOl03CL+pulnklZlmyV2KfkOhs5ZX66+mS/4Pnp+Nft8M/f565TSbE+RIEQRBaABFfQmbSCZ5YKucr7LtTkBh2tBT3NEsrvvYeDdeuhwOmps75yi/x1+uy5FrF6hPngAyy/DX44p+Z+8iErSMW/Nxgrltd0fy+BUEQhE6DiC8hM+lCfDrmixCbixV0vlLlhRVZ8eWVpcgrNvXBQuHUox3zSsiIm2Nm7ysdDTWphVOuZAo7rvgX3DEEKjc2v39BEAShUyDiS8hMMOzYe5i3v8F3qAq6mGVSzledJ3aUv6/PCLO0OV/RYv/cVAn3+RnEV9l+MOiowP1mCD021O2i+PKcr1Rhx+1rzefdWd78/gVBEIROgYgvITOuk7Tf4fCNP5r1WKMRSSoEBV4SfaqE+7qdvtDqPQx6DDHr1tHKSyW+dkJBWWK7VHznJTj8B4n7Mk0xtMvOlw07phBf9pgUehUEQRCyIOJLyIwrvkIREx4EI5RqK01CvA01pkq4r6+GQZOhuDecerc/BZEVXTb8GIoYQddYb5Lzi3uZ/flOwn2QcDT5eEbnqzZ5PsqmEE+4TzPxN7R+uQtBEASh3SPiS8iMG3YMRxPLPdTuMGFBN+cr5DhfDTVGSPUZBVcuhb5jksVXPOwYhtrt8JvRZjuYmJ+KUATyu6S/3yCNtS2U8yXOlyAIgtB8RHwJmUlwvqJ+gnus0Yw+zCuBsCfIgmHH2u1mad0tgIg3BVGqsGNNBWxfY7YHHGmWhWVw4m0w6OjkewtHk3PCMomfhhpzvLnuVDznK4XzZa+bKewpCIIgCIj4ErKhg86XI75qK03YL2G0oyO+araZZdSZdmifcTDuPJM/BolhR8v0+2Hgf3nXzIOJF0OvA5PvLdTUsKPnXAXrk2WivtrM6Qi+wErpfNUlLgVBEAQhDSK+hMwkhR3dnC8v7Jgu58vWvYo61ezzimHab/2cLuuAueIrUgjRIv+aAJH85HsLR007d+qjdOJLaz/fqymhx3f+F+4+3M9HS3e+ddNau8q+IAiC0O4R8SVkJhh2dHO+6ioDYcdAkdVUzpeluCegoEtf/1xLJB+iXnjSijkbrozfSwSUMj/5pf5104UUG+sBrwBrU8RXxVdQuw12rMsx4V7CjoIgCEJmItmbCJ0WrYkLFjAiSwVGO+Z3MaMIIdn5suIr1TyO3QfCj5akFl/RQl+wpXO+3MT+A0+GyvWw7JX0zpOd3giaJr6qNptlxarMCfcxSbgXBEEQckOcLyE9wZGDrvMVi5mE+/wSXyDlkvPlYoWXPdcSyTehR0jvfLnXOfUPJo8M0ouvBkd8NSXnq2qLWW5blbnIqj0mpSYEQRCELOQkvpRSJyql/qOUWqaUujrF8Xyl1Bzv+LtKqQHe/kOVUgu9n0VKqW8456xUSn3sHZvfUh9IaEGCU/WEoxDyXhmb85WXQXxZtymd+HJJEF8FpnBrXimU7u3tCzhf7nXc89M5T259r/om1PqKO19fOTlfqcKOVnxJwr0gCIKQmaxhR6VUGPgDcBywGnhfKfWs1nqJ0+wCYKvWen+l1LeA24AZwCfAeK11g1Jqb2CRUuo5rbW1B47WWst8LG2V4LyO4Txf5NTvNAIsoc5XODHsaMk2PyMki6+8IvjBR371/KScr4D4smIsXZ0v1/lKFTZMh50uaJsTdkzpfHnHJOwoCIIgZCEX5+tQYJnWernWug74G3BKoM0pwAPe+hPAFKWU0lpXOUKrgIQEIqHNkxR2dHK+bEgxv0ui8xUURZC5Sn28bzfh3hNaRd39/VmdLycXLRUJYccaWDQH3vpN5nuKxaDaCztWOGHHlDlf3nUl4V4QBEHIQi7iax9glbO92tuXso0ntrYBPQCUUhOUUouBj4GLHTGmgblKqQVKqQub/xGEViNl2NFzqGwZibwSf184miyKbJtsuM5XtCD5eKrRjqm204mfhLBjNXw0B97/S+Z7qqnwn0GC85VhtKOUmhAEQRCy0OqjHbXW7wLDlVIHAQ8opf6uta4BjtBar1FK9QZeVkp9prWeFzzfE2YXAvTv37+1b1dwCYYdQ06drxpPfGWq82VpTtgxSKaEe3tvkGG0o5OLVV9tqu/bfK502GT7op7G+bLuW8o6X1LhXhAEQciNXJyvNcC+znY/b1/KNkqpCNAVSPhm01p/ClQCI7ztNd5yI/AUJryZhNb6T1rr8Vrr8b169crhdoVmUbEKNixJ3BcLOl8Rv6Cpdb7yS52wYzhZFEWL/ST9TLjiK5yioGpS2DEg8uIJ9+lyvmoS12u2mTplbvJ9Q50ZRGCx4qznUDNC0jpeMrejIAiCsAvkIr7eB4YopQYqpfKAbwHPBto8C3hj/TkdeFVrrb1zIgBKqf2AA4GVSqlipVSpt78YOB6TnC/sKX49Au6elLgvKeyY5xU2DfvOV55T4NQd7WidquDci+lwc75SiTXbn805Swo72pyvdGHHQMJ9jTfvZJUz3uOBk+GWfv62FV9l+yb2XbfTq4HmIHM7CoIgCDmSNezojVS8FHgJCAP3aq0XK6V+BszXWj8L/AV4SCm1DNiCEWgARwBXK6XqgRhwida6XCk1CHhKKWXv4a9a63+09IcTcqBqS3rBkCrsCEboxBPuS1In3OcVG4cpl5CjPTcT1vnKLzXCL+iwhdOEHb941Qi3BPFV49//zk3Q1RNcq95JPNeKr66OIAtFzDUa6xLdOKlwLwiCIORITjlfWusXgRcD+6531muA6SnOewh4KMX+5cDopt6s0Ar842rYtjr1sWAIL+w4XNW2er1basLJ+YoWA5tzG+loz81E3EnzxFdwVGU87BgQX6/eZM4Zc46/r3a7X2h1Z4a8r7j4cqLuRT2gcoMRbyW9/f3xuR1FfAmCIAiZkQr3nZ3qrebH4gquYNjRCh4VNvMdQoqEe08E2SmFWkx8Oc4X+NcJnh+sMN9QZ8KEbs5X5QZ/3YYdU+WKVW8xn9kVWbboazBZP+58yWhHQRAEITMivjo7scZE4VFXaZaN9f66xU2st0SLEifWtkLMiq+mhh1VmlfSdb4gfcJ9Q3Xi52n0xJettq9CsGO9f9wWUd2xzt9nBxrUV5tir9Ei/5gVXzsDtYFlbkdBEAQhR0R8dXZ0LDFUV+sJrnm3w5+nJLYNBcSXCiXmeblhx7jz1cSE+3QOWND5Shd2fPYyeOlaf78VXzbnq6BrovO1c5NZbv3S32cFVH21mWMyQXz1Mcsk50sS7gVBEITcaPU6X0IbRzcmJtZbt2vryuQ5DN3EejBulFKBhHsbdixJXGYj3ShGi3W+bH/pwo4Aaz/w1xvrTcjRhh0LymBHirBjhSO+tiyHyo3mnKg31ZElHnYMOF8ivgRBEIQcEeersxOLJYbpbJ2rVFXcw07OF/gu1+BjYPwFUNwr2flyXaNMxMtVpKiQD8b5yivxxU+6uR3BuFiL5sCyV4yLVV9lcr/AOF82Xy0U8RPuXefrzbvgsXMzO1/BRP1Mczs2OAVeN32eXKZCEARB6FSI+Ors6MZA2DGD+AqlcL4Aeg6Bk+40ocN+h8D+x0KZNxtBsDhqOuJTFKVxvpSCC/8Fky7x2qWZ2xGgcj08dSE8/E0jiuqrTC5YOM8XhQBl+/kOljvis2qLeQ5x58s5J6/YCLhg2DHd3I6bPodf7A2b/gOr3oM/HALv35P+OQiCIAgdHhFfnY3VC2DNAn/bJtxbYWXDjqmquMcT673XJpWw6jUUznnST5zPWXzZsGMa5wug5/4mbAgpxFca0WbFUPVWUznfDYP2HArbvUR7WzQWTCkK3WgKsUYKIVqYeJ2iHinCjmmcr4ovjTDbvMzPL1s6N/1nFARBEDo8Ir46Gy9fD6/c6G9b58uG1uLOVwrxleR8ZRBWdnRhU52vbCUn4mUt0sztmHQfnhiq2mLupe8Y/1ifkbB9jUnGr3bFl/cMqrcY5yvqOF/hPDPXY3C0YzznK1Bqws4DWV3hO4XuaEtBEASh0yHiq7PRWJs4ybT2cr6su2NHOwbLTEByzlcmYWVHF6aapzEVIadcRSas+MrF+eoxxP+s1VuN+Ok/0Tk+GNAm36smhfiq2mKcr7AzijOcB8U9/Um3LenmdrSJ/jUV/ro72lIQBEHodIj46mzEAjleMW+0Y9RzZeo84ZEy7Jgm5ysVVmg0OecrQ9gRTMjTLW8R359CtIXzAC+5vWoLRPKgnzN/e7eBZrl1hXGminqY7Vpv3sfqrf5zseI0HIWi7unDjq6wBf85Vlf4LpiIL0EQhE6NiK/ORqwhUXzZOl9W/KRKuA+ORLQ5X5lcLet8ZRJoLrmGHQEGHgV9xybuM/OEGop6mqXr3lVtNvfi1h3r7omvLSu86YL2Mtv2GaCN8wV+6DEcNf1XbfZHLWrtTKwdDDta52tbYpX9WGD2AEEQBKHTIHW+Ohs6BjGn1IF1wmy5idpKIyZc8RXOM22SnK8M4qvHYLO0ox6zYQVdpoR7y8z/y3z823+Hv18J6z/x91Vvha77mPXvvGQ+X3EvI6o2LzWOX3Evc9ydVsk6X7bWVzjPOGSNdUakFXQJOInBsKPndtU4zheYEZld+mb/rIIgCEKHQ8RXZyPWkDiFj25MnGKortJzaByBFs4z4bNQMOcrg6t1+A+h/yQYcERu92UFT7acr1zoOQRK+8JX7/j7YvW+U+fmfXUfBGsXmnV3DkeL/Yx2QEI46lfZr6s04sstLxEsNWGdr+oK3w0EU8RWxJcgCEKnRMKOnY1gzpeOAU7YrHZHco2veLJ5IDQYCcyvmHBOJHfhZe/L7XtXUMrcmxvmAz+ny6X7AFj/sVkvTiG+bK5X1HG+bLkK+5zcPK9YA6z5AGZ1NTW+XOerwXW+NjbpIwmCIAgdB3G+Ohu6MTnsCL4rk0l8Bed2zDWfKxesY5Qt4T5XUuWj2bCjS7eBflmMTM6XG3a0RVftc3LFbGM9fP6SWV/0aGLOV70jBoOJ+YIgCEKnQZyvzoab3wX+vI5WDNRVphBfniCyIqypBVRzwYqb7oNapr9UIq5LCvFlk+4htfgKOl+hiH+vqZyvxjrotp9Z37zUd7uqKxKduKArJwiCIHQaxPnqbMRiiQnldtSd63wFy0ykCzvmWsMrF/qOhTMehP2Pa5n+UgnDVOKrmyO+UoUd486XHe2YKuzo5Hm5LtjmL2CvEWY9mHDv5n8JgiAInQpxvjobwbkcrfNlc77qa5ILrKYNO7ag+AIYdorvKjWHoh4w7FSznmvYMWfny9b5csKO9QHxFSk061ZYlS/1hWxDjRFg+V28bRFfgiAInRVxvjobSWHHQL2phprkqYXiYccmjHbcE/zPcn8917Bjl37GyYs1ZBnt6NT5igbCjla45hWZdbfavZ3PEWDHBjMpd+12P89MEARB6HSI89XZcMtK2G2Xxtr0Ycd4DS4vYT/TaMc9TSpXrnTv5H3hiKlFFs7zXSkX63jl5TDaMVpsiqy6wsqOpATYsU6cL0EQBEHEV6cjXdjR0lCXHHaM5Jkke1sI1Z7f1pwvl3AKYZhOLHYbCAVlqQWb3bffYTD0a2Y7ONrRVrXPKzJCzE3Ar6/yncLKDUbMhfNFfAmCIHRiJOzY2Uia2zFV2DFFqQm38nw8x6mdia90jD0HNv3H5LKpUGIo1k4vNPgY8wNGgKlwCufLCzs2BMpIlPaB7WuMqI0WmvNFfAmCIHRaRHx1NpKKrDrOVzjPCIma7YnnDP8G7OtUhbehyqYInN2N62Id8cPkuSBdRpzmr4fzE4uhRlMITKWM+xXM+YoWmWcbzOfq0teILzCCNZIvOV+CIAidGBFfnQ3d6P1oIyLcnK+oFzar3pJ4Tv9J/lyN0P7CjqNmQO+DcjsvkpcovqzzFSSv2A/PWufL5oXVV5tnY2t5udMIRQsk7CgIgtDJkZyvzoYVTnbphthsLlPVFkD5+1XgNYmLrxYuNdGSuOKrKQ5dsG0q5wvMs7IDE2zOl03Ot+HF/K5mu7h34gjR1go71u1MrKIvCIIgtElEfHUmtPbFVlx8BZwvMM5XQVd/f5L4sjlfbVh8uffWlCmLbH2wuFhK43xFi1KPdgSzP5wPhWXe/kIo7Ob114phx0fPhL//T8v3KwiCILQoIr46EwmV7a0D5ogvGzarrvCFA/hFVePneue0ZfHlOlihpogvr60VS2mdr5LUdb7AE195Th+FUNTdXw/ntY7ztW0VbF3R8v0KgiAILYqIr86Em2hfscr84EyybZ2bmgrIK/X3q6D4amc5X00JO1pBGXeqMuV8eeLLCql48dUqkzvmiq9CT3xFCrx8sFYQX/XVyYMlBEEQhDaHJNx3JlyX6/kfJofjXOerqKe/P13OV0vO7djSuDW9mhR2dJwvFUp/bl4RVHxl1suXmir53QaY7bpKL+zoCDhXiEVayfmqr4aabS3fryAIgtCiiPjqTLj5XTs3+U6NxW7XVPjJ45Ai7NgeEu7dnK+mJNx75/UcAjvWmxGhqXDDjus/hl4HJRZfzStODF3aY9b5qt2R+z3lSn118u9KEARBaHNI2LEz8NZvYdHfEsOO9dXJSd9WIOhYojBLcr5szld7CTs2wfmygvLIH8P/ezN9O7fUxPqPoM9I436BEV+RgPPlTtDdGjlfsUbz+6zZZgZWCIIgCG0WEV+dgZd/Ck9dlFjNvn5nsgBwBZfrfAXFV7zCfVsusurdmwo1zQ2yQi2/NHHEZ5BokSk1sWODmTaoz0j/mdVsCyTcF/jPVuvm5XzV1yTPw5lw3KtNFmvw1wVBEIQ2iYivzkSS8xWYBifPFV/OetqwY1t2vjwHq6lV+O152UKqeSXm+T1+vtnuM9J3DusDox1d56u+qnl1vn5/CLw7O3n/vNthybOJgqtWku4FQRDaMiK+OjruPI1uzldjXQrnq9hZd4RV0mhHO71QE8J5uxsruposvrzPlE1YWmdtzXzod4iZvsh9fumcr/rqptf5aqiDbV/BhsXJx979Iyz8a2JVfkm6FwRBaNNIwn1HZ+cmfz0YtsrofGUIO/bcH9YsaB+jHZsqECM5OmZDvwZbv4RjrvNreOU54iuSB/sdBuPOg77jYMtysz9aAHVNdL5sbtmO9Yn7tYbqraa+l+t8pSo38cIVJjw646HcrysIgiC0CiK+OjqVrvhqSDyWMecrQ9jxrMdh3YeJYq2tYYVhUwqs2vPC+elHOVp6DYWT7kzc5z6PcJ4pVDvtt2Z77EyTt3Xw+fD6LenF1xevGictrwSevMC0L+tvjlVuTGxbu8P8Tret8qc6gtTOV/l/YPvazJ9JEARB2C1I2LGjs9P5wnYr3INfmd3iOjcJzldAfBX3gP2PbZn7ay2s49WcsGNzc9nySpx+Aq5gKAwTLzbOlw07BkclVm2Bh06DDx40TtYnT8KDp/hlKSoDzlf1VrOs2ZYozGpTiK+6KglHCoIgtBHE+eroxL+UVebRcpDe+crmArVFlDLCq6lhx/2nND+XzX1mmfqwgrCxLjGxv3oroE3x1gZvgmwd88XXznIziXfY+2dbvcU/t3ypv55KZNkCrFq3z9+nIAhCB0LEV0fH5nwVdEkOOwZJ63y10y/rcH7Tna9hp5if5hAtBBSgM4+WtM5aQ01AfFWY5fY1vvgCpyCrNk5ml75e+61+m/L/+Oupcr7qq7xBFjWJv1tBEARhtyNhx46OFV+axNGOqYjk+8n16eY0bE+Eo7t3RKZSvoDNdF0ruBoCAx5qrPhamz6B3k26TxBfjvP18eOwMlAg1vYnoUdBEIQ9joivjo4NOzbWZg87hiJ+rlJHcEci+bu/HEZcfGVyvrxjjbWm8O07s2HnZl8YbV+XmJDv1u1yc7uq3LDj5/76hk/g/qmJ1xTxJQiC0GaQsGNHxzpfDVkqpINJrI/km5pRHUF8hfOaHnbcVWzeV6awoxVmDbXw1b/hH1clnlO5PrE+mzsPpJt0b8OUXfYxoUqXUOCfth0NKeJLEARhj5OT86WUOlEp9R+l1DKl1NUpjucrpeZ4x99VSg3w9h+qlFro/SxSSn0j1z6FFsINTTVkmXYmFPEFQHDS7fZIcxLud5UmhR1rYdnLZn3rCl8Y6RhUfOm3r92OySXDTGdkqd5qRlh23dff12Ufs+w2wN/XWO+PbBXxJQiCsMfJKr6UUmHgD8DXgGHAmUqpYYFmFwBbtdb7A3cBt3n7PwHGa63HACcCf1RKRXLsU2gJ3EKqdVXp2wGEQo746gDOV6QZCfe7SlPCjs9eBm/eZda3rPBzvgA2f+Gv71hvBkwUlCUWza3eYqrol/T29106H0afmZgnlq0GmCAIgrBbycX5OhRYprVerrWuA/4GBIeDnQI84K0/AUxRSimtdZXW2g6xK8Ckfefa527n/rdWcNfLn2dv2J5ImM8xm/gK5Hy5jkp75ICv7/56ZNYxzMX5WjPf3+c6XwBbHPG1dSXkdzFCyxVo1Vs98bWX12+hKfRa3DMxTywhed85XxAEQdgj5CK+9gFWOdurvX0p23hiaxvQA0ApNUEptRj4GLjYO55Ln7udt7/YzEuL12dv2J5obIL4sjlfYMTXRfPgv99rvXtrbY65Fib+v917Tet85ZLzBfD1O2D8BWaqouqtxt0CfzoiMMfyS03FfDeMHBRfdiaC/K4mx8+OphTnSxAEoU3R6qMdtdbvaq2HA4cA1yilmlQ+XCl1oVJqvlJq/qZNm7KfsAtEwyHqG2PZG7YnYg1+XSk3iTsVwZyvou7Q64DWvb+ORjzsmCHc6QqzQ78HfUaYkY8bP0s9ldD21b7zZZPsn7kUVr1rBJkNO9o5IPNLzdIm6tfLpNuCIAhtiVzE1xrAjT/18/albKOUigBdgc1uA631p0AlMCLHPu15f9Jaj9daj+/Vq1cOt9t8ImFFQ0xnb9ieiDX4gqA+W8J9uGOVmtgT5CK+7DRP+x1hlt0GmuWmT6GohwkfVm1OPCe/1Lhi1vn68GGzHDzFd74sBV3M0oYeW1N8aQ3ly1q2T0EQhA5OLuLrfWCIUmqgUioP+BbwbKDNs8B53vrpwKtaa+2dEwFQSu0HHAiszLHP3U40HKK+oaM5X/UQteIrW86XE3bsCEVW9wTxnK8M4qvvWJh0KUy/z2x3H+gfK+hqBJx1sSz5pZ7z5YmvUASO+BEcfB6UBsRXfkB8uY5nqur3u8KyV+D346FiVfa2giAIApBDnS+tdYNS6lLgJSAM3Ku1XqyU+hkwX2v9LPAX4CGl1DJgC0ZMARwBXK2UqgdiwCVa63KAVH228GdrMtGwor7DOV+NvhuTa9gxUmBGPgpNx06unTHnKwon3Oxvl+0HJX1MDa/CMsgvgapy76A3XZEVXzUVpkRFrN4k10Oy82XDjjUB50uFW975qtxg7q+qHMra+QANQRCE3URORVa11i8CLwb2Xe+s1wDTU5z3EPBQrn3uaSKhEA0dMecrL0fnyybcR5qUlie45OUw2jGIUjD4aFj0qBFveaX2ABx1FfzrVjOpds8hJmRppxiyQq84EI6Phx1tzpf3ey/tk5iw31S+fBvm3Q5nzoGI5+xZYZctpC0IgiDEEXvDIRoO0dDYwZyvxnrH+cqh1ESkoGMUWN1T5FLnKxX7HWaWFV86IyYL4Mgfw8jpJjG/sJvZv2114FoBoRcMO1phtM/BsP7jxGmJLJs+h8VPZ77HR8+EL15NLABrhV02YS8IgiDEEfHlEA0r6jqS86W1mUzbOiS55HyNvwCOvaH1762jEs0h4T4Vg6f4y3zv9xUtMA7TN+8xzpgVX3YqIVckF3SFoV8z6/lpnK9x55pw5ZJnkq//xh3wxHcy54TZGmE7y/199TXeUpwvQRCEXJG5HR063GhHW2DVhsJyEV/9J5gfoXmU9jHLou5NO6/rPnDtBhP2Xf662Rcc9GBrgMWdrxL/2NVfOe088WXzu6ww6j8Reg6FT56E8d9O7Hv9J0aof/k2HHBi8v2584JWOlMcxZ0vEV+CIAi5Is6XQyQUojGm0bqDCLC4+Mox7KjCrXs/nYFBk80UP+4IxlyJFpj8r3RJ+0HnKy9NeDiSb8Ketdvh0+dNLhkYp2zv0bAtMDKxoQ7K/2PWrfALUu7M/ODWIIvnfEnYURAEIVdEfDnkRczjqO8IeV8L7jdlAMAJO+Yw2lHYNZQyifG7QjzsGHC+0uV8peyj1IQdX7sZNi4xYiwU9spYBIRS+edGqIeiieLrgwfhwVNM+HrtQn9/SuerJuePJwiC0NmRb1uHSEgBUN8Yiwuxdsubd0H3wWY95yKr8jq0CdJNUVRow47W+SohLQVdzCTc1rGyE6xHi41gWr0Atn0Fw78BGz4xx0adAQsfMaMp63aaib/BtN/uCb7CbgHxJc6XIAhCU5FvW4dI2AiuDjHiMdbofzHmT0VdnQAAIABJREFUPNpRwo5tgnjYMeB8RQvNCEgrhDKNSs3vAqvnOxOre+90XpERVvccY7ZjjfDKjcYZO+QCI76W/wvWOU5XXZWZ1ihaZCZbd8OODZJwLwiC0FTaub3TskTDnvMV6wAjHmMNvhthv6QzhR1VyITMhD2PLZKaqlBrUQ+/VlemsGOfEbBjXfL+vGLiQgzghR+Z3/3UO2DvsVDY3YQeV7/vt6nfacRXQZkp6Joy7CjOlyAIQq6I+HKIdijnyxFfkQLzBWvdiVCgLpQKS7J9W8KKqlTza7oFVTOJrzFnm2WSe+adY921mm1m5OO4c82sBoOOgi/+CesW+ZN811WZMhOF3TzxlSrhXpwvQRCEXJGwo4Ob89XuiTX4X4jhqBFgcTGWD3X1fttIvkmqFtoG8bBjipkGSnqbZSiSuZZY/0km56+4J3Tp6/9+rWBTzt9dQ53SEiO+CYufMusDj4IPHzLvTXWFyTkr6W1yyWIxI9ak1IQgCEKTEfHlYJ2vjiG+nJwv+0WdIL6ciZvD0cQ6TsKeJT+D+Cr2xFdeceYwsVJw9uNm2X2Qv9+Wp7DV73sOhb2G+8cPmGr2lX9uymZ8+JDJEaveaspnlOxlCrVWbzHCriUT7rU2ozOHnQJ9Ru56f4IgCG0UCTs6xMOOHaHQqht2DIUTv8iDU9+E8yTZvi2R51S4D1LihR2jGUKOlh6DE4WX2zeYUOOFryeKuFAIvvZLOOR75nww4qvGy/my++woylwr3DfUwbw7Ms8tWVdp5o6cfUS2TyYIgtCuEfHlEAl3tLCjFV9RfyJkSFwHI8Yk56vtkCns6DpfzcEdIVnYPXU/g482CfhRZ0L26q0m7Nj7ILPvzbvgF/38pP5szteyl+HVn8NHj6dvU+u4sbY6vyAIQgdExJdDfLRjR0m4t9gJs8Hk+gRzhSJ5UuOrLeFOrB3EJtw3V3y55+VnqBMGfoiyeqsRV4Vl0GUfU8Zi6Vyo22FCkJDd+fr8H2a5Zn76NnYuSoDPXsjcnyAIQjtGxJdDJGRHO7Zz5ysWA+18hnDEDzWqcLLQCov4alPES01kCDu2iPjqklvb7WvNsrCbCVFa98slk/iKxeDzl8z66gziq84RX+50RoIgCB0MEV8OfsJ9O3e+XNcLjLCy+UOhsPfjlJsIRyXnqy2R3wV6Dze1uoLsatjRPS9ThXzww45WfNmJvVOJr4YM4qv8P6Y2WM8DYMsXULUldTs37FhdkbrNUxfD/Psy37fl0+dg4V9zaysIgrAbEfHlYMOO4R2rYcPiPXw3u0Aq8WXdlFDE/LiTMtt5/4S2QTgCl7wNB05NPmZLTWSqbp8J97xsYUcbjrYTedvpjVKNRMzkfNm5KEedYZZrPzAJ+EHcEbg1acTX5y+ln/w7yDuzTQK/IAhCG0PEl4OdXmjfhXfBk9/bw3ezCySJr6gvvmzY0f0SDudJwn17obC7+V1lc63SkRB2LM3ePlrsiyc7sfeYc+C85/w2bg25VOxYb5b7HWaWGxbDbQP8id8tNuersHv6hPv6alPmIheqt0DFKimjIghCm0PEl4Mtshqq35n4V3h7I6PzFfLEl1P5XBLu2w+hkBExe49q3vnhqD/gIi8H8ZVX5DtfNuwYLYCB/+W3KeqR2fmy4qvnULPcsNhMWbRhiTnvb2fDqvd88dV1HxN2rK8x+WKWWMyEN6ucchWZigNXbTEDAmzYVBAEoY0g4sshL2Ieh441JAuY9kTwL/1wxE+uVmHzZVm6t3+8pE/itDVC2+b852HCRc0/37pfOTlfRdDohQit8+UeAyjqbto0pvk3U7neCLeiHub927rS7K8qh0WPwmfPmxpg9g+ervuasOPvxsG/f+/3Yyfxts6X1nDnMHjhiuRrau23q/gy++cUBEHYjYj4crDOl4rVQ2N9ltZtmEzOlwrBKX+A0/7sH//arfCth3ff/Ql7FptIny3nC3yhFs73na/4Me/8oh5mufy11E7UjvVQ2seMlCzoCltWmP2VG+HffzDry16GTZ+b97O0jwl1bl9j+rRYd80m7K98E3ashfedd9lSt9MXjVbs7So7N8Pbv5epuARB2GVEfDnY0Y66sb07XxnEV0OtSZx2na78rsmuhtBxsYMtcskbs+KrbF8T8nSx71Rhd7N85PREsWSp3GCmJQLz7u30JuZe8wFsXgaHXW5Koyz6qwmFFpT5wmnth77Yqd9plg3VRoi9O9ts99g/+ZpuXtjWFnK+PnkC5l5r7hmgZju8fmt6x08QBCENIr4cbIV7pRs7rviyX2BujlfwS1Xo2DQ17AgmFBjEOmcFTr2whY8mt9uxwQ9zF3T199taXvsfm+jGFToOW/VWP2zo5pXtWO8n7Nspjiybv0isJ9ZSYUcr4nZuMsuXr4fXb4H/SEFYQRCahnzrOljni8b6di6+AjlfrviynysUApSMcuyMRIuNqMqlvIh1ycr6pzjmvVNVm/19nz6XWKlea5PzVeo5XwmhS8/RKtsXuvT1+ixJFGhgHDJIHFG59GWTA9Z9cPLox+e+D09826xHClou7FgREF92IIJbM08QBCEHRHw5RD0HSOnGjpXzFY6mrmYeipgcG6FzkVecm+sF/vtRlsL5mvorGHQ0TL0TTvgFfPvvJiT49u/huR8Y16p6qwkhlvQx5xSWJffTZR8zwhHMfbkCTYVh/cdm3XW+ljxj7u3AqUaU1dfA4qdh5VvG+bJ0H5y+qKuloS57G4CKr8zSii9bDkNGCguC0ETkfw2HeNgxVm+GqGttkoTbG5nCjsH9SPJwp6Okt5+DlQ0rSsr2Sz7W+0A492mzPum/zb+XPqPgX7eafX3HQu12sx53vgKuVsleEMk3AgxM2NG2KehqymJYZ8t1vr58E/YeA90Hmu2515nE+/yu/jXBOHbrFmb+jC9dA+/fA9esyTwIIS6+ys2yxrtOpur+giAIKRDbw8GGHZUN2+l2OsdjU8SXhB07H8f/HM6ak1tbK75S5XwFUQoOdYoTz7vdiKJ9J5q8LkgeMWn7tWHHaJHvjpX0MY6tDWMGa4kNOspP9n//HrOs3UbCHxRl/f3zGxugLkUx2E+9grFLnk7/2Wq2+VX3rfNlRV62ScUFQRACiPhysNMLoT3x0l5Dj5lyvhL2h2Vaoc5IYTdf7GTD5nN17Zdb+zFnwzlPwoEnwbZVZlTt+c/7bpYVVjZUZ/u1zlddpd+2tI95b63DFBRO+x9naowBoI3rFaSwm+kzFoMnzodf7J1cKqLUC4l+8GD6z1Wxyl8POl+ZqvsLgiCkQMSXg1KKcEgRss5RrL2Kr6Y4X/IKCBk49X+h/2G5i7VQ2LhcA48y26NmmJxDi3W+unnhQptLZsVXzXa/TeneZiRlbRqR03+i73wBDD0h+X7se19X6Ttcq95NbGNHMa56F9YsgDfuNALNdbRsyDGvxBdfdWkcOUEQhCzIN2+ASEiZhHtovyMeU4mvVFPJiPgSsrH/FPjO35vukB74ddhnPIz/TuJ+62r1PtAsbS6ZFXe1202bUNQk4acLO+aVGlFXlEJ8ue+0FV+1O/wRm4v+5h+34cS+Y832yzfAP2+Ed+6GOw8ydfHAF199x5qwY91Ovw9xvgRBaCKScB8gLxzyna/2WjwxlfhKVcsrFAEtkw4LrUDXfvC9fybvt2HHvUbA6LNM3hb4dcB6DDFC79ynodeBMPenyeG9/34fint6/TnFgftP8oq0djXnN9bDpk/NsdodfthymXdfWkO5VzB18DGmoOuXb5ntT581IzUrNxp3ruJLU6Kj51AzN6WdrxLE+RIEocmI7REgElYom/PVEZwvFUpfRDUUloR7YfdiQ4qF3Yw7Zgu+FveAc5+F0/5ktgccYQRWQcD5UmHoOcR3vKKFECk00x+V7g17DYceg0ybvYb5JVaqt/r5a3b05Ou3wD3HmPVBR5ulHWSzZoFZ2nMqvjLOWUlvc76t8QVGoL18gy/uGhvMtEluvTNBEAQHcb4CRMIhQjZhvd3mfDluVqYCkKEIhNqpwBTaJ132MeUjug9KPmZdMJf8UhOKjMWM+IoWJZd/KepuRFwoBN/8M6ASzwfYugLQZgRl5Xrjin38hN9ur+FQ3Nuf+shOb1Tl5XdVfGnEl3XcNn7mn/vhQ2YZLYTJV8PKefDST0wO2RkZkvizUbPNCMtIXvP7aCp1VVD+Hz8MKwhCqyDOV4BoSBFq96MdHUGVqQBkOCo5X8LupXQv+NFnfumJbOR3AbSZFqt+pxE4Qcr2M+IJjEByC8La+Stt4dVeQ82yuiJRABZ2gx6Dk/veGXC+7CTim5eaZTjfb7tukdf3VrNc8oyfM9Yc/nwMvPGr5p/fHD58GO451g/1CoLQKsg3b4BoJEQI63y103yodOJr5tOmCrl7TMSXsLsp7pF78WI7b2TNds/5SiG+ZjwEJ/069fnW+driia+eB3j9VRiRNOBIuPILcz/dPfHlDk6pKjdCrWabEV82x2zLCrN0R4Fu8tywHRv8fUueye1zpqJilS/yLJ/PhbVZisbuCpUbzP8ftpaZIAitgnzzBoiEFGHdgUpNhB3xNfho2O8wf1vqfAltnfhoxe0m4d5O9O1S3DP1tEXu+Zu9xPpenviq9sRXSW8/lHjAiTB4Cuwzzj+/arOpVwYB8bXcuF7uaMstK8woyMoN5g+bsv0y1w7LRKwRGmv9shaWF38M//pl8/rMBZun5s7XKQhCiyPiK0A0HCJkRwC227Cjm/OVIewoFe6Fto4tnFq7I73zlfF8K76s8+WFHa3z5Y6WPOhkmPl/iWHLneV+mQlXfFV8ZQRfghjUcMcBsPJNk/w/diasfMN3ydLx7p+S29hSFq4I0hoqN8GOdVk/Nus/NvNcBln8FPzjJ+nPszXVgqJPEIQWRcRXgGhIEcYb8dQhwo7ZEu5FfAltmGDY0Y6OzJVwFCIFxjXL7+KXtKjeagSYK74sPYaYQQFl+xnxs32t2d9lH799rN6sWzFY2hdO/q0pvLpmvnHURp1hji2dm/7+qrbA36+E9/6UuN+W1XBFUN1OM4+kW+YiHf+4Bp69LHFfzXZ4/Hx45w/pz4s7XyK+BKE1EfEVID/sz+eo7Yin9kaC+MogriTnS2jrJIUdm+h8uX2U7OWHJyu+NGUlUomvQ78HF75unK6d5f6cjoVlJoHf/kFTUObfT1F343RFCv1rddvPzF355dvp782GNN08rnf/BKveM+tVm/3pkGweVuWGzH8Yag3rP4JtqxOnUrLzXwLU16Q+t2abf11BEFoN+eYNEMH/T237znZaPDHX0Y5S50to69g6XbXbTRmE5ogvO+Kx71i/ztiWlWaZSnzlFZvRk0U9jAip2W7yuyL5JjHfnuOGHQvKTKkLO5qyZC+z7D8RvnoneT7Jxnp4ZDp8/LjZXrfICKq6ncYJe2ym2R+rTxZEujF1WPCt38Lrt5mQaM225JyxTU55DCsog2QLO1ZuhGculcKygrCLiPgKsLHCnzakfHs7nTYkIeE+W9hRXgGhDWNdq/hoxxQJ99mwoqX/xP/P3nmHx1Fee/idrWqr3ost994LtrFpppjeQyeFBC6BwE2AhBS4qTcEErgkJKEn1JDQMWDAYDrGvfduq/feVtq5f3wzmtnVriQ3SYbzPo+e2Z39duablbT729853zmqZpY7xqj7RXjxZRKbqsJvLbVW+NP+HHvY0WyblDZGbTvF12xVV8w8X1OVqllWs1+FI1c8qfb7G6Fih/qJNH/7CsRweV+r/glf/FmFPU1qbQ3BWxus22Y5jFA6w45V4R/f84mqa2aW1RAE4ZCQIqshVNY3QZRxu66RMJV/Bj69TbhPHyvhBWFg44kDNOXUNFVaIudgMN2cQbPVNjrJSnDvTnzFpCqR0lxtOXD250QlWmH90J6VPkN85R2ntoWr1f/lQ9PhnAesxuJ+W4/IojXh/18bK1QNMrsb1VAaPKatSa3ARIflj1n7awsgNk2JyDZbxf3mCM6XWd8rUs6X6ZhJKQpBOCx6Jb40TVsAPAg4gcd1Xb8n5HEv8DQwDagELtN1fa+maacB9wAeoA24Q9f1JcZzPgKyANO/Pl3X9bLDvqLDxB52rKpv7GbkAKa3Ycczfnf05yIIh4PDocJ/OxYroZIz7dCPlWYIo6hEKNukbvfkfAFU743gfCVa/2tmLlmo85WUr7Y1+y0Xqnh9cLg/cbCqqL/s76rURSimEOrO+SrfChihzf1L1QKA+iLVBskMYWZPVYKyqaJn5ytS2NEMgTb0+1u1IBzT9Bhz0jTNCfwVOBMYC1yhadrYkGHXAdW6rg8HHgD+YOyvAM7VdX0C8E3gmZDnXaXr+mTjZ0D8N9vFV3X9VyDs2J34EoRjgSEnqARygLyZB//8a16FCx62Quz2mmDdOl9GNfuqPcGOW3dhx2Enwyl3qUbdAN44Na6uUOV+Afgyg8OBiYPg9N+qUJ7dtTJ54UpY928liMyK+qErHksNMWmKv0v/qZL/awusMa31VhmNcOKrvVXliYGR61YLG18OHtMipSgE4UjQm4SfmcBOXdd367reBrwAnB8y5nzgKeP2S8B8TdM0XdfX6LpurNNmExBtuGQDllf/y3pzr2kU8SUI/c7IBWobl6FcooNl2Ckw+QrrflRi+NuhmM5XW33ksGNnwr0hvlxeOOH24IUBCbkqJGjW3WqtV9XrOx/Pg/EXw9CTgkODdl69XjlWvgwVRrSLKoCyzUpsXfce/LQQBh2nzmsXeS216lwQXnyZwsrpUeJr48vw0negrjj4GPDVCDtW74PVoX6AIPQNvRFfOYDtP5gCY1/YMbqutwO1QErImIuB1bqu25ud/UPTtLWapt2lab3tN3J0yfZZYqXqmHW+DPfOFdV9wr0gHAsMP1WVRMmb2fu2RN0Ra3tr6q5pdYxtXKSwY6fz1Y2IS8hThVdNYdVar0SRz2hNlJCrruusPwX3irTjiVOCJyZVhQ/XPmcJh8pdsGUhZIxV8/TGWcctXGMdo7FctUPSHOFXO5q5cUn50NZg1TdrsyXqd4qvARGo6J79X3YvrlY/BW/Iyk2hf+iTpW6apo1DhSJvsO2+yghHzjN+ronw3Os1TVupadrK8vI++LYVCA47FtYcg/+YgXZAU2/kUkRVONaJTVEFTOfdfmSOd+JPYPp1MOeW7sfFpFq3vfawoyG0opO6Ol/hiM9ROV2gvhC1NSjnK38uzLtNuV4AqcPh5uUw7VvWc0/7NYy/RD2ncJVyvS55AgbNgfd+rtyqhbeqx8/4ffB5M8ZB7X7bDl05eFGJ4Z0vu/gClesGVsFXsImvHsKO7a2qkn5dL6rxHy1WPAGL7478uBm6jbT4QBCOIr0RX4WArd8Guca+sGM0TXMBCajEezRNywVeBa7VdX2X+QRd1wuNbT3wPCq82QVd1x/VdX26ruvT09LSenNNh4ctZOeig8WbelFNeqARaFfhRqdbwo7CV4Op10D25CNzrIRcOOd+OP033Y+z9220O19JQ1TCfHxO15yvSOcDJZzSRivhU1eocr3m322tkAQlfMxkfc2pBOLsm9T9llpIHaHKb5z+G3V/5RNQvg1Gn6NCjXZGnNZ1LmYOWljxZThziYPU1lwRaneGOuuA9fBFeO9nqpL+tre6H3eoHFihVod2R1sjNFdFbhNnLlqIVPNMEI4ivRFfK4ARmqYN0TTNA1wOvBEy5g1UQj3AJcASXdd1TdMSgbeAO3Vd72w0pmmaS9O0VOO2GzgH2Hh4l3KEsDXTzvK5WLyltJvBA5RO8eXpvr2QIAiRcbqtcKI952v4fPjhRkjIUQ7U5Ku6F4am+MqeooRT5S5VKDU+O/x4s7aZJ1aFI9Nt65vmGC2DcqZC7kzY8JIKAZpulZ1Bc8DjC97niSC+6ktgzbPqtplX153zFWm1Y/U+tTUXF9TsDz9u1T+tvLeitZHdp0BH+Gr+T5wKj54U/jkmZri0YCXs/qjr4/XGe7s4X0I/0KP4MnK4bgbeBbYA/9F1fZOmab/WNO08Y9gTQIqmaTuBHwF3GvtvBoYDdxu5XWs1TUsHvMC7mqatB9ainLMwy3z6AZvzNSTJy7aShm4GD1ACHTbnS8KOgnDImEn3dudL0yzhFJsCF/yt+56TZpJ79lQl4sxk+bj08ONN8WWGNN1RMOsmFXr1ZVrjsiZCqfGdNZz4cnlg1IKux45O7Co43rsL1v9b3TadL7PEhd35MsVXSw20h7RfK9sCD06ET+9X5S7AEl+Fq1Uo0jzGwltVzlX1Xnj0RHjrtnCvBPzjzK6hw9BQ5ge/gbd/3PW5ZnPyfyyAp8+35m7SnfPVUgernuramUAQjhC9iknpuv428HbIvrttt1uAS8M877fAbyMc9jAK9hxFOizxFePSaWxt72bwACXQrkSXN956IxcE4eCJSYHKncHO18GSMRby58HY85TY0A0nx55TZqfT+bJV81/wv13HpdnDlUPCH+vcB2H2zUrgmMeOTlLum0ljBWx+zXaskBWloeLLG6/Cj02VEJ9lPWYWfv3gV5ZwrNkPjZXw2Mlqjmfdp8K1oJyvz/+sbldsCz//0k1dE+L3fR58f8e7SiyddW/w/raQOo3bFsHIM9Sc0karkKR5TaG8cTNsfl0J3Owp4ecWiZZa+OssuPgxldcnCGGQhKBQbM5XtDNAs78Df0cAt/MYasNjhh0vedJa+SQIwsETE8b5Oli8PvjWm8Zt2/9jbCTxZZzL3Y2bBsHhyHDOFyhHLtkmzDxxSlA2VihXR9Ng3QtqQcCkK1R4LlTImSKmvRXaWyB9jMq3aiwLFl92seNvUnOq3meFOKv3wHOXqDAtQNUuKNmgbjfXwus3KZdw7g9VyY62JhU6LN+q8rbMldum+DJFXF2REl+BjmCnvy0karHpVVWSY8WT8H1bs/NwYcfONk+HsLq2tkAVuC1eJ+JLiMgxpCj6CFvOV5RDWc7HnPtliq+0kZHzSgRB6BmzLMXhOF927E50TGg1npAxPTURTzcKqnp8wYsDQvH46BQR3jj1vLZ6K6dr1xJVnPXCh+G2rWrxgD1XtKUGdn5g1QEzxVlo/0ezd+S0b8O3F8HUa1Xo0kzOn2AER8xQaeEqJebSRqtVmWuehY9+D/+bo3LCzHIWHW2WGCpaC+uNZuTtLcoVa6pU79uheWih4mv3x+o4bfVGKybb9YViXqseJt+sJzpbNEnrNiEyIr5CsTlfUc4AAPUtEcTXgeXw5g8HXl6AmfMlCMLh0el8HUJPyXCYwsrhilwbLFzYMezckiE2XTlM3dU/czgs587jgyxjcUDRGpW3tX+p6iJgomnBlf8X3w3PXgQ731f3TSetuQpqC+GhGXDfcCheq/af9FMYPMdK3C/foraTrwJflhXy1NX7K+MutM519v1qrns/Dy5nYQq2hbeo38WES5XTZtYig66FZ+1OXM40aG+GghXqftkW6zG783VghcoPMwWjv4WDprflOISvNSK+QrGtrOlRfO18H1Y+GXkpc39h5nwJgnB4xGcDWvdtiA4G00GLSbHaHXUZYzpfPYQdASZfCeNCG46EoXPVpk+FK50eJb4KV6kQ4ZB5wePDXa9Z2sHufO1fChXblVg5sDx4/qb4Ktuqtp5YJWbtjpTDpcpkgBKRM65T26aKYCerZIMKXxavV3XQUkYo58u+mrLOJr7a26zaagBDT1ZbU1SVbVZbV3Sw8/XMBSr0arZZau9FnceKHcFfwFvF+RJ6RsRXKDYh5dWU+GqIFHY0/7k72sI/3l+YYUdBEA6PyVfCt97qPqx3MHiMnK9IyfbQe+cL4LRfwQl39DzOdO68cWoVZNoo+OLPqm8kGgw+Pnh8OPFlhurMhPzmarUYwaT2gKqeb4ZLzZWZ5vPcMcHdBQBShqu5eOJgxBlqX0yqco3MsKPHp0KkB1YAOgyaZb02Qee3iS9/SLL9sJOD75dtVXXUkocq56u9TQnR0FBleyvdUrweHpoOez629pnOV2hYNpQ9n/Z9g/LCVar47UCL1nwNEfEVii3s6HGY4iuCs2UKtcBAdL5EfAnCYeOJhfzjex7XW0xhFSpCgs4ZUmriSBCVYAgj45h5RkHWwXPgG091FZfhxFfhKrWNy1RzbKpU4scc21Cqrs8MgZqhzgajULXpfNlJG60S6b+3BObfpfbFpKhjNxguVcowJWj2L1XvaznTrOuo2K62oU3E7SFHhxtyZwS/J5ZtUcVsY5LVsVc8Do+d0vWazZWWkUKIxeuMeeyw9nWKL2NRwwe/hoqdwc+rLYCnz4NP7gt/3KPFljdV8VvT+RP6DRFfodjEl9fRQ9jRFF8DMuwo4ksQBhym+OrO+XK6lKA5Um4bKPHlsQmjU34B310Clz8HY8OELcOJr+YqFTZNGQYxScrZqdihcsg0I83BXtTVdPnMYqaeWGuFZ0yqejzHqDiUNipYmDZVKufLm6DqobXUwIFlkDVJuV7msSu2Q3SyClWGE1/DT4WJ31CrJ+1N2dvqVZPyqAR1bHPVZepIFZI1aW9R6SV/HAk7FsPDc6F8u/W4WSLDfu5O8VWpHLtP/9RV2K15VuW8mTloR4rGyq5Cz445NzN/71Bob1W11cyFBcIhIeIrFLvzpan8r8jia6CGHTsk50sQBiKdAqOHVmnfXNhz78mDIXGwqshvEp0Eud2UWoyU45Y7Q723RCcrMVa5S7U8irbllJk4nFZDcDDCjob48mXBTcvguP/qeo6YVCV6qvdBXJohkGpVXbCUEWqMGXas2KGuKyEnWACZKy9nXq+K4EJwyQ1QDp5ZcLZqFwyeCzevgMyJ1hh/s2qVpHfAJ39UIu3fV1thO1OIBZ3bECXN1VZOWmutldgfCFjdBEo2Wkn9z14CXz7c9fU4GO4bCg9183s9EuJr76fw6R9h+zvdj2usUNcqhEXEVyg28eXuMedLnC9BEA6CTvHVjfPod/cEAAAgAElEQVQFkDnhyDpfp/wCvvlm78eb4ssbsspz0Gy1jUlWeVNt9SpvqzOhP6SuoNcHGELFHWM5ftGJqu2Sy0MXzNembItazRmVqERDc5VVnsPsKFBXqMKHcRkqf6qxQgkeM3fL3nkgbbQ6litK3fdlGseuUf0xUw1hZxdp7a2qvAVA4Uq1rdhmhRvLt1rzMDEFjh6wVmkCbDVe/5J1Kj9u1FkqZaV0o/rCvGuJlTu2bZF1jt5id+QiYc5t31Krl+fBYq5WtRfqbaqC3w9S7a5AHfu+YbD4rp6Pt+IJtbr1a4aIr1BMIeVw46Qdp0OjoUfnS8SXIAi9IDYVnF6V6N2XeMIku3dH6ggVQgytdm82745JsVYXpgyzibWQjhrmfVe0Wt1pCqvuVo+aAq12v+V8NdcoQWUKUvtK0Ng0FZpsLIdFP1aLCMywo118nfhj+O77yrUDJb5iktVqz5YaFXKE4CKz/mZrlWeg3Qqv1uxTj5nOVq1dfNnCcaZwA2vhwfZ3AQ1OMrrwFaxUwlHvMDoCVMC/Locnzoj8GoVj9VPB8w5Ha51yIwN+lfB/KJj5bVU28VW6Sbl7L1+nXEFzscGKJ7o/Vocf3rlThWa/Zoj4CsV0vtzRaIEO4rwuWe0oCMKRIToJblkTXNtqIDL2fLh9m9V/csQZMP9u1awbLAEDkDnJCjt6wjlfWGHCmN6IL5tITBpiHNtwz0zxZV8JGpOiHLKAXwmZyt0258s2n6gEI0RqnNuXCaPOth43na/Bs9XxQLlc9lIUZj20+hLD9dJVvll9kVWmqKXWev8tXqfEoS/Lyn3b/q4K32ZOVI+VbLDqldUcgJX/ULd7KnNRtlU5diZm0VyIvNKypVZdgzu2a+ixpU6de+cHSij+60o1fs+nweFDc4WpfaWp/dz7vrBCrz1dQ+VO9flZsCJ8A3U7hauNFa9fDUR8hWKKL5cXOvzEeV3UtRxrqx0l50sQBiwJOQP//1PTlGtklo1Iyod5t6nFAGCJoIRBhjtlhh1DOgF0ii/DgeqN82V36AbPCS5wGxp2NI9pisSafarMhNk0O1zDc3PucZmQbuuPaYqvYafAHTtUmNRMiDf7Ow6apVZP1hdb4mXCpepzo6EU9n+pcr3MZuoV29TtuAy16rOxEopWw4jT1WucPlatPKw3xFdrLSyz5X2FrrIs26JE18on4W/HwROnW/ln9jBis018bXgJnr9c3W6pVa/X0BNh5+LgkhOf3Adv/rcqqPv4abDtLfjoHnjqHHjv59Y4e9jRfL69Y8CBL3ufjF9ihGVb66wQbiTe/Rm89aPeHfcYQMSXnR2LVZ8zUG86gXZ8Ua7IYcdAP+V86To8MB6WPRr+cXG+BEE4EpglHUJ7W5rOlylYorvL+cIKE8alq5IXplgKh30laN5xwZ0ATPFlDzvGpHY9nlmCItSJs8/VrEN2/ccqMT9hUPA4V5TlSJl10NJGqefVl8DWtyFnuvoB5Rg9eYYKx5khTFC5bb5MJc72GqG+oUaj8/SxSnTYE/abKixHzlyFCarX5VPnwePz4W2jtltLjRJzoNw+M5/NXuB15/uwfZESZ2Zj9LzjVIjTdKhaapXrZYrMJkP0meHLL/+mFkD4m1W+Wkyqem5jhXLGKneoUHpcpnIe7c3Ku/t8LN1AZ+ur/V9GHgfKFbQX1T3GEfFlZ/VTVosMlxJfAzLsaP4DLIpQXFHElyAIRwLT+Qptr9RmuCwpw9U2Ys6X2dbIFHEJKvF/6rWRz2k/RnRi8Lmjw4QdY1OtMKGJWW4hnPNlDzsCZE+Gs+7r2nHAHW29t484XeXA5UxTzytcrUTP6LMh0RBtW96wnps2EoYYAsvMSas3xJc71hI5GWNVztn+pcHnnnGd2trF14rHVPkNzanEzy1r1Pv85tfV4631VjkNe9ix5oDaVu1W54pKtMR0W5Pabn1L/U7P+mPw622uVAVV6qNqN6Cr1wPUIoKnzoEtC5X4Sh6qxrTanK+SDZFXPZZshMzx6vdXsDL8GICOduUOttQcfImL4nXw4e8P7jl9gIgvO/ZvXO4o6PAr52ugrXa0f6sIh4gvQRCOBKbDFCq+hp6ktlOuMh7vIefLXjA2//iuIs2OpimHa9KV6n50OOfLnvMVwflyRYcP78ZlKoeop3IfpouEBvnz4GcFSmj5Mq36XiNOU+6f06ucLxNvglUqJDpRnbOxHHZ9qEKpTqNxefpYtd25xFpZ6o5Vws2Xba2W3PsZLPmtEj03LVNFaZOHqt/D+v+ochWtDZYQtIcdaw23yAzxRSVYv1e/Ib7qS6z55NhKVTSUWrdLN1nJ9mbHAHuPzKQhlviyf0Y9djKsNBLvy7eref77ajXvkg2QMUGt7rWvDA2lvtjqBVp7IPK4cDxyAnx8z6Gv7jxKiPiyY/9ndEVBwE9clLsXdb76WHzZv1WY3ygeOQE+f9DYJzlfgiAcASI5XznT4Je1qugphK/zZb8fzoHqjh/vtupzBeV8Gc6Xw6nEFagcsegklYtl0lwV+ZyzboRvv93ze6Qpvrzxwa6YL0ttPWafTLdyb3RbwnhzFQyfD1e/DHN/qAq6oquQpL2PZtootW2rVy6YOxbyZqjcuswJlmv02o3K1brwEYjPsmq2HX+rEiar/mE4X4b4aqpWIdN9X1ih01Kb+DKdQ3NhQnO1EpDuaCuMCpb4ShujxJeZZJ89VW3tQsjhVGU6GkqCG56Dcp92fQh/naHyyrYshI/vVU5ezlR17eXblMMVyvb3YNU/rfvdhR43vARv3W7dtztuyx+Fh2aqVlIDABFfduy1dxxuCHSQFueluLaZD7eW8dcPQyoH91fY0f6toq5A/cEWr1dWeCCgctHE+RIE4XAxxVdoIn0oPZWaOJRWSZ1tikxhl2A5RmAJiNg0NdZ0vyLlqZnEJAe7O5FwR4U/jhmuzBhnCThThJp5YynD1JyGn6peg7gM6/nmiklQjw0/zbo//y44/r/V7czxysHb/o4SHCf/tGvttyEnQN4sWP20UYojxWr99NE98M+zrUVkZggzKt4SpmbYsblaHVvTVCjVl632N5QpUZY1yRJfvmyj4TyWEMqapPLmUoap+8Vr1e/htF8bE9Xh5e+qmxteVNtKw0XLmwnp41Qzc3v5CpP3f6mKuprURHC+/C2q1MWKx6zrKllvPb7xFeVYVvSiHlofIOLLjl18OV3Q4eeEkam0+AN8+58ruO/dbej21SH9tdrRLr7KtxvJlbr6BvT4fPXHJc6XIAiHi9uWq9UdZi5WT6sdDwVvPKCplkZ2PLFKGJihztg0JTxMB2zsBYd+TrCctS7XZNxPtCXoZ01W20mXwS1rYdp3gp8TZwi2qITgCvoAlzyhRNSUa5QrZ4b0Mico4fTOT9TrO/qc8PNMH2OUetDVgoeYJOW8lW+zQnVgE1/2sKNRD6252hLQ2ZPhViP3ub1Fvc4Z41TO1YHlkDpciXKH2xJfV72khJdZv65orTrP8bcq16x6n5XEb8cdo4RXhhF+3b80uEZZIBC8ktLhVitat70Dq59RUR5/s8pxe+dOa5wZFt72trWvdFPwtp8R8WUnJtT58jN7WAqxHkvIVDfZhNZAyPkq32olRdbst1a+2GPxgiAIh4Knl+Irdwac8b9WLphJZ8L9YYgvh0Mdx17/C5SAiE21HLKEXFUU1hQUM68/9HNCz86XKZJAXT8oEZY8pGvyvs9wvgYf3/WLcVSCaidl5s+ZZExQ25r9aoGCyxt+njEpVu6WJ04JtaYqy1kC5VaZ9cqCwo5hxBeo/pZm9MQTp4QgKGcqZbh6zaMSrBQY8/dsFqhtqbH2RSdZ5SmGGq+ZKeqzpyqjI9UIvy68FV61tZxqKAmuFZaUr0Kdyx5WhVlfvwl+l6kcr1X/sBzNsq1qJeaXf1f5ekBnrbiyTSr02M/hRxFfduw5Xw4XBNrxupycMNLaX1rXYo3pt7CjrehffbGKm5u3QeUqzLutb+ckCMJXj1FnwSl39VyR3+mC2TdZgsXkcMKOdqITuoovT0zwvgX3wCX/UKspL3pM5UYdDvacLzujz4HvfgCTrrD2ZYyF73+pQnbhiMuE+JyDK65rb3M068bI4+wRG1OkVu0OLjcxeLZ1OyohfNjRLr7MOm+gtvnzrNehc4WrEQ52eoOFqvk5agr26ESrjtkII8Q6eI5aVDDOcCfdUaqQb1SCygerL1Gh1M8eCL7WxDwVdmwsV+Jq3b/U/l1LVAut77ynhGPZZlU6o7Uezv5T8EKQ0s2w6VX40yio2hP5dT3KSGKQnS5hRxUrv/vcsUzOS+T3i7ZSVt/KmMAq2PF+PzpfxrcNd6yyhUML8V39ilpRJAiCcDjEpcMJt/c8LhKhFe4PlZnXK/FiZ+SZwfcT82x3ZnPYdIqvkDw2TYPc6V3Hp4/p5lge+NHmgzu/wwnTvq1+B6bbFg67APXGqdytXcbKy7xZ6sv6qLNg48vGmHgrtGdPuA8tfOuJM+qCxanPw+P+Cz651wq3muIq1BVNHqbEUZTN+TLJm6V+j4OPh3khBVOv+o9aTfnQdLV4bMXjlrFx5r1qwcGWhSqqowfUIoWkIVC9R4WbZ3zPctHKtijRlzTYqs1mLhYo3aQWR3jirNIc/YCILzv2lhmG8wWQlRDNmeOz+P2irWwprmNU+QtkbnzU+qPvj7Cj06v+qP3NKinSTlJ+385HEAQhHKFFVg+VOT/ouu/ECHUOjxSRwo59ybn/1/MYexK+16eq8JtJ7ef/1UqC//helQvliVOFbkGFK81ejKHiy3QrTQfspDtVPtjIBep+p/gKeX2Sh6oq953Ol+24cWnwg1XKnQpH6gi1SOHLvwXvn/FdJUYLVhgrMI1Qc/VeNZ9TfmE5celj1CrPhByr04AvS4mv3JlQsFxFiebd1jU83IdI2NGO06ZFnV7lKhmkx6t4+z2LtvLuWsOqNOuG9Mdqx6gE9ebgb7LCjqD+qH2HabcLgiAcCcwP3p5yxgYikcKOAw17rrInDiZfbd1PGqycOk2DG7+AH21VgsMUVm1N6gt8R2vXlZSdYUcjZOdwqrCqmbMW0fkyQtTm6xbUoSDVSNbvZkHYBQ8rRyrfVpLDHB+fpVyvzrIeulppaeakgRJfdQUq78t0tkzncP5dVreCSZdHnkMfIM5XJOLSjU7zOmgaUW4nCdFuapv9RBOS69Uv4ite/RH7jbCjsUCAxEH9quYFQRA6SciFK16wqr0fS7gGgPPVG4LCjj71pfwbT6vSQ/bSHE6XlQfncKjVnG0NKuQI4cOOEHmxRCTxlTI0eL/pSLljehd+jkuDm1cAGjx5enAkJ5yxEFos1yxc21prhUhN8ZU6Uq3MrNpttcbqJ0R8RcKXpb4NNFV1NnpN93mV+NJag8cGIhRhPVqYzpfDpZyvhnZIG62K6EnIURCEgcSoM3seMxDpbY2z/iZUfAGMPV/9dIcnVn1+mNXwu4iv2OBtKJGaqZvOV2jOV+iCie4wV3Ze/1Hw/nC5b6HdDezN0s08wFFnQUO5amPkcKgaav2MWCShmCs5zG8I5gpCICNefROKIsTp6rewY7SyjBvL1HzTx1qVhwVBEIRDp9P5GuAhU0+MLT8rTCPx7p7X1mhzvkLDjiE5X6FEdL5GqFwrs1SG6XwdjPiKRFjnK0R8JQyycgxN52vwHLjokQEVFRLnK5Qbl6qYcvE6db++uFMlp/uUGo8mxPnqa/HVWqcUfXsrNFaqby4Z4+GyZ6WyvSAIwpHgWMn5AiVs6lott643eOJUDbGP/qDuR3S+Igi6SAn33jj4oa1P46E4X5GITVeLBfSAtQ11vhwO5X4VrgouhDvAkE/qUFzGKgzT+bL1qDo9vYaY6C+JDhVb4fpRHU1M56u1XtnGTVUqWTJSET5BEATh4BgIqx17S0yK+lJuFpztDe4YVVHexF5qCQ495ysUU3yFHv9QcLqUAGuqVJ/RNfu7ii9QSfdFa602SQMQEV+RMNtBmN3egQVNb3K68xm2dmQEj+2vsGNHuxJg7c2q75kgCIJwZIjUXmggEpMSXFS1N9iT3y97tms+VU85X2Y40b6aMRxRRzDsCGqemkOJsIby8M7cnFthyEnBFQwGGAN3Zv2Ny6OWxdbburM3V+Nob8FHU9DQkuo6Sg7UMDmvhz/CI0F7myqB4fWpZcJmv6zQQoCCIAjCoTN4tqpmb9bJGsiMOO3gF1uZoiVnGow5t+vjPeWRRUq4DzcueajVfPxwyRin3DR3tCrmGs7tSxupfgYwIr66Iz4L6qyEezMxMVWrDRr26dZi7tj4OVt+vYBoz1FuaG328HLHqj8+s3GqiC9BEIQjR/JQuPy5/p5F7+iu/VAkTHEVqcp7T2HH9DEw7iLIn9v9eRwOuGXNwc8vEuc8oD73yjarnOdjFBFf3eHLDlrtaPZUjNaCw4z5SW4oh7oWfx+Kr+jgfmkivgRBEITeYi4oiOSY9RR29MTCpf844tPqETO32WyifYwycNZdDkR8mUE5XzTXhB2W41Matr6lDxLvzUaontjglS0ivgRBEITe0mpEcJIiOV89rHYUDgsRX90Rm6ZyqgJGaM+shxKCW1OtDhpa+0B8+RuNk8aEiK9jIClUEARBGBg0GvnCkcKOg2bD+Iu7bxguHDIivrojLl3FlpurlABrCe98uVGiq6FPna9Q8SXOlyAIgtBLzEhOuKrxoHKeL3kycthROCxEfHWH2TOqoQza6q3k9hBcpvhq9R/+ORsrVD/JSHTmfMUE53wdC7VoBEEQhIHBhQ/DpCtVv0OhzxHx1R2m+Gosj5jvhebApSvxddg5X3VF8KdRsGtJ5DFB4kucL0EQBOEQyJ4MF/4dHEd5kZgQFhFf3WFWzm0sj5jvhTu2U3y9u6mE0x/4mBZ/x6Gdr7ZQNemu3Bl5TLiEe80R7IIJgiAIgjBgEfHVHXbnK0K+F54YHLoKN36yo4LtpQ3sqWg8tPO11lnni0RQwr0huLy+g2srIQiCIAhCvyHiqzuiElWj6oayyGFHTyyOQDtel4O2dpUTdqCqKfzYnjDFV0NZ5DH2hPtjqfGrIAiCIAiAiK/ucThUi6HGMlvYUQveumOhow1flFWv9kB186Gdr8V0vioij/Ebxw51vgRBEARBOCYQ8dUTcWlKDJlhR1+W2ppNRT1KfMV6beLrcJ2v0LBj2VboMFZS+hvB4Qan28r5EvElCIIgCMcMvRJfmqYt0DRtm6ZpOzVNuzPM415N0/5tPL5M07R8Y/9pmqat0jRtg7E9xfacacb+nZqm/VnTBmjSUmy6EXasBqdHiTGwOrR7YiHQTpxNfBUctvNlhB1rC1TvqoePh5VPqn1tTVY3enG+BEEQBOGYo0fxpWmaE/grcCYwFrhC07SxIcOuA6p1XR8OPAD8wdhfAZyr6/oE4JvAM7bn/B34HjDC+FlwGNdx9IhLVyUgyraoSsBmflV0stp6YqCjLUR8HarzVa+2jRVKdP15CnzxoFoBuf9L9Zi/UYU6QZwvQRAEQTgG6Y3zNRPYqev6bl3X24AXgPNDxpwPPGXcfgmYr2mapuv6Gl3Xi4z9m4BowyXLAuJ1Xf9S13UdeBq44LCv5miQOwMaSlTtrSEnWELHdL5Ccr40TYUd9e4KpUbCDDu2NUDVLuhog10fqn1Fq43HmizRJQn3giAIgnDM0RvxlQMcsN0vMPaFHaPrejtQC6SEjLkYWK3reqsxvqCHYw4MRp2ltoH28OLLEwsd/k7na1SGj8a2DpbvqTr4c7XUWrfLt6pt6Sa1rd4LTVUq4d4MOzocyoEz65EJgiAIgjDg6ZOEe03TxqFCkTccwnOv1zRtpaZpK8vLu6l/dbSIz1LuFyjx5YkDNFvCfYwSX4bz9b3Z2ZybtI8bnl1F48E22jadL4DybWqr2wq2Fq0JDjsCfOddmPODgzuPIAiCIAj9Rm/EVyGQZ7ufa+wLO0bTNBeQAFQa93OBV4FrdV3fZRuf28MxAdB1/VFd16fruj49LS2tF9M9CpzwY/UTkwxpoyF1hCrA6vGppPeAn1iPatEwq+Jl/tL8U5Ka97OvMiT3q7kanr8c7h0G1fu6nqe13nLUTPEF6jwON2x+PTjhHiBtJEQlHOELFgRBEAThaNEb8bUCGKFp2hBN0zzA5cAbIWPeQCXUA1wCLNF1Xdc0LRF4C7hT1/XPzcG6rhcDdZqmzTJWOV4LvH6Y13L0GHk6nPJzdfu46+HmFTDze3DDx2oFJJDgUTleyaVfADDXsYGakt3wxi3Q3qaeu28pbF8ETRVWOLGjHZY9osKJLXWQMkLtL9tsnT95CEz/Nqx5Fko2SCshQRAEQTiG6VF8GTlcNwPvAluA/+i6vknTtF9rmnaeMewJIEXTtJ3AjwCzHMXNwHDgbk3T1ho/ZoLS94HHgZ3ALmDRkbqoPsETCynDlBsGzEzXmTUojqji5QDMdWxk2Od3wOqnoGCFeo7f5oQ1lKrtisdg0Y9h1T9V2DF1BGhOqLMZgfHZcMIdKpu/o1XElyAIgiAcw7h6HgK6rr8NvB2y727b7Rbg0jDP+y3w2wjHXAmMP5jJDkjiMgGYntzCC3OK4LUmdF8Ws+s2E2g0ylFohsb12+p/mS2E9imnDHe0cr6ik5TYqrWtcfBlqqT6nGlwYFlw2FEQBEEQhGMKqXB/uPgy1Pbje+G1/wJ3LNq824jXmkhsMRZ0mon0QeKrRG3N8GNbo3K1ouIhwUiHM0tJ+LLVNn+e2h5KGQtBEARBEAYEIr4OF8P5Yu+nKjH+hxth2CnBY8ziqWbYMSlfOV/+FlXPC1QhVwBvgiW+sqeqsbnT1f38uWpbsv5oXIkgCIIgCH1Ar8KOQjfEpQOaElaZE1QOWFQijVossXqjGmPW7zKdr8TBKuerZIN1HDPHy+58JebBd2ypcHkz1XbswKxHKwiCIAhCz4jzdbg43VZ5iKQhautwUBQ9qnNIYWkpn+4oR/c3gStaNeeuL7UKqYLq4wiqbIQpvswWRiaeWLirAub+91G6GEEQBEEQjjYivo4EPiP0mJTfuasqYVzn7de+3MI1TyynrLIK3R3Nyko3ekMpVGxTpSp82VBjJNhHJ0GCUVYtJrRJAErsCYIgCIJwzCLi60hgiq/kIZ27KnNPYVcgi3bdwfD4AB6Xg6KKKpp0D4v26mgdrVCwElKGq2r5ZgJ+dJLlfMUk9fGFCIIgCIJwtBHxdSSI6+p8jZ55OremPUYxqczOcTNveCpV1bU0BjyU60Zrov1LIXWk0bLIIDpJVdE/8U4YfW7fXYMgCIIgCH2CiK8jgVluwia+hqbF8eYP5pGblUG81swZ4zPR/U1UtDrYr9saYaeOtJp1A0QlgsMJJ//UOq4gCIIgCF8ZRHwdCYbNh1FnQcKgLg9p3gRorefcidkkutpp1D2s1YdR5MxRA5KHgNdwvjxx4PL04cQFQRAEQehrRHwdCfKPhyv+Bc4wlTui4qGljmiPk6GJDlrwMiEnke9xtyoZMfxUy/mKlhwvQRAEQfiqI+LraOONh1ZV5yvZ08FxI3M4e2IWmxp91J33uKoT5hHxJQiCIAhfF0R8HW0M5wsAfxOeqDiGp6kw492vbcTfEbDCjiK+BEEQBOErj4ivo403XrUX0nVV4d4dzUmj0rh8Rh6vrS3is50VEnYUBEEQhK8RIr6ONlHxoHeoxtn+JnDH4HI6uOnk4QCU17dapSZEfAmCIAjCVx4RX0cbb7zattZ1Ol8AKXFqVWNlQ5s1RsSXIAiCIHzlEfF1tEkarLbPfQM62sAdA0CMx0WMx0lFQ2tnzlezK55rn1zOluK6/pqtIAiCIAhHGRFfR5uhJ8Npv4HSDeq+4XwBpMZ52VfZxF+/KANgaVGAT7aX88Di7f0xU0EQBEEQ+gARX0cbTYPxF1n3beIrJc7DR9vKeGm7H4DXdoPbqbF4Sym7yxv6eqaCIAiCIPQBIr76Al+2ddsT23kzNc5Le0Bnj57FxfyJNxpG8evzx6PrsGhjST9MVBAEQRCEo42Ir77A4QA0dTsk7GiyqiWLdF8Ul07LZXSmj6W7Kvt4koIgCIIg9AUivvoKX5baGgn3AKnGikeHocsunJqDy+lg1tAUVu6rorW9o69nKQiCIAjCUUbEV18Rb4gvp9U423S+FozP5IqZeXx7zhAA5gxLocUfYO3+mj6fpiAIgiAIR5cwnaCFo4LpfLVYgsqs9TUuO6Gz6CrA1MGq3teGwlpGZ8YTH+1C07S+m6sgCIIgCEcNEV99xZl/UCsfh5/auSsjPgqA4elxQUNTYj34olysPVDDb9/awrjseNJ8Xn5wynCmDU7u02kLgiAIgnBkEfHVVyTkwmXPBu2aNiiJBy+fzPzR6UH7NU0jPyWWJVtV/a9NRaroqtvp4LFrRXwJgiAIwrGM5Hz1Iw6HxvmTVZJ9KINTYmhqUwn3r990PNfOHszH28qpbfb39TQFQRAEQTiCiPgaoOSnqHpgqXFeJuUlcvHUXNo6Ary3Sep/CYIgCMKxjIivAcrgFFWSYnSmD4CJuQnEG3lggiAIgiAcu4j4GqDkpyrna5QhvjRNY3RmPFtL6vtzWoIgCIIgHCYivgYoI9N9+Lwu5gxL6dw3OsvHtpJ6dF3vx5kJgiAIgnA4iPgaoCTEuFn3P6czf0xG577RmfE0tLZTUN3cuW/RhmL2VjT2xxQFQRAEQTgERHwNYByO4MKqo7NUCPKNdUU0tbVT3+LnpudX8/tFW/pjeoIgCIIgHAJS5+sYYlSGD4/LwX3vbuOt9cVcf8JQAjp8uK2c+hY/vih3f09REARBEIQeEOfrGCLW6+LtW+bxwGWT2FXewI9fXg9AW3uA9zaV9vPsBEEQBEHoDSK+jjGGp8dx4ZRcrpg5iLb2AGOz4smMj2LJtrL+npogCIIgCL1AxNcxynVzh+B0aBw3NJk5w1L4clelrIIUBE2sgGsAACAASURBVEEQhGMAEV/HKHnJMbxy4xxunT+CWcNSqGxsY3tpQ39PSxAEQRCEHhDxdQwzKS+RxBgPs4eqWmAPfbiT0rqWfp6VIAiCIAjdIeLrK0BecgzzRqSycF0R331qJR0BCT8KgiAIwkBFxNdXhGeuO44HL5/MhsJaXlx5oL+nIwiCIAhCBER8fYU4b1I2UwYl8pclO/F3BPp7OoIgCIIghEHE11cITdO46aThFNY0s3BdUX9PRxAEQRCEMPRKfGmatkDTtG2apu3UNO3OMI97NU37t/H4Mk3T8o39KZqmfahpWoOmaQ+FPOcj45hrjZ/0I3FBX3dOGZ3OuOx4frVwMzvL6nl5VQHL91TxxGd7eGlVQX9PTxAEQRC+9vTYXkjTNCfwV+A0oABYoWnaG7qub7YNuw6o1nV9uKZplwN/AC4DWoC7gPHGTyhX6bq+8jCvQbDhcGg8fPU0Lvjr55z+wCcEdMhNiqa8vpWshCgumZbb31MUBEEQhK81vXG+ZgI7dV3fret6G/ACcH7ImPOBp4zbLwHzNU3TdF1v1HX9M5QIE/qIvOQYFv5gLpfNyGP+6HQKqptpbQ+wt7KJklr5VQiCIAhCf9Ib8ZUD2JfPFRj7wo7Rdb0dqAVSenHsfxghx7s0TdN6MV7oJdmJ0fz+oon85copxHqcxHqcACzbU9nPMxMEQRCErzf9mXB/la7rE4B5xs814QZpmna9pmkrNU1bWV5e3qcT/CoQ43Hxp29M4uFrpuGLcrF0lyW+/vzBDn75xqZ+nJ0gCIIgfP3ojfgqBPJs93ONfWHHaJrmAhKAbi0WXdcLjW098DwqvBlu3KO6rk/XdX16WlpaL6YrhLJgfBbzRqRx2pgMXl1TyIGqJgDe21zCK6sLpCekIAiCIPQhvRFfK4ARmqYN0TTNA1wOvBEy5g3gm8btS4Alejef6JqmuTRNSzVuu4FzgI0HO3nh4Lj9jFE4NI3fvbUFgILqZupa2imobu7nmQmCIAjC14cexZeRw3Uz8C6wBfiPruubNE37taZp5xnDngBSNE3bCfwI6CxHoWnaXuB+4FuaphVomjYW8ALvapq2HliLcs4eO3KXJYQjOzGam08ZzjubSnhnYzE1TX4ANhXVAlDT1EZBdVN/TlEQBEEQvvJox1LIafr06frKlVKZ4nBobe/g9Ac+obG1nYqGNgAm5SZwy/wRPPLxbpbvrWLd3aeTEOPu55kKgiAIwrGNpmmrdF2fHrq/xzpfwlcLr8vJGeMyefST3Z371hXUct1Tlqj96avrGZ7uIzHazXfmDumPaQqCIAjCVxYRX19Dpg5K7Lx922kjefjjXTS2dXTue3tDCVACwLfm5ONwSBUQQRAEQThSSG/HryFTByUB4HE5uOnk4bx967zOx164fhZr7jqN208fCUB5QysAgYBOW7s06xYEQRCEw0XE19eQ9PgocpOiyU2MxuHQGJQcQ0a8F02D8TkJJMV6GJ+TAMCusgZqm/w8+fkeTrrvQwIBndb2jh7OIAiCIAhCJCTs+DXlhhOH0epXIkrTNOaPyWBbST1xXvUnMSg5BoA7XlpPeyDA+OwEimpb+N+3t/D4Z3v4w8UTeGHFAV68YTYup2h4QRAEQegtstpRAKC9I0BAV6FIUKsiR9/1DuafR5TbQYu/a9jxw9tPYkhqLADvbSrhtbWF/PHSScR4RNcLgiAIX28irXYUy0IAwOV0dAovUKsiM+OjOu+HE14AeysaAXh3UwnXP7OKtzeUsGRr2dGdrCAIgiAcw4j4EiKSZ4QeQ9E0yyF77NPdTP71e/zk5fWMzYonzuvinY0ltPg7WLqrko2Ftcz/00edLY0EQRAE4euOiC8hIoOTY3A7NcZlxwNgVpy4c8Fotv1mAXFeF1/sqqSmyU9jazv3XjKRcydl8+HWMq5/ZhVXPPYl1z+9kl3ljTy3bH+P5yuta+m2z6Su61z/9Ere3VRyRK5PEARBEPoDEV9CRG48aRh/v2oaJ4xMIynGzVhDhA1JjUXTNPJTlTN25vhMlv/sVMbnJHD1rEG4nA4+2V5OcqyHotoWnA6NF1ce4E/vbaOhtT3suZbvqWL27z/gbx/tijifgupm3ttcytsbio/8xQqCIAhCHyFZ0UJEhqbFMTQtjrkjUrl29mB+++YWNhbWdSbY56fEsrGwjllDU0iK9QAwLjuBZT+bz4GqJqoa27jtxXXceNIwfv7qRv6yZCeVjW34olwcNySZzUV1xHpdVDW28c7GEgI6/GXJDi6YkkNOYnSX+awvUD0ot5XUA8oJ0zQpACsIgiAcW4j4Enokyu0kKyGaIamxuJ1aZy5YfooSYTOHJHcZPyLDB8BnPzkFgNPHZvKrhZt43gg/PvKx1d5I00DX4c4zR/Pg+zv46Ssb+MvlU7j+mZWkxnm588zR5CXHsL6wBoBd5Q00tLZz+aNLmT86gx+eNpIWfwdupwOnVOMXBEEQBjgivoRe8715Q5k/Jp0otxOAC6bkoKMzyhBa3ZHm8/KTBaNpauvgW3PyqW9pZ0Z+EpqmkRTjpkPX8bqcxHqc3PX6Js596DMKa5rxOB34OwI8eu10NhjOl79D5+7XN7KxsI7NRXWcNjaDW15Yw7zhqfzq/PFH9TUQBEEQhMNF6nwJAwpd17nv3W38/eNd3HTScDQNHvpwJ4tuncelf1/KiIw4Vu9XDticYSmdIcjKxjYSot2s+PmpQSUz+oPG1nZivUf3e02LvwOvyyFhV0EQhAFMpDpfIr6EAUllQyvJsR6qGts4/g9LSIn1UljTzCPXTOOGZ1YB8OVP5/PZzgpuf3EdLodGe0AnLzmacyZmc9GUHKI9TnKTYmhqU0n+ZuHX55bto6bJT2Z8FB9sLWVibiIXTM4hMyEq4nx6y3ubSvj+c6t54LLJnDsp+7CPF46y+hbm/uFDvn18Pj89c8xROYcgCIJw+Ij4Eo5Z7n9vG39espOhqbG8/6MT+WJXJUPTYslOjEbXdX726gYm5Sbyu7e3UN/STrTbicflINrt5N3/PoEbnl3Jyr3VTBmUSLTHxac7ytF18DgdeN0O6lvaGZwSw/+cO5Z/fL6XhGg318waTJrPy/qCWtxOB08t3ctj10wnoOu8taGY0Zk+pucnd5nrNx5ZyvI9VXhdDv59w2wm5yUe8dfjmieW8emOCqLcDrb+5swjfnxBEAThyCDiSzhmaWxt58rHl/Gd4/M5f3JOxHHrC2rYU9HIrS+sBcDp0DhxZFpnxf2JuQnsqWgkMcZNIADVTW18cNuJLNlaxs9f3UhqnIeOgI5D06ht9pPu81JU20JSjJvqJj9nT8xiXHY8976zDYA3fzCX8TkJtLUHWFdQQ5zXxZkPfsoNJw5l0YYSapv9TMpL5HcXjI9YsPZgOVDVxLx7P8TjdNAeCLDuf07HF+U+IscWBEEQjiyRxJck3AsDnlivi9dvOr7HcRNzE5mQk8Dzy/YzKDmGhGg3j3+2B4B3//sERmX6qGvxowegprmN+pZ2shKiOXVMBj9/dSMVDW387KzRXDZjEGf/+VMKqpuJ9TipbvIzJiuet9YX4zByrOK8Lh77dDcPXj6F55ft45cLN5MY4yYh2s1/nTCMS6flcc+iLSzdVcntL67j6etm8urqQuaPySDN56WsvoUL//oFxw1JZtawFP5v8Xb+eOkk5gxP5a31xVQ3tbG7vJFvzMhldGZ85zW+v6UUgLvOGcNdr29i7YEa5o1IA1Qe2P2LtzMkNZYrZg7q9rV69st9/Gv5fs6akMVNJw/v9e+ioqGVdzaWcNVxgyTfTBAE4RAR8SV8pdA0jX/fMBtd16lsbOO5ZftJj/cyMiMOgHjDJUqIsdyijPgoxmbFs7m4jrMmZJEQ7ebZ645ja0kddc3t/GflAa6dk88t/1rDst2VjM+JZ9aQFP7xxV5+cfZYPttZAUBNk58/XjqJpFgPSbEeHv/mDP69Yj8/eXkDNz67miVby4iP2sKpYzIYkxVPYU0zr6wp5JU1hQC8vbGYrMRobnp+defctpbU8fz3ZnXeX7y5lBHpcZw/JYe739jEF7sqO8XXrS+s4d1NpUS5Hcwfk066z8phq2psw6GBhkZjWzuPfLKL4poWNhXVsWB8JsPS1OsTCOg0tLV3vk4vLN/PI5/sZtGt84hyO3lg8XaeW7afcdnxTBmUdMR+b3Utfi79+1J+vGAU88dkHLHjCoIgDEREfAlfSTRNIzXOy4OXTyba4+zRpbnhxKGsO1BLbpJRwyw1lnyjmOw3ZuSxsVCVuSirb2VyXiIXTMnh8c/28MGWUpbtruKiqTlcMXMQ0wcHC5JLpuXxt492sWRrGYNTYpiQk8Brawt5Y10RKbEe3v/RiXy2s4K/friTjYV1nSHSV74/hy93V3LvO9t4c30RZ43PYmNRLUt3V3LLKSOIj3Jz8qh0Hv1kNxsLa7lgcg4fby9n/uh0Ptpezi/f2MSfLp1MbbOf3729hbc3FDM4OQZflIttpfW0+APcccYoHnx/B498vIvfXjCB+hY/Nzyzit0VjTx27TQ+3l7Bc1/uo7KxjcWbS4nzunjVEIr/WXmAlXuruWhqDnsrm/j7Rzu588wxaJqq81ZW18K6AzX8Z2UBB6qbePjqaRw/PBVQodPcpOig38k/P9/LttJ6nv1yX5D4qjaK8rqc1grWQEDHcYj13Opb/NQ0+Y9YGFgQBOFQEPElfKU5fVxmr8adPzmn23wy+4d1dmI047LjyYj3ct+726hvbWf+6AxmhEnAdzo0vjUnn18t3My1s/O5bu4QvvvUSt7fUsrsYaozwLmTstlQWMs/v9iL1+VgeHocUwclMSI9jmeW7uPm59fw3bk1rNpfTUqsl+/OGwLAn6+Ywh0vruPTHRWsO1BDiz/AOZOymDo4ifve3cbW4k+pbfbT0NrOWROyWLiuqHNeDg0un5FHQXUT/1p+gEUbSkCD5rYO2gM6Vz62jNb2AKCaqN/2n3W0daj72QlR/Gv5AQDuX7wdTYOmtg6WbC0jEJJCOik3gfgoN7f8aw06qi/oT15ZzxUzB+HQ4IYThtEe0Hn8U1V01+ty4u8IsGx3FSMz4zjljx+TGuchIdrN5LxEmv0dvL62iG/OyednZ0Ve6dnQ2s6PX1rH9tIG5o9O57IZeQxOieXqJ5azvaSeV2+aExTO7Q2BgE5A13E5HbT4O1ixt4rjh6UGCcHG1naeWrqXsVnxRLmdzMhP7iz86+8I8H/vb+eKmYM6RT4Ed2oor28lKcaNy+k4LJEpCMLARsSXIPSChGiVz1Xb7CcnUbk2J45M4z8rC0iIdjPXcHXCccXMQQR0uNLIw7rquEG8v6W00wkCtRigrT3Asj1VfM8QV74oNx/cdiI/e2VDZ+7a/d+Y1JlgH+d18ferp/HnD3Zw/+LtAEzISeDCKblMyUvk9hfXERfl4t83zGZYWizFNc2UN7RyzazBlNW3khLn5dfnj2f+6AwWby6l2d/BTScP5/YX17GhsJbbThvJmKx4Fq4v4vW1RVw9axBnjc9iU1Edv3t7Cz86bSRl9S1UNbZx6fQ8Xl9TyAyj20GGL4r81BiGpcWxbE8Vlz/6JQC/XLgJXaez00Gs18Vb64txOR0MTolhf1UT5z/0OZuL6xiUHENDazsNre3EeV2sK6jtLCmycF0R184ezH9WHGDq4CRmDU3h2S/38e6mEibmJrK9tJ4vdlUyfXAST36+h1fXFHL9CUNZd6CGGI+Tm55bzZ1njuGzHeX89KwxRLmddAR0/vTeNlbvr+aZ647DbXPbXl9byB0vrcfrdPDOD0/g9v+sY+nuSr59fD53nzMWTdPQdZ0fv7yet9ZbvUcfu3Y6Q1Jj+dXCTYzM8PHEZ3sormnh/ssm0+Lv4Mvdldz03Gp+d+EEzp2UzZx7PiDdF8XIjDg+31XJmKx47r14IqMyfZTVtfDuphIy4qOYPyYDp0OjtK6FhtZ2Khva+HRHOSMyfJwXUuLE3xGgtT1AnFF7bm9FIyv2VnH2xCyi3U4aWtv7bNHGQGsJJgJX6C9ktaMg9JJz/vIpGwvr+OuVUzl7Yhabi+q4f/F2fn72mM5+l71B13U+3FbG3OFpnQVhC6qbmPuHD3E5ND758clk23pbltS2cMqfPmLu8FQeuWZalw+vFXuruPThpUS7nWz81RmdTktreweg3CRQeVXtHTrJRh/OSHy2o4JFG4v5zfnjcTg01hfU8K/l+/nleePwupy0+DtYvb+a2UNTev1B2tzWwR0vrePN9cWMzIjjzPFZPPjBDlLjPFQ0tPG3q6by+c4Knl++H12HdJ+XsvpWfFEunvvuceQmxbBybxU5SdGsPVDDz1/diC/KRX1LO/kpMeQlx/DpjgpyEqMprGkG4FfnjeObc/L5ZHs51z65HIB5I1K5cuYgbnxuNW6nhr9D5+RRaTzxzRk8+ulu7lm0FYB/fW8WozN9LN9bxdiseBb83yckxngorGnmpFFpfLStnLnDU/lsZwUj0uNo6wgQ43GxpbiOHxmi9XtPr+Tbx+ezcF0RFQ1tna+Fy6GR5vNSXNvSue/UMen88rxxzP3Dh4AS++dNyubN9UWMyYrnmeuO4+Q/fsT+qiYAThqVxmPXTufMBz9lZ1lD0GudnRBFVmI00/OT2FJcT0ltM0U1LXz/5GFcMjWXm59fw/K9VWTGRzE6y8fH28s5YUQaF0/L5fMdFVwyPZcPtpTx8uoCfnDKcE4YkcZv39rC1pI6HrhsMjPykymra+Hppfv4bGcFf796Kum+KF5bU8hZE7KI9jipa/Fzz6KtXDlzEONzEgCobfJz1p8/5fzJ2bS2Bzh1TAazh6UEzd0uzsrqWqhu8lPf4mdEho9NhbW4XY6wDnNrewcdAZ0Yj4vnlu3jrfXFzB2RyvdPGs720np2lzeyYHwmpXUtrNlfw8mj03hjbRF3vLSeBeMy+d4JQxiblUC0x9nt37Gu6yzfU8XE3MSgsbqu88a6IuaPyWD9gRqqm/xB3UD8HQF++soGrpiZx8TcRJyadtCiz/ysHkjiVegZWe0oCIfJoOQYNhbWkZ2oEtnHZsfz+De7/E/1iKZpnDI6OKk8NymG1246nmFpsV1ciMyEKD66/SSSYz1h33gn5SYS7XYyNjs+qLelKbpM4nvpbswdkcrcEXZXLpGJuVa9sii3kznDIjt94Yj2ODl9XCZvri/m9LGZ/PC0keyvaurMIZs9NIW9lY2Y3wXvPncsNz+/hpNHpXee2wwhm9dR39LOuZOyWbiuiL2VTfzotJH814nDuPThL0DTuHrWYEAJrtGZPvZUNPLbC8aTlxTD0LRYdpc3ctVxg3hu2X4e/2w3izeXMjw9jv1VTSzeXMrzy/ezcF0RF03JodnfwcLrZnLG/33CR9vKyYyP4unvzOTfKw/w4soDpMR5OVDVxP9eOIErZuahaRr5KTG8uLKAhtZ2bjllOH9espPvzh3Cc8v2k5MYzdWzBpPu87JwfTFbiuvZX6mE1S/OHsNFU3NJjvWQ5vNy/+LtvLK6gP1VTfzh4gk0tHbwmzc3c9Xjy9hZ1sBl0/OYOSSZU8dmcM+irWwrqWPVvmpW7asm1uMkyu1kYm4C976zjb98sJNmfwdXHTeID7aU8dG2ci6aksObG4r5eHs5AP9eqULKw9Jiufv1TUS7nbidGr4oN1c9toyRmXFsLa6nQ9fRdVi0oYQxWfHc9uI67l+8HYcDkmO9rDtQw9sbinnjprkMSolh8ZZSCmua+dtHuwB46ou9/OPbM8hPieU7/1zB1bMG86/l+0mN8/LHSydx+aNL2Wu8JjmJ0ZTVt+Dv0BmfE8+FU3K5bq5yiAMBnaseW0ZlYxuXzcjjnkVbifE4WV9Qy40nDuMXr25kXUEN6/7ndL771Eo2FNYyNiueaUZ+5tLdlbyzqQSA0Zk+XrvpeCoaWnnisz18Y3oeDy3ZyZDUWOpa/AxPj+Pu1zeRGuflyW9N544X1/P7iyfgcTo6S9yY3DJ/BLfOH8G972ylobWdl1YVsKW4jpLaFmqb/dx36UQunJLLyr1V5CXHkBEfxaOf7KKguplTx2QwKtNHQrSbVn8AhwNufFZ9YVgwPpOEaA+t7R3Ut7R3/p2b1Lf4qWtpJ8f4ArdybxWf76zkppOH4XI62FJcx/I9VSwYn0lGvLUop7bJzytrCjh+eCojM3xUNLSyraSe44ensvZADU9+todfnD2GFXurOXVsetD7S01TG5WNbQxNje18jwoEdF5ZU8jc4alBBaz9HYFOV/mt9cWMyfIx1FjwA7CpqJZYj6sz57Y7Ko05pvq87Klo5Pll+2lsbefvV08jNS78++VAQcSXIPSSPCNPJ8fmSh1JuivImh4fufq+x+XgZ2eNJjPh6MzrSHHamAwun5HH5TPzABiXHc+rawoZkR5HUqynMw/K43JwxrhMHrpyChNzur4meckxjM2KZ0haLL84ewwL1xXhdmpcddwgPC4HL904B12nU4hqmsZj106nsrGNwUYz+Hsumsjmolq+OSefioZW/vjedjoCOt8/aRgbC2t5ZU0BNU1+AF5bW8i47ASGpsUxLjuBtQdqmDMsBYdD44qZgyKW9RiZ4WNvZSkel4ObTxnBxdNyyUuK4YenjSTGtgikttnPJ9vLWbmvGoAF4zM73cmLp+XywPvb+Z83NhHjcXLupGxiPC4qG1r520e78Hld/PK8cZ0uzO8vmgDA00v/v707j4+yOhc4/ntmksm+7wvZSCCEfUvAgCKbG3VfcMOrqL2tVrpYq35qXe61rbeL3i63brVoWy9aFKVuVRSXUkTDIlvYF0kISSBkD1lP/3jfGbIRIsJMxOf7+fDJvO+8M/POPMPMM+c855w9lFY38f1ZQ+josJLf7eV13LFoHZV1zdx7/jAWzMxhd2UDBVkxXG13hd8wOYP1JTXEhLqYkB7Fb97dwV9X7eXJeRNIiQzi98t3UFxWy81Ts7g6fxA3/ulTPtpeSUiA9fil1U2EBfixr6qJOaOSeG19GW9vPsDNU7N4a2MZyRGBXDUxjXHpkdz+/Fpe+6yM1Z8fZkdFPQ+/XmzXFdbxyFtb2HOokdzEMG4qzOS+VzcSHxbInNFJfLjtIP/9+mZGpUYwMSOal9aUeF67n7+5hdl5Vovag3/fzD93HOSTPVUAPPj3TWworaEgM5pVu6sIdjnJTQzjxf+czPtbK1mx/SAvFO1jc1kt7xVX8KcVe1j4rz0IeGoZHQIhLicH65t56qPdbC2v44rHV/J/147zxP28EYk0t3WwcMVuzhoSyxMf7vK8rzftr8Xl5yDY5eSjbQfJz4xh7pMfMz49iifnTeCRt7bSYQzPrdwLWI/V3NaBQ8RTc7l8ayUOsUoM9h1uIjrERXntEW4szGRfVSPX/XEVFbXN/EdhBuW1R3i3uIKaplbe3Gh1h++srKe13fDYsm28clshj3+wi4bmNiuhXLEbEbhkTIpnFPZjV43hrpfW09LWwY6KejaX1fLtaYO569xcAFbvPcw1T31Mc1sHl41L5Sdz8rj3lQ3UNLbyzx0HGZUawcVjUhiXHkWQv5Nrn/6YaUPjGRQVzKPLtjE2LZIl3y7kN+9u50DtERavLsEh8NurxzErL4HN+2upbmxh8uCjLe3LNpez+2ADSz/bzwZ7MBRAQngAtU1t5P90GTEhLs4dkUheUgTXFKTR3NZOS1sHYYH+7Kys5/H3d/JfF4/wtE56myZfSvXTnFHJNLW2Exsa4OtT6eH6yRm+PoXjCnI5+fllozzbI+3uKPdKAalRVvI4LDEMf6eDOaOOvTzTy98+A6dD8Hc6mJWXQHxYADF2XDrXarkNig7uMmgiPzOafLs+7b45eSzf8gHtHYapOXGMTo1k0/5a0mOCqWmyRke6jx2XFsW6fdU9ust6MzQxjLc3lzM6NQKXn8OT+HVf93O0nXS/uq4Ul9NBUqckOiUyiO/NHMLv3tvBxWOTPUtk3Tl7KB0GUiIDe+0qm9fL+yEnIYyltxfS2NJOSIAfIQF+nulIJmZEe7rzOnd5L5iZwx0zsj1feg9cOLzLfU7NieXFohKy40OtdVivHseZQ2JZX1LDxIxoPt5VxdYDdXy2r5oPtx/kuoJ0FszMAazk+53icqoaWjxduDEhLjJiQ3jXns/utrOz+cboZEYPiiQ8yI+kiCDumN7G7Ec/5EeL1/PGgqk8u3IPw5PDmZ2XyN6qBn526UjW7LXWf/3pG1s85/rCp/sYkRLOjYWZrNpdxfrSGs4YHEN4oNXFOz49iheK9rGptIYGe0kyY+B7s4Zw65lZPLB0E4s+3ce3pg3ml29vY42d8LV3GJassRKV3149ltnDEyguq+Pi36/wdGOPS4vkznOGct8rG7luUjrvbalgR2U9T3ywk7YOw6rdVTz8+mbaOwx/mV/AoYZmappa2XKgjmB/Jzsr68mODyU2NICfvbkFP4eD3QcbqD3Sxr1LNtDY3M4FI5O4+dkiqhtbiQr25w/v7yTE5cThEG47ezDvb60kKSKISVkxjE2LZMGidcx75hNKDjfRbmeXhdkxCMLLa0sZkhDKtvJ6nlmxm5a2DjJjQ9hcVgvAkx/uYu3n1czNH8Sv39lGXFgAs/MSeWbFbpYVl1Pf3IZD4OyhcSzfWsn6khpEwCnW/9nFq0s8cdlZUc/nhxp5dNk2jIGc+FAcIjzy1hZmDovnpoWfcqD2CHMnDiI0wI+qxhaWrC31tJLfe34uSRFBhAb4MTUnluKyOt7cWMbWA3X8raiE5rbPaW3v4P6lmwgL9OO170zh+qdX0dJuqKxr9tnIZ02+lOqnkakRjEyN8PVpnDZGpkYwLCmcb4xKAiDV/tLPSz7+a9z51+pT8754129nqVHBzJ+ayUurSxibFom/08HMPKtb+Ft/Wc2bGw94kq/pufH8rWgfZw6JO+79DkkIA2Bcet/zoY1ItpKzhMJXLgAADhxJREFUnZUNZMWFdOk6Bqv76obJGQS6jiaVDodw93m5X+h5gpWYRgR9sYXn++q6mZITx7Mr97L0s/2kRwdzgR1L92CSoYmhfLKnijc2lJEQHsBNUzI8t81LCudfOw8BcM/5udz8bBGXj0/lcGMLq+3EJsNOWIcmhnluFxLgxyOXjeK6P67igaWb2LS/lu/NHMIdM3I8x2THW91YxWW1nDE4hqI9h2lp76AwO9aT5Le0dZDUqTssOSKQyGB/Nu2v5WB9Cy6ng4vGJHPTlEwC/Z3cfZ71JT9/ShaPf7DLU1sIsGLnQZwO4fyRSTgdwujUCJIjAvl0z2EC/By8+M3J+DkdLPv+WYgIew818mLRPraV13HJ2BQ+K7GmZQlxOSnIiu71B4RbWKA/9y7Z4GkJc7fQXvmE1U278MaJdqtrA+PTo2hqaScy2MUPzzn6fmlt7+DHr2xk76FGLhiZRH1zGx9sq+T6SRlMGxrHqt1VnDE4huH3/4P1JTWEBfrxzTOzuPvlDSyYkcP+6iZW7jrEgkXrCA3wY+GNExmfHsXQxFB+8Y9tPHjhcOZOHISf08H7WyuIDQ1gWXE5zW0dXDVhEB9sqyQhPJC6I638cPF6HnptMwIsua2Q7PhQXlpdwv1LN/HGhgMcqLVqIxevLsHhEFrsUdgpkUGcMzyRW6ZmdXmPdv6c/mR3FVc+sZJfvm2tSlJ3pI1fvb2N/TVHeP2OKT6dckaTL6WUTwS7/HhzwVTPdlxYAFdNGMTl44895cepctc5Q1kwI6fHl96MYQl8tP0g+Xar0JScWDY8eE6/7nPMoEhcfg6mDYnv87ggl5M5I5N4eW3pMVtVO08KPJDkZ0YjAuW1zb0mpDnxYazYYSVYi64Z32WKjWFJ1lQf4YF+DEsMZ/md03A5HTy3co/nmPTY3r8cp+TEcunYFBZ9atWnFWZ3bYmMDXURGexPdWMrM4clUN3YyuayWgoHH02+gC41TyLCiOQINu6voa3dMDUnll9cMdpzfWSwy9NqlxIZxNbyOrJiQyivPULdkTYSwwO7dHVPHhzLS2tKyE0M88xT504SBseH0thiDYi5cHQy1xakccUTK5k8OLbPxAsgo5fXJDbUxZ5DjXx3Zg7ThlrvN3cLZvfaT7CS8DOHxPH6+jK+MTqJ3MRwe469ePydDs6yYzk4LpTislpyE8O4ZFwKrR2GK8anEujvpLGljSVrS5mRm+Cp6bpqYhpXTezaDe8+H/fAC+s5WEn1PnsAybLicqbnxntKL2YPT+D+pZu4f+lGROCv8wu45ulV0GEQAQFevb3wuL0Qo1Ij8HcKdUfaGJUawfqSGpZvrSAlMojh/fiRdypp8qWUGhBEhEcuH3X8A0/RY/dW+3HZuBTOG5HYo6uwPwZFB7PxgXM8I1r7ct3kdF5eW0qr3ZrxVRER5E9uYjjFZbWkRfesOXS3WCVFBDIipeu8annJ1va49CgcDiHQYb3+OXaLYUyIq89BIrdPz+aVdaUE+Tu7DAgBK57ZcaEU7T3MtKFxFJfVsqOinokZ0QT6OwhxOWloae/S8gVWV6hV9yRMyjp213JKlJV8pUQFEeDvpLisloRu91WYHcNLa0o8z7OzbLvA3OV0UJAVTbDLjyeuG9+l8PxYOo+sTosOxiHwqytHs/dQI5eOSz3u7d2uyU+jqr6FaUOtUZk/npPX45iceCv5GpIQRoCfk+s7FfcHu/y4tiC9x22+iEHRwYxOjcDpEH56yUjP/qSIIE9tXkFmNGdkxzI2LZKmlnam5sRyqKGlX+Ufgf5OT53mNflprC/ZQN2Rtj7ra71Fky+llDoGETmhxMutP4kXwNhBkdxzXq6nleCrpCAzmuKyWtKje45Ocy/rNWNYfI/uS3cN07RuLWbuLsP0mL67hLLiQrllahbQe51fQVY0zXat0ndnDeGSsSme+rjUqGC2ltf1GKQyISPaLpA3XVrIunMPukmJDCLEnmIkMbxrMjAlOxaXn4Px6T2nxnA/xwkZUZ46vv5OCJ0QFkiQv5Pmtnaev6UAY6wkprfH6UthdmyXuQZ7k2OfZ26nbt+T7ZXbCnvt2n5ufj7by+s9LXhPzZtAhzFdlk3rj/zMaNaXVDN7eCL/84+tVDW0eF5/X9LkSymlfExE+OZZg319GickPzOahf/a02v9zPDkCM4dnthrC4m/08GKu8/G39E1cYoPCyAq2L9fX5D39LHKwQ/PyeUHs6x5w1Iig7qMUna3XCV2G0V85pCjyUhf9UDJnZKviCCrdS6h233Fhwey4kfTiellXr3YUBdTc2K5fHz/W6rcHA4hPSaYhpa2Lt24p4K7a3h4yqnrojtWTWGAn7NLV+WJDnS6bVo2s/MSiA5xkRETTFVDi2ctW1/S5EsppdQJmzksgfvm5HF2bs+ar0B/J49fP/6Yt+2tHklE+PP8AuLCvvyo4mNNZOpOxBK7dRUG+Dk9E/z2NaVMit0qlhwZ5FmGq3vyBRzzObif44m6Oj+Nptb2E759f03PjWfRrZMYOwC66U5URLC/Z0R1RmwIaz6v1uRLKaXUV5vLz+GZ8PRkGXEKW1rAmourqbWd8MCeX4HP31LAH97f5Rmt2psRyeEE+jsYmRpBZV0zQI9WtFPphjMyvPI4DkfftW9fNe6kayB0O+ryQkoppdQJampp56HXNvH9WUNPSmudOnVqmlpZt6/aM5rTG3R5IaWUUuokC3I5+dmlvhmlq76YiCB/ryZefflis+0ppZRSSqkvRZMvpZRSSikv0uRLKaWUUsqLNPlSSimllPIiTb6UUkoppbxIky+llFJKKS/S5EsppZRSyov6lXyJyLkislVEdojI3b1cHyAiL9jXrxKRDHt/jIgsF5F6Efldt9uMF5EN9m1+I8da4EkppZRS6jRy3ORLRJzA74HzgDzgahHJ63bYfOCwMSYbeBR4xN5/BLgPuLOXu/4DcAuQY/8790SegFJKKaXUV0l/Wr7ygR3GmF3GmBZgEXBRt2MuAp61Ly8GZoiIGGMajDH/xErCPEQkCQg3xnxsrPWNngMu/jJPRCmllFLqq6A/yVcKsK/Tdom9r9djjDFtQA3Q12qcKfb99HWfSimllFKnnQFfcC8it4pIkYgUVVZW+vp0lFJKKaW+lP4kX6XAoE7bqfa+Xo8RET8gAjh0nPtMPc59AmCMedIYM8EYMyEubmAsiKmUUkopdaL8+nHMp0COiGRiJUhzgWu6HbMUuAFYCVwOvGfXcvXKGFMmIrUiMglYBcwDfnu8E1m9evVBEdnbj3P+MmKBg6f4MdQXp3EZeDQmA5PGZWDSuAw83ohJem87pY8c6ehBIucDjwFO4BljzMMi8hBQZIxZKiKBwJ+BsUAVMNcYs8u+7R4gHHAB1cBsY8xmEZkALASCgDeB7/SVsHmLiBQZYyb4+jxUVxqXgUdjMjBpXAYmjcvA48uY9KflC2PMG8Ab3fb9pNPlI8AVx7htxjH2FwEj+nuiSimllFKngwFfcK+UUkopdTrR5KunJ319AqpXGpeBR2MyMGlcBiaNy8Djs5j0q+ZLKaWUUkqdHNrypZRSSinlRZp8dXK8BcTVqSEiz4hIhYhs7LQvWkTeEZHt9t8oe7/YC7HvEJH1IjLOd2d+ehORQSKyXEQ2i8gmEVlg79fY+IiIBIrIJyLymR2TB+39mSKyyn7tXxARl70/wN7eYV+f4cvzP92JiFNE1orIa/a2xsXHRGSPiGwQkXUiUmTv8/lnmCZftn4uIK5OjYX0XFj9buBdY0wO8K69DVZ83Iux34q1QLs6NdqAHxhj8oBJwG32/wmNje80A9ONMaOBMcC59nyJjwCPGmOygcPAfPv4+cBhe/+j9nHq1FkAFHfa1rgMDGcbY8Z0mlbC559hmnwd1Z8FxNUpYIz5EGt+uM46L9b+LEcXXr8IeM5YPgYi7YXa1UlmjCkzxqyxL9dhfamkoLHxGfu1rbc3/e1/BpgOLLb3d4+JO1aLgRkiIl463a8VEUkFLgCetrcFjctA5fPPME2+jurPAuLKexKMMWX25QNAgn1Z4+QDdrfIWKwVKTQ2PmR3ba0DKoB3gJ1AtTGmzT6k8+vuiYl9fQ0Q490z/tp4DLgL6LC3Y9C4DAQGeFtEVovIrfY+n3+G9WuSVaV8yRhjRESH5fqIiIQCLwHfNcbUdv6BrrHxPmNMOzBGRCKBJUCuj0/pa09E5gAVxpjVIjLN1+ejuphijCkVkXjgHRHZ0vlKX32GacvXUf1ZQFx5T7m7udf+W2Hv1zh5kYj4YyVefzXGvGzv1tgMAMaYamA5MBmre8T9Y7rz6+6JiX19BHDIy6f6dVAIXCjWcnqLsLob/xeNi88ZY0rtvxVYP1byGQCfYZp8HeVZQNwekTIXa8Fw5Rvuxdqx/77aaf88e1TKJKCmU/OxOonsGpQ/AsXGmF93ukpj4yMiEme3eCEiQcAsrFq85cDl9mHdY+KO1eXAewNhDd3TjTHmHmNMqr2c3lys1/laNC4+JSIhIhLmvgzMBjYyAD7DdJLVTqSXBcR9fEpfCyLy/8A0rBXmy4H7gVeAF4E0YC9wpTGmyk4Ifoc1OrIRuNFeJ1SdZCIyBfgI2MDROpZ7seq+NDY+ICKjsAqEnVg/nl80xjwkIllYLS7RwFrgOmNMs4gEAn/GqterAuYaY3b55uy/HuxuxzuNMXM0Lr5lv/5L7E0/4HljzMMiEoOPP8M0+VJKKaWU8iLtdlRKKaWU8iJNvpRSSimlvEiTL6WUUkopL9LkSymllFLKizT5UkoppZTyIk2+lFJKKaW8SJMvpZRSSikv0uRLKaWUUsqL/g3T9it+sw5/3AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o2zh6N9t1b7"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement[400:500])),erreur_entrainement[400:500], label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement[400:500])),erreur_validation[400:500], label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6Gq2CkeGR_1"
      },
      "source": [
        "**4. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRxXtHRXGXfF"
      },
      "source": [
        "taille_fenetre = 20\n",
        "\n",
        "# Création d'une liste vide pour recevoir les prédictions\n",
        "predictions = []\n",
        "\n",
        "# Calcul des prédiction pour chaque groupe de 20 valeurs consécutives de la série\n",
        "# dans l'intervalle de validation\n",
        "for t in temps[temps_separation:-taille_fenetre]:\n",
        "    X = np.reshape(Serie_Normalisee[t:t+taille_fenetre],(1,taille_fenetre))\n",
        "    predictions.append(model.predict(X))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd2nfDcgG8EO",
        "outputId": "2f7fdb05-5611-4628-fe9e-ba086a7b3945",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        }
      },
      "source": [
        "# Affiche la série et les prédictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps,serie,label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title('Prédictions avec le modèle GRU + Attention')\n",
        "plt.show()\n",
        "\n",
        "# Zoom sur l'intervalle de validation\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps[temps_separation:],serie[temps_separation:],label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title(\"Prédictions avec le modèle GRU + Attention (zoom sur l'intervalle de validation)\")\n",
        "plt.show()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmQAAAGDCAYAAACFuAwbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd3hUxfrHP7Ob3miBUCVIr9IUBZFqB7EhCv4U7N17r1zEcoErIGK5iooFG4qiXhC9IgooGkCkV+m9BEMggZCQumV+f5yzy7Ykm5BC4P08zz7ZM2fOzHtmT7LfvPPOO0prjSAIgiAIglB5WCrbAEEQBEEQhPMdEWSCIAiCIAiVjAgyQRAEQRCESkYEmSAIgiAIQiUjgkwQBEEQBKGSEUEmCIIgCIJQyYggE4QzRCnVQym1WilVs4g605VSE8z3PZVSO0rZ13tKqX+V1taqiFIqSSl1X2Xb4YtSar9Sqr/H8QNKqTlKKeVTL1EppZVSIRVvpaCUGqaUWljZdghCcYggEwQT8ws2Vyl1SimVaoqomGKuaQS8CFyvtT4eTD9a66Va65ZB2DNcKfW7z7UPaa3HB9OPULForacBi4EJZdmuUqq5UuorpdQxpVSmUmqXUuotpVRD83xvpZTTfG6zlFI7lFIjPK4PKAg9/0koa5RS48w+uxXXp6+wPcN+/e5Va/2F1vqqsmhfEMoTEWSC4M1ArXUM0BnoCjzvW8Hnj/0hrXUvrfXRCrRROEvRWk/RWj9XVu0ppZoBK4G/gE5a6zigB7AHuNyj6l/mcxsH/B34QClVrOgP0oZEpdT+EtRXwF3AcfOnIAhBIIJMEAKgtT4M/AS0AzD/635UKbUL2GWWDVBKbVBKZSil/lBKdXBdr5TqpJRaZ3osvgYiPM71Vkolexw3Mqe6jiml0pVSbyulWgPvAZeZno8Ms66Xh0Epdb9SardS6rhS6nulVH2Pc1op9ZDpUclQSk11TacppZoppRYrpU4qpdJMGwOilJqllDpi1l2ilGprlnczy60edW9SSm0y31uUUqOVUnvM+/qv57SuUupyc9wylFKHlFLDg/lslFL3KKW2KaVOKKUWKKUaF1LP5S0ZYbZ/whyPi5VSm8x+3/aob1FKPa+UOqCUOqqU+kwpVc3j/P+Z59KVUs/59OV7r7OUUvGF2FVNKfWRUipFKXVYKTXBcwx9GAcs01r/Q2udDKC1Pqq1fkNr/ZVvZW3wI4YY6uB7voLoCdQDngBuV0qFgTGlCwwDRpnP9Fyl1AzgAmCuWTbKrHupx7OxUSnV29W4Mqawxyullpm/Xws9xnqJ+TPDbO8y5eNpVkp1V0aIwUnzZ/cg2xaEckUEmSAEQBlTkdcB6z2KbwS6AW2UUp2Aj4EHgVrA+8D3Sqlw8wvoO2AGUBOYBdxSSD9W4AfgAJAINAC+0lpvAx4ClmutY7TW1QNc2xeYBNyG8QV4APD9kh4AXIzx5XwbcLVZPh5YCNQAGgJvFTEcPwHNgTrAOuALAK31SiAb6OtRdygw03z/OMaY9QLqAyeAqabtjc123wJqAx2BDUXY4LrnQcCzwM3mdUuBL4u5rJtp/xDgDeA5oD/QFrhNKdXLrDfcfPUBLgRigLfNftsA7wL/Z95LLYxxc/G4aVMfjM/wpFk/ENMBO9AM6ARcBRQWI9cf+KaY+3NjCsMbgHhgd7DXlTF3A3OB/5rHA8E9pfsF8LL5TA/UWv8fcBDTM621flkp1QCYhzH1WxMYCXyjlKrt0cdQYATGMxlm1gG4wvxZ3Wxvuadh5j8E84A3MT7D/wDzlFK1gmhbEMoXrbW85CUvrQH2A6eADAxx8w4QaZ7TQF+Puu8C432u34EhPq7AmGJSHuf+ACaY73sDyeb7y4BjQEgAe4YDv/uUTfdo5yOMLzfXuRjABiR62Hy5x/n/AqPN958B04CGJRyj6ma71czjCcDH5vtYDIHW2DzeBvTzuLaeaV8I8AzwbZB9JgH3me9/Au71OGcBclx9+lyXaNrawKMsHRjicfwN8Dfz/SLgEY9zLT3sHYMhlF3nooECoL/HvV7pcb6+x7UuO0KABCDf9VyZde8Afivk3u3ANR7Hj2E8n6eADzyeJ6dZng84XPfkMw4hPm27n6Vixj8R2B/kZxUFZAI3msfvA/8rqk+M37v+HsdPAzN86iwA7vZ4Hp73OPcIML+we8Xj9whDUK/yaXs5MLy4tuUlr/J+iYdMELy5UWtdXWvdWGv9iNY61+PcIY/3jYGnzCmVDGVMKTbC+CKuDxzWWmuP+gcK6a8RcEBrbS+FrfU929Van8IQHA086hzxeJ+DIdoARgEKWKWU2qKUuidQB0opq1LqJXMqLhPjyxMMDwwY3rCblVLhGB6idVprl02NgW89xmcbhlhIMO97TynuuTEwxaPN4+Z9NCjimlSP97kBjl1j4jWe5nuXiKqPx+evtc7GGGtPu95TSm1XSm0HfsXwkiUEsD8USPG4h/cxvDGBSMcQsq5+39aGt/QNsx0Xf5nlcRjeH0+vpevZ8qzvOrYF6lQpNdTDvk3ABZ7PulLqgkLsvcns70fz+AvgWh/vVnE0Bgb7/G5djsc4UPhzXRy+nzHmcTC/M4JQroggE4Tg8RRYh4CJpnhzvaK01l8CKUADpbzSHxT2BXYI48suUEoEHaDMk78wvrwAUEpFY0zDHC72RrQ+orW+X2tdH2Pa9R1lBJD7MhQYhDF1Vg3DAwGGCEJrvRXjC+1avKcrwbi3a33GKEIb8XmHgKbF2RmAQ8CDPm1Gaq3/KEVbvniNJ8ZnZscQcCkYIhIApVQUxlh72jVca93K4xVv3quv/flAvIf9cVrrtoXYtAhD6AaF1jofw8PUXil1o1mcguk59anehEL+UdBaz3TZhzHdfdBnzA8WYsLdGALmoFLqCMZ0fSjGswGBn2nfskMYHjLP/qK11i8Vdt9FtOWL72cMxudc7O+MIJQ3IsgEoXR8ADykjMB2pZSKVkpdr5SKxZgCsQNPKKVClVI3A5cU0s4qjC/Ml8w2IpRSPcxzqUBDV1B0AL4ERiilOpoeqheBlVrr/cUZr5QarMy0CRixXRpj2suXWAwBkY4xHfVigDozgScxpmpneZS/B0w048VQStU2Y8DA8Jz0V0rdppQKUUrVUkp1LM5us81n1OmFBdWUUoODuC4YvgT+rpRqoox0Jy8CX5vey9nAAGUsRAgDXsD77+d7wItKqSamXZ736kZrnYIRu/eaUirOjPlq6hHH5ss4oKdS6j9mbBVmkHnrwm5Ca10AvIYxzYrW2oExNTvRHOdQpdQdQBuMKeAywbSvH0bcYkfzdREwmdOrLVMx4vM88S37HBiolLra9NBGKGMhTEOK5xjGc+zbh4sfgRamBzBEKTUEYxx+CKJtQShXRJAJQinQWq8B7scI+j6BEUA93DxXgOHVGI4xpTYEmFNIOw6MoOdmGMHNyWZ9MKa9tgBHlFJpAa79BfgXxpdtCobH6fYgb+FiYKVS6hTwPfCk1npvgHqfYXhRDgNbgRUB6nyJETv3q9ba084pZtsLlVJZ5rXdTNsPYiyaeApjjDZgfHkXidb6W4wv+K/MKdTNGN65suBjjIUYS4B9QB5GsD5a6y3AoxjiMwXjM0/2uHYK8C0w3/deA3AXRrD4VrOd2XhPx7nRWu8022kIbDTbXobh6SkqQfDHGJ7XgebxIxjjvAk4ihGLdr3WOrWQ60vD/wEbtNYLTQ/sEa31EYwp1A5KqXYYcY9tzKnI78zrJgHPm2UjtdaHMLyyz2IIrEPAPwni+0prnQNMBJaZ7V3qcz4dQzA+hfFPxihggM9zKwiVgvIOcxEEQRAEQRAqGvGQCYIgCIIgVDIiyARBEARBECoZEWSCIAiCIAiVjAgyQRAEQRCESkYEmSAIgiAIQiUTKBlllSE+Pl4nJiaWez/Z2dlER0eXez9VCRmTwMi4+CNjEhgZF39kTAIj4+JPVR2TtWvXpmmtA+5cUW6CTCn1MUa+l6Na63Zm2dcY+8OBsSdehta6o1IqEWNblR3muRVa64eK6yMxMZE1a9aUtel+JCUl0bt373LvpyohYxIYGRd/ZEwCI+Pij4xJYGRc/KmqY6KUKmwbvXL1kE3HSJr5matAa+1KeIlS6jWMvd5c7NFaB5OpWxAEQRAE4Zyi3ASZ1nqJ6fnyw9zj7za8N8AVBEEQBEE4L6msoP6eQKrWepdHWROl1Hql1GKlVM9KsksQBEEQBKHCKdetk0wP2Q+uGDKP8neB3Vrr18zjcCBGa52ulOoCfAe01VpnBmjzAeABgISEhC5fffVVudnv4tSpU8TExJR7P1UJGZPAyLj4I2MSGBkXf2RMAiPj4k9VHZM+ffqs1Vp3DXSuwldZKqVCMDZe7uIq01rnA/nm+7VKqT1AC8AvYl9rPQ2YBtC1a1ddEUF9VTV4sDyRMQmMjIs/MiaBkXHxR8YkMDIu/pyLY1IZaS/6A9u11smuAqVUbeC41tqhlLoQaA7sPZNOnE4naWlpZGRk4HA4zsjgatWqsW3btjNq41xDxiQwRY2L1WqlevXqxMfHY7FICkBBEAThNOWZ9uJLoDcQr5RKBsZqrT8Cbge+9Kl+BfCCUsoGOIGHtNbHz6T/5ORklFIkJiYSGhqKsY6gdGRlZREbG3sm5pxzyJgEprBx0Vpjs9lITU0lOTmZCy64oBKsEwRBEM5WynOV5R2FlA8PUPYN8E1Z9p+dnU3Lli3FEyGcFSilCAsLo0GDBuzYsaP4CwRBEITzinNarYgYE8425JkUBEEQAiHfDoIgCIIgCJWMCLIqjt1uZ/LkyWzcuLGyTREEQRAEoZSIIKviPPPMM6xYsYJ27doVW3f69OlVMm9LZaKUYvbs2YUeC4Jw7lNgd7I/LbuyzRDOcUSQnWUcO3aMRx55hMTERMLDw0lISKBfv378/PPPfnX/97//sXz5cmbOnInVai227SFDhrB37xllExFRJwjCecfY7zfT+9UkTmQXgNN5+kR+VuUZJZxzVEYeMqEIbrnlFnJycvjoo49o1qwZR48eZfHixaSnp/vVHTRoEIMGDQqqXZvNRmRkJJGRkWVtcpXEZrMRGhpa2WYIglAF+H13Gp3VTmq8MhSaXwVDPodProXDa2HYbGh+ZWWbKJwDiIfsLCIjI4OlS5fy0ksv0a9fPxo3bszFF1/MyJEjuf322931CgoKePrpp2nYsCFRUVFcfPHFLFiwwH0+KSkJpRQ//vgjl1xyCWFhYSxYsCCgd2vu3Ll06dKFiIgImjRpwnPPPUdBQUFA+5KSkhgxYgTZ2dnExcWhlGLcuHElsumnn36iS5cuREZG0rNnT5KTk1m8eDEXXXQRMTExDBgwwEt8Dh8+nAEDBjBhwgQSEhKIiYlhxIgR5Obmuuvk5+fzt7/9jYSEBCIiIrj00kv5/fffix0PrTUvv/wyTZs2JTIykvbt2/P555+X6DM7fPgwt99+OzVq1KBGjRrceuut7Nq1q/gLBUGoMliU4irrWuNg10KY+zdDjAEsebXyDBO8OJlr42SOrXQXp++B/FNla1AJEUF2FhETE0NMTAzff/89eXl5hdYbMWIEixcvZubMmWzevJm7776bgQMH+gX2P/3000yYMIHt27fTrVs3v3YWLFjAsGHDeOyxx9iyZQsff/wxs2fP5tlnnw3Yb/fu3XnjjTeIiopi165dpKSkMHLkyBLZNHbsWN544w1WrlzJiRMnGDJkCC+88ALTpk0jKSmJLVu2uEWei8WLF7Nx40YWLVrEN998w8KFC3n66afd50eNGsXXX3/Nxx9/zPr162nfvj3XXHMNKSkpRY7H888/z0cffcTUqVPZunUrzzzzDA8++CDz5s0rdOw9ycnJoU+fPkRERLB48WKWL19O3bp16d+/Pzk5OUG1IQjC2Y9FKZqrZLQyvzI3zoSwGLh6EhxaAYdWQcom+PpOKJBYs8qi879/YuOkPvBn0XG+x7LyOZju8zd6zv0wc0g5Wlc859WU5b/nbmHrX377lReLw+EIKkYrEG3qxzF2YNug6oaEhDB9+nTuv/9+pk2bRqdOnejRoweDBw92C6o9e/bw5Zdfsn//fne298cee4xffvmF999/n3feecfd3rhx47jqqqsK7W/ixIn885//ZMSIEQA0bdqUyZMnc+edd/LKK6/47W4QFhZGtWrVUEqRkJDgzkhfEpvGjx9Pz549AXjooYd4/PHHWbt2LZ07dwbg7rvv9guat1qtfPLJJ8TExNCuXTsmT57Mvffey6RJkwB49913+fDDD7n++usBeO+99/j111+ZOnUqEyZMCDge2dnZ/Oc//2HhwoVue5o0acKqVauYOnWqu62i+Oqrr9Ba88knn7jHasqUKTRt2pQffviB2267rdg2BEE4+1EK6qrj5FzQF1uX+zi04A2aDXmJyITmsHgy/PB3iKgGB5ZB86uh8/9VtsnnBesPnqBetUjqVosAoIlK4Qrrn/DNvdD+1kKvu3jiLwC8fEsHbru4EdjyDEF92SMVYndhnFeCrCpwyy23cP3117N06VKWL1/O/Pnzee2115g4cSLPPvss69atQ2tNmzZtvK7Lz8+nb9++XmVduwbcUN7N2rVrWbVqFZMnT3aXOZ1OcnNzOXLkCPXq1QvK5pLY1KFDB/f7hIQEANq3b+9VdvToUb9rPKdaL7vsMgoKCtizZw9gxIP16NHDfd5qtXLZZZexdetWr3Y8x2Pr1q3k5eVxzTXXeAlPm81GYmJiUPe9du1a9u3b57dVUk5Ojts2QRCqJgu2HKFp7Wia1YlFAbVUJgUR8by4oz6z0p9gcmothlwQA90fh1/Hn74wZQMggqwiuOmdPwgPsbBjwrWQvIZfwkedPmnLg9CIIq8f9c0mQ5ClbASnDRpeUs4WF815JciC9VT5UtH7NkZERHDllVdy5ZVXMmbMGO677z7GjRvHyJEjcTqdKKVYvXq1X1C6b8B+dHR0kf04nU7Gjh3L4MGD/c7Vrl07aHtLYpPneZcQ8i1zeq5iOgN8PXye4+HqY+7cuX77SgYb7O90OunYsSNfffWVu+zUqVPExMRQs2bN0potCMJZwIMzjBix/S9djxVNLTI5HlEL7D4VG3c//T40Go5uqzgjzzK01ry6cAc3XNSAlnXL9zvz5wM2apDJjXoZFPSC1R96V0jfBXXbB74YsOKgh2UzFPSBg8sBsNfvUqmi6LwSZFWVNm3aYLfbycvLo1OnTmitOXLkCH369Dmjdjt37sz27dtp1qxZ0NeEhYXhcDi8ysrSpkD8+eefZGdnuwXVihUrCAsLo2nTpm6bli1b5j52OBwsX76coUOHFtpmmzZtCA8P58CBA35evGDp3LkzX375JfHx8VSvXh2QTdcF4VwkjhxClYP8sJpuQaYw/+GrdxHE1IV6HSC2HmybC1ob85znGZl5dqb+toeZKw+yfkzh4TJnitaaL7YV8FroF9xiXQozdkG2MbOy21mfZpa/4NiOIgXZEyHf8mTIHHjrM/SpVLY6G/PRglT+c1twM0PlgQT1n0Wkp6fTt29fPv/8czZt2sS+ffuYNWsWL7/8Mv369SMuLo4WLVowbNgwhg8fzuzZs9m7dy9r1qzh1VdfZc6cOSXqb8yYMcycOZMxY8awefNmtm/fzuzZsxk1alSh1yQmJpKXl8evv/5KWloaOTk5ZWpTIOx2O/fccw9btmzh559/ZvTo0dx///1ER0cTHR3Nww8/zNNPP82PP/7Itm3bePjhh0lNTeWRRwqPB4iNjWXkyJGMHDmSjz/+mN27d7Nhwwbee+89pk2bFpRdw4YNIyEhgUGDBrF48WL27dvHsmXLeOqpp2SlpSCcI9z36RpOpB0G4PUVGazef9y7Qlg0/O1PIxVGndaQexyyj1WCpZWPS4PaHLpc+8m3O6nNCQZYDM8Wh1bA8b08a7uX6womoa1hkLwm4LVHTuZRmxOGGAPISkFpJ+NsdzNn3eFytbs4xEN2FhETE8Oll17KlClT2L17N/n5+TRo0IChQ4fy/PPPu+t98sknTJw4kVGjRpGcnEzNmjW55JJLSuyduvrqq5k3bx7jx4/n1VdfJSQkhBYtWjB8+PBCr+nevTsPPfQQ99xzD8ePH2fs2LGMGzeuzGwKRK9evWjbti19+vQhJyeHW265hZdfftl93hUDN2LECDIyMujUqRPz588vNgZu/PjxJCQk8Oqrr/Lwww8TFxdHx44dixSknkRFRbFkyRJGjx7N4MGDOXnyJPXq1aNv377UqFGj9DcsCMJZwy/bUummjMVgqY5Y9pur88bN3WLEHwG5TmPRV2QdM442dQvE1Kl4YysZbUabOHX5CrLsfDs3WP8gXNm5OX8cc4bUhYN/MOuPXtgIIa9xHyK3fgdXvwgWb7/TpZMWMcEUY08WPMKUDgc5GNaM1ataMePeyo0hU7qcB6486dq1q16zJrAK3rZtG61bty6TfmQayp+KGpPhw4eTlpbGDz/8UO59lQXBjEtZPptVgaSkJHr37l3ZZpx1yLj4c7aMidaaJs/86D6+3rKCqWFvclX+ZHbqRu7y/S8Zq7G7T1pEdoGDjSO7wCsXwlUTjGD/MuJsGZfiSD+VT5cJvxAZamXb+GvKrZ+D6Tn8+cYg2qn99Cp4w/05JI42UhatGXSS+AUPw/B5UK8jhISD1YgN7jZ6BsvDH2etbs7ggnG8Nvgith/J5IOl+5j3xOW0rV+t3OwGUEqt1VoHXHEnHjJBEARBMMmzObjro1VeZfHqJADpOi7gNX+dNPJG5oRWIyqiOhw/sy3qKpOsPBs2h6ZmdFiJr3WYDp7y9JDtSs3iVL6dC1UKO3XDgHVOJV5JfEgkTPdIX9TtYeg3hsstm7EozfMF9wDw1KzTuTJjwyt39xYRZIIgCIIArNp3nNveX+5XXk+lk69DOE7R3u+MHBtRNRLhxIFysrD86T7pV7Ly7W6vU0lwOA0hVl56LKfAzpWvL6FhDCxSKSx2dnSfGzX7tLCyWyMNj5j99I4urHwXajahuSWZfB3Kbt3Ar/3YiMqVRCLIhLOa6dOnV7YJgiCcJ/yVkRuwvKFK47CORxezDq77S7+yv3OikdeqipKV75vXI3hcgsypNYmj5zG8eyLjbihFuqn1X0BsAtRqDjUaG2VOB5Ylr9BJRVIjJ4vwMDvLnEbbGw5l8N81yd52XDsZ9i6GG9+BE/vh0xtg9y90UMns0fVx4J/sPUYEmSAIgiBUPprArp0GpiDzxeHUWHyzWyS0ha3fQW6G4aUJKzof5LmE20NmHk//Yz/9WydwefN42LcUlrwMQ/8LoZGFN3IyGf7nsUL+X2lgDaXgk4FEHFrGt+GnT61ytgLg/s+8Y8kdTg0dhxovgFpNodV1sGoa3a0wxX6TX7c9m8cTaq3cxBOS9kIQBEEQKHyqrTBBZnM4ybV552Wk4cXGz8mJ8GJ9mHJRpW9aXVF4eshc3PnRSiNr/qcDYN8S2G1sW5Samcegqcs4mumzb/Oe37yPV03j6Jr/EXZomVfxJNsd5GPEuUWHeXu7PPvXWvPf1YcoaNzbXfa9ozu+XBhf+cJZBJkgCIIgEFiQhVNAHZURUJB9+sd+2oxZ4F3YpBeEROL2E53YDzvnl7mtZyOFxpAdWnn6/WFjB4QvVhxg46EM3vp1N+3HLuDLn37F/uMo+P4xiIqHMcehaT9Y8Cx1friLnc4GPBQ3FYBfHR153zHQ3eR+n43C7c7TBizbnc6obzbx0vY6OC/ozou2O9gTIH6ssr1jIFOWgiAIggAEXh1YT6UDBBRkk37a7t+IxQJ3/Q8OreRAppPGK8dW6VWXJcHhM34h2LnL+jP8aRwf1dWp8/vr0OF2lDIE0JJdx8jKt9Ft+ROEWI4YFa+eCBYrXDES9iwC4B7bP6kZlsj1+RMDBuR72eEhyFxTypuP2Vl91edM27ki4DVhIZUvyCrfAkEQBEE4Cwg0Y9lApQFwWBe/v29CnBngdEE36PEEvRY3J0XXhPTdZWjl2YvdI0N/fdL4W8g3jAmdAetnADDD3t84OX80SkE8J+mZl8Q7oVO40HKEb6MGw7MpcNHtRr3G3RlV/1MuzptKsq5DqNXCFt3EPVVZGJ7C2mIqsrTsfHakZhV6jQgyoczJysrihRde4MCBqrvsWhAEoVIIoMjcggx/D5kn1aNCvTwzLtY6W5C9dSE4nWVi4tmMSwjFkMMfEU/wWMj/3OfG2e7iLcfNcPH9sPc36mdt4l+hM5jgeIPrrKv4ztGdz6LuZu9JJ5sPG3nftNb8d28oxzB2Pll74ERQdnh+Dvl2Y9wzc+1k5zsKu0QEmVD23HPPPaSlpdG4ceMi682ePRvlsfnt9OnTiYmJOaO+k5KSUEqRlpZ2Ru0IgiBUBoFWWTZQaTiwMHZovyKvjQy1UmA/LbrSTuUD8LuzHdH2E3DyYNkaexZixG5phloXeZUn5s1kusPM3H/53yAqnv47x9Pfspa/dC3etQ/kGdt92DT0fW0xA976nQ+X7uXlBTtKZYfDqcmzOTiZY+Puj40kv06tyS0oPKVH2FkQQ1b5FgheDB8+HKUUSilCQ0O58MILGTlyJNnZ2cVe++abbwLwxhtvlLjfIUOGsHdv8HEO7dq149VXX/Uq6969OykpKdSqVavE/QuCIFQ2gYL6G6o0ssNqc2X701smTb6lvV+9yFArBY7TgqzrBGM14S6nGe90rHTiotLY+j35+/7gWFZ+0Jc4nZqrLGt4NvRLd9kSh89YVWsIA16nZu5+olU+79oHMtl+B7lEeE15Tpi3jXeT9pTK9JSTeQyZtoKLXljoLrPZnWTmFS7I7AG8mxWNBPWfhfTv358ZM2Zgs9lYunQp9913H9nZ2bz77rte9ex2O1ar1e3peuKJJ3jiiSdK1WdkZCSRkUXkhgmCsLAw6tate0ZtCIIgVBaur+Tr29dj3p8pgOEhOxlWF89Nk2pGh/tdGxFqxebw/1Lf5QpAP7oNWlxdxhaXD9XJgv8+SDjQLe9z9r40sNhrwBA13SzGQoeXbLcz29GLU0T4V2x5nfvtLo/tjwJN+ZaGkbP8E/Nm5duZ/sf+Qq/JOYOEuGWFeMjOQsLDw6lbt+z/8TUAACAASURBVC6NGjVi6NChDBs2jO+++45x48bRrl07pk+fTtOmTQkPDyc7O5uTJ0/ywAMPUKdOHWJjY+nVqxe+m65/9tlnNG7cmKioKAYMGEBqaqrX+UBTlj/++CPdunUjMjKSWrVqMXDgQPLy8ujduzcHDx7kn//8p9ubB4GnLOfMmUP79u0JDw+nUaNGTJw4Ec8N7RMTE5kwYQIPPvggcXFxNGzYkFdeecXLjvfff58WLVoQERFBfHw8V199NXZ75f/yCIJwbuGKgbqokbHBdAh22qgDHIvwDgGpHesvyKLCrDic2k9UZBJDqq4OR/4sJ6vLnt6W04JmRugkFq3dQv6pE/B+Lwq2zefb9clef8ddOJ2aBiqN3c76vOe4gTSqkYf/WGENYUWj+9nkbOJO7gplJ8hKw6ki4ssqChFkVYDIyEhsNhsA+/btY+bMmcyaNYuNGzcSHh7O9ddfz+HDh/nhhx9Yv349V1xxBX379iUlxfgPb+XKlQwfPpwHHniADRs2MHDgQMaMGVNkn/Pnz+eGG27gyiuvZO3atfz222/06tULp9PJnDlzaNCgAWPGjCElJcXdjy9r165l8ODB3Hzzzfz555+89NJLTJo0ibffftur3uuvv0779u1Zt24dTz/9NKNGjWL5cmM/uTVr1vDoo48yduxYduzYwaJFi7jmmmvOdEgFQRD8cGmMGzs2oHfL2nRRu4hTOeyKvdSrXssE/z0tI83kpAV2J3k+yWJXOlvD5tkw76nyMbwQjpzMK75SAPpb15m51KCHdQv95nbH+Xp7SNmA7ZsH+fvXG1myyz9W2O7UNFDHSA5iReqSBvdxQ8FEnB4yZG9a8aE55UX2WeAhO7+mLH8aXar/UiIddrCWcqjqtodrXyrdtcCqVauYOXMm/foZAaUFBQXMmDGDhIQEAH799Vc2bNjAsWPH3FOO48ePZ+7cucyYMYNRo0YxZcoU+vXrx3PPPQdAixYtWL16NR999FGh/Y4fP55bb72VCRMmuMs6dOgAQFRUFFarldjY2CKnKP/zn//Qq1cv/v3vf7v73bVrF5MnT+bxxx9317vqqqt47LHHAHj88cd58803WbRoEZdddhkHDx4kOjqaG264gdjYWBo3bsxFF11U4nEUBEEoDrd/RkHP5rWJ27OYfB3CrpiuXvUiw6zUiArlRI7tdFmoKcgcTob4bFD+ku0OBtY7iVr7KVz+dyOOqpxZsvMYd328imn/14Wr2gb+O71ibzpLdh5jVN9GMPdJqNaQRNWQPpb16PZDuHNlA74ImwRApMNIGRFtz6C92ktmbieOnMxj54of6JadRPhNb+HQmnrqOH86mxRrX769cladRoRayLP59z202wWVYI034iE7C5k/fz4xMTFERERw2WWXccUVV/DWW28B0LBhQ7cYA8MLlZOTQ+3atYmJiXG/Nm/ezJ49RkDktm3buOyyy7z68D32Zf369W4RWFq2bdtGjx49vMouv/xyDh8+TGZmprvMJfRc1K9fn6NHjwJw5ZVX0rhxY5o0acKwYcP49NNPycoqPJeMIAhCqTFdZApF/Wi4wbqcWY5e5Fii/Kr+46qWXsdRHh6y7Ue8/0b9RTx3ZD0JaFg+FfIyKW/+NFNHbDiUUWid26et4J2kPThXfgB/zoLfXycp/CmiVT6OLveyzNmeVnmf8In9aubEPwxPbsSBhUHWZZw8msyzk1/hiuX3Er5pBmQcoObBn4lXmRzSCYX26aLgDAVZhMduSde1r0vvlsV75QAujD8dmtOwhuHEeLxvMy5qVP2M7CkLzi8PWSk9VblZWcTG+ruoy4srrriCadOmERoaSv369QkNDXWfi4723m/L6XSSkJDA0qVL/dqJi4vzKztb8Ey54Xl/rnNOM2dPbGws69atY8mSJfz8889MmjSJZ599ltWrV1O/fv0KtVkQhHMbl4dMKWhSsJNwZeM3Z0fiAwTr+8ZQuaYsbY7AQmPFiVho2wtWvAN7k+CR5QHrlRWuP7HFRWXFcxLb0tdJs9QjwZFKiDLsf2ChMd2ZRzj/tt/NwFr1ublGIsfCGnIfP8Gyn7jTMz/rohdos+MXdjob8KnjqkL7m702mVu7NCS7iBQUwVAjQnFv71ZMmLeNOrERpJzMDeq6JvHRbE0xBLHVTBpbmbFrnpSbh0wp9bFS6qhSarNH2Til1GGl1AbzdZ3HuWeUUruVUjuUUlVjKUo5ERUVRbNmzWjcuLGfWPGlc+fOpKamYrFYaNasmderTp06ALRu3ZoVK7y3i/A99qVTp04sWrSo0POhoaE4HEUHQbZu3Zply7w3hP39999p2LBhiQRuSEgIffv2ZdKkSWzatIns7Gx++OGHoK8XBEEIBpfGsihFbdtfAOzWDfy2BPKs6yLCnLJcuS+98A5uNFfKH90KDlvh9cqbvJOc3PoLr4W9z0dhr6Dzs7kv9wm65r+LUyvWOpvz607v+7CaAu94aD2v8vXOZkZqi83foBU8YXucnEArK01GztrIt+uTycw9s/t3aO9/7HMKggvKDwuxUDfOsK9+NcNDdrYIsvL0kE0H3gY+8yl/XWvtlcBKKdUGuB1oC9QHflFKtdBaV/6yh7Oc/v3706NHDwYNGsTLL79Mq1atOHLkCPPnz6d///707NmTJ554gu7duzNp0iRuvfVWkpKS+Pbbb4ts97nnnmPgwIE0a9aMoUOHorVm4cKFPPjgg0RFRXHBBRewdOlS7rzzTsLDw4mP989i/dRTT3HxxRczbtw4hg4dyurVq3nttdd48cUXg76/H374gT179nDFFVdQs2ZNfvvtN7KysmjdunWJx0oQBKEonO4pS4jLS8auLRzW8XQJ8IXt6yFzTVn+/Wv/lAtuYhMMUfbdw8b+lrVbFl73DFEovzKnU3Pb+8t58dS/aJG9hltMl8xLttvZpo2VpJ3z3yMff0eAawui7THdaJO9kicKHuN752WAog4nWNVhEbeu68R2XXws1t+/3kh4gMz417ary0+bjwR1f3YnXBhvzBi1rBvLlr9O+tWxWhQOp+aNIR2pExfO0A9WklvgYNnovizfk05GbgHL96a7py4rm3LzkGmtlwDHg6w+CPhKa52vtd4H7AYuKS/bziWUUvz444/07duX+++/n5YtW3LbbbexY8cO95TepZdeykcffcS7775Lhw4dmDNnDuPGjSuy3euuu45vv/2Wn376iU6dOtGrVy9+++03LBbjkXnuuec4dOgQTZs2pXbtwHP3nTt3ZtasWXzzzTe0a9eO0aNHM3r0aHcAfzBUr16d7777jv79+9OqVSteffVVPvzwQ3r27Bl0G4IgCMHg0lhKQUj2EdKohp0QLm/u/w+nr0RzBfUXS6Nuxs+9i0tvaAnw1I15dgfrDqTTIttIi/SK7Ta+c3RnmmOAu04GseQG8HBZTW/U7zVvpn/+y3zv7A6m6FNxdUlceyubdNOg7QoU1B8XUfSMkCd2J/RpVYe5j13O7Rc3Cugh69OyDoue6sWgjvXp1qQWT/RtxvMDWmO1KC5vHs/17evx+b3dGNat6J1tKorKiCF7TCl1F7AGeEprfQJoAHjOoSWbZecd06dPL/TcuHHjAgqp2NhYpkyZwpQpUwq9dsSIEYwYMcKrzFMYDR8+nOHDh3udv+GGG7jhhhsCtnfJJZewcaP3f4K9e/f2+6/x5ptv5uabby7Urv379/uVJSUlud9ffvnl/Pbbb4VeLwiCUFa4Y8hQkJ1GeLUEPrymK/3b+Aep+zrNIsOK/zrNszmIqNUUYhIgpQhPWhmSb3cwaOoyerWozX09m5CoDA/USNuDzHb0giDnofamZfPMnE2AYrf2XiWaVUQG/GBwrXwMNDXs4tnrWvHij9vdx6667RsaOeNyAwiysBBF09pGEL9V+S/EUEoFFNuVRUULsneB8RjP/XjgNeCekjSglHoAeAAgISHB68vbk2rVqpXZajyHwyEr+3yQMQlMMOOSl5dX6HN7LnLq1Knz6n6DRcbFn8oek937jbim35f9Ts+UPeiQaEKObiPp6DaveklJSWQd9RYhh/btLrb9nxYtpkaEhUscVqI2fM4G3YKMGsWn8SnNuOzbWwDAmh2H+DPNwcZDGTTTh2mr9gOwxZlYovbWHjjB2gMnaFrNf2KtwHZm0UVPdw0jq0Dz++HAOS0BWjgPEaLAbmo2u1N7jclNje28mwE5Hh/L1gNHq9TvWIUKMq21Oz28UuoDwBWZfRho5FG1oVkWqI1pwDSArl276t69ewfsa9u2bWW2MjKrgldZVgVkTAITzLhERETQqVOnCrKo8klKSqKw39PzGRkXfyp7THYv3Qvbt9Gz5+XEbSmARp297Zk/DzBmA3oDfbtnMPi95eTbnVzUrjVsKdrr1bpjV655YynfXtiLTrlf0jF3Odz0ZLF2lWZctqs9sHM7oVGxgJH6IjmsEW0tByjQVnbr0k1C1axRjT0nT3iV2c8wJn5gv8uJjwln9Ucr4Yh/wlkwxjwqaYF7P0qHVl5j0hsYdl0Bncf/7C5Ly7dUqd+xCs1DppTyXJ5xE+Bagfk9cLtSKlwp1QRoDqyqSNsEQRCE8xvPVZZkp0FULa/zj/dtxpCup30HHRpWd6eXiAwt3r9xNNPYqHvowYHQ9mY4tDLwjuZlgCuk/3DG6XQQL8/fQTOVzB5dH1sp/TGr958ovlIJccXfDbnYGNtVzwbOgRnlOS0cYNhcaSxcKP91DWc15eYhU0p9iSFa45VSycBYoLdSqiPGUO4HHgTQWm9RSv0X2ArYgUdlhaUgCIJQkbhXWdpywZbtJ8ieusp/VaRLT7nykBVFaqaR2yvEGgJN+8CWOfDLWLjyhTO03B9XjFuqKQJdNFDpHNZnT9wUnE4ZMqBDfQZ0KDy/pGsl6zVt69Itzj/hra8gOxu2QyoJ5SbItNZ3BCgudK8erfVEYGJ52SMIgiAIReFyuljyzAQB0cELl6ggBNnRLEMchYZYoLmZbnPZFLj8HxBZtpniC0tQW0+ls9p5Wli2rR/HE/2a8+CMtWXaf0nwFVKe3NSpAT2aGZ9DVLgxxo/2aUb67vV+daN9PoOzJL1Y0JzTWye5sr0LwtmCPJOCcPbiTnuRYyZFjQpekAWT9uLnrUYYdahVGTnJ+jxvnNg+r0R2BkMgQVafNKqrbA7qOu6yhLgI+rSs41f3TPl7/xZcdmGtIuv0alGbOrHhRdYZO7ANt3YxVnW+fUdnbr+4Ea3rBY7TVUpRv9rplB1P9GteQqsrl3N266To6GgOHz5MQkICoaGhXhl9BaGi0Vpjs9lITU312/5KEISzA236yCy5piArgYfMd8qydb04tqV471np2lcyNTOfeZtSaNrsHloufgmVXvwKzZJSEECQ9bYaiw5+dZ5eVBRqVYT5JGlNiAv3m+oMljCrhQKHk5Z1Y6gdG87yvYXvXPDsda1pWbfoRVCe3rPE+GheuqVDEbVh/t+vwOHQ1IgOK7Le2cg5K8gaNmxIWloaBw4cwG4/s3nkvLw8IiIK3wrifETGJDBFjUtISAjVqlULuKuBIAiVjzu+3u0hK9rD40moxVvU1IgqOsnpozPXAfBnrYbEloMgs3ksfRzcpSGz1ibTQh3ilI5grz69vi7E6j9R9tSVLRn1zaZS9RsRanGLQV3MTprVIotPBBtiKdlEXkmSy55tnLOCzGKxUKdOHfd+jmdCUlLSeZWmIBhkTAIj4yIIVR9LCQRZtwtrsWTnMaLDvT1kT1/TikFTT+/lq1TgBZUpIQ2ITd9zRvYGwnPKsnGtKADaW/axQzcCj22VLAFmj8JDSx/NFBUWQmaeHa297zc2IoSsPDudL6jOuoOGpzAusngJEmI9f2a3zukYMkEQBEEIFqcZBa5y00FZIaL4QPt3h3Vm/t96UismnDfvOP3P2EWNqnN129MZ/hvViAp4fXrEBXB8D5RxfKnTQw0lxkeTqFK4SO1hmbOtV71AGe4Li4d75tpWxfY78aZ2XNUmgd4t63j5x14bbCTA9dzIO5i4O+t5FG4kgkwQBEEQ8Ng6KScdompCENNl0eEhtKobB0D7BtW8zrmm5BpUj3R7qXxJD28E9jzI+qv0hgfAdS9PX9OKfq0SeDX0fXKIYKbdyPHlCn6PCOANC5TCIyY8JKjUHonx0Uy7q6tX3WHdLuDSpoa38Z7Lm7jLg4ntthSxAvNcQwSZIAiCIOAxxVaQDeEl34kk1Gd6zbWP4vUd6tEyIXB7c5NNoVbGcWRaa+rEhvNw76ZE5qfR1bKT9+wDOIIhjG4zk7AGiuOK8PBcfTLiYjaOuYoVz/Yr0qPluvcQTwHl4aWLiwhl/0vXM6hjAz/hKhiIIBMEQRAETgehK1s2hAb2aBVFmE+A/NBuFzCgQz1G9EikSe3Aq6tX59QFoOCLoXww7/cS91kYWntkqk81NsVZa+Yfm3l/N3duryvNjdOXP9PXfa2n8OrTsg7VokKL9ZB9cFdXxgxowwU1T49brBlgX8tnxeM3D3dn2wvXlPLOzl3O2aB+QRAEQSgJboeOLadUgsx3xWJsRChvD+0MFB4vdZw4aHQpYYdWcPKP6XD95SXuNxBOrU8H7Jubozdp04WVW3LMKdRoNo69yu0hq1ct0n1tRKiVX/7Ri/1p2V5tFuUha1A9kt4++cxuuKg+2QV2dx4xF75pNgQDEWSCIAiCgEcMWUEOhJVckPlOWXpSlAhxDJ2FdXIj6qvAG2uXBq091lIe3QbRdZh0Z2/+cSqfOrFG/FhhaSfiIkOoExtBszoxXuVFecgCpc+wWBTDujUulf3nIyJTBUEQBAEj7kopTA9ZyRM4hwYQJS7CQwoXM7aQaFY5W9LTshnspUvI6ovGI2j+2Hao0wqllFuMFUXtmMDZ84OJIRNKjwgyQRAEQcDDq1SQXSoPmSuGrGdz/+TPRXnI7E7NNPsAGlmOwba5Je43EE6XuAQ4sQ9qNi32muZ1YgizWgpd/ViUh6woMVoavn7gUl4Y1Lb4iucQMmUpCIIgCBhB/UqpUseQWSyKX/5xBfWrR/qdCy9KkDmcLHJ24piOo/auhdD+1hL37YcrqD//lLHzQPULir3kxyd7euUv8yUqtHDJEFLG6Sm6XViLbsXshXmuIR4yQRAEQcDTQ5YDYaXbc7ZZnViiwvyFS1GC7FS+HY2FVc5WcHBFqfr1RWNm4c84aBTUKD6WK9RqKXJqNSKs8HsIFEMmlAwZQUEQBEHAFXeloZRpL4qiqCnLG6f+AcAWZxPIOAB5J8+4P6fWhrjMOGAUVD/z4Ppwq8SQlScyZSkIgiAIGB6ycGUH7SxVDFlRFOV5SjtlBPJv1aZoSt0CjbufUX9GHjIPD1kQU5bFERcZwuAuDQkPtRAdFsLIq1vS/LmfgLKPITsfEUEmCIIgCBirLKMxVzmWYpVlUQTjQdriNAVZyqYzFmTuoP7Mv8ASAtG1z6g9MATeK+aelC6iwqzkFDjKPIbsfEQEmSAIgiBgTFlGKVOQlbGHzBLEvo3HqI6Oro068ucZ96cx4+Fyj0NkTY+0/WXL/x7twbLdaUHtSykUjQgyQRAEQcDwkEW5PWRlK8hCgoqxUjjqtCPkyKYz79A1ZZlzHKLKb7Vi84RYmheyT6dQMmTSVxAEQRAw4q4iVYFxUMpVloVRNy6Ch3oVnwusIL4NHNkEabvOqD9j6yQg9wRE1TyjtoSKQQSZIAiCIGBOWZJnHJSxh0wpxehrWxVbLze+vfHmv3efUX9GCg/TQxZZ44zaEioGEWSCIAiCgCFiosvJQxYs+2r340h0S/SJfeB0lrodI8ktRgyZeMiqBCLIBEEQBAFjmi/SFdQf6p9tvyJ4/vvtvJ5xBcqWczqHWClwupLc5p4wgvqFsx4RZIIgCIJgUl5TlsGSmpnHOmdz42DXwlK3ozVEkgeOAvGQVRFEkAmCIAgCxirLSMp3yrJV3aJXJGbm2dmlG3KyehvYMLPU/WitqUaWcSAesiqBpL0QBEEQBFx5yPKMN+XkIZv7+OV8t/4wFyfWxO500v8/S7zOO5zG5t7p9XpTbft7YM8vVT8aiNOnjAMJ6q8SiCATBEHwIc/m4FS+nfiY8Mo2RahAtIZIXT55yFyEWi0M7trIfWy1KLcI8yS/2oXGFk4n9peqH6014VRuPJxQMmTKUhAEwYcRn6ym64RfKtsMoYLRaCNTf0gkWCrm67GwHYdGLjbFVErpksQ6NYRhMw5CIkrVhlCxiCATBEHwYfne9Mo2QagEnBoiyS/zbZOKIiI08Kbj23Rj8kNi4dsHCS3IKHG7GgjXIsiqEiLIBEEQCkFr/6kk4dxFuwRZGW8sXhSzH+pOj2b+Wxs5sbCu4V2gHdRLKbm3VmtNmGuBQqgIsqqACDJBEIRCKHB4J+bcn5ZNboGjkqwRyh9tpIqoQA9Zy7qxPNG3ecBzKxsOh4T2VM8o+bSl1hDuEmTiIasSiCATBEEohAL7aUFmczjp/WoST361vhItEgKhtebtX3dxOCM34PnZa5PJyrMF0Y7LQ1axOcishQSSOZwaGnQiNmtvibP2a7RHDJksTqkKiCATBEEohOQTuczd+BcAKRlGwtCV+45XpklCAPamZfPqwp088vlav3MbDmUwctZG/vXd5mLbcSdTreBtk1Qhgf02h4amfQm1Z8HO+SVq0+mEMC0esqpEuQkypdTHSqmjSqnNHmWvKKW2K6U2KaW+VUpVN8sTlVK5SqkN5uu98rJLEAQhWO6dvprHv1zPiewCFm49AkCtmLBKtkrwxWZOLefa/KeTs/PtAKRm5nMwPYd8u4N1B0/w39WH/OpqNBEUVEKW/sI8ZE5oNZC88Fqw4YsStSgesqpHeeYhmw68DXzmUfYz8IzW2q6Umgw8Azxtntujte5YjvYIgiAUy9HMPPf7v04a77f8lcmEedsAiAmX9I1nG648XpYArianuTAjz+7gild+48aO9flug+H1vO1iIx+Y1prPVx7kWFZ+hceQQeEeMrtTgzWE7OgmRJRwX0utPT1kkoesKlBuHjKt9RLguE/ZQq213TxcATQsr/4FQRBKw+6jp9zvI0KNP5F3frTSXRYwiafdQeLoeXywZG/5G1jFcDo1ryzYzpGTecVXLnUfxs9AsViuhbIuL9qibUf96uxPz+Ff323mtx3HiNAVu8oSCvOPwcH0HH7Zmkp+eC3ITClRm+6gfmUFq/wTURWozE/pHuBrj+MmSqn1QCbwvNZ6aaCLlFIPAA8AJCQkkJSUVN52curUqQrppyohYxIYGRd/qtqYbE0/Pe2VZ/MPpN7yVyZjP/sZiwV6NQwF4GS+8a3/1i/baO48GFQ/VW1cSsueDAdTV+Tx68Z9PH1J0Z6a0o7J3pPGZ5YT4PpNxwwfwMlMQ2jn2+zuc/N+/o18h+YfSacXA0SQR/KxE+yuwM9mT0bglbuLth9l0faj/K9xLPVz0liyaAFOa3DTjxknc3HaMnGoUJaeg8/Zufj7UymCTCn1HGAHXJPiKcAFWut0pVQX4DulVFutdabvtVrracA0gK5du+revXuXu71JSUlURD9VCRmTwMi4+FPVxiRkVxqsXllknU+3GlNBY++8EoBV+47Db8sJDw8P+l6r2riUltgDx2HFcqJjq9G7d/ci65Z2TOIOnoDlf1CtWhzfpERxc+cG9GlZBwD71lRYu4ZDWYa4LvDQ2BPWOBl1TUtgo7ssknyqJzanYQV+NtUPZcCKZYWef/vQBXwQBp0aRhDbsldQbb697Q9iT1mwOqPOyefsXPz9qfBVlkqp4cAAYJg2sy5qrfO11unm+7XAHqBFRdsmCILgLGEyWKdTc9v7y4HCt8ERDNYfPEHSDv8pw9KSU2CnwO7E7jA+M6Vg7sa/GPHJanedPHvheeNSTuaR4jGVasFJOLYKD+ov7rFZ7WwJgOXQiqDbdGozqF9WWFYZKlSQKaWuAUYBN2itczzKayulrOb7C4HmgARjCIJQ4QSSY01rB44p2paSycHj7j9lAYPKhdPc9M4fDPcQSyVhzf7j/LEnzauszZgF3Dh1mTs+zBkovi/AtLMnx08VuN9HYYqzCg7qL44MYtnpbED07y/Crp+DukYDobpAsvRXIcoz7cWXwHKgpVIqWSl1L8aqy1jgZ5/0FlcAm5RSG4DZwENaa0n2IwhChRNou6RQa+A/lddOWcr+9Gz3sQgyA6dT89OfKQEFki8nsguC2qLq1veWM/QD/6nkrSmZTPllFwAbk0/6nX9q1ka/Mk88PWiRmBt6V7SHLIjHZrTtfuPNvsVBtel0rbIUD1mVoTxXWd6hta6ntQ7VWjfUWn+ktW6mtW6kte5ovh4y636jtW5rlnXWWs8tL7sEQRCKIpA2CLEW/o3pmSjW4vEX1eHUTP1td1AZ4iubjJwCZq3xz8tVGOmn8skLkPPLxYwVB3j4i3XMWX/YPZ6+21AB7Dl2ik7jf+ahz9fSfdIi8s2px8U7j/HWol0B2/5w6V4Opud4la3a7///e2aeLSihl+sRVBalTEFWwYlhW9eLY0CHevzv0R68PuSigHXW6RaciGoCx/cF16hrL0vJQVZlkEz9giAIHmifSctLmtTEain8T6VnglGXh6zva0nc8cEKXlmwgxd/3FY+hpYhT361gX/O3uSV8qMoukz4hbs+WlXo+U2mp8ruIcJcCVo9OWRO9y7YkspfJ/M4km3Uv/vjVbz2886AbU+Yt417Py1+2rPDuIU0eebHYuudyjcE8/WWFSwO/4dRWMEeslCrhbeHduaiRtUZdFEDr3OTbm7vfr8yK57MAxuCatM9ZSkesiqDCDJBEAQPfJ0qXz9wKZNuah+4MpCefToGyaIUB9Kz2Xss21h5CWTm+QuRsw1XjjBbAC9WYQTySrnbyzTSSNSMDnPL25wAm7JHhXkv9M/MD25Bhc3hDMr7VRRN1WGGWX8hNyebOLKZGvbm6ZOVGENmjBzkNAAAIABJREFU8VkZUiMqlAc6GF6uP5xtiMtNhuPFh1g7tSZU28RDVoUQQSYIguCBb9iTUoo29eOCutaiYO+xbK+yMxUOFYHLK1jSEDibw8kJD0HqwrVy0XPFaiAPme+q1PS84MaqXrVI8u3Bi8fanGBu2LP0tGwCoK9lHYvC/8nE0I9pl7mEt0Lf8r6gghPDFkVcZCjVwoyBWursYBSuKH53QXemfsnSX2UQQSYIguDBmQioPceyGfO99ybWzuB1Q6XwzJw/2ZlqTFWqYhMweDP6mz/pNP5nv+D90x437fY4BtpnssBHVOXa/c8/M2cTqZneWf4jw6xFxrD58mHCHNpb9vNkyBysOPg47FX3udisvVxk2cNCR5fTF1TyysQhXRu53yfWiibE/Kbep+vyu6MtbPyq2AdLawjV+eIhq0KIIBMEQfDAJS1u7tSAD+/qGrBO18Y1Cr3+0PFcr+OS5jUrSzYfPsnk+duLFJlfrjq9s0CgbaF88azz/cbDAGQXeCsp1/Sk3el017c5/Nv2DfT/ekcBR7NOi68Wz//El6sO0e3FRV71Qq0q4C4KgUjgOO2zlgDQ1bKTPRH/53W+L6uorrJZ6WzN87YRRmFMQlBtlxeTb+1Ag+qGZ6tuXASh7m9qxffO7pB/Ek4Ywf3frk/2SwcCnlOWEkNWVRBBJgiC4IFLvNzX80L6twn8xewrLYpKCBuExik3bn3vD95N2hNweu/P5JN+e2/anU5W7k3nq1UHmThvK9+uT3afczg1u1KzsHt4ZsLMdCCnPKYjPcWfzaGLFHmBRNo+nynfQCzYksrJXCMYPxQ7Vhz0saznGoux0CCKPB6xfkcE+dwT8hMWp43HCx5zX5+po7g4byqv226hpcW4x7XOFnzuuJLWeR9DXP1ibShvvnm4O9883B2LReGZdWW30wj6z0s1Fj38/euNAdOBgCuoXzxkVQXZcVQQBMEDl54oKp7KV2S8M6wLD32+trAWy8awMyCQl27g27/7ldkcmiHTvLPBD+hQn6NZ+Xy96iBv/rqb7x7t4T4XFmIhu8DBqTw7VDPKPMWf3aFxBOg7O99OdHhIwEUEv+04FtQ9Xf2G4fX6I/xxaqvT+cc65r3PuvCHsChNc8thmqoUChp0Y+6e7jgLLKxytuIY1QH42HEtj4Z8RzrV2KgvBCCXs8OjVLdaBHWrGbZYPR7GQ9rYEip1zmgat7nWXe6awo0ItQIuD1kBhEoMWVVBBJkgCIIHLvlQVJJX3ynAZnWiiYsICbiisjI9ZCEWC+DEHqQR9gAC6Z7pq1m6K42OjQwRc8hjZ4LwECtgI8vDQ+bpLTOmLP3bbDt2Aftfut4vhgzgvcV7grIVDE+YpxgD+D38SSzKuN+brMb+kPbGN8IemOe81KtuFlF0z3+bXMLQZ/GEkaeoPWYq38b2/bA3yV3ebuwCIkKtbP731YArhkw8ZFWJs/cJFARBqARc3qSiPGS++iY8xOqefnu8bzPeGNLRfc7h1Pzn550kn/BOZloRuKZSHR5Tg6v3HyftVH7A+oGmEJfuMuKTXJ6v/2047D53xAy2P5VnJzUzD7vD6bWa0u7QFJZJw+ZwBkwWWxRN1WGut6zgPus8ALpZ/HO8xag88nUIk223GzZgJaTdIK86beqdXjWbRjWW/usGmtWJKZEtFUlUiOfDqOif/7Lx9qthWHHF62m/qeMwnS8xZFUIEWSCIAgeuKcsfcpH9Eh0v/edAgwPtbin3y69sBY3djqd3HNXahZvLtrFozPXl4e5RRJiBh95esgGv7ecW9/9//buO06q6vwf+OdM214pS4elV2kLoiIsooiK3VhjbN9ojJrEGPOzYTcSTTTR2KOJGkuMmthRVEDpCkjvsCC977J9yvn9ce+duTNzp+20O7uf9+vFy5l779x75u648+xzznnOAsPjnWFm7mn348t1wYuD7zragOP/8BUe/mSdQYbMODu360hDTHXP8lGPr7JuxzOOp3CP/Q380fYipuZvRpO0Y3LT4/jKPRJ/cv4EAJAlXHjOfQ56N/4LFxa9DXTxBch9OuTh+N6l3udnD++C0jwHinLsUbcl1TrkWjB92mDv882ymzIBobkW/cVOw9dYpbpCBDNkGYMBGRGRjhY+BGbI7jt7CC4bq5QjCAwysu1Wb9CT47D67XOq2xuaU18gVut21Qbia12SVYeMs3UugwyZJtxs0cNqLbLPVu9BXZOvHEW4Qf1Vh+rw8rdRLgMEoJ/Y5ff8EtscDPRsxQ7ZEVtkV1znvB2vuacAABZ5BilthgW2bCXz9a/rjsc/rxmDr26rREG2L/gKnJBx7UnlUbcplYZ3K/J7/o1ak2ykZbPh8Q6p1odjhixjMCAjItKR3i7L4D7Ls4Yps+/Glpf6bc+1W5GrBmK5gQGZGgTFWuMrEWxqtKEFWheEyIxpjMZ0acLNltTOv6+mCbuO+oI9p9vjNytTb/uhemw9GHlGpWagRSnP8YH7RFzXfBsAYLhrBbarg9wBoAZ5uLDpPjzd4X7vtrwsZaj0+H7tUTlAPVYXXAaOFZw6tFPUbUolW8AC9ztkRxyUhaiwbDA83u7NkDEgyxQMyIiIdEJ1WQLKl3rVjLPQp4OSdblyXE+seeB02KwWvPnzcbj6xF7efZqj9elbXNyqBmROtwczV+/xrjEZSmDx1tOH+Mp+bAqzzmW905f9e3XBdm928S9fbsJjM40DBv3kgEiKUItH7S/DIwV+7bwJCz1DvPv0jwFgqRyArp07e58XZIWfu6a1VQvErSb9VrQF1VYRWIjhmGBZCaOZvDapjhNkQJYxOMuSiEjHt4xQ5FmWQvgyMCO6F3tnIpqFFpCd8ue5UR0fGJD17ZiPz9fsi/w63TqVa/fUoEtRDnYdVQrkav+NdK1wfmr9EgDwonsaAIF6ZGO9pzsGWn7Ee+6Tg44vK/QFIXlZ1qD9+vBFy5D5tqU+kxkNmzW4XZtt/XG2+1u0Rw0Owtel6fFIpUo/wDFkGcSkfwsQEaWHliELV+z1gtHdcMrAjrhpUt+oz/vjkXr88o2lhms6Jos13JswEDjOrWdpdGs66hcOb3Z5UJAd+W/9wGK1HQpCBw7Pus/BytPfxQzXZbBbBU7s0w6XNE/HH3v9HUdREHS8/n1H+hnFeIvSJjhDBuy0KZNHrrTNwnDhG0vW+65PcaT6mPpCZsgyBQMyIiIdj7fLMvQ3dWG2Ha9cPcYvExNJfbMbn67ai09W7Ym3iVGLPSDzD5La5Tuie11AtitwYoORY41KV+44dcZjefvQwZ+EBdm9lRpigzoXIttuRTXycaRgQNCxD503FHa13/H20wegZ7vwQaU3Q5b++r1hKTXl/O20KAHZr23v44OsewH4srdZ4BiyTMMuSyIiHRlFHbJ47Dxcj283HYhrEfNoGWVVwgkMrAqjLAWh77IEgBx75IBMG1uXn6VcI8tmQZc8gd11vvsytGshVu+qAQB0LMjCP64eg+Hdi3HTG8sAAL07KMFWrsOKBXecAo8ESvMcaGh2o7rBGdWMyXBd02ZiFFzvRQe/5w44vX9QZAk1IEvzQukUPQZkREQ6ocpeJMpTXytdSzccl4VJybmEV7jVBoxUNzT7PY+m6xEAvl7vX5ss12HFwE4FWL/3WMjXLN1+BACQn+Vb6qdbgQW763zB3YCyQrxwZQXmbjiA4lwHJg1UZklu2q+cd0T3EiybfhrsVuFXyiLHYcVdZw4KeW19LBx4i8wanxm1yyMElnv6YqjYBrtw407bm3C5pwFghiwTscuSiEgnXNmLWDx5yXDvY6Oio4cbY6tS3xJGA8HD2VfjX8E/P8IMxVCy7VZ8dMt4jOrhP8mhd4c8fPbrkzG8e7G3blujU7kPbo8MmiuY67Cia3EOLj++h9/2u84chDOHdcKYXiUozXP4BWOx0hJPJu+xNPTj4Qac3/wgLm2+BwBwje1zeHYpBYizodUh46D+TMGAjIhIJ1zZi1icP7IbsmzKr1ijgCwVmZhYM2R7qxv9nitrVcYu12GF3WoxHEs2qHMhsnS1JYarM1ONypzlGsyQBIALRnXDs1eMbnHQrA/9tHt0szr4P7BsidkYZS03Sd/KEKhSFl1nhizzMCAjItKJZnHxWBkFZBYTllfQ1qbUOEIU5dJmRN4wobfhfm0MWeBAdC3YtatrM14/obc3i+bRRWQ3TeoDAJimFuJNtNI8X9ZIm5hx2uAyVM04y7RLKHUuysEZQzvhtWvHBu2rQT76NL6OnbI9xN7VAIAswQxZpmFARkSkE83i4rEyGpCdinIL4arrG9GWQNLouzzPHeELjo5XVyoY0KkAd54xMOg8OQ7jrs4rx/UEAFQ3KNmbbiU56KXOrjxnRBdvwDaocyGqZpyFYQHLBSXKVSf0xB8vHIbHLzoO14cIKs3GahF47qejMbJHieF+N6xY5SmHbddiAFKXIctJXSMpLhzUT0Skk6guSz2jdSBT0WWpX1R8ZI9i3Hf2ELg9EhdGWEJJY9dlyP566Uj8/OTeuO7V7/DguUNx1Ym9UNGzBEIIPP75Br9rlRUqWZk91f5FYa8dr8x6PHBMGavWtTgHZYXZ2PDwVDisFny4aH3L3miMbFYLLhnTI/KBGWauZzjOOPYdTrEs1wVkzJBlCmbIiIh0fLMs44+YtFMYLdqtZciq653odccneGvJjrivF0hbTBwAhndTVhIY3dM4w2LEHjApYGjXIiy+61SU5jkwplep9x5pGUBtzFxpnlK/TFsbc2CnAvzm1H7e8xysVTJxXUty1NdZM6b8hBkErpeqec89AUdkPs6zzkcWuLh4pmFARkSkk4w6ZEZdh9qmHeqajq8v3J64C6r0X9xZdt+v+6cvG4nfTenvd2xHg0r5QgjM/l0lFtxxStjraOPtrlHrfo1Su9W0rNldZw7Cb071XU+7H12L2Z3WEnNvn4RXDcaSOWHDIs8gnGNdiHKxFx4IwGrOMXEUjAEZEZFOMros+5UFz9zTVg7yeBe1TnyGSD/eSD+z8ezhXYJmE/Zsl2t4jvL2eegSIXDSml45oAOqZpyF7qXKubTMYHZAodj26goA8ZSraMs6FGRhYv8OGNgpeNmo19xTAAA/sX2DJtjNW1iNgnAMGRGRjpYhS8QsS7vVgkanBw+cMwQfr/RfMknLHrm16yUhINOvBpAVEBQFJu2Kc6NbJsmIJaDrUqO9x8DK/R/dMj6oxAYAXDHIgR5dS3HqoLIWt6UtsRvMgl3h6ePbL1O3birFjxkyIiId71qWCYiP/n39Cbixsg9K8xz41eR+OKlvO+8+LUPmVB/EWMM1KvqYK3A8mJaZ69cxH9eNL0ehLls1pEthTNfR7lVgEOvyKO8t2+7/VdO5KMdwtmBJtgVPXDwiKKNGxgJ/psO6FqEe2XjKdR62eDpjRb9fpqll1BIMyIiIdLyD+hPQaTm4SyH+39SBEELgt6f1x5MXj/Du0wKyRvXBsh1HsWTb4bivqaef3BkYLGkB2YBOBZg+bTDcHt8EgJevGoOqGWdFfR0tMxaUIQvRZUmJEbi4/Us/q0CP0lw84boYk5v/jC5n35OmllFLMCAjItLxDupPwm/HPN1SRFp3nn5h7lveWpbQ6+kr0g8OyHppwZoWROnXftTKVkTL22UZEPQ51VmeWXZ+1STDryb3w82T+nrH8NmsAj1KfWMBs1u40gKlB8eQERHpJGNQv0Y/lsqpXqfJ5QvIAivbx0t7L49deBxO6N3Ob5+3AK76vGNhNh46byi27K+NuQSFdnxg87XZlMyQJcegzoUY1LkQbyzejiP1Tgj4Zyl53zMLAzIiIh0tq5SMulgWi0BZYRb21TShUR1vrc+QBY4JipeEMqPx4jHdg/ZpY+X0XZlaJf1YaTFAqEH9zNQk1/kju+GV+duQ47DCpvsZaHXhKDMk9aclhHhFCLFfCLFat61UCDFLCLFJ/W+Jul0IIZ4SQmwWQqwUQoxKZtuIiIxIb6CSnPMvvutUDOxUgAW7XXj447VocPoCskSXvlDei/E5S3KVQfzdSuKvBRaqy/KR84eiXZ4j4YEm+bv7rEFYcd8U5Dpsfp+hZMzcpeRJdvj8TwBTA7bdAeArKWU/AF+pzwHgDAD91H/XA3guyW0jIgrinWWZxMW/tRmNf5+3DTNX7/VuT3SXJSBDzhY9ZWBHPHvFKNwyuZ/xATHQgoDAAOCK43ti6fTTWIU/yawW4V0UPRn17Cg1khqQSSm/ARA4behcAK+qj18FcJ5u+2tSsQhAsRCiczLbR0QUyNdlmbxr6OOuxbqZlbZEd1nK0GPhhBA4c1hnw1pWsdLuVWCGjFIvN8TC7mR+6fjJlUkptQqJewFoFQC7AvhRd9xOdZtfNUUhxPVQMmgoKyvDnDlzktpYAKitrU3JdTJJW7snW466UeeUOK5D+P9l2tp9iUam3ZOtW5Q1AL/99hvYk5Rt2HewwXB7fV1i79XuPU1obnYn/f43NSpFXpcsWYyq3JYHeJn2WUmVWO7LpGKJ9wD0L7G06nvZGj8raQ2lpZRSCBG8yFv417wI4EUAqKiokJWVlclomp85c+YgFdfJJG3tnlx9xycAgKoZp4Y9rq3dl2hk2j1Z7dkEbNqIiRMmwpGkQdH/2v49ttXsC9peWlyEysoTE3adTw+uwMaag0m//3nfzcaBhnqMGXs8ytvntfg8mfZZSZVY78u0Kclri1m0xs9KOqZg7NO6ItX/7le37wKgnwrUTd1GRJQyMoGV+kOZPm0QpvYKXsfRFpCRa3Z58Ou3l2PbwboWXUfK1CxlqA3qN1pEnYiik46A7EMAV6mPrwLwgW77z9TZluMAVOu6NomIUsJXqT95erbLw6UDg9eODBzPtWHvMXzww27c+K+lLbqORHLfh0YbzK9fO5OIYpPsshdvAVgIYIAQYqcQ4joAMwCcJoTYBOBU9TkAfApgK4DNAF4CwEW4iCjlpEF9rmS5bny533OXbvkiwFfhfmtcGbLkv49HzhuKoV0L0aNdbuSDichQUseQSSkvC7FrssGxEsBNyWwPEbV+S7cfxto9x1pc5NRbwT4FqaWpQzvh5XnbvM87FvivTagtPdTs8g/UoiWRmozV8b3b4eNbTk7JtYhaK86PJaJW5cLnFgJoedV5b5dlCiKyPF2Jgk6F2ci2W3Ckrhl5WTY4bJb4x2SlaAwZEcWP6yoQxWjepoN49NN16W4G6fzm7eW47p/fJeZkMnQx1UTrqC7ifc1JvWC1CLzz/U6MfGgWfvmGMmbM6Y4vIJNgQEaUKZghI4rRT19eDAC488xBaW4Jaf73w+6EneuLtfuQqrHp7fOzsGz6aSjNc+Crdfu9279UH8ebIZNSJnXFASJKHGbIiGLAaf2t3/q9x1J6vdI8Zbal0ZI3LnfLxo5pmCEjyhwMyIhi4NR9QXKKf3Jt3n8Mr+gGvLd21Q3OoG1Ogz8AXl9YhXV7aqI6Z7ilk4jIXBiQEYXh9kgcqm3yPtcHZC5my5LqnL/Nx4Mfr20zge/huma/50/O2gi3JzhDNv2DNTjjr99GdU4lQ8aQjCgTMCAjCuOvX27E6Ie/9D7XD7J2GQy4rmmWaHS6U9K21q6+WbmPbSQeC/LXrzbhYG1z5APDUMaQEVEmYEBGFMacjQf8nr+79Efv47pmF372yhKs3+vrPvrV1/W45h8Jmu1HAHx1wWIVbWatyeXG2t01Mb0mVb5YE7zeZSwkwD5LogzBgIwojPws/4nIf/h0vffxd9sO45uNB3BmQPfRwq2HUtK2tqKlPcPRTsC4939rcOZT3+LmN5ehOc5B9Ik2d+P+yAeFwzFkRBmDARlRGIEBmV6D2jXpkcAjn6zFjkP1qWpWm9LSDJk7ytd9v/0wAODjlXuwbk9qZ1gaubGyj/dxYB2yWDN4EpJjyIgyBOuQEYWRnx36f5FGpy+b8tK32zBzzd5UNKnNaWkvosF4eEP6NSutJg5ePC1IFXKWJVHmYIaMKIQmlxu7jzaE3B84eP9QnAOwyVgiMmTHGp34z/c/Gh6nD8jqm10tulYiXTiqG/KzbCjJtftt/+tXm2K+F5JLJxFlDAZkRCHc8uZyLNp6OOT+Rpd/QOY0GH/U6HSjycVZl/FocUCm6+6767+rcfu7K7Fy51HvNmmwiHiDCWbI9u2Yj9UPnI5+ZQV+22dv2B/zeDoJVuonyhQMyIhC+GJt+Blujc2BAVnwt+Wge2fixEe/Tmi7WiMpJR76eC1W7awO2tfiQf26QG5fTSMAXzfz64u2o+/dn+HO91f6VeZvaE5/QKbJsvn/erZZRIuCU2bIiDIDAzKiFjrWFLl7S0rgUB27MiNpcLrx8rxt+MkLC7Bh7zH0uuMT775QA9lrm1y48LkF2LzfF1D9eNg3sSLcLMsFmw/C7ZF4a4l/N2a9mQMyqwV3/3d1TOcwWRUPIgoj7KB+IYQVwGtSyitS1B6ijPGP+VXpbkKroQUOUgKfB0yOCBVXzdt0AEu3H8FjMzd4t13x98Xex34Bmfrwq3X7sOLHo8h1GP/qu+0/K2JvfII8/9NRfssnZdmsfvstAnhv2c6Yzsl4jChzhA3IpJRuIURPIYRDSsk/84naoGU7jgAARvUogdsj8cmqPZg2rDMsBotht5TWvSgRPGYsVDedtlnfJXdQt8yVUdmLF77ZCgAob58XR2uTY+rQzn7PHQEZspZku5RB/eyzJMoE0XRZbgUwXwgxXQjxW+1fshtGlE7NrvgKhP5r0fYEtSR5pJR4Ye4WHK0P/7fWBc8uwAXPLgAAvDxvK3711nJ8tHJ3QtviHYAvg8s7BAZke6sbsXzHEW/2Rz9LUr++aLgyEdsO1sXX4BQI7LKMttCtPy6dRJQpognItgD4WD22QPePqNXSdx0FmjK4LOLr7/lfbGN90mH+5kN49LP1uO/DNVG/ZttBZYzWscaWl4eornfi5699j7m6Zam0bJZHyqAuysDM0KlPzMX5zy7wBmr6BJBLN9PV7ZFYu7vGuyxSONl28w2nDcyQRVvoVo9lL4gyR8TCsFLKB1LRECIzqWkMHZD17pAPIPIag2ZaF3HlzqPo2zHfb+yU9h6jXQy91x2fYFzvUgBAtt0a4ejQ1uypxqy1+7B5fy1m/64SgH/2J1KXZa06meLmN5cDgF9ZB30wt3ZPDX75xjIAwHHdisK2Kcdu9Sv0awYOq39AFm7ViFAkGJARZYqIfxYKIWYLIb4O/JeKxhGlS7jurm4lOUHbuhYHb2uKs9szUWoanTjnb/Pxq7eW+23XArFYgquValkKo+FjC7YcxAc/7AIA7Klu8MuA6Wn3ZdvBOox48AtIKb1djRLBmaA6NQB7ZvZmPPrpuqDzLakyrhW3Xy11oT9HKIGD/J+6bGTY41Mhx6H8XDoVZht+vgCla/03by8PuWyXlKxDRpQpovmT63e6x9kALgSQ/nLWREkUbrhOl+LsoG0DOxVgV0BV/xfVAeTp5lLHZ31XdcRvuxYYZduiD8i0wqlGBVQvf0mZ4XjuiK44++n5OFjbhKoZZwUd16TLRB2td+JYkwsnzVD+xvNIadBF+Q22PXomHv98A4wcONZkuP1PX2z0Po5UziLX4X8PbAmcsNBSRTlKpX6XxwO7zYo1Bl2vC7cewv9+2I1Ddc14/brjg/YzQ0aUOaLpslwasGm+EGJJktpDZAoyTMEAhzU4gDH60nti1sbgjWmgxRaBKwn4MmTRj5+yCgGXlBELqGqzHY26bQNXLtBnd6TBoH4AKL/z06jbqKnVZcUiVeC3BXQPWkwQxRRmKwFZo9ODgyGW5dJ+tqF6x7mWJVHmiKbLslT3r70Q4nQA4QdkEGW4cAtT26wCz10xKmBr+K+9Xnd8gsNpKhCrjc9yuj1wuT14f9lOeDzSO2Yq226Fy+3Bih+PhjsNAOW9A9FXtDeaGRjYlRs4QaAlg9cjqW8Kbq8+5gqIx2A1QYasMEcLyIzv9ZG6Zm93ZMjSIABTZEQZIpo/jZcC+F7970IAtwG4LpmNIkq3cBkyu1XArvsGv2BU16i+89JVakGLiZxuidcXbcdv31mBt77b4f2iz7Jb8ZcvN+HcZ+Zj9S7f0kUvzN3iVzEf8GVi6tXX/ni4Hs/P3RJyAkPgclLVDU7c+4H/DNRjARMoklFwt9lgndFJAzp6HwdmxAIDtHTwdVka39tpT8+LIkPGshdEmSKaLsvyVDSEyEzCJWmybFbYbUpAMrZXKZ64eASuf+37iOdMV9ZFnz2paVCyUXuONsKppgHnbz4IqxqQ7D/WCC0B/uhn64POpWW3dh1Rxsv9/LXvsX7vMZw7oovhtZ0BqcbZ6/cHzWasjWIJqmT42+UjMXP1Xvz2nRVBAZkZuiyLc+1h9+862uAt+hpujUsTvBUiikI0XZa5Qoh7hBAvqs/7CSGmJb9pROkTLiArzXN4AxgtyCrJdUQ8Z7p6wfTdhtp4sSaX21uMden2I96ZiuG6avXmbT4IwDc26/1lu7z79LXAnAHdk0bBwZ7qxuCNMSorzMK/rx8X02tyHTb0KM0FEBwsm6HLsme7XO/j168ba3iM4BgyolYjmsT8PwA0AzhRfb4LwMNJaxGRCYTLOJTmOeBSIxdtTNU90wZFPOeTszZi5c7I47QSTf9etBIXjU6P4UzSD1bsxlNfbYp4Ti3Q0qrJ62dA6seIBXZZGs12DDV7Mhb5WTYc37tdzK9TasoB140vx7z/N8m73WqCtJJWiiM/y4aT+3VAWWFW0DFaK9fvrcH6vcGzMCUkl04iyhDRBGR9pJSPAXACgJSyHvyji1q5wFhFX74h2271Zp208ggF2XZcUtE97DlnbziA89UliFJJn/XSAqgGp9sw6Pxoxe6oZoc2uz14fWGVYVA35L7PvY+dbg+ONnrwgjrOLFI9sJZqaaHa0jwHqmachTOHdUa3klyUqN2EZsiQAcDs31Xi69smAoDfuEUTHeGsAAAgAElEQVSNdvtrGl2Y+pdvg/czQ0aUMaIJyJqFEDnwTtgRfQAYF/4haiWMgpXORb76Y1rmR18u4ZbJfSOe1+2RWLLNuJBpsujfyx3vrwIAvLt0J/65oKrF52xyeTD9gzXYvL827HHNbg9eWNmERz9bj/V7j0U9OzNWoQKyP144rEXnMUtAVt4+Dx0Llc9dYOV+IHLBWy6dRJQ5ognI7gMwE0B3IcQbAL4C8PuktooozYx6LD+/dQIW3zUZADC2vBR5Dit+MbGPd3+3klyc1zf8QGwAuPiFhQlrZzSSUUYiWi63RL0aMzS7PKhLUkBWkG08P2nacb7JBn075kc8T3d1TFmkQrLpYJQhi7SmqAQr9RNliogBmZRyFoALAFwN4C0AFVLKOcltFlF6GZVxKMy2o0zNVpTmObDmwakY3bMk4HW+xzlhutEGTZ8Jl9uD6vrQa2YmSrhloJLN6fZAHWaHc5+Zj3/M34biXDtumtQn/AvDuO/swUHbxvRS1tjsX+YfdOl/BuHGBWrOGtYZAJDXgnUjky1wsXEgeM3V+mb/AE1KsM+SKEOEDMiEEKO0fwB6AtgDYDeAHuq2FhFCDBBC/KD7VyOE+I0Q4n4hxC7d9jNbeg2ieCUihDl1cFnIfQ1ON+79cA2GP/hFUOX6REtjPIZmt8dvdmmTy4M8h81wtYNonDuiCy4ZEzxWT8ts/ev//JcPsuguHk2i8Gcn9MSXv50QFGibgd0aHFkFVvA/VNuMD37Y5f2DgvEYUeYI92fgn8PskwBOackFpZQbAIwAACGEFcqszf8CuAbAk1LKP7XkvESJFG9W6deT++GXk/qgXZ4j5FitNxfvAKDMeMyKYT3JWBlVy4/krSU7EnLtbQfqsPmof+mLXIe1xYVX/3qp8aLfBWpGy2iclWZAWUHE4rxCCPTtWNCyxiWZUZdl4IzYxz/fgA9X7Eaew6b8QSABYYIit0QUWciATEo5KdS+BJoMYIuUcjunZpOZxJtUEkIpINunQ17kayV5jFc0XXWB7lQH/8frtv+sCNqWm2Xzy1y1xC8m9sGuow34aMVuAEC+OoYscE1KvVtP64+rTuyFy15aFNe108WoyzJQ1SEl4DzWpHRlKmPIGJERZYKoBkoIIYYCGAzAO81MSvlaAq5/KZRxaZqbhRA/g7JU021SyiMGbbkewPUAUFZWhjlz5iSgGeHV1tam5DqZpLXfk3WH/LsRo32vzc3NAASqqqowZ85uVO2MPEbsm2/nozAreX+QbKs21wD15roaVG1t2TJS2s9hXA6AHOAjNd5bv+oH1FVZ4AzIBup/bquXf4d2ORbDfekQ6/9Dxxe5EVzYwt+ho0otsoU/rEVJ9WYcPdoAmyX97zVarf33SkvxvgRrjfckYkAmhLgPQCWUgOxTAGcAmAcgroBMCOEAcA6AO9VNzwF4CEpy4iEoXabXBr5OSvkigBcBoKKiQlZWVsbTjKjMmTMHqbhOJmnt98Sx+SDw3WLv82jfa71zNnI6lOD+c4agMNuO2pW7gdXLvftH9ijG8h3+xWHHjjsBnXQlNRKt+MejwML5STt/rLqWdUC/8lJg47qYXxv0c5iprLVZedI4dC/NVbqav/jU/3j1mEkTxqM0z+F7TZo/v7H+P1QJYM6BBfh+e9DfqV67apWA9J0NThSXdUdh0RFk2y2orIxtFYN0ae2/V1qK9yVYa7wn0eSyL4LStbhXSnkNgOHQFruLzxkAlkkp9wGAlHKflNItpfQAeAmA8VohRCnQ0iFkuXaBJy4egcJspfyFfpaf3SoMxzg5DRa+TiRtDNmTlwyPeGzHguBq8LE4c1iniMfkOqwxdVkO6xr5141W9iLcebXFugHg9CGhJ1yYmf792SLcwxe/2aouLs7hIESZIJqArFENklxCiEIA+wGEL0kencug664UQnTW7TsfwOoEXIOoRWRC5ln6B2S3TRlgOA4o2QGZNoasfX4W5t9xCs46rnPIY39+cu+4rpVjjzwKIi/LGtO6nnlZVqy4dwqWTT8t5DEF2cH137653X8YrFbsde2Dp+OZy1s8UTyt9EFYNJ9QNwvDEmWMcGUvnhFCjAewRAhRDCVrtRTAMgBxVbYUQuQBOA3A+7rNjwkhVgkhVgKYBODWeK5BFI9ElYrIdigBWb+O+fjFxD7epYv0XEmuS6HNGLUKga7FORgTpqRDvBXqtcXLw8l12KK6zk9GdwMAWIRAUa5d6W4MIfB8E/p3QA/d4tyB1w83+N/M9O8zmtmztY3Jr3NHRIkR7s/ZjQAeB9AFQB2UbNZpAAqllCvjuaiUsg5Au4BtV8ZzTqJEStTMx2y1nIV2NqPSBUnvslTfizaT2R5mtp7NoNZVLKKZCZjrsMISRdrmnBFd8J+lO2PO8Ky4dwpyHMkrI5JOWkD25CXDceu/g2ewBqppdKErU2REGSHkb08p5V+llCcAmADgEIBXoCyhdL4Qol+K2keUFlo8ZrUIjO/bvsXn0Yp5agGeUcDS6PTguPs/x3+X72zxdcLRvxfAuFZXp8JsCBF/hkwfcA7vXhzymFAB2ZY/+OpBa+0ONwbqTz8Zjpd+VuG3rSjX7nefl9w9Gd/fc2rEtmcCq3rfCrIiL9EFADUNTo4gI8oQ0SydtF1K+Ucp5Ugo477OA7A+6S0jSiNtDNn7N54YVP09FlrcoWXIjIKhw3XNqGl04cGP1rb4OuFoXVtarGWUpfv2/03ChofOiHsAuD7QeueGcYZdtHarBWPLjbtNrQZjpMIleC4a3Q2nhVkRAQA6FmSjfX58kxXMQrs/0a5P2uTyoNmV3AwsESVGxIBMCGETQpytLiz+GYANUNa2JGq1POp3WLy9PV2Lc2G3CvxuygAAxmskal+Y0XTjtYQ2qF+boaeNWRvYyVeR3m61wGGzwO2J78vb7fFg8sCOuP/swciyWfH3qyqCjrFblWr4VTPO8m6bdesEfHzLeL/jkl0wNxN5A7IYxh0u3HooWc0hogQKOYZMCHEalIzYmQCWAHgbwPXq+C+iVk37uos3SMpxWLHpEV83nL70gqZOXRA6WatVeAMy9fzamLXjuhVh/d5jfsc63aG/6J+4eDju+3ANhncrxrzNBwEAf7lkBPKzbPjxSD0e+Ggtml0evHz1GO9rbJbgv/mMyjX0K/Nfrqi8fZ4uQ8ZON42lBQEZEWWGcBmyOwEsADBISnmOlPJNBmPUVrRkuaFoGC0QXdekBGRanCKlxOdr9ibsS1ebM6CNP3KpG4xmGoabYFCYbceq+0/HLyf18W47b2RXnDq4zBvsBZ7T6P2Gm1QAKNmy//7yRG9UzHDMR/sZJuvzSUTpE24tyxYtHk7UGmjfd4nuRjQqcVHfrCxtpHVHfbpqL256cxnuPGMgbpjYJ+j4WPm6LJXnWhbMbpCpCheQad2tRlmvUwZ2xMZ9x/Cryf7zfYyCPrvB6/W0bJk2jo8JMh8tu+gKk8kkosyUmcV4iJJMyuQEA/pCsZr3liqzK7VL7atpBADsqW5MyDU9Hv8uS23248QBHYKODddlqS3vZJT16l6ai0fOHxbUJWvUPdkzRH2wQJIZsiAXqbXZKnqFriVHRJkpqsXFidqaRI0hC3TVib3Q4HTjL19u8m7belAZCSACuqMSdW1PQNmL0T1LsOaB0w0nGITKkD17xSiUt88DgJhqfAXWNbvrzIE4vne7EEf78wZkTJF5ndi3vd9kCCJqPRiQERnwJClDlm234jen9vcLyDS7jjbA45G6QCQx13RL/7IXgK/78eNbxvvV7BrcpdDwHGN6lXofa8VuoxE41Glol+iXwfUO6o/6FUREmYsBGZEB3xiy1F537qYDugxZYs4Z2GWpNzRg4e5px3XBkC5FeHVBFf65oMq7Xd9NGUuGLHBiQuAQund/cULI82llOc4Z0SXq6xERZSoGZEQGfLPYUhuRudwy4eUetPcSbRX+8vZ5uP+cIbhhYm+c8OjXAIAsXVYsngxZ+wL/9SgrdJm3QN1Lc9k9R0RtBgf1E4WR6gyZQOK7S91hMmTh6MeY6RcNz3ZE/2tjcJdCTO5hw9e3TcRHN4/HwE7GXaIUuyuO72G4/dvfT8KbcawuQUTpwYCMyIAnYEHuVBEi8SU3vOeLMbrMc/gCMv19MFr+KRSrReDKwVno3SEfw7pFP36MInv4vKGG27uX5mJseejMIxGZEwMyIgPJHkN277TBhtvdHqkb85WYaxkN6o9GqC5Ozno0h3A/h3gXiSei1GNARmTA462BlZwvtmvHlxtub3R5vNd+ZvYWHDjWFPe1El1Gg8xpxb1TsO7BqQD8g7UPbjopXU0iohgwIIvS6wursHHfsYjHUeuQrMKwkTQ63fAN6wce/Hht3OdMdBkNTUVPFic1ixX3TUFRrt1wxqpWCJiIzI2zLKM0/YM1sAhg66Oc9dUWJCuIiaRJlyEDgGaXO+5zeoPLFmb7uqgV+vXWPTg1qOgrpZ425pA9lESZjwFZFLQvtASt9UwZQMtSpbqbr8np9n7eAN/C4PHwldGI/bXrH5pq+LpYapFR8ggoP192RxNlPgZkUWAg1vZ40pQha3S6dTXQALcn/ogsnlmb2QZrb5J5CDVFxoCMKPMxIItCmPWWqZVKdOmJSMrb52HbwTrM2XDAr2RBIj573hIe8Z+KTEb7mRp9TJ//6Sh0K4luIXciSj8GZFFghqztSXUQo32hfr/9CA7VNfvakYAPX6qDS0od7Udq9KOdOrRzahtDRHHhLMsoMEPW9iR6+aJIuhbneB9vO1jnfexKQJeltwuU8Virc9YwJeiyMtgmynjMkEWBGbK2JxVlL/5xzRgU5dix/VAdKvt3xMiHZgUd0+j0QEqZkMCQ39mtz2MXDcfdZw2GLYbVE4jInBiQRYEBWduTim6+SQM6AgBG9Qhdz+uHH4/i7e9+xGVjjdctjAYLw7ZeDpsFHQqy0t0MIkoA/lkVBf2st0BSSjz11SZsYtHYViWdA+G7l+b4Pb/z/VVYufNoi8/HHksiIvNjQBYF/RiyHYfq/fat3FmNJ2ZtxGUvLU5xqyiZ0jkQ/uObTw7ads//Vrf4fNrHlxkyIiLzYkAWhTqnLyI755l53sfHGp0495n53sdu9m22GukcCO+wBf9vWZRjb/H5PGlaBoqIiKLHgCwCp9uD+xY0ep8frXfi4ucXAgDqmnzL2jS5PLj93RUpb19b1exKQAl7A99VHcY/52/zPk/HkjR2gyWJ4inQmq5loIiIKHoMyCJodAavJbik6rDhvveX7UpJm9q6tbtr0P+ez/Dl2n1RHb+/phEzPlsfVQbzJ88vxP0frcXDn6wDkLqyF3pGM+aMPofRinctSyIiSj4GZBG4QhQhk1KiKUlZGgpvybZDAIBvNh3wblu+4whmr99vePzv31uJ5+duwaQ/zYk5sDHLos21Ta4Wv5YZMiIi82NAFoEzxOrOh+qa48paUMvVq/ddv8D1+c8uwDX//M7w+IZm5fgdh+vxv+WxZTHNklXS3kNLcFA/EZH5sQ5ZBKGyYBUPf4lzR3RJcWsI8AUneY7oPr76qiX6ZYmiYZYYJp7gn2tZEhGZX9oyZEKIKiHEKiHED0KI79VtpUKIWUKITep/Q1fMTJHmEBkyAPh45Z4UtoQ02mSKXEd0A90lfBHZ459vwOsLq7zPP1u1J2x3oFkCsvp4MmTssiQiMr10d1lOklKOkFJWqM/vAPCVlLIfgK/U52kVqssSAMtcBKhvduFgbVOLX1/XFN3rG5xKAKXNPFyzuzrs8YF1fZ+fuxUAsGpnNW58Yxke+HBNyNemspuvS1F2yH0NiRjUz4iMiMi00h2QBToXwKvq41cBnJfGtgDwL6/giGG9uMVbD8EVJphrjaY9NQ8VD3/Z4tef9dS3Ub2+yel/X896al6IIxWBKy0UqjW99lQ3AACqDtXhP9//6A1c9FIZwsz67UQsm34aAOCtn4/DNSf1QkGW0i17rNGFz1YpGdnj//AlHv98fdTnlTDP5AQiIjKWzoBMAvhCCLFUCHG9uq1MSqn1A+4FUJaepvnoxxy9fHUFbj99QMTXbN5fi0teXIR7w2ReWqOtB+vien1VwCoIeh6PxOuLtqPXHZ9g84FaAIg64A2Vxzza4AQAfFd1BLe/uxIrdwZn2lKZIcvLsqE0zwEAOKFPO9x39hAsv/c0/HpyPwDAjW8sw5+/2IB9NU14ZvaWqM/rSdDi5ERElDzpHNQ/Xkq5SwjREcAsIYTfn/xSSimECPouVYO36wGgrKwMc+bMSWojr5npCzI2rVmJ6vrgr/dcG1CvDkOaM2cOttco3UvvLNmBKSWHktq+dKmtrQ157+P9mRi9/rNtTvx7gxIca4HThk2bMce5PeJrq9VMmGbdnhp8PXs2Fm11+m1/5L1FQa+dO3dOTMFMuPvSUrt2+P4oePrrzd7H0V5n+/ZmQMqk/78SSjLuSWvA+xKM98QY70uw1nhP0haQSSl3qf/dL4T4L4CxAPYJITpLKfcIIToDCCosJaV8EcCLAFBRUSErKyuT29CZn3gfjh1Tge2H6oGVy7zb/nrpCDw2cwPqjypf+pWVlVi+4wiwYAFcUnmu9+qCKgzoVIBxvdslt91JNmfOnKD3pt2r8SdPwBOzNuKGiX1iW/JHfb3Rz/TDfT8A8C9Z0au8Nyon9vH7GVVWVmL30QbkZ9tQmK1c+4nV84Bq/+zXntzeyCqpAbDDu23J3uBxWpMmTYq+/QhxX+L0dfVqYNP2oO0nnTwB9ii60Rc3rod1+7aEtytaybgnrQHvSzDeE2O8L8Fa4z1JS5elECJPCFGgPQYwBcBqAB8CuEo97CoAH6SjfaFYhIAtYFmb9vlZQbPXwhWMve/DNbj0xeBMTGvy2eq9eHbOFjz66bqEndNiMAhq5c5q3Pn+qqDtJ874GtN048oOG5S6OHis2TuGLJTHLjquBS1NvF9M7GO4/aVvt0b1eo+UrHlBRGRy6cqQlQH4r9oVZAPwppRyphDiOwDvCCGuA7AdwMVpah8ABA3ydtgssAUEBg6bxS8gG/3QLO+g8dbmUG0T8rNtyLKFLzehzT6Np1RDIKN44pNVwWVHtJ/ZjsPKeLRjjU7sPBIceNmsAvtqws/oPMEkWcwuxTm4sbIPnpvjP27s4LEoa6pJDuonIjK7tARkUsqtAIYbbD8EYHLqW2QssD5Vls0CV0CpC4fV4jfw+1Bdc8zFRzPF6Ie/xIT+HfDatWO9295duhOTB3ZEiToYHfDVuwqc3Rir2Rv2I89hw3HdiqKuoaWvG/fJyj2o6GVcyu5IXTNqGpyG+zSB2dB0yjFYXDzaSb8eKU2z4gARERljpf4w7FYLbj99AB7/fAMAIMtu8SuDARhnzfTu/WA1Hjx3KIDgjJtZVB2sw4zP1mPGhcNQnOvw27dh7zF4pMQf1O7Hbzb61o9cv7cGv/vPCpw+pAxXndjLu10LUKN9uz8ervfr5pXqrMBr/mG8FFI4tY2+IPqmN5fh5H7t/fa/ft1Y3P/hGuw4XI9jjeEDMquJ0krZ9uDoS+vGlVKiptEVcryeZIaMiMj0GJCFkW234qZJfX0BmdVqGJCFG1j92sLtGNq1CBdXdMeyHUeT2t6W+nzNXsxcsxedirJx/zlD/Pad/pdvQr7uwDGly6+mwYXLX1rs3a4FZIEZsn/M34bvq47gmStG+W0/+bHZfs/L7/w09jehOljrn538dtNB/2v164CBnQvxSRSrLNgs5inTZ9RNrN3nF77ZihmfrceiOyejk0FxWY9kUVgiIrNjQBaDLLsl6AvPYQ0fkAHA799dic5F2bjy5SXJbF7UFm09hA4FWejTIR8AvLWvDtU1o8nljjhGTFPToGSjCnP8P0aWEF2WD3y0FgDwTItbHtnFLyyMeEy3kpyozmWmLsuqQ8E13rT7rBWM3VvTaBiQSUh2WBIRmZx5UgAZwGG14KS+7fHejSeiIFsJQrJsFtij+OK++c3lyW5e1C59cREm/3mu97k2CP+jFbsx4J6ZUZ9H6/LTyksECrWyVLjlqOJVHWFcGGCcbcozWBczXFd0qp09PHghey1DtkKtyxaqtVJyHUsiIrNjQBYDbczO6J4l3i9rm9UCWxSjq7Ns5r3Vzhasyfn9XhfuUEtOBI5d0iY+hBozd6Q+8ZMeOhWGXgcykNHPwmhmrJnGkI3qUYL7zh7st+3przdj+v9WR3ytZKV+IiLTM2+UYHJTh3YGoAy2jmaNy6yAQdmrdlZj8/7apLQtVu4YM1aNTjf+9oOvZIQjIMDRxtmFivOO1EXOYsXKHcOECS0gu3RMd5S3z/PbpmemMWSAcYD4+iJfwdhQMRfXsiQiMj9zfeNkkAfPHYLFd01GrsMWVZdlYDfZ2X+bh1OfmBvi6NQKLOURybSn/RfzDuyCbHQp9cf0Y8i+XLvP+1jrVpy9fj8a4qxV1k4d/+bRvYd2eQ7DY7WARR98XTa2OwDganWW6K/UdSMBc2XIgMiBrDvEz5FrWRIRmR8H9beQ3WpBmdpNpg3q790+z7vA9tjyUizZdjht7YtFrAFZYGbP6fZ/fZPTP0PW7PLg/1773rv/9++uwJhepfjP0p34yehuLWixT7Zan0tfyb9X+zzDWnCXjukRdOzPT+6Na04qh91qwSVjeiDHYcVTX22Kq03J0uBUgte7zxyERwxWQQj1c2ShfiIi82OGLAGKc5XxR/pZef83vtzvmFDZi3Rzuj2Y8Znfuu6474PV6HXHJ1i7uyaqc/xzQZXfc62mmDaGLDALVnWoHv9ZuhMAsHHfsZY0GwBw++kDvFmse84a5N3eq11e0LEf3zIeD507JGi7EMIbUOcYDOw3k19M7I1rTuqFK0/oabjfGWLJLgmWvSAiMjsGZFEY0s4SdpHsu88ajN+c2g9TBnfybuvbMR+bHjnDOxBbAOhQkJXspsak0enGwOnBsypfXaiMS7r0xcglJIw8P1dZ4ufbTQex62iDN7NjJJ4w9aZJfb1jo7oW+0pZ9GqXG3Rsx4Is7+SLSFXrX76qAleOMw560qk414H7zh7izQoGCjU5QxnUn8yWERFRvBiQReH2MTlYcd+UkPuLcuz4zan9/cYmFWTbYbdacPWJvVCQZcPWg3XeQqrp4vFIv5mPu482hM3c1TS6Qu4LR1964o73VqK+OfR5YskcXlLRPWibVvpBP0avXX5w4NveYFsokweV4aHzhkZ9fDoY/YEQMkPGSv1ERKbHgCyBrj6pl/exVqdMCIG6MAFJqszbdBC97/rUW7MqUSLNMHW6PWEDu1h6cs8b2TVom3egvt2CV68di+nTBgdlg165usJv3JjGpCtZReWr2yYGbXO6PYalRriWJRGR+TEgS6CCbDt6qt1l+m4lMwwfW7LtEABfVXcAmL/lUFznHNWjOGI1+9omF857Zn7I/ev2RDdODQActuBr6WdCTuzfAdeNL0dZoZINu2BUV9x6an9MGtAx6mtkivb5WUFZvxvfWIY/zlSW+Vq09RC2HlAmXzBDRkRkfgzIEuzdX5yI9248Id3NCNJFHWO1/VC9d9tzszfHdc7A+mNGDtUmsgis8BsrBgBDuhQBAHJ0AfCkAR3xytUVeOzC4/DrU/sFDWhvl6+UxehcHH0xWTN6+/rjg7Zp4/cufXERTlFXY+BalkRE5seALME6FGRhdM/SdDcjiDYTdOaavd5ttU3xdaXarZaI3X57qhujOlc0xXVdbg8+umW837Y/nD8Mb/18HLqX+gbyCyFwysCykCsoTBlchr9dPhI3TeobVdvMqm/HAqx/aGrYYzbsPQa3J3lLVRERUWKwDlmapWpZG6PAqT7OoqyRFlWPRVGu3W/SQ/fSHLjc0i+gc3kkSnL9B7PnOKw4oU+7mK4lhMC044LXhsxERjMuX19Y5X18+l++AaDcTyIiMi9myNIssIZXshglsmItCBvIbhUJG5sUGGj96pR+uGCUbxB/WWEWRvcsYdebgSV3T/Z7Pv2DNUHHcFA/EZG5MSBLs//9sDsl10nGjML8LHtUC6vrnW8wUxIAupX41w6zWYXfWpJv/XxcyPpbbV1OFPeFg/qJiMyNAVmapWp8j4yrBKux6dMGRbWOp97x5cbj6x45fyh+Nbkf+pflA1AW9rYxiohKQXboosUaZhaJiMyNAVkK3HnGwJD7XO7U1MRoae/k8z8dHXKVguJch18W6w6D99k+33+hb6N6YABQVpCN357W3zsuLTAYM+vSU2ZROaBD2P2Mx4iIzI0BWQrcMLFPyH3xjuOKllHBUAC4YWLvsK/LslmQG2aNR30dMo/BNQK7Ga0hIgMtUNMq71stAlbdufXrTLbPz8JPx/UI2+62JlRXsIbxGBGRuTEgS5F/Xz/O+3jdg75SBenO/JTkOvDuL0LXTYsUkOlnWurjsdMGl+Hiim5B45usFoF/XDMm5Pm0wMxqEd5xZe3zs/zGmH1/z6l4+LxhIc/RFp07oqt33VQjzW6WviAiMjMGZCnSqchXhDTHYUXvDnkAlAW+pzw5F7PX70/q9UMN6i/NdaCiVynG9TYe2+WwWZDrCF0dRd+1qA8uT+7XHo9dNNyvkj6gBFzhKucP6VIIADhwrAkjuxcDAH4RIYtHinDB/Z6j0dWDIyKi9GAdshQJrGrvUb8891Q3Yk818Nt3fsAHN41Hj3a5Ri+Pm1F3IgB0LclR22ecBXPYLGFn8elnWeqv0awudB04mDxUl6Xm/00dCCklzhjWGUU5diybfhqKQ4xhI3/7apSg69ZT+6PR5cZzc7Z496Wqa5yIiFqGGbIUCaxE7w4IkI7UOzHh8dlJu36oDFkPtcJ9qEr5DpsFfdWZjwBw6iD/7NYNE3zZK/13vrYKwNQhnfyOD7zM3y4fib9dPtL7vCjHjkcvOM47kaA0zxFyIgD5u3Z8OU4bXIarT+oVVSkMIiIyDwZkKRKcITM+7r/Ld1uFFRgAABQVSURBVCbl+kbxmM0i0FntSs0KsS6lw2rBvdMGe9t/xtDOfvvPG9kVmx45AxsenurN+gHwLrJ+yyn+yxNZAjJk047r0mqq5qdb56IcvPSzChTl2KNaZ5SIiMyDv7VTJPALMtR4n1v/vSIp1zeaZbn6gdO9XY4hAzKbBdl2K6YNUwIxo1bbrRZk2azeLsszh3XCeSOUWX+B2a3AMWWUHIlc1oqIiJKPv7VTJFKXpV51gzPh1ze6nP5L+6KKbt7HnXUTELIMxpb9+/pxmHFy8NqI2usmDywLWYiU3Y+pwQwZEVFm4aD+FAkMUDxhBllPe/pbfPv7UxJ6faNK/fps1Yl92qNqxlnYtO8Y2uVn4aQZX6PB6fZ9sauHSilxfO92aNgR/IV/xfE90akoJ2icmR6r76eGI8YVFIiIKL34Z3SKleYplesHq+UdjPx4uCHh19VnyF67diwuHNXN8Lh+ZQUozXMgP1uJ1bWlkbTFqcPN1bNYBE4bHDo7BkSeZUmJwQwZEVFmYYYshd654QTvrMZnrhiF4+7/IuSxC7ccwgl92iXs2vpAakL/DpjQP/xSO29fPw5frt3nrUGWqDiKXZap4bByliURUSbhn9EpNLa81FsgtjDb7i0Oa+TZOZsTeu1QdchC6dMh33jJpzjLWcXaDmqZWBd9JyKi9Ep5QCaE6C6EmC2EWCuEWCOE+LW6/X4hxC4hxA/qvzNT3bZU61SYHXLflv21eGvJDry/LDFlMOKNg8oKswAABdnxJVW1grHv3HACvrh1QnyNopDYZUlElFnS0WXpAnCblHKZEKIAwFIhxCx135NSyj+loU1p8fRlIzH64S8N9+2ubsSd768CAFwQYrxXLLR47OWrKlr0+l9P7o/y9vmYOrRT5IPDaFIDsrHlxks1UWKEG8dHRETmk/KATEq5B8Ae9fExIcQ6AF1T3Q4zaJeflbqLqSmy4er6kLFy2Cy4aHT8gaGWIaPk4n0mIsosae3XEEL0AjASwGJ1081CiJVCiFeEECVpa1iaFCVxzUatyka68yYdC1IYhLZhWkCmrZjApZSIiMxNGFVwT8mFhcgHMBfAI1LK94UQZQAOQuldewhAZynltQavux7A9QBQVlY2+u233056W2tra5Gfnx/5wBa4emad9/Evhmfh+RVNQcf8c2rowf/RmrXdiTfWNePpU3JR4Ig/LIvlnjywoAEHGzy4tSIbvYtad2CQzM9KLHbVenD3vAbccFwWKjop99yephmuZrknZsP7Eoz3xBjvS7BMvSeTJk1aKqU0HDuUlrIXQgg7gPcAvCGlfB8ApJT7dPtfAvCx0WullC8CeBEAKioqZGVlZdLbO2fOHCTtOjM/8T4cMWwosGJp0CGJuPa2+duAdWsx/qSTUKLWQotHLPckBT8i00jqZyVG509xecuWpJOZ7omZ8L4E4z0xxvsSrDXek3TMshQAXgawTkr5hG67ftXq8wGsTnXb0s1hC53BePSzdbj8pUUtPreWCA1c3JtaLzMEY0REFJ10/MY+CcCVAFYJIX5Qt90F4DIhxAgoXZZVAG5IQ9vSymYJHR+/MHdrXOf21v9iPEZERGQ66ZhlOQ/GYcGnqW6LGZS3z8O2g8o4slSs88gEGRERkfmwemSafX3bRJyoLpHkTuIECybIiIiIzIsBWZoJIbxV1Z3u5NWOkmppWI4hIyIiMh8GZCZgtyo/hmaXxMn92iflGt46ZIzHiIiITIcBmQlcOa4nAOC4bkX4+1UVePXasSGPbWndOF+XJSMyIiIis+G8eBOY0L8Dqmac5X0+sX8Hv/0uXVem2yNhs8YeVGldlsyQERERmQ8zZCaVbff9aJp1AZnTHWeGjAEZERGR6TAgMyl912KT0xeQNbdw4L/W1ckuSyIiIvNhQGZS+kzWhyt2ex+3dCYmM2RERETmxYDMpPRx030frvE+bnFAZnBeIiIiMgcGZCYlQqSynK7IY8hqm1xodLr9tnEtSyIiIvNiQJZhohlDNvS+z3HO3+b5bdPWsmQ8RkREZD4MyEwqVNy0/1gjbnj9e2zeX4sFWw6GfP3GfbV+z71dlozIiIiITId1yMwqRNx0+UuLAQCfr9kHAFj34FTkOKyRz5fEdTKJiIgoPsyQmdSoHiVRHRftguQSgIXJMSIiIlNiQGZSz1wxCjn2yJkvtye6gMwjJbsriYiITIoBmUnlZ9kwsHNBxOM8UQZkUrLkBRERkVkxIDOxZlfkGZWuaAMycIYlERGRWTEgM7FoisB6oh1DJjnDkoiIyKwYkJnY9RP6RDwm6gyZlOyyJCIiMikGZCZ20ehu6NkuN+wxbje7LImIiDIdAzKTi7TUUdRlL6SEYI6MiIjIlBiQmZw+HutlkC37at2+qM4jJeuQERERmRUDMpPTZ8h+UtE9aP/Dn6zD3upG73MZImPm4aB+IiIi02JAZnJaVuuMoZ1CBlv68hihCsVKcFA/ERGRWTEgMzktQ/bzCb1DLkepH0cWakyZlGBlWCIiIpPi4uImpwVkViEQavh+o9MNAJjy5FycP7JbxHMRERGRuTBDZnIW9ScULpZqcnng9khs3FeLP85cb3iMspZlEhpIREREcWNAZnJWNYrySHi7LMvb5+G20/p7j2lyulHd4Ax7Hq5lSUREZF4MyExOeAMyCal2Wp59XGeMLS/1HtPo8uBIfXPQaxudbm93poTkLEsiIiKTYkBmctosSykltAmUQghYdUXFmpxuHKkLDsgGTp+JcY9+pb6edciIiIjMigGZyRXm2AEoQdglY7qjR2kuLh7T3S8ga3R5QnZZHq1XtivBHCMyIiIiM+IsS5P700+G49/f/YiR3YshhMA3v58EADhU2+Q9pkntlgyPg/qJiIjMigGZybXPz8JNk/oGbQ/MkDXqisMa4aB+IiIi8zJdl6UQYqoQYoMQYrMQ4o50t8esbBbfj27P0QZM/9/qsMcrY8gYkhEREZmRqQIyIYQVwDMAzgAwGMBlQojB6W2VOeljq9kbDkQ8nnXIiIiIzMtUARmAsQA2Sym3SimbAbwN4Nw0t8mUmpy+Lsp1e2oiHs+Vk4iIiMxLhFqwOh2EEBcBmCql/D/1+ZUAjpdS3qw75noA1wNAWVnZ6Lfffjvp7aqtrUV+fn7SrxOLTUfceGRxY1TH/nNqHl5a2YT1h934c2VuQq5vxntiBrwvwXhPjPG+BOM9Mcb7EixT78mkSZOWSikrjPZl3KB+KeWLAF4EgIqKCllZWZn0a86ZMwepuE4sTnJ7sN65Cmt2V2P93mNhj62srMSH+3/A9obDCXsfZrwnZsD7Eoz3xBjvSzDeE2O8L8Fa4z0xW5flLgDddc+7qdsogN1qwZ8vHo7+ZQVRHS9l+PUwiYiIKH3MFpB9B6CfEKJcCOEAcCmAD9PcJlNz2JQfYa92uXjpZ4ZZUGzadwz/Xb4LB48FV/MnIiKi9DNVl6WU0iWEuBnA5wCsAF6RUq5Jc7NMTQvIRnQvxol92hke8/mavQCAhqgKyBIREVGqmSogAwAp5acAPk13OzKFw6oEZIU5dtitvoRnWWEW9tUo1fxzHKb7MRMREZGO2bosKUY2tWJ/UY4ddqtvkJh+bNlDH68FAPxuSv/UNo6IiIiiwoAswzndSj2ywmw7hG7Ufr+OwYP9rxzXK1XNIiIiohgwIMtwjWqB2Pxs/27J4d2Lgo612zjNkoiIyIw4uCjD1asD9XPsVr/tZw7rjHV7jmFo10Lc/OZyAL7xZkRERGQuDMgyXEOzGpA5fAGZzSJgt1pwxxkDsWmfr2is1cIMGRERkRkxIMtwTS4lIMtWM2Qf3HQSOhVle/cX5ti9jwUrwxIREZkS+7Ay3OAuhQCArsU5AIDh3YtRVqgLyLLthq8jIiIi82CGLMP9bsoAnDO8C/p2NF5kNdvOmJuIiMjs+G2d4exWC4Z0CZ5RqWE3JRERkfkxICMiIiJKMwZkRERERGnGgIyIiIgozTiovw14/qejUd3QnO5mEBERUQgMyNqAqUM7pbsJREREFAa7LImIiIjSjAEZERERUZoxICMiIiJKMwZkRERERGnGgIyIiIgozRiQEREREaUZAzIiIiKiNGNARkRERJRmDMiIiIiI0owBGREREVGaMSAjIiIiSjMGZERERERpxoCMiIiIKM2ElDLdbWgxIcQBANtTcKn2AA6m4DqZhPfEGO9LMN4TY7wvwXhPjPG+BMvUe9JTStnBaEdGB2SpIoT4XkpZke52mAnviTHel2C8J8Z4X4LxnhjjfQnWGu8JuyyJiIiI0owBGREREVGaMSCLzovpboAJ8Z4Y430JxntijPclGO+JMd6XYK3unnAMGREREVGaMUNGRERElGYMyMIQQkwVQmwQQmwWQtyR7vakkhCiuxBithBirRBijRDi1+r2UiHELCHEJvW/Jep2IYR4Sr1XK4UQo9L7DpJHCGEVQiwXQnysPi8XQixW3/u/hRAOdXuW+nyzur9XOtudTEKIYiHEu0KI9UKIdUKIE9r6Z0UIcav6/85qIcRbQojstvhZEUK8IoTYL4RYrdsW82dDCHGVevwmIcRV6XgviRLinjyu/v+zUgjxXyFEsW7fneo92SCEOF23vVV9RxndF92+24QQUgjRXn3e+j4rUkr+M/gHwApgC4DeABwAVgAYnO52pfD9dwYwSn1cAGAjgMEAHgNwh7r9DgB/VB+fCeAzAALAOACL0/0eknhvfgvgTQAfq8/fAXCp+vh5ADeqj38J4Hn18aUA/p3utifxnrwK4P/Uxw4AxW35swKgK4BtAHJ0n5Gr2+JnBcAEAKMArNZti+mzAaAUwFb1vyXq45J0v7cE35MpAGzq4z/q7slg9fsnC0C5+r1kbY3fUUb3Rd3eHcDnUOqOtm+tnxVmyEIbC2CzlHKrlLIZwNsAzk1zm1JGSrlHSrlMfXwMwDooXzLnQvnyhfrf89TH5wJ4TSoWASgWQnROcbOTTgjRDcBZAP6uPhcATgHwrnpI4D3R7tW7ACarx7cqQogiKL9IXwYAKWWzlPIo2vhnBYANQI4QwgYgF8AetMHPipTyGwCHAzbH+tk4HcAsKeVhKeURALMATE1+65PD6J5IKb+QUrrUp4sAdFMfnwvgbSllk5RyG4DNUL6fWt13VIjPCgA8CeD3APSD3lvdZ4UBWWhdAfyoe75T3dbmqN0nIwEsBlAmpdyj7toLoEx93Fbu11+g/GLwqM/bATiq+0Wqf9/ee6Lur1aPb23KARwA8A+1K/fvQog8tOHPipRyF4A/AdgBJRCrBrAU/KxoYv1stPrPTIBroWR/gDZ+T4QQ5wLYJaVcEbCr1d0XBmQUlhAiH8B7AH4jpazR75NKfrjNTNMVQkwDsF9KuTTdbTEZG5RuhueklCMB1EHphvJqg5+VEih/wZcD6AIgDxnyV3qqtbXPRiRCiLsBuAC8ke62pJsQIhfAXQDuTXdbUoEBWWi7oPRba7qp29oMIYQdSjD2hpTyfXXzPq17Sf3vfnV7W7hfJwE4RwhRBaV74BQAf4WSKrepx+jft/eeqPuLABxKZYNTZCeAnVLKxerzd6EEaG35s3IqgG1SygNSSieA96F8ftr6Z0UT62ejLXxmIIS4GsA0AFeogSrQtu9JHyh/1KxQf+92A7BMCNEJrfC+MCAL7TsA/dRZUQ4oA20/THObUkYdv/IygHVSyid0uz4EoM1auQrAB7rtP1NnvowDUK3rkmgVpJR3Sim7SSl7Qfk8fC2lvALAbAAXqYcF3hPtXl2kHt/qMgFSyr0AfhRCDFA3TQawFm34swKlq3KcECJX/X9Juydt+rOiE+tn43MAU4QQJWr2cYq6rdUQQkyFMhziHCllvW7XhwAuVWfilgPoB2AJ2sB3lJRylZSyo5Syl/p7dyeUyWZ70Ro/K+meVWDmf1BmcWyEMpPl7nS3J8XvfTyUboSVAH5Q/50JZVzLVwA2AfgSQKl6vADwjHqvVgGoSPd7SPL9qYRvlmVvKL8gNwP4D4AsdXu2+nyzur93utudxPsxAsD36uflf1BmN7XpzwqABwCsB7AawOtQZsm1uc8KgLegjKNzQvlCva4lnw0o46o2q/+uSff7SsI92Qxl7JP2+/Z53fF3q/dkA4AzdNtb1XeU0X0J2F8F3yzLVvdZYaV+IiIiojRjlyURERFRmjEgIyIiIkozBmREREREacaAjIiIiCjNGJARERERpZkt8iFERJlFCKGVVQCATgDcUJZ3AoCxUln7j4jINFj2gohaNSHE/QBqpZR/SndbiIhCYZclEbUJQojRQoi5QoilQojPdUv3zBFCPCmE+F4IsU4IMUYI8b4QYpMQ4mH1mF5CiPVCiDfUY95V19mDEGKGEGKtEGKlEIJBHxG1CAMyImoLBICnAVwkpRwN4BUAj+j2N0spKwA8D2UZn5sADAVwtdr9CQADADwrpRwEoAbAL9V95wMYIqU8DsDDKXk3RNTqMCAjorYgC0qANUsI8QOAe6AsOqzR1gBcBWCNlHKPlLIJwFb4Fir+UUo5X338LyjLi1UDaATwshDiAgD6NQiJiKLGQf1E1BYIKIHWCSH2N6n/9egea8+135OBA26llNIlhBgLZfHwiwDcDOCUxDSZiNoSZsiIqC1oAtBBCHECAAgh7EKIITGeo4f2egCXA5gnhMgHUCSl/BTArQCGJ6zFRNSmMCAjorbAAyWD9UchxAoAPwA4McZzbABwkxBiHYASAM8BKADwsRBiJYB5AH6buCYTUVvCshdERBEIIXoB+FhKOTTNTSGiVooZMiIiIqI0Y4aMiIiIKM2YISMiIiJKMwZkRERERGnGgIyIiIgozRiQEREREaUZAzIiIiKiNGNARkRERJRm/x8LdBLehFBJegAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDS7BJvZG_e1"
      },
      "source": [
        "# Calcule de l'erreur quadratique moyenne et de l'erreur absolue moyenne \n",
        "\n",
        "mae = tf.keras.metrics.mean_absolute_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "mse = tf.keras.metrics.mean_squared_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "\n",
        "print(mae)\n",
        "print(mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7MYRUhhcUGq"
      },
      "source": [
        "# Création du modèle LSTM avec couche d'attention personnalisée simple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-J0Du8hceO7"
      },
      "source": [
        "**1. Création du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dk-OVhlxcYoN",
        "outputId": "0273ec79-bcbc-4a3c-9c8b-5fd7dc74f3dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dim_GRU = 40\n",
        "\n",
        "# Fonction de la couche lambda d'entrée\n",
        "def Traitement_Entrees(x):\n",
        "  return tf.expand_dims(x,axis=-1)\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.Input(shape=(taille_fenetre,)))\n",
        "model.add(tf.keras.layers.Lambda(Traitement_Entrees))\n",
        "model.add(tf.keras.layers.LSTM(dim_GRU,return_sequences=True))\n",
        "model.add(Couche_Attention())\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "model.save_weights('model_initial.h5')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda (Lambda)              (None, 20, 1)             0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 20, 40)            6720      \n",
            "_________________________________________________________________\n",
            "couche__attention (Couche_At (None, 40)                60        \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 41        \n",
            "=================================================================\n",
            "Total params: 6,821\n",
            "Trainable params: 6,821\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RV7DMcPcg9p"
      },
      "source": [
        "**2. Optimisation du taux d'apprentissage**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVHSl4sp8PyP"
      },
      "source": [
        "model.load_weights('model_initial.h5')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUC-JS47cdKL",
        "outputId": "2a88b2c1-0244-4d68-b0ab-7a51e20c503c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(lr=1e-8)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur, metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset_norm,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "30/30 [==============================] - 2s 10ms/step - loss: 0.5260 - mae: 0.9291\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.52293, saving model to poids.hdf5\n",
            "Epoch 2/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.5073 - mae: 0.9049\n",
            "\n",
            "Epoch 00002: loss did not improve from 0.52293\n",
            "Epoch 3/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.5194 - mae: 0.9220\n",
            "\n",
            "Epoch 00003: loss did not improve from 0.52293\n",
            "Epoch 4/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.5117 - mae: 0.9075\n",
            "\n",
            "Epoch 00004: loss did not improve from 0.52293\n",
            "Epoch 5/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.5337 - mae: 0.9361\n",
            "\n",
            "Epoch 00005: loss did not improve from 0.52293\n",
            "Epoch 6/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.5384 - mae: 0.9443\n",
            "\n",
            "Epoch 00006: loss did not improve from 0.52293\n",
            "Epoch 7/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.5323 - mae: 0.9358\n",
            "\n",
            "Epoch 00007: loss did not improve from 0.52293\n",
            "Epoch 8/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.5271 - mae: 0.9284\n",
            "\n",
            "Epoch 00008: loss did not improve from 0.52293\n",
            "Epoch 9/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.5492 - mae: 0.9539\n",
            "\n",
            "Epoch 00009: loss did not improve from 0.52293\n",
            "Epoch 10/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.5367 - mae: 0.9391\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.52293\n",
            "Epoch 11/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.4890 - mae: 0.8840\n",
            "\n",
            "Epoch 00011: loss did not improve from 0.52293\n",
            "Epoch 12/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.4970 - mae: 0.8947\n",
            "\n",
            "Epoch 00012: loss did not improve from 0.52293\n",
            "Epoch 13/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.5520 - mae: 0.9594\n",
            "\n",
            "Epoch 00013: loss did not improve from 0.52293\n",
            "Epoch 14/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.5409 - mae: 0.9459\n",
            "\n",
            "Epoch 00014: loss did not improve from 0.52293\n",
            "Epoch 15/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.5278 - mae: 0.9306\n",
            "\n",
            "Epoch 00015: loss did not improve from 0.52293\n",
            "Epoch 16/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.5167 - mae: 0.9172\n",
            "\n",
            "Epoch 00016: loss did not improve from 0.52293\n",
            "Epoch 17/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.5394 - mae: 0.9412\n",
            "\n",
            "Epoch 00017: loss did not improve from 0.52293\n",
            "Epoch 18/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.5068 - mae: 0.9050\n",
            "\n",
            "Epoch 00018: loss did not improve from 0.52293\n",
            "Epoch 19/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.5325 - mae: 0.9363\n",
            "\n",
            "Epoch 00019: loss did not improve from 0.52293\n",
            "Epoch 20/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.4902 - mae: 0.8856\n",
            "\n",
            "Epoch 00020: loss did not improve from 0.52293\n",
            "Epoch 21/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.5415 - mae: 0.9480\n",
            "\n",
            "Epoch 00021: loss did not improve from 0.52293\n",
            "Epoch 22/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.5158 - mae: 0.9182\n",
            "\n",
            "Epoch 00022: loss did not improve from 0.52293\n",
            "Epoch 23/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.5221 - mae: 0.9226\n",
            "\n",
            "Epoch 00023: loss did not improve from 0.52293\n",
            "Epoch 24/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.5184 - mae: 0.9141\n",
            "\n",
            "Epoch 00024: loss did not improve from 0.52293\n",
            "Epoch 25/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.4947 - mae: 0.8934\n",
            "\n",
            "Epoch 00025: loss did not improve from 0.52293\n",
            "Epoch 26/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.5201 - mae: 0.9165\n",
            "\n",
            "Epoch 00026: loss did not improve from 0.52293\n",
            "Epoch 27/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.5131 - mae: 0.9107\n",
            "\n",
            "Epoch 00027: loss did not improve from 0.52293\n",
            "Epoch 28/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.4937 - mae: 0.8934\n",
            "\n",
            "Epoch 00028: loss did not improve from 0.52293\n",
            "Epoch 29/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.5881 - mae: 0.9951\n",
            "\n",
            "Epoch 00029: loss did not improve from 0.52293\n",
            "Epoch 30/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.4983 - mae: 0.9013\n",
            "\n",
            "Epoch 00030: loss did not improve from 0.52293\n",
            "Epoch 31/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.5197 - mae: 0.9134\n",
            "\n",
            "Epoch 00031: loss improved from 0.52293 to 0.52152, saving model to poids.hdf5\n",
            "Epoch 32/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.4989 - mae: 0.8945\n",
            "\n",
            "Epoch 00032: loss did not improve from 0.52152\n",
            "Epoch 33/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.5419 - mae: 0.9450\n",
            "\n",
            "Epoch 00033: loss did not improve from 0.52152\n",
            "Epoch 34/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.5416 - mae: 0.9459\n",
            "\n",
            "Epoch 00034: loss did not improve from 0.52152\n",
            "Epoch 35/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.5103 - mae: 0.9119\n",
            "\n",
            "Epoch 00035: loss did not improve from 0.52152\n",
            "Epoch 36/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.5285 - mae: 0.9301\n",
            "\n",
            "Epoch 00036: loss did not improve from 0.52152\n",
            "Epoch 37/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.5266 - mae: 0.9271\n",
            "\n",
            "Epoch 00037: loss improved from 0.52152 to 0.51943, saving model to poids.hdf5\n",
            "Epoch 38/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.5193 - mae: 0.9168\n",
            "\n",
            "Epoch 00038: loss did not improve from 0.51943\n",
            "Epoch 39/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.5081 - mae: 0.9075\n",
            "\n",
            "Epoch 00039: loss improved from 0.51943 to 0.51722, saving model to poids.hdf5\n",
            "Epoch 40/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.4927 - mae: 0.8944\n",
            "\n",
            "Epoch 00040: loss improved from 0.51722 to 0.51624, saving model to poids.hdf5\n",
            "Epoch 41/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.4808 - mae: 0.8767\n",
            "\n",
            "Epoch 00041: loss improved from 0.51624 to 0.51107, saving model to poids.hdf5\n",
            "Epoch 42/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.5058 - mae: 0.9073\n",
            "\n",
            "Epoch 00042: loss improved from 0.51107 to 0.51021, saving model to poids.hdf5\n",
            "Epoch 43/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.4813 - mae: 0.8742\n",
            "\n",
            "Epoch 00043: loss improved from 0.51021 to 0.50842, saving model to poids.hdf5\n",
            "Epoch 44/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.4875 - mae: 0.8800\n",
            "\n",
            "Epoch 00044: loss improved from 0.50842 to 0.50284, saving model to poids.hdf5\n",
            "Epoch 45/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.4859 - mae: 0.8822\n",
            "\n",
            "Epoch 00045: loss improved from 0.50284 to 0.49567, saving model to poids.hdf5\n",
            "Epoch 46/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.4817 - mae: 0.8764\n",
            "\n",
            "Epoch 00046: loss improved from 0.49567 to 0.48859, saving model to poids.hdf5\n",
            "Epoch 47/100\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.4860 - mae: 0.8790\n",
            "\n",
            "Epoch 00047: loss improved from 0.48859 to 0.47596, saving model to poids.hdf5\n",
            "Epoch 48/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.4428 - mae: 0.8337\n",
            "\n",
            "Epoch 00048: loss improved from 0.47596 to 0.46841, saving model to poids.hdf5\n",
            "Epoch 49/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.4513 - mae: 0.8405\n",
            "\n",
            "Epoch 00049: loss improved from 0.46841 to 0.45471, saving model to poids.hdf5\n",
            "Epoch 50/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.4375 - mae: 0.8216\n",
            "\n",
            "Epoch 00050: loss improved from 0.45471 to 0.43633, saving model to poids.hdf5\n",
            "Epoch 51/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.4137 - mae: 0.7924\n",
            "\n",
            "Epoch 00051: loss improved from 0.43633 to 0.41160, saving model to poids.hdf5\n",
            "Epoch 52/100\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.3808 - mae: 0.7472\n",
            "\n",
            "Epoch 00052: loss improved from 0.41160 to 0.38759, saving model to poids.hdf5\n",
            "Epoch 53/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.3584 - mae: 0.7179\n",
            "\n",
            "Epoch 00053: loss improved from 0.38759 to 0.35899, saving model to poids.hdf5\n",
            "Epoch 54/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.3416 - mae: 0.6977\n",
            "\n",
            "Epoch 00054: loss improved from 0.35899 to 0.32290, saving model to poids.hdf5\n",
            "Epoch 55/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.2828 - mae: 0.6280\n",
            "\n",
            "Epoch 00055: loss improved from 0.32290 to 0.28475, saving model to poids.hdf5\n",
            "Epoch 56/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.2509 - mae: 0.5873\n",
            "\n",
            "Epoch 00056: loss improved from 0.28475 to 0.24278, saving model to poids.hdf5\n",
            "Epoch 57/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.2203 - mae: 0.5577\n",
            "\n",
            "Epoch 00057: loss improved from 0.24278 to 0.20334, saving model to poids.hdf5\n",
            "Epoch 58/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.1780 - mae: 0.4976\n",
            "\n",
            "Epoch 00058: loss improved from 0.20334 to 0.16667, saving model to poids.hdf5\n",
            "Epoch 59/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.1373 - mae: 0.4413\n",
            "\n",
            "Epoch 00059: loss improved from 0.16667 to 0.13253, saving model to poids.hdf5\n",
            "Epoch 60/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.1079 - mae: 0.3861\n",
            "\n",
            "Epoch 00060: loss improved from 0.13253 to 0.10247, saving model to poids.hdf5\n",
            "Epoch 61/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0808 - mae: 0.3259\n",
            "\n",
            "Epoch 00061: loss improved from 0.10247 to 0.07725, saving model to poids.hdf5\n",
            "Epoch 62/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0612 - mae: 0.2673\n",
            "\n",
            "Epoch 00062: loss improved from 0.07725 to 0.05689, saving model to poids.hdf5\n",
            "Epoch 63/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0447 - mae: 0.2210\n",
            "\n",
            "Epoch 00063: loss improved from 0.05689 to 0.04377, saving model to poids.hdf5\n",
            "Epoch 64/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0368 - mae: 0.1910\n",
            "\n",
            "Epoch 00064: loss improved from 0.04377 to 0.03557, saving model to poids.hdf5\n",
            "Epoch 65/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0350 - mae: 0.1799\n",
            "\n",
            "Epoch 00065: loss improved from 0.03557 to 0.03204, saving model to poids.hdf5\n",
            "Epoch 66/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.0331 - mae: 0.1761\n",
            "\n",
            "Epoch 00066: loss improved from 0.03204 to 0.03089, saving model to poids.hdf5\n",
            "Epoch 67/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.0299 - mae: 0.1703\n",
            "\n",
            "Epoch 00067: loss improved from 0.03089 to 0.03008, saving model to poids.hdf5\n",
            "Epoch 68/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0274 - mae: 0.1646\n",
            "\n",
            "Epoch 00068: loss did not improve from 0.03008\n",
            "Epoch 69/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0313 - mae: 0.1733\n",
            "\n",
            "Epoch 00069: loss improved from 0.03008 to 0.02981, saving model to poids.hdf5\n",
            "Epoch 70/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0281 - mae: 0.1663\n",
            "\n",
            "Epoch 00070: loss improved from 0.02981 to 0.02953, saving model to poids.hdf5\n",
            "Epoch 71/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0285 - mae: 0.1630\n",
            "\n",
            "Epoch 00071: loss improved from 0.02953 to 0.02930, saving model to poids.hdf5\n",
            "Epoch 72/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0310 - mae: 0.1721\n",
            "\n",
            "Epoch 00072: loss improved from 0.02930 to 0.02899, saving model to poids.hdf5\n",
            "Epoch 73/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.0281 - mae: 0.1651\n",
            "\n",
            "Epoch 00073: loss did not improve from 0.02899\n",
            "Epoch 74/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0291 - mae: 0.1663\n",
            "\n",
            "Epoch 00074: loss improved from 0.02899 to 0.02860, saving model to poids.hdf5\n",
            "Epoch 75/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0284 - mae: 0.1681\n",
            "\n",
            "Epoch 00075: loss improved from 0.02860 to 0.02848, saving model to poids.hdf5\n",
            "Epoch 76/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0294 - mae: 0.1700\n",
            "\n",
            "Epoch 00076: loss improved from 0.02848 to 0.02765, saving model to poids.hdf5\n",
            "Epoch 77/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0291 - mae: 0.1700\n",
            "\n",
            "Epoch 00077: loss did not improve from 0.02765\n",
            "Epoch 78/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0253 - mae: 0.1615\n",
            "\n",
            "Epoch 00078: loss did not improve from 0.02765\n",
            "Epoch 79/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.0342 - mae: 0.1962\n",
            "\n",
            "Epoch 00079: loss did not improve from 0.02765\n",
            "Epoch 80/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0455 - mae: 0.2244\n",
            "\n",
            "Epoch 00080: loss did not improve from 0.02765\n",
            "Epoch 81/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0495 - mae: 0.2396\n",
            "\n",
            "Epoch 00081: loss did not improve from 0.02765\n",
            "Epoch 82/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0591 - mae: 0.2731\n",
            "\n",
            "Epoch 00082: loss did not improve from 0.02765\n",
            "Epoch 83/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.1012 - mae: 0.3652\n",
            "\n",
            "Epoch 00083: loss did not improve from 0.02765\n",
            "Epoch 84/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.4318 - mae: 0.8004\n",
            "\n",
            "Epoch 00084: loss did not improve from 0.02765\n",
            "Epoch 85/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.6899 - mae: 1.1545\n",
            "\n",
            "Epoch 00085: loss did not improve from 0.02765\n",
            "Epoch 86/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 1.1324 - mae: 1.6124\n",
            "\n",
            "Epoch 00086: loss did not improve from 0.02765\n",
            "Epoch 87/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 1.4159 - mae: 1.9008\n",
            "\n",
            "Epoch 00087: loss did not improve from 0.02765\n",
            "Epoch 88/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 1.9764 - mae: 2.4741\n",
            "\n",
            "Epoch 00088: loss did not improve from 0.02765\n",
            "Epoch 89/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 2.6811 - mae: 3.1809\n",
            "\n",
            "Epoch 00089: loss did not improve from 0.02765\n",
            "Epoch 90/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 3.3943 - mae: 3.8943\n",
            "\n",
            "Epoch 00090: loss did not improve from 0.02765\n",
            "Epoch 91/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 4.3219 - mae: 4.8219\n",
            "\n",
            "Epoch 00091: loss did not improve from 0.02765\n",
            "Epoch 92/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 5.5256 - mae: 6.0256\n",
            "\n",
            "Epoch 00092: loss did not improve from 0.02765\n",
            "Epoch 93/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 7.0200 - mae: 7.5200\n",
            "\n",
            "Epoch 00093: loss did not improve from 0.02765\n",
            "Epoch 94/100\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 8.9817 - mae: 9.4817\n",
            "\n",
            "Epoch 00094: loss did not improve from 0.02765\n",
            "Epoch 95/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 11.3433 - mae: 11.8433\n",
            "\n",
            "Epoch 00095: loss did not improve from 0.02765\n",
            "Epoch 96/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 14.2983 - mae: 14.7983\n",
            "\n",
            "Epoch 00096: loss did not improve from 0.02765\n",
            "Epoch 97/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 18.1157 - mae: 18.6157\n",
            "\n",
            "Epoch 00097: loss did not improve from 0.02765\n",
            "Epoch 98/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 22.8818 - mae: 23.3818\n",
            "\n",
            "Epoch 00098: loss did not improve from 0.02765\n",
            "Epoch 99/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 28.9782 - mae: 29.4782\n",
            "\n",
            "Epoch 00099: loss did not improve from 0.02765\n",
            "Epoch 100/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 36.3891 - mae: 36.8891\n",
            "\n",
            "Epoch 00100: loss did not improve from 0.02765\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "No3DCTltd1Hl",
        "outputId": "f7938b26-feb5-446f-b024-abd5e23eabaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 1])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, \"Evolution de l'erreur en fonction du taux d'apprentissage\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKmlwKcHcnBK"
      },
      "source": [
        "**3. Entrainement**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4TH6_RLcoxK",
        "outputId": "9a2200b2-9ffe-4a7c-f3e6-04d9edb2b9a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "class TimingCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, logs={}):\n",
        "        self.logs=[]\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.starttime = timer()\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.logs.append(timer()-self.starttime)\n",
        "\n",
        "cb = TimingCallback()\n",
        "\n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.1,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0.5)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(learning_rate=lr_schedule,momentum=0.9)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur,metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle\n",
        "historique = model.fit(dataset_norm,validation_data=dataset_Val_norm, epochs=500,verbose=1, callbacks=[CheckPoint,cb])\n",
        "\n",
        "print(cb.logs)\n",
        "print(sum(cb.logs))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "30/30 [==============================] - 2s 26ms/step - loss: 0.1901 - mae: 0.4823 - val_loss: 0.0961 - val_mae: 0.3731\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.10214, saving model to poids.hdf5\n",
            "Epoch 2/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0309 - mae: 0.1763 - val_loss: 0.0464 - val_mae: 0.2421\n",
            "\n",
            "Epoch 00002: loss improved from 0.10214 to 0.02940, saving model to poids.hdf5\n",
            "Epoch 3/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0255 - mae: 0.1573 - val_loss: 0.0494 - val_mae: 0.2501\n",
            "\n",
            "Epoch 00003: loss improved from 0.02940 to 0.02547, saving model to poids.hdf5\n",
            "Epoch 4/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0234 - mae: 0.1520 - val_loss: 0.0475 - val_mae: 0.2459\n",
            "\n",
            "Epoch 00004: loss improved from 0.02547 to 0.02524, saving model to poids.hdf5\n",
            "Epoch 5/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0281 - mae: 0.1688 - val_loss: 0.0469 - val_mae: 0.2413\n",
            "\n",
            "Epoch 00005: loss did not improve from 0.02524\n",
            "Epoch 6/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0249 - mae: 0.1540 - val_loss: 0.0460 - val_mae: 0.2400\n",
            "\n",
            "Epoch 00006: loss improved from 0.02524 to 0.02509, saving model to poids.hdf5\n",
            "Epoch 7/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0271 - mae: 0.1601 - val_loss: 0.0457 - val_mae: 0.2371\n",
            "\n",
            "Epoch 00007: loss improved from 0.02509 to 0.02500, saving model to poids.hdf5\n",
            "Epoch 8/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0249 - mae: 0.1552 - val_loss: 0.0434 - val_mae: 0.2318\n",
            "\n",
            "Epoch 00008: loss did not improve from 0.02500\n",
            "Epoch 9/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0255 - mae: 0.1568 - val_loss: 0.0430 - val_mae: 0.2308\n",
            "\n",
            "Epoch 00009: loss improved from 0.02500 to 0.02439, saving model to poids.hdf5\n",
            "Epoch 10/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0222 - mae: 0.1476 - val_loss: 0.0417 - val_mae: 0.2257\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.02439\n",
            "Epoch 11/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0244 - mae: 0.1551 - val_loss: 0.0433 - val_mae: 0.2308\n",
            "\n",
            "Epoch 00011: loss improved from 0.02439 to 0.02435, saving model to poids.hdf5\n",
            "Epoch 12/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0255 - mae: 0.1541 - val_loss: 0.0438 - val_mae: 0.2318\n",
            "\n",
            "Epoch 00012: loss did not improve from 0.02435\n",
            "Epoch 13/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0242 - mae: 0.1576 - val_loss: 0.0431 - val_mae: 0.2291\n",
            "\n",
            "Epoch 00013: loss improved from 0.02435 to 0.02417, saving model to poids.hdf5\n",
            "Epoch 14/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0235 - mae: 0.1492 - val_loss: 0.0434 - val_mae: 0.2301\n",
            "\n",
            "Epoch 00014: loss did not improve from 0.02417\n",
            "Epoch 15/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0221 - mae: 0.1470 - val_loss: 0.0414 - val_mae: 0.2238\n",
            "\n",
            "Epoch 00015: loss did not improve from 0.02417\n",
            "Epoch 16/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0232 - mae: 0.1527 - val_loss: 0.0414 - val_mae: 0.2234\n",
            "\n",
            "Epoch 00016: loss did not improve from 0.02417\n",
            "Epoch 17/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0246 - mae: 0.1562 - val_loss: 0.0402 - val_mae: 0.2200\n",
            "\n",
            "Epoch 00017: loss improved from 0.02417 to 0.02365, saving model to poids.hdf5\n",
            "Epoch 18/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0239 - mae: 0.1518 - val_loss: 0.0423 - val_mae: 0.2268\n",
            "\n",
            "Epoch 00018: loss did not improve from 0.02365\n",
            "Epoch 19/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0213 - mae: 0.1478 - val_loss: 0.0405 - val_mae: 0.2215\n",
            "\n",
            "Epoch 00019: loss did not improve from 0.02365\n",
            "Epoch 20/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0251 - mae: 0.1559 - val_loss: 0.0401 - val_mae: 0.2192\n",
            "\n",
            "Epoch 00020: loss did not improve from 0.02365\n",
            "Epoch 21/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0240 - mae: 0.1523 - val_loss: 0.0385 - val_mae: 0.2163\n",
            "\n",
            "Epoch 00021: loss did not improve from 0.02365\n",
            "Epoch 22/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0237 - mae: 0.1522 - val_loss: 0.0393 - val_mae: 0.2178\n",
            "\n",
            "Epoch 00022: loss did not improve from 0.02365\n",
            "Epoch 23/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0218 - mae: 0.1412 - val_loss: 0.0393 - val_mae: 0.2176\n",
            "\n",
            "Epoch 00023: loss improved from 0.02365 to 0.02355, saving model to poids.hdf5\n",
            "Epoch 24/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0275 - mae: 0.1627 - val_loss: 0.0408 - val_mae: 0.2209\n",
            "\n",
            "Epoch 00024: loss improved from 0.02355 to 0.02313, saving model to poids.hdf5\n",
            "Epoch 25/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0230 - mae: 0.1492 - val_loss: 0.0397 - val_mae: 0.2191\n",
            "\n",
            "Epoch 00025: loss did not improve from 0.02313\n",
            "Epoch 26/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0203 - mae: 0.1405 - val_loss: 0.0396 - val_mae: 0.2182\n",
            "\n",
            "Epoch 00026: loss did not improve from 0.02313\n",
            "Epoch 27/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0228 - mae: 0.1492 - val_loss: 0.0385 - val_mae: 0.2148\n",
            "\n",
            "Epoch 00027: loss did not improve from 0.02313\n",
            "Epoch 28/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0225 - mae: 0.1487 - val_loss: 0.0391 - val_mae: 0.2146\n",
            "\n",
            "Epoch 00028: loss did not improve from 0.02313\n",
            "Epoch 29/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0249 - mae: 0.1538 - val_loss: 0.0393 - val_mae: 0.2156\n",
            "\n",
            "Epoch 00029: loss did not improve from 0.02313\n",
            "Epoch 30/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0228 - mae: 0.1471 - val_loss: 0.0387 - val_mae: 0.2133\n",
            "\n",
            "Epoch 00030: loss did not improve from 0.02313\n",
            "Epoch 31/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0217 - mae: 0.1468 - val_loss: 0.0395 - val_mae: 0.2162\n",
            "\n",
            "Epoch 00031: loss did not improve from 0.02313\n",
            "Epoch 32/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0252 - mae: 0.1552 - val_loss: 0.0386 - val_mae: 0.2124\n",
            "\n",
            "Epoch 00032: loss did not improve from 0.02313\n",
            "Epoch 33/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0237 - mae: 0.1490 - val_loss: 0.0378 - val_mae: 0.2108\n",
            "\n",
            "Epoch 00033: loss improved from 0.02313 to 0.02268, saving model to poids.hdf5\n",
            "Epoch 34/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0244 - mae: 0.1532 - val_loss: 0.0382 - val_mae: 0.2123\n",
            "\n",
            "Epoch 00034: loss did not improve from 0.02268\n",
            "Epoch 35/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0268 - mae: 0.1552 - val_loss: 0.0361 - val_mae: 0.2089\n",
            "\n",
            "Epoch 00035: loss did not improve from 0.02268\n",
            "Epoch 36/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0222 - mae: 0.1495 - val_loss: 0.0380 - val_mae: 0.2099\n",
            "\n",
            "Epoch 00036: loss did not improve from 0.02268\n",
            "Epoch 37/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0224 - mae: 0.1425 - val_loss: 0.0387 - val_mae: 0.2129\n",
            "\n",
            "Epoch 00037: loss did not improve from 0.02268\n",
            "Epoch 38/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0253 - mae: 0.1535 - val_loss: 0.0383 - val_mae: 0.2119\n",
            "\n",
            "Epoch 00038: loss did not improve from 0.02268\n",
            "Epoch 39/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0213 - mae: 0.1442 - val_loss: 0.0355 - val_mae: 0.2050\n",
            "\n",
            "Epoch 00039: loss did not improve from 0.02268\n",
            "Epoch 40/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0257 - mae: 0.1557 - val_loss: 0.0369 - val_mae: 0.2085\n",
            "\n",
            "Epoch 00040: loss improved from 0.02268 to 0.02253, saving model to poids.hdf5\n",
            "Epoch 41/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0198 - mae: 0.1406 - val_loss: 0.0376 - val_mae: 0.2105\n",
            "\n",
            "Epoch 00041: loss did not improve from 0.02253\n",
            "Epoch 42/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0236 - mae: 0.1490 - val_loss: 0.0377 - val_mae: 0.2110\n",
            "\n",
            "Epoch 00042: loss did not improve from 0.02253\n",
            "Epoch 43/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0218 - mae: 0.1458 - val_loss: 0.0359 - val_mae: 0.2065\n",
            "\n",
            "Epoch 00043: loss did not improve from 0.02253\n",
            "Epoch 44/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0232 - mae: 0.1491 - val_loss: 0.0380 - val_mae: 0.2109\n",
            "\n",
            "Epoch 00044: loss did not improve from 0.02253\n",
            "Epoch 45/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0237 - mae: 0.1488 - val_loss: 0.0367 - val_mae: 0.2073\n",
            "\n",
            "Epoch 00045: loss did not improve from 0.02253\n",
            "Epoch 46/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0225 - mae: 0.1459 - val_loss: 0.0364 - val_mae: 0.2061\n",
            "\n",
            "Epoch 00046: loss did not improve from 0.02253\n",
            "Epoch 47/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0222 - mae: 0.1473 - val_loss: 0.0362 - val_mae: 0.2070\n",
            "\n",
            "Epoch 00047: loss did not improve from 0.02253\n",
            "Epoch 48/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0230 - mae: 0.1494 - val_loss: 0.0350 - val_mae: 0.2026\n",
            "\n",
            "Epoch 00048: loss did not improve from 0.02253\n",
            "Epoch 49/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0229 - mae: 0.1471 - val_loss: 0.0362 - val_mae: 0.2048\n",
            "\n",
            "Epoch 00049: loss did not improve from 0.02253\n",
            "Epoch 50/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0199 - mae: 0.1438 - val_loss: 0.0356 - val_mae: 0.2043\n",
            "\n",
            "Epoch 00050: loss improved from 0.02253 to 0.02229, saving model to poids.hdf5\n",
            "Epoch 51/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0247 - mae: 0.1520 - val_loss: 0.0354 - val_mae: 0.2034\n",
            "\n",
            "Epoch 00051: loss did not improve from 0.02229\n",
            "Epoch 52/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0250 - mae: 0.1550 - val_loss: 0.0364 - val_mae: 0.2047\n",
            "\n",
            "Epoch 00052: loss did not improve from 0.02229\n",
            "Epoch 53/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0219 - mae: 0.1446 - val_loss: 0.0363 - val_mae: 0.2066\n",
            "\n",
            "Epoch 00053: loss did not improve from 0.02229\n",
            "Epoch 54/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0230 - mae: 0.1500 - val_loss: 0.0366 - val_mae: 0.2059\n",
            "\n",
            "Epoch 00054: loss did not improve from 0.02229\n",
            "Epoch 55/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0220 - mae: 0.1429 - val_loss: 0.0364 - val_mae: 0.2052\n",
            "\n",
            "Epoch 00055: loss did not improve from 0.02229\n",
            "Epoch 56/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0221 - mae: 0.1445 - val_loss: 0.0359 - val_mae: 0.2061\n",
            "\n",
            "Epoch 00056: loss did not improve from 0.02229\n",
            "Epoch 57/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0237 - mae: 0.1514 - val_loss: 0.0367 - val_mae: 0.2054\n",
            "\n",
            "Epoch 00057: loss did not improve from 0.02229\n",
            "Epoch 58/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0224 - mae: 0.1460 - val_loss: 0.0361 - val_mae: 0.2040\n",
            "\n",
            "Epoch 00058: loss did not improve from 0.02229\n",
            "Epoch 59/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0220 - mae: 0.1480 - val_loss: 0.0355 - val_mae: 0.2033\n",
            "\n",
            "Epoch 00059: loss did not improve from 0.02229\n",
            "Epoch 60/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0248 - mae: 0.1538 - val_loss: 0.0362 - val_mae: 0.2035\n",
            "\n",
            "Epoch 00060: loss did not improve from 0.02229\n",
            "Epoch 61/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0219 - mae: 0.1452 - val_loss: 0.0356 - val_mae: 0.2023\n",
            "\n",
            "Epoch 00061: loss did not improve from 0.02229\n",
            "Epoch 62/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0233 - mae: 0.1489 - val_loss: 0.0349 - val_mae: 0.2004\n",
            "\n",
            "Epoch 00062: loss did not improve from 0.02229\n",
            "Epoch 63/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0240 - mae: 0.1486 - val_loss: 0.0343 - val_mae: 0.1994\n",
            "\n",
            "Epoch 00063: loss did not improve from 0.02229\n",
            "Epoch 64/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0229 - mae: 0.1454 - val_loss: 0.0356 - val_mae: 0.2011\n",
            "\n",
            "Epoch 00064: loss did not improve from 0.02229\n",
            "Epoch 65/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0235 - mae: 0.1513 - val_loss: 0.0348 - val_mae: 0.2020\n",
            "\n",
            "Epoch 00065: loss did not improve from 0.02229\n",
            "Epoch 66/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0247 - mae: 0.1500 - val_loss: 0.0349 - val_mae: 0.1998\n",
            "\n",
            "Epoch 00066: loss did not improve from 0.02229\n",
            "Epoch 67/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0221 - mae: 0.1462 - val_loss: 0.0357 - val_mae: 0.2020\n",
            "\n",
            "Epoch 00067: loss did not improve from 0.02229\n",
            "Epoch 68/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0244 - mae: 0.1486 - val_loss: 0.0348 - val_mae: 0.1985\n",
            "\n",
            "Epoch 00068: loss did not improve from 0.02229\n",
            "Epoch 69/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0207 - mae: 0.1380 - val_loss: 0.0357 - val_mae: 0.2022\n",
            "\n",
            "Epoch 00069: loss did not improve from 0.02229\n",
            "Epoch 70/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0259 - mae: 0.1537 - val_loss: 0.0347 - val_mae: 0.1990\n",
            "\n",
            "Epoch 00070: loss did not improve from 0.02229\n",
            "Epoch 71/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0209 - mae: 0.1456 - val_loss: 0.0346 - val_mae: 0.1992\n",
            "\n",
            "Epoch 00071: loss improved from 0.02229 to 0.02209, saving model to poids.hdf5\n",
            "Epoch 72/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0222 - mae: 0.1472 - val_loss: 0.0348 - val_mae: 0.1991\n",
            "\n",
            "Epoch 00072: loss did not improve from 0.02209\n",
            "Epoch 73/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0251 - mae: 0.1532 - val_loss: 0.0343 - val_mae: 0.1989\n",
            "\n",
            "Epoch 00073: loss did not improve from 0.02209\n",
            "Epoch 74/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0219 - mae: 0.1470 - val_loss: 0.0351 - val_mae: 0.1993\n",
            "\n",
            "Epoch 00074: loss did not improve from 0.02209\n",
            "Epoch 75/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0273 - mae: 0.1576 - val_loss: 0.0338 - val_mae: 0.1971\n",
            "\n",
            "Epoch 00075: loss did not improve from 0.02209\n",
            "Epoch 76/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0214 - mae: 0.1429 - val_loss: 0.0351 - val_mae: 0.1988\n",
            "\n",
            "Epoch 00076: loss did not improve from 0.02209\n",
            "Epoch 77/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0216 - mae: 0.1454 - val_loss: 0.0349 - val_mae: 0.1989\n",
            "\n",
            "Epoch 00077: loss did not improve from 0.02209\n",
            "Epoch 78/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0212 - mae: 0.1449 - val_loss: 0.0353 - val_mae: 0.2005\n",
            "\n",
            "Epoch 00078: loss did not improve from 0.02209\n",
            "Epoch 79/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0211 - mae: 0.1454 - val_loss: 0.0349 - val_mae: 0.1993\n",
            "\n",
            "Epoch 00079: loss improved from 0.02209 to 0.02185, saving model to poids.hdf5\n",
            "Epoch 80/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0211 - mae: 0.1436 - val_loss: 0.0328 - val_mae: 0.1943\n",
            "\n",
            "Epoch 00080: loss did not improve from 0.02185\n",
            "Epoch 81/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0198 - mae: 0.1371 - val_loss: 0.0345 - val_mae: 0.1973\n",
            "\n",
            "Epoch 00081: loss did not improve from 0.02185\n",
            "Epoch 82/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0268 - mae: 0.1567 - val_loss: 0.0347 - val_mae: 0.1987\n",
            "\n",
            "Epoch 00082: loss did not improve from 0.02185\n",
            "Epoch 83/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0222 - mae: 0.1478 - val_loss: 0.0349 - val_mae: 0.2001\n",
            "\n",
            "Epoch 00083: loss did not improve from 0.02185\n",
            "Epoch 84/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0220 - mae: 0.1413 - val_loss: 0.0341 - val_mae: 0.1971\n",
            "\n",
            "Epoch 00084: loss did not improve from 0.02185\n",
            "Epoch 85/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0212 - mae: 0.1460 - val_loss: 0.0328 - val_mae: 0.1942\n",
            "\n",
            "Epoch 00085: loss did not improve from 0.02185\n",
            "Epoch 86/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0210 - mae: 0.1433 - val_loss: 0.0316 - val_mae: 0.1911\n",
            "\n",
            "Epoch 00086: loss improved from 0.02185 to 0.02162, saving model to poids.hdf5\n",
            "Epoch 87/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0235 - mae: 0.1483 - val_loss: 0.0343 - val_mae: 0.1970\n",
            "\n",
            "Epoch 00087: loss did not improve from 0.02162\n",
            "Epoch 88/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0229 - mae: 0.1446 - val_loss: 0.0340 - val_mae: 0.1962\n",
            "\n",
            "Epoch 00088: loss did not improve from 0.02162\n",
            "Epoch 89/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0202 - mae: 0.1402 - val_loss: 0.0335 - val_mae: 0.1945\n",
            "\n",
            "Epoch 00089: loss did not improve from 0.02162\n",
            "Epoch 90/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0221 - mae: 0.1449 - val_loss: 0.0343 - val_mae: 0.1968\n",
            "\n",
            "Epoch 00090: loss did not improve from 0.02162\n",
            "Epoch 91/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0207 - mae: 0.1377 - val_loss: 0.0334 - val_mae: 0.1960\n",
            "\n",
            "Epoch 00091: loss did not improve from 0.02162\n",
            "Epoch 92/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0237 - mae: 0.1473 - val_loss: 0.0326 - val_mae: 0.1932\n",
            "\n",
            "Epoch 00092: loss did not improve from 0.02162\n",
            "Epoch 93/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0222 - mae: 0.1464 - val_loss: 0.0336 - val_mae: 0.1933\n",
            "\n",
            "Epoch 00093: loss did not improve from 0.02162\n",
            "Epoch 94/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0209 - mae: 0.1399 - val_loss: 0.0347 - val_mae: 0.1981\n",
            "\n",
            "Epoch 00094: loss improved from 0.02162 to 0.02119, saving model to poids.hdf5\n",
            "Epoch 95/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0234 - mae: 0.1493 - val_loss: 0.0340 - val_mae: 0.1942\n",
            "\n",
            "Epoch 00095: loss did not improve from 0.02119\n",
            "Epoch 96/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0252 - mae: 0.1535 - val_loss: 0.0346 - val_mae: 0.1981\n",
            "\n",
            "Epoch 00096: loss did not improve from 0.02119\n",
            "Epoch 97/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0230 - mae: 0.1491 - val_loss: 0.0348 - val_mae: 0.1984\n",
            "\n",
            "Epoch 00097: loss did not improve from 0.02119\n",
            "Epoch 98/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0217 - mae: 0.1427 - val_loss: 0.0327 - val_mae: 0.1913\n",
            "\n",
            "Epoch 00098: loss did not improve from 0.02119\n",
            "Epoch 99/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0199 - mae: 0.1400 - val_loss: 0.0335 - val_mae: 0.1931\n",
            "\n",
            "Epoch 00099: loss did not improve from 0.02119\n",
            "Epoch 100/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0217 - mae: 0.1419 - val_loss: 0.0333 - val_mae: 0.1943\n",
            "\n",
            "Epoch 00100: loss did not improve from 0.02119\n",
            "Epoch 101/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0231 - mae: 0.1494 - val_loss: 0.0339 - val_mae: 0.1953\n",
            "\n",
            "Epoch 00101: loss did not improve from 0.02119\n",
            "Epoch 102/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0202 - mae: 0.1413 - val_loss: 0.0330 - val_mae: 0.1923\n",
            "\n",
            "Epoch 00102: loss did not improve from 0.02119\n",
            "Epoch 103/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0232 - mae: 0.1438 - val_loss: 0.0333 - val_mae: 0.1931\n",
            "\n",
            "Epoch 00103: loss did not improve from 0.02119\n",
            "Epoch 104/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0205 - mae: 0.1406 - val_loss: 0.0325 - val_mae: 0.1906\n",
            "\n",
            "Epoch 00104: loss did not improve from 0.02119\n",
            "Epoch 105/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0218 - mae: 0.1473 - val_loss: 0.0322 - val_mae: 0.1897\n",
            "\n",
            "Epoch 00105: loss did not improve from 0.02119\n",
            "Epoch 106/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0209 - mae: 0.1441 - val_loss: 0.0321 - val_mae: 0.1905\n",
            "\n",
            "Epoch 00106: loss did not improve from 0.02119\n",
            "Epoch 107/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0230 - mae: 0.1445 - val_loss: 0.0337 - val_mae: 0.1944\n",
            "\n",
            "Epoch 00107: loss did not improve from 0.02119\n",
            "Epoch 108/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0210 - mae: 0.1428 - val_loss: 0.0333 - val_mae: 0.1934\n",
            "\n",
            "Epoch 00108: loss did not improve from 0.02119\n",
            "Epoch 109/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0216 - mae: 0.1442 - val_loss: 0.0333 - val_mae: 0.1924\n",
            "\n",
            "Epoch 00109: loss did not improve from 0.02119\n",
            "Epoch 110/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0204 - mae: 0.1418 - val_loss: 0.0314 - val_mae: 0.1891\n",
            "\n",
            "Epoch 00110: loss did not improve from 0.02119\n",
            "Epoch 111/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0187 - mae: 0.1405 - val_loss: 0.0326 - val_mae: 0.1924\n",
            "\n",
            "Epoch 00111: loss did not improve from 0.02119\n",
            "Epoch 112/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0241 - mae: 0.1492 - val_loss: 0.0332 - val_mae: 0.1920\n",
            "\n",
            "Epoch 00112: loss did not improve from 0.02119\n",
            "Epoch 113/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0221 - mae: 0.1451 - val_loss: 0.0333 - val_mae: 0.1920\n",
            "\n",
            "Epoch 00113: loss did not improve from 0.02119\n",
            "Epoch 114/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0216 - mae: 0.1433 - val_loss: 0.0313 - val_mae: 0.1886\n",
            "\n",
            "Epoch 00114: loss did not improve from 0.02119\n",
            "Epoch 115/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0232 - mae: 0.1482 - val_loss: 0.0334 - val_mae: 0.1926\n",
            "\n",
            "Epoch 00115: loss did not improve from 0.02119\n",
            "Epoch 116/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0217 - mae: 0.1449 - val_loss: 0.0321 - val_mae: 0.1899\n",
            "\n",
            "Epoch 00116: loss did not improve from 0.02119\n",
            "Epoch 117/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0216 - mae: 0.1453 - val_loss: 0.0314 - val_mae: 0.1885\n",
            "\n",
            "Epoch 00117: loss did not improve from 0.02119\n",
            "Epoch 118/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0247 - mae: 0.1498 - val_loss: 0.0331 - val_mae: 0.1913\n",
            "\n",
            "Epoch 00118: loss did not improve from 0.02119\n",
            "Epoch 119/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0211 - mae: 0.1420 - val_loss: 0.0299 - val_mae: 0.1856\n",
            "\n",
            "Epoch 00119: loss did not improve from 0.02119\n",
            "Epoch 120/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0192 - mae: 0.1389 - val_loss: 0.0319 - val_mae: 0.1882\n",
            "\n",
            "Epoch 00120: loss did not improve from 0.02119\n",
            "Epoch 121/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0249 - mae: 0.1508 - val_loss: 0.0336 - val_mae: 0.1935\n",
            "\n",
            "Epoch 00121: loss did not improve from 0.02119\n",
            "Epoch 122/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0201 - mae: 0.1451 - val_loss: 0.0327 - val_mae: 0.1907\n",
            "\n",
            "Epoch 00122: loss did not improve from 0.02119\n",
            "Epoch 123/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0207 - mae: 0.1424 - val_loss: 0.0313 - val_mae: 0.1869\n",
            "\n",
            "Epoch 00123: loss did not improve from 0.02119\n",
            "Epoch 124/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0232 - mae: 0.1489 - val_loss: 0.0320 - val_mae: 0.1902\n",
            "\n",
            "Epoch 00124: loss did not improve from 0.02119\n",
            "Epoch 125/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0203 - mae: 0.1417 - val_loss: 0.0334 - val_mae: 0.1930\n",
            "\n",
            "Epoch 00125: loss did not improve from 0.02119\n",
            "Epoch 126/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0234 - mae: 0.1471 - val_loss: 0.0326 - val_mae: 0.1903\n",
            "\n",
            "Epoch 00126: loss did not improve from 0.02119\n",
            "Epoch 127/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0224 - mae: 0.1443 - val_loss: 0.0329 - val_mae: 0.1910\n",
            "\n",
            "Epoch 00127: loss did not improve from 0.02119\n",
            "Epoch 128/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0227 - mae: 0.1514 - val_loss: 0.0317 - val_mae: 0.1900\n",
            "\n",
            "Epoch 00128: loss did not improve from 0.02119\n",
            "Epoch 129/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0221 - mae: 0.1456 - val_loss: 0.0317 - val_mae: 0.1874\n",
            "\n",
            "Epoch 00129: loss did not improve from 0.02119\n",
            "Epoch 130/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0231 - mae: 0.1484 - val_loss: 0.0316 - val_mae: 0.1867\n",
            "\n",
            "Epoch 00130: loss did not improve from 0.02119\n",
            "Epoch 131/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0193 - mae: 0.1393 - val_loss: 0.0329 - val_mae: 0.1904\n",
            "\n",
            "Epoch 00131: loss did not improve from 0.02119\n",
            "Epoch 132/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0226 - mae: 0.1482 - val_loss: 0.0321 - val_mae: 0.1885\n",
            "\n",
            "Epoch 00132: loss did not improve from 0.02119\n",
            "Epoch 133/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0223 - mae: 0.1449 - val_loss: 0.0323 - val_mae: 0.1884\n",
            "\n",
            "Epoch 00133: loss did not improve from 0.02119\n",
            "Epoch 134/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0225 - mae: 0.1496 - val_loss: 0.0308 - val_mae: 0.1843\n",
            "\n",
            "Epoch 00134: loss did not improve from 0.02119\n",
            "Epoch 135/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0229 - mae: 0.1455 - val_loss: 0.0327 - val_mae: 0.1903\n",
            "\n",
            "Epoch 00135: loss did not improve from 0.02119\n",
            "Epoch 136/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0221 - mae: 0.1436 - val_loss: 0.0320 - val_mae: 0.1878\n",
            "\n",
            "Epoch 00136: loss did not improve from 0.02119\n",
            "Epoch 137/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0223 - mae: 0.1468 - val_loss: 0.0328 - val_mae: 0.1901\n",
            "\n",
            "Epoch 00137: loss did not improve from 0.02119\n",
            "Epoch 138/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0229 - mae: 0.1457 - val_loss: 0.0285 - val_mae: 0.1807\n",
            "\n",
            "Epoch 00138: loss did not improve from 0.02119\n",
            "Epoch 139/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0223 - mae: 0.1431 - val_loss: 0.0322 - val_mae: 0.1874\n",
            "\n",
            "Epoch 00139: loss did not improve from 0.02119\n",
            "Epoch 140/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0208 - mae: 0.1442 - val_loss: 0.0326 - val_mae: 0.1900\n",
            "\n",
            "Epoch 00140: loss did not improve from 0.02119\n",
            "Epoch 141/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0228 - mae: 0.1440 - val_loss: 0.0325 - val_mae: 0.1885\n",
            "\n",
            "Epoch 00141: loss did not improve from 0.02119\n",
            "Epoch 142/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0212 - mae: 0.1422 - val_loss: 0.0308 - val_mae: 0.1853\n",
            "\n",
            "Epoch 00142: loss did not improve from 0.02119\n",
            "Epoch 143/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0215 - mae: 0.1453 - val_loss: 0.0321 - val_mae: 0.1870\n",
            "\n",
            "Epoch 00143: loss did not improve from 0.02119\n",
            "Epoch 144/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0188 - mae: 0.1361 - val_loss: 0.0307 - val_mae: 0.1855\n",
            "\n",
            "Epoch 00144: loss did not improve from 0.02119\n",
            "Epoch 145/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0200 - mae: 0.1410 - val_loss: 0.0316 - val_mae: 0.1883\n",
            "\n",
            "Epoch 00145: loss did not improve from 0.02119\n",
            "Epoch 146/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0235 - mae: 0.1504 - val_loss: 0.0302 - val_mae: 0.1836\n",
            "\n",
            "Epoch 00146: loss did not improve from 0.02119\n",
            "Epoch 147/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0255 - mae: 0.1539 - val_loss: 0.0314 - val_mae: 0.1868\n",
            "\n",
            "Epoch 00147: loss did not improve from 0.02119\n",
            "Epoch 148/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0217 - mae: 0.1449 - val_loss: 0.0315 - val_mae: 0.1870\n",
            "\n",
            "Epoch 00148: loss did not improve from 0.02119\n",
            "Epoch 149/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0226 - mae: 0.1448 - val_loss: 0.0317 - val_mae: 0.1876\n",
            "\n",
            "Epoch 00149: loss did not improve from 0.02119\n",
            "Epoch 150/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0214 - mae: 0.1422 - val_loss: 0.0305 - val_mae: 0.1836\n",
            "\n",
            "Epoch 00150: loss did not improve from 0.02119\n",
            "Epoch 151/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0214 - mae: 0.1455 - val_loss: 0.0312 - val_mae: 0.1840\n",
            "\n",
            "Epoch 00151: loss did not improve from 0.02119\n",
            "Epoch 152/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0214 - mae: 0.1478 - val_loss: 0.0318 - val_mae: 0.1867\n",
            "\n",
            "Epoch 00152: loss did not improve from 0.02119\n",
            "Epoch 153/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0208 - mae: 0.1439 - val_loss: 0.0318 - val_mae: 0.1877\n",
            "\n",
            "Epoch 00153: loss did not improve from 0.02119\n",
            "Epoch 154/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0222 - mae: 0.1483 - val_loss: 0.0317 - val_mae: 0.1851\n",
            "\n",
            "Epoch 00154: loss did not improve from 0.02119\n",
            "Epoch 155/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0215 - mae: 0.1492 - val_loss: 0.0322 - val_mae: 0.1883\n",
            "\n",
            "Epoch 00155: loss did not improve from 0.02119\n",
            "Epoch 156/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0244 - mae: 0.1498 - val_loss: 0.0318 - val_mae: 0.1874\n",
            "\n",
            "Epoch 00156: loss did not improve from 0.02119\n",
            "Epoch 157/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0188 - mae: 0.1381 - val_loss: 0.0318 - val_mae: 0.1852\n",
            "\n",
            "Epoch 00157: loss did not improve from 0.02119\n",
            "Epoch 158/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0227 - mae: 0.1449 - val_loss: 0.0307 - val_mae: 0.1837\n",
            "\n",
            "Epoch 00158: loss did not improve from 0.02119\n",
            "Epoch 159/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0255 - mae: 0.1532 - val_loss: 0.0310 - val_mae: 0.1860\n",
            "\n",
            "Epoch 00159: loss did not improve from 0.02119\n",
            "Epoch 160/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0233 - mae: 0.1467 - val_loss: 0.0314 - val_mae: 0.1859\n",
            "\n",
            "Epoch 00160: loss did not improve from 0.02119\n",
            "Epoch 161/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0232 - mae: 0.1459 - val_loss: 0.0320 - val_mae: 0.1871\n",
            "\n",
            "Epoch 00161: loss did not improve from 0.02119\n",
            "Epoch 162/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0205 - mae: 0.1413 - val_loss: 0.0301 - val_mae: 0.1835\n",
            "\n",
            "Epoch 00162: loss did not improve from 0.02119\n",
            "Epoch 163/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0241 - mae: 0.1522 - val_loss: 0.0301 - val_mae: 0.1813\n",
            "\n",
            "Epoch 00163: loss did not improve from 0.02119\n",
            "Epoch 164/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0228 - mae: 0.1484 - val_loss: 0.0318 - val_mae: 0.1854\n",
            "\n",
            "Epoch 00164: loss did not improve from 0.02119\n",
            "Epoch 165/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0232 - mae: 0.1474 - val_loss: 0.0318 - val_mae: 0.1860\n",
            "\n",
            "Epoch 00165: loss did not improve from 0.02119\n",
            "Epoch 166/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0226 - mae: 0.1466 - val_loss: 0.0300 - val_mae: 0.1811\n",
            "\n",
            "Epoch 00166: loss did not improve from 0.02119\n",
            "Epoch 167/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0225 - mae: 0.1461 - val_loss: 0.0307 - val_mae: 0.1847\n",
            "\n",
            "Epoch 00167: loss did not improve from 0.02119\n",
            "Epoch 168/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0204 - mae: 0.1412 - val_loss: 0.0300 - val_mae: 0.1812\n",
            "\n",
            "Epoch 00168: loss did not improve from 0.02119\n",
            "Epoch 169/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0223 - mae: 0.1452 - val_loss: 0.0302 - val_mae: 0.1842\n",
            "\n",
            "Epoch 00169: loss did not improve from 0.02119\n",
            "Epoch 170/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0248 - mae: 0.1503 - val_loss: 0.0317 - val_mae: 0.1865\n",
            "\n",
            "Epoch 00170: loss did not improve from 0.02119\n",
            "Epoch 171/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0208 - mae: 0.1399 - val_loss: 0.0309 - val_mae: 0.1839\n",
            "\n",
            "Epoch 00171: loss did not improve from 0.02119\n",
            "Epoch 172/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0225 - mae: 0.1454 - val_loss: 0.0315 - val_mae: 0.1852\n",
            "\n",
            "Epoch 00172: loss did not improve from 0.02119\n",
            "Epoch 173/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0229 - mae: 0.1512 - val_loss: 0.0310 - val_mae: 0.1832\n",
            "\n",
            "Epoch 00173: loss did not improve from 0.02119\n",
            "Epoch 174/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0215 - mae: 0.1421 - val_loss: 0.0314 - val_mae: 0.1845\n",
            "\n",
            "Epoch 00174: loss did not improve from 0.02119\n",
            "Epoch 175/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0240 - mae: 0.1517 - val_loss: 0.0304 - val_mae: 0.1829\n",
            "\n",
            "Epoch 00175: loss did not improve from 0.02119\n",
            "Epoch 176/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0222 - mae: 0.1459 - val_loss: 0.0310 - val_mae: 0.1832\n",
            "\n",
            "Epoch 00176: loss did not improve from 0.02119\n",
            "Epoch 177/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0215 - mae: 0.1434 - val_loss: 0.0315 - val_mae: 0.1858\n",
            "\n",
            "Epoch 00177: loss did not improve from 0.02119\n",
            "Epoch 178/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0202 - mae: 0.1403 - val_loss: 0.0311 - val_mae: 0.1831\n",
            "\n",
            "Epoch 00178: loss did not improve from 0.02119\n",
            "Epoch 179/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0222 - mae: 0.1454 - val_loss: 0.0292 - val_mae: 0.1808\n",
            "\n",
            "Epoch 00179: loss did not improve from 0.02119\n",
            "Epoch 180/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0181 - mae: 0.1364 - val_loss: 0.0317 - val_mae: 0.1862\n",
            "\n",
            "Epoch 00180: loss did not improve from 0.02119\n",
            "Epoch 181/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0241 - mae: 0.1480 - val_loss: 0.0315 - val_mae: 0.1844\n",
            "\n",
            "Epoch 00181: loss did not improve from 0.02119\n",
            "Epoch 182/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0246 - mae: 0.1520 - val_loss: 0.0308 - val_mae: 0.1827\n",
            "\n",
            "Epoch 00182: loss did not improve from 0.02119\n",
            "Epoch 183/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0224 - mae: 0.1449 - val_loss: 0.0312 - val_mae: 0.1837\n",
            "\n",
            "Epoch 00183: loss improved from 0.02119 to 0.02095, saving model to poids.hdf5\n",
            "Epoch 184/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0206 - mae: 0.1414 - val_loss: 0.0315 - val_mae: 0.1849\n",
            "\n",
            "Epoch 00184: loss did not improve from 0.02095\n",
            "Epoch 185/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0209 - mae: 0.1413 - val_loss: 0.0318 - val_mae: 0.1858\n",
            "\n",
            "Epoch 00185: loss did not improve from 0.02095\n",
            "Epoch 186/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0218 - mae: 0.1458 - val_loss: 0.0309 - val_mae: 0.1834\n",
            "\n",
            "Epoch 00186: loss did not improve from 0.02095\n",
            "Epoch 187/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0218 - mae: 0.1445 - val_loss: 0.0298 - val_mae: 0.1802\n",
            "\n",
            "Epoch 00187: loss improved from 0.02095 to 0.02094, saving model to poids.hdf5\n",
            "Epoch 188/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0200 - mae: 0.1405 - val_loss: 0.0308 - val_mae: 0.1825\n",
            "\n",
            "Epoch 00188: loss did not improve from 0.02094\n",
            "Epoch 189/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0208 - mae: 0.1404 - val_loss: 0.0304 - val_mae: 0.1813\n",
            "\n",
            "Epoch 00189: loss did not improve from 0.02094\n",
            "Epoch 190/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0233 - mae: 0.1476 - val_loss: 0.0316 - val_mae: 0.1849\n",
            "\n",
            "Epoch 00190: loss did not improve from 0.02094\n",
            "Epoch 191/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0195 - mae: 0.1383 - val_loss: 0.0316 - val_mae: 0.1855\n",
            "\n",
            "Epoch 00191: loss did not improve from 0.02094\n",
            "Epoch 192/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0207 - mae: 0.1456 - val_loss: 0.0315 - val_mae: 0.1850\n",
            "\n",
            "Epoch 00192: loss did not improve from 0.02094\n",
            "Epoch 193/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0172 - mae: 0.1319 - val_loss: 0.0312 - val_mae: 0.1832\n",
            "\n",
            "Epoch 00193: loss did not improve from 0.02094\n",
            "Epoch 194/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0231 - mae: 0.1479 - val_loss: 0.0303 - val_mae: 0.1813\n",
            "\n",
            "Epoch 00194: loss did not improve from 0.02094\n",
            "Epoch 195/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0189 - mae: 0.1375 - val_loss: 0.0313 - val_mae: 0.1842\n",
            "\n",
            "Epoch 00195: loss did not improve from 0.02094\n",
            "Epoch 196/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0194 - mae: 0.1391 - val_loss: 0.0307 - val_mae: 0.1831\n",
            "\n",
            "Epoch 00196: loss did not improve from 0.02094\n",
            "Epoch 197/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0223 - mae: 0.1444 - val_loss: 0.0307 - val_mae: 0.1817\n",
            "\n",
            "Epoch 00197: loss did not improve from 0.02094\n",
            "Epoch 198/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0222 - mae: 0.1411 - val_loss: 0.0298 - val_mae: 0.1811\n",
            "\n",
            "Epoch 00198: loss did not improve from 0.02094\n",
            "Epoch 199/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0239 - mae: 0.1482 - val_loss: 0.0312 - val_mae: 0.1832\n",
            "\n",
            "Epoch 00199: loss did not improve from 0.02094\n",
            "Epoch 200/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0206 - mae: 0.1431 - val_loss: 0.0302 - val_mae: 0.1805\n",
            "\n",
            "Epoch 00200: loss did not improve from 0.02094\n",
            "Epoch 201/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0197 - mae: 0.1390 - val_loss: 0.0306 - val_mae: 0.1811\n",
            "\n",
            "Epoch 00201: loss did not improve from 0.02094\n",
            "Epoch 202/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0195 - mae: 0.1390 - val_loss: 0.0314 - val_mae: 0.1848\n",
            "\n",
            "Epoch 00202: loss did not improve from 0.02094\n",
            "Epoch 203/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0197 - mae: 0.1429 - val_loss: 0.0310 - val_mae: 0.1833\n",
            "\n",
            "Epoch 00203: loss did not improve from 0.02094\n",
            "Epoch 204/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0222 - mae: 0.1454 - val_loss: 0.0303 - val_mae: 0.1812\n",
            "\n",
            "Epoch 00204: loss did not improve from 0.02094\n",
            "Epoch 205/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0213 - mae: 0.1413 - val_loss: 0.0308 - val_mae: 0.1826\n",
            "\n",
            "Epoch 00205: loss did not improve from 0.02094\n",
            "Epoch 206/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0249 - mae: 0.1531 - val_loss: 0.0310 - val_mae: 0.1822\n",
            "\n",
            "Epoch 00206: loss did not improve from 0.02094\n",
            "Epoch 207/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0209 - mae: 0.1429 - val_loss: 0.0313 - val_mae: 0.1838\n",
            "\n",
            "Epoch 00207: loss did not improve from 0.02094\n",
            "Epoch 208/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0203 - mae: 0.1403 - val_loss: 0.0302 - val_mae: 0.1808\n",
            "\n",
            "Epoch 00208: loss did not improve from 0.02094\n",
            "Epoch 209/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0262 - mae: 0.1537 - val_loss: 0.0310 - val_mae: 0.1838\n",
            "\n",
            "Epoch 00209: loss did not improve from 0.02094\n",
            "Epoch 210/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0209 - mae: 0.1422 - val_loss: 0.0289 - val_mae: 0.1780\n",
            "\n",
            "Epoch 00210: loss did not improve from 0.02094\n",
            "Epoch 211/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0206 - mae: 0.1386 - val_loss: 0.0292 - val_mae: 0.1793\n",
            "\n",
            "Epoch 00211: loss did not improve from 0.02094\n",
            "Epoch 212/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0252 - mae: 0.1541 - val_loss: 0.0299 - val_mae: 0.1811\n",
            "\n",
            "Epoch 00212: loss did not improve from 0.02094\n",
            "Epoch 213/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0202 - mae: 0.1395 - val_loss: 0.0286 - val_mae: 0.1787\n",
            "\n",
            "Epoch 00213: loss did not improve from 0.02094\n",
            "Epoch 214/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0185 - mae: 0.1367 - val_loss: 0.0300 - val_mae: 0.1798\n",
            "\n",
            "Epoch 00214: loss did not improve from 0.02094\n",
            "Epoch 215/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0236 - mae: 0.1508 - val_loss: 0.0297 - val_mae: 0.1791\n",
            "\n",
            "Epoch 00215: loss improved from 0.02094 to 0.02059, saving model to poids.hdf5\n",
            "Epoch 216/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0245 - mae: 0.1522 - val_loss: 0.0294 - val_mae: 0.1790\n",
            "\n",
            "Epoch 00216: loss did not improve from 0.02059\n",
            "Epoch 217/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0227 - mae: 0.1460 - val_loss: 0.0307 - val_mae: 0.1808\n",
            "\n",
            "Epoch 00217: loss did not improve from 0.02059\n",
            "Epoch 218/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0185 - mae: 0.1351 - val_loss: 0.0306 - val_mae: 0.1813\n",
            "\n",
            "Epoch 00218: loss did not improve from 0.02059\n",
            "Epoch 219/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0214 - mae: 0.1436 - val_loss: 0.0298 - val_mae: 0.1811\n",
            "\n",
            "Epoch 00219: loss did not improve from 0.02059\n",
            "Epoch 220/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0209 - mae: 0.1466 - val_loss: 0.0306 - val_mae: 0.1808\n",
            "\n",
            "Epoch 00220: loss did not improve from 0.02059\n",
            "Epoch 221/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0197 - mae: 0.1346 - val_loss: 0.0307 - val_mae: 0.1819\n",
            "\n",
            "Epoch 00221: loss did not improve from 0.02059\n",
            "Epoch 222/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0232 - mae: 0.1483 - val_loss: 0.0306 - val_mae: 0.1826\n",
            "\n",
            "Epoch 00222: loss did not improve from 0.02059\n",
            "Epoch 223/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0237 - mae: 0.1482 - val_loss: 0.0294 - val_mae: 0.1778\n",
            "\n",
            "Epoch 00223: loss did not improve from 0.02059\n",
            "Epoch 224/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0218 - mae: 0.1453 - val_loss: 0.0294 - val_mae: 0.1774\n",
            "\n",
            "Epoch 00224: loss did not improve from 0.02059\n",
            "Epoch 225/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0215 - mae: 0.1439 - val_loss: 0.0307 - val_mae: 0.1817\n",
            "\n",
            "Epoch 00225: loss did not improve from 0.02059\n",
            "Epoch 226/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0209 - mae: 0.1401 - val_loss: 0.0292 - val_mae: 0.1782\n",
            "\n",
            "Epoch 00226: loss did not improve from 0.02059\n",
            "Epoch 227/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0220 - mae: 0.1422 - val_loss: 0.0307 - val_mae: 0.1813\n",
            "\n",
            "Epoch 00227: loss did not improve from 0.02059\n",
            "Epoch 228/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0212 - mae: 0.1445 - val_loss: 0.0308 - val_mae: 0.1817\n",
            "\n",
            "Epoch 00228: loss did not improve from 0.02059\n",
            "Epoch 229/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0223 - mae: 0.1432 - val_loss: 0.0306 - val_mae: 0.1812\n",
            "\n",
            "Epoch 00229: loss did not improve from 0.02059\n",
            "Epoch 230/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0192 - mae: 0.1360 - val_loss: 0.0302 - val_mae: 0.1792\n",
            "\n",
            "Epoch 00230: loss did not improve from 0.02059\n",
            "Epoch 231/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0217 - mae: 0.1428 - val_loss: 0.0305 - val_mae: 0.1821\n",
            "\n",
            "Epoch 00231: loss did not improve from 0.02059\n",
            "Epoch 232/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0210 - mae: 0.1445 - val_loss: 0.0306 - val_mae: 0.1806\n",
            "\n",
            "Epoch 00232: loss did not improve from 0.02059\n",
            "Epoch 233/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0221 - mae: 0.1408 - val_loss: 0.0302 - val_mae: 0.1785\n",
            "\n",
            "Epoch 00233: loss did not improve from 0.02059\n",
            "Epoch 234/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0237 - mae: 0.1490 - val_loss: 0.0305 - val_mae: 0.1811\n",
            "\n",
            "Epoch 00234: loss did not improve from 0.02059\n",
            "Epoch 235/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0200 - mae: 0.1392 - val_loss: 0.0298 - val_mae: 0.1787\n",
            "\n",
            "Epoch 00235: loss did not improve from 0.02059\n",
            "Epoch 236/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0208 - mae: 0.1447 - val_loss: 0.0291 - val_mae: 0.1769\n",
            "\n",
            "Epoch 00236: loss did not improve from 0.02059\n",
            "Epoch 237/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0191 - mae: 0.1358 - val_loss: 0.0304 - val_mae: 0.1803\n",
            "\n",
            "Epoch 00237: loss did not improve from 0.02059\n",
            "Epoch 238/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0188 - mae: 0.1362 - val_loss: 0.0306 - val_mae: 0.1812\n",
            "\n",
            "Epoch 00238: loss did not improve from 0.02059\n",
            "Epoch 239/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0190 - mae: 0.1349 - val_loss: 0.0298 - val_mae: 0.1790\n",
            "\n",
            "Epoch 00239: loss did not improve from 0.02059\n",
            "Epoch 240/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0185 - mae: 0.1369 - val_loss: 0.0297 - val_mae: 0.1780\n",
            "\n",
            "Epoch 00240: loss improved from 0.02059 to 0.02036, saving model to poids.hdf5\n",
            "Epoch 241/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0214 - mae: 0.1435 - val_loss: 0.0295 - val_mae: 0.1782\n",
            "\n",
            "Epoch 00241: loss did not improve from 0.02036\n",
            "Epoch 242/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0208 - mae: 0.1407 - val_loss: 0.0280 - val_mae: 0.1731\n",
            "\n",
            "Epoch 00242: loss did not improve from 0.02036\n",
            "Epoch 243/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0223 - mae: 0.1453 - val_loss: 0.0297 - val_mae: 0.1783\n",
            "\n",
            "Epoch 00243: loss did not improve from 0.02036\n",
            "Epoch 244/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0224 - mae: 0.1441 - val_loss: 0.0297 - val_mae: 0.1781\n",
            "\n",
            "Epoch 00244: loss did not improve from 0.02036\n",
            "Epoch 245/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0185 - mae: 0.1350 - val_loss: 0.0298 - val_mae: 0.1794\n",
            "\n",
            "Epoch 00245: loss did not improve from 0.02036\n",
            "Epoch 246/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0211 - mae: 0.1398 - val_loss: 0.0293 - val_mae: 0.1763\n",
            "\n",
            "Epoch 00246: loss did not improve from 0.02036\n",
            "Epoch 247/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0220 - mae: 0.1445 - val_loss: 0.0307 - val_mae: 0.1815\n",
            "\n",
            "Epoch 00247: loss did not improve from 0.02036\n",
            "Epoch 248/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0231 - mae: 0.1489 - val_loss: 0.0287 - val_mae: 0.1763\n",
            "\n",
            "Epoch 00248: loss did not improve from 0.02036\n",
            "Epoch 249/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0210 - mae: 0.1439 - val_loss: 0.0300 - val_mae: 0.1787\n",
            "\n",
            "Epoch 00249: loss did not improve from 0.02036\n",
            "Epoch 250/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0209 - mae: 0.1388 - val_loss: 0.0302 - val_mae: 0.1786\n",
            "\n",
            "Epoch 00250: loss did not improve from 0.02036\n",
            "Epoch 251/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0239 - mae: 0.1481 - val_loss: 0.0296 - val_mae: 0.1775\n",
            "\n",
            "Epoch 00251: loss did not improve from 0.02036\n",
            "Epoch 252/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0195 - mae: 0.1384 - val_loss: 0.0301 - val_mae: 0.1793\n",
            "\n",
            "Epoch 00252: loss did not improve from 0.02036\n",
            "Epoch 253/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0239 - mae: 0.1470 - val_loss: 0.0295 - val_mae: 0.1764\n",
            "\n",
            "Epoch 00253: loss did not improve from 0.02036\n",
            "Epoch 254/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0195 - mae: 0.1367 - val_loss: 0.0303 - val_mae: 0.1801\n",
            "\n",
            "Epoch 00254: loss did not improve from 0.02036\n",
            "Epoch 255/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0203 - mae: 0.1422 - val_loss: 0.0289 - val_mae: 0.1767\n",
            "\n",
            "Epoch 00255: loss did not improve from 0.02036\n",
            "Epoch 256/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0239 - mae: 0.1472 - val_loss: 0.0299 - val_mae: 0.1783\n",
            "\n",
            "Epoch 00256: loss did not improve from 0.02036\n",
            "Epoch 257/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0239 - mae: 0.1488 - val_loss: 0.0297 - val_mae: 0.1774\n",
            "\n",
            "Epoch 00257: loss did not improve from 0.02036\n",
            "Epoch 258/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0231 - mae: 0.1448 - val_loss: 0.0270 - val_mae: 0.1718\n",
            "\n",
            "Epoch 00258: loss did not improve from 0.02036\n",
            "Epoch 259/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0201 - mae: 0.1395 - val_loss: 0.0285 - val_mae: 0.1752\n",
            "\n",
            "Epoch 00259: loss did not improve from 0.02036\n",
            "Epoch 260/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0192 - mae: 0.1398 - val_loss: 0.0282 - val_mae: 0.1742\n",
            "\n",
            "Epoch 00260: loss did not improve from 0.02036\n",
            "Epoch 261/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0222 - mae: 0.1430 - val_loss: 0.0279 - val_mae: 0.1740\n",
            "\n",
            "Epoch 00261: loss did not improve from 0.02036\n",
            "Epoch 262/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0236 - mae: 0.1480 - val_loss: 0.0290 - val_mae: 0.1777\n",
            "\n",
            "Epoch 00262: loss did not improve from 0.02036\n",
            "Epoch 263/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0225 - mae: 0.1440 - val_loss: 0.0294 - val_mae: 0.1772\n",
            "\n",
            "Epoch 00263: loss did not improve from 0.02036\n",
            "Epoch 264/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0257 - mae: 0.1559 - val_loss: 0.0279 - val_mae: 0.1742\n",
            "\n",
            "Epoch 00264: loss did not improve from 0.02036\n",
            "Epoch 265/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0229 - mae: 0.1490 - val_loss: 0.0297 - val_mae: 0.1771\n",
            "\n",
            "Epoch 00265: loss did not improve from 0.02036\n",
            "Epoch 266/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0206 - mae: 0.1423 - val_loss: 0.0294 - val_mae: 0.1765\n",
            "\n",
            "Epoch 00266: loss did not improve from 0.02036\n",
            "Epoch 267/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0188 - mae: 0.1328 - val_loss: 0.0297 - val_mae: 0.1764\n",
            "\n",
            "Epoch 00267: loss did not improve from 0.02036\n",
            "Epoch 268/500\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.0203 - mae: 0.1436 - val_loss: 0.0300 - val_mae: 0.1781\n",
            "\n",
            "Epoch 00268: loss did not improve from 0.02036\n",
            "Epoch 269/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0196 - mae: 0.1400 - val_loss: 0.0291 - val_mae: 0.1755\n",
            "\n",
            "Epoch 00269: loss did not improve from 0.02036\n",
            "Epoch 270/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0185 - mae: 0.1349 - val_loss: 0.0295 - val_mae: 0.1775\n",
            "\n",
            "Epoch 00270: loss did not improve from 0.02036\n",
            "Epoch 271/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0224 - mae: 0.1432 - val_loss: 0.0294 - val_mae: 0.1758\n",
            "\n",
            "Epoch 00271: loss did not improve from 0.02036\n",
            "Epoch 272/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0219 - mae: 0.1422 - val_loss: 0.0297 - val_mae: 0.1777\n",
            "\n",
            "Epoch 00272: loss did not improve from 0.02036\n",
            "Epoch 273/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0223 - mae: 0.1467 - val_loss: 0.0268 - val_mae: 0.1713\n",
            "\n",
            "Epoch 00273: loss did not improve from 0.02036\n",
            "Epoch 274/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0210 - mae: 0.1395 - val_loss: 0.0278 - val_mae: 0.1735\n",
            "\n",
            "Epoch 00274: loss did not improve from 0.02036\n",
            "Epoch 275/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0171 - mae: 0.1321 - val_loss: 0.0284 - val_mae: 0.1737\n",
            "\n",
            "Epoch 00275: loss did not improve from 0.02036\n",
            "Epoch 276/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0218 - mae: 0.1448 - val_loss: 0.0290 - val_mae: 0.1759\n",
            "\n",
            "Epoch 00276: loss did not improve from 0.02036\n",
            "Epoch 277/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0214 - mae: 0.1446 - val_loss: 0.0293 - val_mae: 0.1773\n",
            "\n",
            "Epoch 00277: loss did not improve from 0.02036\n",
            "Epoch 278/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0217 - mae: 0.1429 - val_loss: 0.0289 - val_mae: 0.1751\n",
            "\n",
            "Epoch 00278: loss did not improve from 0.02036\n",
            "Epoch 279/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0240 - mae: 0.1471 - val_loss: 0.0295 - val_mae: 0.1770\n",
            "\n",
            "Epoch 00279: loss did not improve from 0.02036\n",
            "Epoch 280/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0206 - mae: 0.1403 - val_loss: 0.0299 - val_mae: 0.1779\n",
            "\n",
            "Epoch 00280: loss did not improve from 0.02036\n",
            "Epoch 281/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0220 - mae: 0.1470 - val_loss: 0.0293 - val_mae: 0.1758\n",
            "\n",
            "Epoch 00281: loss did not improve from 0.02036\n",
            "Epoch 282/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0209 - mae: 0.1407 - val_loss: 0.0288 - val_mae: 0.1748\n",
            "\n",
            "Epoch 00282: loss did not improve from 0.02036\n",
            "Epoch 283/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0199 - mae: 0.1403 - val_loss: 0.0289 - val_mae: 0.1757\n",
            "\n",
            "Epoch 00283: loss did not improve from 0.02036\n",
            "Epoch 284/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0206 - mae: 0.1415 - val_loss: 0.0288 - val_mae: 0.1763\n",
            "\n",
            "Epoch 00284: loss did not improve from 0.02036\n",
            "Epoch 285/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0221 - mae: 0.1432 - val_loss: 0.0276 - val_mae: 0.1734\n",
            "\n",
            "Epoch 00285: loss did not improve from 0.02036\n",
            "Epoch 286/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0233 - mae: 0.1492 - val_loss: 0.0296 - val_mae: 0.1765\n",
            "\n",
            "Epoch 00286: loss did not improve from 0.02036\n",
            "Epoch 287/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0215 - mae: 0.1437 - val_loss: 0.0289 - val_mae: 0.1751\n",
            "\n",
            "Epoch 00287: loss did not improve from 0.02036\n",
            "Epoch 288/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0224 - mae: 0.1423 - val_loss: 0.0292 - val_mae: 0.1743\n",
            "\n",
            "Epoch 00288: loss did not improve from 0.02036\n",
            "Epoch 289/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0192 - mae: 0.1413 - val_loss: 0.0293 - val_mae: 0.1756\n",
            "\n",
            "Epoch 00289: loss did not improve from 0.02036\n",
            "Epoch 290/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0193 - mae: 0.1378 - val_loss: 0.0293 - val_mae: 0.1766\n",
            "\n",
            "Epoch 00290: loss did not improve from 0.02036\n",
            "Epoch 291/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0236 - mae: 0.1482 - val_loss: 0.0288 - val_mae: 0.1745\n",
            "\n",
            "Epoch 00291: loss did not improve from 0.02036\n",
            "Epoch 292/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0203 - mae: 0.1412 - val_loss: 0.0289 - val_mae: 0.1751\n",
            "\n",
            "Epoch 00292: loss did not improve from 0.02036\n",
            "Epoch 293/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0214 - mae: 0.1423 - val_loss: 0.0296 - val_mae: 0.1765\n",
            "\n",
            "Epoch 00293: loss did not improve from 0.02036\n",
            "Epoch 294/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0216 - mae: 0.1447 - val_loss: 0.0276 - val_mae: 0.1724\n",
            "\n",
            "Epoch 00294: loss did not improve from 0.02036\n",
            "Epoch 295/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0218 - mae: 0.1444 - val_loss: 0.0294 - val_mae: 0.1756\n",
            "\n",
            "Epoch 00295: loss did not improve from 0.02036\n",
            "Epoch 296/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0227 - mae: 0.1439 - val_loss: 0.0279 - val_mae: 0.1734\n",
            "\n",
            "Epoch 00296: loss did not improve from 0.02036\n",
            "Epoch 297/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0209 - mae: 0.1422 - val_loss: 0.0295 - val_mae: 0.1762\n",
            "\n",
            "Epoch 00297: loss did not improve from 0.02036\n",
            "Epoch 298/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0234 - mae: 0.1502 - val_loss: 0.0292 - val_mae: 0.1753\n",
            "\n",
            "Epoch 00298: loss did not improve from 0.02036\n",
            "Epoch 299/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0210 - mae: 0.1436 - val_loss: 0.0283 - val_mae: 0.1730\n",
            "\n",
            "Epoch 00299: loss did not improve from 0.02036\n",
            "Epoch 300/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0175 - mae: 0.1360 - val_loss: 0.0289 - val_mae: 0.1755\n",
            "\n",
            "Epoch 00300: loss did not improve from 0.02036\n",
            "Epoch 301/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0198 - mae: 0.1373 - val_loss: 0.0293 - val_mae: 0.1752\n",
            "\n",
            "Epoch 00301: loss did not improve from 0.02036\n",
            "Epoch 302/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0249 - mae: 0.1485 - val_loss: 0.0284 - val_mae: 0.1737\n",
            "\n",
            "Epoch 00302: loss did not improve from 0.02036\n",
            "Epoch 303/500\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.0234 - mae: 0.1456 - val_loss: 0.0279 - val_mae: 0.1711\n",
            "\n",
            "Epoch 00303: loss did not improve from 0.02036\n",
            "Epoch 304/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0204 - mae: 0.1385 - val_loss: 0.0282 - val_mae: 0.1712\n",
            "\n",
            "Epoch 00304: loss did not improve from 0.02036\n",
            "Epoch 305/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0232 - mae: 0.1475 - val_loss: 0.0286 - val_mae: 0.1745\n",
            "\n",
            "Epoch 00305: loss did not improve from 0.02036\n",
            "Epoch 306/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0221 - mae: 0.1435 - val_loss: 0.0293 - val_mae: 0.1758\n",
            "\n",
            "Epoch 00306: loss did not improve from 0.02036\n",
            "Epoch 307/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0204 - mae: 0.1410 - val_loss: 0.0275 - val_mae: 0.1723\n",
            "\n",
            "Epoch 00307: loss did not improve from 0.02036\n",
            "Epoch 308/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0210 - mae: 0.1419 - val_loss: 0.0295 - val_mae: 0.1766\n",
            "\n",
            "Epoch 00308: loss did not improve from 0.02036\n",
            "Epoch 309/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0184 - mae: 0.1368 - val_loss: 0.0291 - val_mae: 0.1752\n",
            "\n",
            "Epoch 00309: loss did not improve from 0.02036\n",
            "Epoch 310/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0223 - mae: 0.1480 - val_loss: 0.0294 - val_mae: 0.1762\n",
            "\n",
            "Epoch 00310: loss did not improve from 0.02036\n",
            "Epoch 311/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0209 - mae: 0.1428 - val_loss: 0.0282 - val_mae: 0.1727\n",
            "\n",
            "Epoch 00311: loss did not improve from 0.02036\n",
            "Epoch 312/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0201 - mae: 0.1416 - val_loss: 0.0295 - val_mae: 0.1765\n",
            "\n",
            "Epoch 00312: loss did not improve from 0.02036\n",
            "Epoch 313/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0216 - mae: 0.1383 - val_loss: 0.0280 - val_mae: 0.1713\n",
            "\n",
            "Epoch 00313: loss did not improve from 0.02036\n",
            "Epoch 314/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0215 - mae: 0.1424 - val_loss: 0.0290 - val_mae: 0.1735\n",
            "\n",
            "Epoch 00314: loss did not improve from 0.02036\n",
            "Epoch 315/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0199 - mae: 0.1388 - val_loss: 0.0290 - val_mae: 0.1746\n",
            "\n",
            "Epoch 00315: loss did not improve from 0.02036\n",
            "Epoch 316/500\n",
            " 8/30 [=======>......................] - ETA: 0s - loss: 0.0186 - mae: 0.1337"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-9b00ff36f610>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Entraine le modèle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mhistorique\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_Val_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCheckPoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qdJe8PYk0x8"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wela4RRGcraC"
      },
      "source": [
        "**4. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiC511R5cqtc"
      },
      "source": [
        "taille_fenetre = 20\n",
        "\n",
        "# Création d'une liste vide pour recevoir les prédictions\n",
        "predictions = []\n",
        "\n",
        "# Calcul des prédiction pour chaque groupe de 20 valeurs consécutives de la série\n",
        "# dans l'intervalle de validation\n",
        "for t in temps[temps_separation:-taille_fenetre]:\n",
        "    X = np.reshape(Serie_Normalisee[t:t+taille_fenetre],(1,taille_fenetre))\n",
        "    predictions.append(model.predict(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1knHYB7-lBrV"
      },
      "source": [
        "# Affiche la série et les prédictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps,serie,label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title('Prédictions avec le modèle GRU + Attention')\n",
        "plt.show()\n",
        "\n",
        "# Zoom sur l'intervalle de validation\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps[temps_separation:],serie[temps_separation:],label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title(\"Prédictions avec le modèle GRU + Attention (zoom sur l'intervalle de validation)\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0WO5Bp7cuqs"
      },
      "source": [
        "# Calcule de l'erreur quadratique moyenne et de l'erreur absolue moyenne \n",
        "\n",
        "mae = tf.keras.metrics.mean_absolute_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "mse = tf.keras.metrics.mean_squared_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "\n",
        "print(mae)\n",
        "print(mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXXKV_rQuWbf"
      },
      "source": [
        "# Création du modèle LSTM avec couche d'attention personnalisée simple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfiypQrDuWbh"
      },
      "source": [
        "**1. Création du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LygZm1-4uWbi",
        "outputId": "98b781a9-9dde-44d1-b829-0a9f554ea606"
      },
      "source": [
        "dim_GRU = 40\n",
        "\n",
        "# Fonction de la couche lambda d'entrée\n",
        "def Traitement_Entrees(x):\n",
        "  return tf.expand_dims(x,axis=-1)\n",
        "\n",
        "# Encodeur\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.Input(shape=(taille_fenetre,)))\n",
        "model.add(tf.keras.layers.Lambda(Traitement_Entrees))\n",
        "model.add(tf.keras.layers.LSTM(dim_GRU,return_sequences=True))\n",
        "\n",
        "# Décodeur\n",
        "model.add(Couche_Attention())\n",
        "model.add(tf.keras.layers.Lambda(Traitement_Entrees))\n",
        "model.add(tf.keras.layers.LSTM(dim_GRU))\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "model.save_weights('model_initial.h5')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda_2 (Lambda)            (None, 20, 1)             0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 20, 40)            6720      \n",
            "_________________________________________________________________\n",
            "couche__attention_1 (Couche_ (None, 40)                60        \n",
            "_________________________________________________________________\n",
            "lambda_3 (Lambda)            (None, 40, 1)             0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 40)                6720      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 41        \n",
            "=================================================================\n",
            "Total params: 13,541\n",
            "Trainable params: 13,541\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j19xcCoKuWbk"
      },
      "source": [
        "**2. Optimisation du taux d'apprentissage**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fg50B9YiuWbk",
        "outputId": "904ecba3-d4e7-4a76-defb-fe2bdb14d019"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(lr=1e-8)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur, metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset_norm,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "30/30 [==============================] - 3s 16ms/step - loss: 0.3647 - mae: 0.7381\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.36642, saving model to poids.hdf5\n",
            "Epoch 2/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.3692 - mae: 0.7393\n",
            "\n",
            "Epoch 00002: loss did not improve from 0.36642\n",
            "Epoch 3/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.3647 - mae: 0.7362\n",
            "\n",
            "Epoch 00003: loss did not improve from 0.36642\n",
            "Epoch 4/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3761 - mae: 0.7512\n",
            "\n",
            "Epoch 00004: loss did not improve from 0.36642\n",
            "Epoch 5/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.3682 - mae: 0.7425\n",
            "\n",
            "Epoch 00005: loss did not improve from 0.36642\n",
            "Epoch 6/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.3721 - mae: 0.7436\n",
            "\n",
            "Epoch 00006: loss improved from 0.36642 to 0.36589, saving model to poids.hdf5\n",
            "Epoch 7/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.3661 - mae: 0.7372\n",
            "\n",
            "Epoch 00007: loss did not improve from 0.36589\n",
            "Epoch 8/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.3940 - mae: 0.7718\n",
            "\n",
            "Epoch 00008: loss improved from 0.36589 to 0.36528, saving model to poids.hdf5\n",
            "Epoch 9/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3683 - mae: 0.7410\n",
            "\n",
            "Epoch 00009: loss did not improve from 0.36528\n",
            "Epoch 10/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.3604 - mae: 0.7331\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.36528\n",
            "Epoch 11/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3689 - mae: 0.7427\n",
            "\n",
            "Epoch 00011: loss did not improve from 0.36528\n",
            "Epoch 12/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3499 - mae: 0.7160\n",
            "\n",
            "Epoch 00012: loss did not improve from 0.36528\n",
            "Epoch 13/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3491 - mae: 0.7182\n",
            "\n",
            "Epoch 00013: loss did not improve from 0.36528\n",
            "Epoch 14/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.3756 - mae: 0.7463\n",
            "\n",
            "Epoch 00014: loss improved from 0.36528 to 0.36310, saving model to poids.hdf5\n",
            "Epoch 15/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3717 - mae: 0.7473\n",
            "\n",
            "Epoch 00015: loss did not improve from 0.36310\n",
            "Epoch 16/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3773 - mae: 0.7568\n",
            "\n",
            "Epoch 00016: loss did not improve from 0.36310\n",
            "Epoch 17/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.3887 - mae: 0.7666\n",
            "\n",
            "Epoch 00017: loss did not improve from 0.36310\n",
            "Epoch 18/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3720 - mae: 0.7506\n",
            "\n",
            "Epoch 00018: loss did not improve from 0.36310\n",
            "Epoch 19/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.3835 - mae: 0.7611\n",
            "\n",
            "Epoch 00019: loss did not improve from 0.36310\n",
            "Epoch 20/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.3657 - mae: 0.7419\n",
            "\n",
            "Epoch 00020: loss did not improve from 0.36310\n",
            "Epoch 21/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.3725 - mae: 0.7491\n",
            "\n",
            "Epoch 00021: loss did not improve from 0.36310\n",
            "Epoch 22/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.3695 - mae: 0.7439\n",
            "\n",
            "Epoch 00022: loss did not improve from 0.36310\n",
            "Epoch 23/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3719 - mae: 0.7415\n",
            "\n",
            "Epoch 00023: loss did not improve from 0.36310\n",
            "Epoch 24/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.3689 - mae: 0.7413\n",
            "\n",
            "Epoch 00024: loss did not improve from 0.36310\n",
            "Epoch 25/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3563 - mae: 0.7255\n",
            "\n",
            "Epoch 00025: loss did not improve from 0.36310\n",
            "Epoch 26/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3592 - mae: 0.7320\n",
            "\n",
            "Epoch 00026: loss improved from 0.36310 to 0.36197, saving model to poids.hdf5\n",
            "Epoch 27/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3647 - mae: 0.7351\n",
            "\n",
            "Epoch 00027: loss did not improve from 0.36197\n",
            "Epoch 28/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3612 - mae: 0.7364\n",
            "\n",
            "Epoch 00028: loss did not improve from 0.36197\n",
            "Epoch 29/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3552 - mae: 0.7280\n",
            "\n",
            "Epoch 00029: loss did not improve from 0.36197\n",
            "Epoch 30/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.3675 - mae: 0.7440\n",
            "\n",
            "Epoch 00030: loss did not improve from 0.36197\n",
            "Epoch 31/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3767 - mae: 0.7537\n",
            "\n",
            "Epoch 00031: loss did not improve from 0.36197\n",
            "Epoch 32/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3618 - mae: 0.7328\n",
            "\n",
            "Epoch 00032: loss did not improve from 0.36197\n",
            "Epoch 33/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.3384 - mae: 0.7026\n",
            "\n",
            "Epoch 00033: loss did not improve from 0.36197\n",
            "Epoch 34/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3518 - mae: 0.7208\n",
            "\n",
            "Epoch 00034: loss did not improve from 0.36197\n",
            "Epoch 35/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.3683 - mae: 0.7373\n",
            "\n",
            "Epoch 00035: loss did not improve from 0.36197\n",
            "Epoch 36/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3784 - mae: 0.7518\n",
            "\n",
            "Epoch 00036: loss did not improve from 0.36197\n",
            "Epoch 37/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3591 - mae: 0.7259\n",
            "\n",
            "Epoch 00037: loss improved from 0.36197 to 0.36014, saving model to poids.hdf5\n",
            "Epoch 38/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.3665 - mae: 0.7365\n",
            "\n",
            "Epoch 00038: loss did not improve from 0.36014\n",
            "Epoch 39/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3738 - mae: 0.7455\n",
            "\n",
            "Epoch 00039: loss improved from 0.36014 to 0.35834, saving model to poids.hdf5\n",
            "Epoch 40/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3745 - mae: 0.7488\n",
            "\n",
            "Epoch 00040: loss did not improve from 0.35834\n",
            "Epoch 41/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.3417 - mae: 0.7008\n",
            "\n",
            "Epoch 00041: loss did not improve from 0.35834\n",
            "Epoch 42/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.3411 - mae: 0.7025\n",
            "\n",
            "Epoch 00042: loss improved from 0.35834 to 0.35357, saving model to poids.hdf5\n",
            "Epoch 43/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3518 - mae: 0.7235\n",
            "\n",
            "Epoch 00043: loss did not improve from 0.35357\n",
            "Epoch 44/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3739 - mae: 0.7456\n",
            "\n",
            "Epoch 00044: loss improved from 0.35357 to 0.34976, saving model to poids.hdf5\n",
            "Epoch 45/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3377 - mae: 0.6959\n",
            "\n",
            "Epoch 00045: loss improved from 0.34976 to 0.34702, saving model to poids.hdf5\n",
            "Epoch 46/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3313 - mae: 0.6968\n",
            "\n",
            "Epoch 00046: loss improved from 0.34702 to 0.34081, saving model to poids.hdf5\n",
            "Epoch 47/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.3307 - mae: 0.6969\n",
            "\n",
            "Epoch 00047: loss improved from 0.34081 to 0.33411, saving model to poids.hdf5\n",
            "Epoch 48/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3373 - mae: 0.6985\n",
            "\n",
            "Epoch 00048: loss improved from 0.33411 to 0.32804, saving model to poids.hdf5\n",
            "Epoch 49/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.3133 - mae: 0.6631\n",
            "\n",
            "Epoch 00049: loss improved from 0.32804 to 0.32244, saving model to poids.hdf5\n",
            "Epoch 50/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3144 - mae: 0.6675\n",
            "\n",
            "Epoch 00050: loss improved from 0.32244 to 0.31471, saving model to poids.hdf5\n",
            "Epoch 51/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3179 - mae: 0.6725\n",
            "\n",
            "Epoch 00051: loss improved from 0.31471 to 0.30283, saving model to poids.hdf5\n",
            "Epoch 52/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.3062 - mae: 0.6594\n",
            "\n",
            "Epoch 00052: loss improved from 0.30283 to 0.29546, saving model to poids.hdf5\n",
            "Epoch 53/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.2976 - mae: 0.6510\n",
            "\n",
            "Epoch 00053: loss improved from 0.29546 to 0.28973, saving model to poids.hdf5\n",
            "Epoch 54/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.2800 - mae: 0.6252\n",
            "\n",
            "Epoch 00054: loss improved from 0.28973 to 0.27999, saving model to poids.hdf5\n",
            "Epoch 55/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.2778 - mae: 0.6313\n",
            "\n",
            "Epoch 00055: loss improved from 0.27999 to 0.27346, saving model to poids.hdf5\n",
            "Epoch 56/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.2671 - mae: 0.6121\n",
            "\n",
            "Epoch 00056: loss improved from 0.27346 to 0.26699, saving model to poids.hdf5\n",
            "Epoch 57/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.2584 - mae: 0.6090\n",
            "\n",
            "Epoch 00057: loss improved from 0.26699 to 0.26492, saving model to poids.hdf5\n",
            "Epoch 58/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.2778 - mae: 0.6400\n",
            "\n",
            "Epoch 00058: loss improved from 0.26492 to 0.26419, saving model to poids.hdf5\n",
            "Epoch 59/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.2584 - mae: 0.6117\n",
            "\n",
            "Epoch 00059: loss improved from 0.26419 to 0.26173, saving model to poids.hdf5\n",
            "Epoch 60/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.2534 - mae: 0.6028\n",
            "\n",
            "Epoch 00060: loss improved from 0.26173 to 0.25970, saving model to poids.hdf5\n",
            "Epoch 61/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.2542 - mae: 0.6133\n",
            "\n",
            "Epoch 00061: loss did not improve from 0.25970\n",
            "Epoch 62/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.2659 - mae: 0.6240\n",
            "\n",
            "Epoch 00062: loss improved from 0.25970 to 0.25703, saving model to poids.hdf5\n",
            "Epoch 63/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.2507 - mae: 0.6086\n",
            "\n",
            "Epoch 00063: loss did not improve from 0.25703\n",
            "Epoch 64/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.2699 - mae: 0.6341\n",
            "\n",
            "Epoch 00064: loss improved from 0.25703 to 0.25454, saving model to poids.hdf5\n",
            "Epoch 65/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.2490 - mae: 0.6057\n",
            "\n",
            "Epoch 00065: loss improved from 0.25454 to 0.24651, saving model to poids.hdf5\n",
            "Epoch 66/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.2248 - mae: 0.5716\n",
            "\n",
            "Epoch 00066: loss improved from 0.24651 to 0.22932, saving model to poids.hdf5\n",
            "Epoch 67/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.1970 - mae: 0.5348\n",
            "\n",
            "Epoch 00067: loss improved from 0.22932 to 0.18935, saving model to poids.hdf5\n",
            "Epoch 68/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.1203 - mae: 0.4004\n",
            "\n",
            "Epoch 00068: loss improved from 0.18935 to 0.10412, saving model to poids.hdf5\n",
            "Epoch 69/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.0488 - mae: 0.2317\n",
            "\n",
            "Epoch 00069: loss improved from 0.10412 to 0.03974, saving model to poids.hdf5\n",
            "Epoch 70/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.0283 - mae: 0.1675\n",
            "\n",
            "Epoch 00070: loss improved from 0.03974 to 0.03060, saving model to poids.hdf5\n",
            "Epoch 71/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.0297 - mae: 0.1708\n",
            "\n",
            "Epoch 00071: loss improved from 0.03060 to 0.02983, saving model to poids.hdf5\n",
            "Epoch 72/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.0275 - mae: 0.1641\n",
            "\n",
            "Epoch 00072: loss improved from 0.02983 to 0.02812, saving model to poids.hdf5\n",
            "Epoch 73/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.0284 - mae: 0.1714\n",
            "\n",
            "Epoch 00073: loss did not improve from 0.02812\n",
            "Epoch 74/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.0402 - mae: 0.2009\n",
            "\n",
            "Epoch 00074: loss did not improve from 0.02812\n",
            "Epoch 75/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.0557 - mae: 0.2626\n",
            "\n",
            "Epoch 00075: loss did not improve from 0.02812\n",
            "Epoch 76/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.0815 - mae: 0.3105\n",
            "\n",
            "Epoch 00076: loss did not improve from 0.02812\n",
            "Epoch 77/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.0337 - mae: 0.1962\n",
            "\n",
            "Epoch 00077: loss did not improve from 0.02812\n",
            "Epoch 78/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.0479 - mae: 0.2309\n",
            "\n",
            "Epoch 00078: loss did not improve from 0.02812\n",
            "Epoch 79/100\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.0429 - mae: 0.2241\n",
            "\n",
            "Epoch 00079: loss did not improve from 0.02812\n",
            "Epoch 80/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.0774 - mae: 0.2994\n",
            "\n",
            "Epoch 00080: loss did not improve from 0.02812\n",
            "Epoch 81/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.0633 - mae: 0.2767\n",
            "\n",
            "Epoch 00081: loss did not improve from 0.02812\n",
            "Epoch 82/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.1217 - mae: 0.4028\n",
            "\n",
            "Epoch 00082: loss did not improve from 0.02812\n",
            "Epoch 83/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.2056 - mae: 0.5463\n",
            "\n",
            "Epoch 00083: loss did not improve from 0.02812\n",
            "Epoch 84/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 2.8759 - mae: 3.2915\n",
            "\n",
            "Epoch 00084: loss did not improve from 0.02812\n",
            "Epoch 85/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 19.6201 - mae: 20.1201\n",
            "\n",
            "Epoch 00085: loss did not improve from 0.02812\n",
            "Epoch 86/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 24.4323 - mae: 24.9323\n",
            "\n",
            "Epoch 00086: loss did not improve from 0.02812\n",
            "Epoch 87/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 30.5165 - mae: 31.0165\n",
            "\n",
            "Epoch 00087: loss did not improve from 0.02812\n",
            "Epoch 88/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 38.0669 - mae: 38.5669\n",
            "\n",
            "Epoch 00088: loss did not improve from 0.02812\n",
            "Epoch 89/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 47.6879 - mae: 48.1879\n",
            "\n",
            "Epoch 00089: loss did not improve from 0.02812\n",
            "Epoch 90/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 59.7928 - mae: 60.2928\n",
            "\n",
            "Epoch 00090: loss did not improve from 0.02812\n",
            "Epoch 91/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 74.9247 - mae: 75.4247\n",
            "\n",
            "Epoch 00091: loss did not improve from 0.02812\n",
            "Epoch 92/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 78.1780 - mae: 78.6780\n",
            "\n",
            "Epoch 00092: loss did not improve from 0.02812\n",
            "Epoch 93/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 103.1962 - mae: 103.6962\n",
            "\n",
            "Epoch 00093: loss did not improve from 0.02812\n",
            "Epoch 94/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 129.7518 - mae: 130.2518\n",
            "\n",
            "Epoch 00094: loss did not improve from 0.02812\n",
            "Epoch 95/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 163.1996 - mae: 163.6996\n",
            "\n",
            "Epoch 00095: loss did not improve from 0.02812\n",
            "Epoch 96/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 205.2862 - mae: 205.7862\n",
            "\n",
            "Epoch 00096: loss did not improve from 0.02812\n",
            "Epoch 97/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 258.2919 - mae: 258.7919\n",
            "\n",
            "Epoch 00097: loss did not improve from 0.02812\n",
            "Epoch 98/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 324.9594 - mae: 325.4594\n",
            "\n",
            "Epoch 00098: loss did not improve from 0.02812\n",
            "Epoch 99/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 409.0139 - mae: 409.5139\n",
            "\n",
            "Epoch 00099: loss did not improve from 0.02812\n",
            "Epoch 100/100\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 514.7180 - mae: 515.2180\n",
            "\n",
            "Epoch 00100: loss did not improve from 0.02812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "eEaAVFisuWbl",
        "outputId": "d7904b4e-8d92-42fd-96a8-654e9b2f26ba"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 1])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, \"Evolution de l'erreur en fonction du taux d'apprentissage\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPcIR0yMuWbm"
      },
      "source": [
        "**3. Entrainement**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PD2YaeKpzCS-"
      },
      "source": [
        "# Charge les meilleurs poids\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DstONd8uWbm",
        "outputId": "ee71b423-207e-4b29-f962-e364d03ad2d1"
      },
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "class TimingCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, logs={}):\n",
        "        self.logs=[]\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.starttime = timer()\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.logs.append(timer()-self.starttime)\n",
        "\n",
        "cb = TimingCallback()\n",
        "\n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.2,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0.05)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(learning_rate=lr_schedule,momentum=0.9)\n",
        "#optimiseur=tf.keras.optimizers.SGD(0.2,momentum=0.9)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur,metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle\n",
        "historique = model.fit(dataset_norm,validation_data=dataset_Val_norm, epochs=500,verbose=1, callbacks=[CheckPoint,cb])\n",
        "\n",
        "print(cb.logs)\n",
        "print(sum(cb.logs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "30/30 [==============================] - 4s 49ms/step - loss: 0.0263 - mae: 0.1642 - val_loss: 0.0586 - val_mae: 0.2818\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.02251, saving model to poids.hdf5\n",
            "Epoch 2/500\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.0173 - mae: 0.1330 - val_loss: 0.0280 - val_mae: 0.1808\n",
            "\n",
            "Epoch 00002: loss improved from 0.02251 to 0.01816, saving model to poids.hdf5\n",
            "Epoch 3/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0176 - mae: 0.1353 - val_loss: 0.0287 - val_mae: 0.1808\n",
            "\n",
            "Epoch 00003: loss improved from 0.01816 to 0.01755, saving model to poids.hdf5\n",
            "Epoch 4/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0217 - mae: 0.1588 - val_loss: 0.0288 - val_mae: 0.1925\n",
            "\n",
            "Epoch 00004: loss did not improve from 0.01755\n",
            "Epoch 5/500\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.0218 - mae: 0.1609 - val_loss: 0.0944 - val_mae: 0.3684\n",
            "\n",
            "Epoch 00005: loss did not improve from 0.01755\n",
            "Epoch 6/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0209 - mae: 0.1514 - val_loss: 0.0528 - val_mae: 0.2675\n",
            "\n",
            "Epoch 00006: loss did not improve from 0.01755\n",
            "Epoch 7/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0164 - mae: 0.1294 - val_loss: 0.0458 - val_mae: 0.2440\n",
            "\n",
            "Epoch 00007: loss improved from 0.01755 to 0.01611, saving model to poids.hdf5\n",
            "Epoch 8/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0139 - mae: 0.1152 - val_loss: 0.0417 - val_mae: 0.2302\n",
            "\n",
            "Epoch 00008: loss improved from 0.01611 to 0.01517, saving model to poids.hdf5\n",
            "Epoch 9/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0136 - mae: 0.1190 - val_loss: 0.0350 - val_mae: 0.2040\n",
            "\n",
            "Epoch 00009: loss improved from 0.01517 to 0.01503, saving model to poids.hdf5\n",
            "Epoch 10/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0133 - mae: 0.1202 - val_loss: 0.0291 - val_mae: 0.1832\n",
            "\n",
            "Epoch 00010: loss improved from 0.01503 to 0.01493, saving model to poids.hdf5\n",
            "Epoch 11/500\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.0148 - mae: 0.1207 - val_loss: 0.0403 - val_mae: 0.2209\n",
            "\n",
            "Epoch 00011: loss improved from 0.01493 to 0.01454, saving model to poids.hdf5\n",
            "Epoch 12/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0157 - mae: 0.1227 - val_loss: 0.0277 - val_mae: 0.1767\n",
            "\n",
            "Epoch 00012: loss did not improve from 0.01454\n",
            "Epoch 13/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0149 - mae: 0.1247 - val_loss: 0.0248 - val_mae: 0.1649\n",
            "\n",
            "Epoch 00013: loss did not improve from 0.01454\n",
            "Epoch 14/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0174 - mae: 0.1322 - val_loss: 0.0282 - val_mae: 0.1788\n",
            "\n",
            "Epoch 00014: loss improved from 0.01454 to 0.01449, saving model to poids.hdf5\n",
            "Epoch 15/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0143 - mae: 0.1205 - val_loss: 0.0253 - val_mae: 0.1649\n",
            "\n",
            "Epoch 00015: loss improved from 0.01449 to 0.01372, saving model to poids.hdf5\n",
            "Epoch 16/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0157 - mae: 0.1197 - val_loss: 0.0254 - val_mae: 0.1664\n",
            "\n",
            "Epoch 00016: loss improved from 0.01372 to 0.01359, saving model to poids.hdf5\n",
            "Epoch 17/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0148 - mae: 0.1217 - val_loss: 0.0229 - val_mae: 0.1547\n",
            "\n",
            "Epoch 00017: loss did not improve from 0.01359\n",
            "Epoch 18/500\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.0123 - mae: 0.1140 - val_loss: 0.0240 - val_mae: 0.1573\n",
            "\n",
            "Epoch 00018: loss did not improve from 0.01359\n",
            "Epoch 19/500\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.0149 - mae: 0.1235 - val_loss: 0.0214 - val_mae: 0.1476\n",
            "\n",
            "Epoch 00019: loss did not improve from 0.01359\n",
            "Epoch 20/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0171 - mae: 0.1314 - val_loss: 0.0287 - val_mae: 0.1785\n",
            "\n",
            "Epoch 00020: loss did not improve from 0.01359\n",
            "Epoch 21/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0133 - mae: 0.1161 - val_loss: 0.0207 - val_mae: 0.1449\n",
            "\n",
            "Epoch 00021: loss improved from 0.01359 to 0.01291, saving model to poids.hdf5\n",
            "Epoch 22/500\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.0136 - mae: 0.1182 - val_loss: 0.0261 - val_mae: 0.1668\n",
            "\n",
            "Epoch 00022: loss did not improve from 0.01291\n",
            "Epoch 23/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0153 - mae: 0.1198 - val_loss: 0.0189 - val_mae: 0.1382\n",
            "\n",
            "Epoch 00023: loss did not improve from 0.01291\n",
            "Epoch 24/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0126 - mae: 0.1139 - val_loss: 0.0213 - val_mae: 0.1457\n",
            "\n",
            "Epoch 00024: loss did not improve from 0.01291\n",
            "Epoch 25/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0159 - mae: 0.1226 - val_loss: 0.0188 - val_mae: 0.1372\n",
            "\n",
            "Epoch 00025: loss did not improve from 0.01291\n",
            "Epoch 26/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0147 - mae: 0.1229 - val_loss: 0.0218 - val_mae: 0.1482\n",
            "\n",
            "Epoch 00026: loss improved from 0.01291 to 0.01280, saving model to poids.hdf5\n",
            "Epoch 27/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0113 - mae: 0.1127 - val_loss: 0.0246 - val_mae: 0.1604\n",
            "\n",
            "Epoch 00027: loss did not improve from 0.01280\n",
            "Epoch 28/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0139 - mae: 0.1183 - val_loss: 0.0207 - val_mae: 0.1436\n",
            "\n",
            "Epoch 00028: loss did not improve from 0.01280\n",
            "Epoch 29/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0128 - mae: 0.1148 - val_loss: 0.0195 - val_mae: 0.1382\n",
            "\n",
            "Epoch 00029: loss did not improve from 0.01280\n",
            "Epoch 30/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0124 - mae: 0.1108 - val_loss: 0.0197 - val_mae: 0.1396\n",
            "\n",
            "Epoch 00030: loss did not improve from 0.01280\n",
            "Epoch 31/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0112 - mae: 0.1078 - val_loss: 0.0179 - val_mae: 0.1323\n",
            "\n",
            "Epoch 00031: loss did not improve from 0.01280\n",
            "Epoch 32/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0134 - mae: 0.1161 - val_loss: 0.0175 - val_mae: 0.1322\n",
            "\n",
            "Epoch 00032: loss improved from 0.01280 to 0.01273, saving model to poids.hdf5\n",
            "Epoch 33/500\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.0118 - mae: 0.1135 - val_loss: 0.0174 - val_mae: 0.1295\n",
            "\n",
            "Epoch 00033: loss did not improve from 0.01273\n",
            "Epoch 34/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0146 - mae: 0.1192 - val_loss: 0.0181 - val_mae: 0.1332\n",
            "\n",
            "Epoch 00034: loss did not improve from 0.01273\n",
            "Epoch 35/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0127 - mae: 0.1145 - val_loss: 0.0188 - val_mae: 0.1367\n",
            "\n",
            "Epoch 00035: loss did not improve from 0.01273\n",
            "Epoch 36/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0137 - mae: 0.1146 - val_loss: 0.0174 - val_mae: 0.1313\n",
            "\n",
            "Epoch 00036: loss did not improve from 0.01273\n",
            "Epoch 37/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0131 - mae: 0.1148 - val_loss: 0.0159 - val_mae: 0.1300\n",
            "\n",
            "Epoch 00037: loss did not improve from 0.01273\n",
            "Epoch 38/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0122 - mae: 0.1118 - val_loss: 0.0168 - val_mae: 0.1311\n",
            "\n",
            "Epoch 00038: loss improved from 0.01273 to 0.01270, saving model to poids.hdf5\n",
            "Epoch 39/500\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.0137 - mae: 0.1155 - val_loss: 0.0189 - val_mae: 0.1357\n",
            "\n",
            "Epoch 00039: loss improved from 0.01270 to 0.01239, saving model to poids.hdf5\n",
            "Epoch 40/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0119 - mae: 0.1130 - val_loss: 0.0188 - val_mae: 0.1354\n",
            "\n",
            "Epoch 00040: loss did not improve from 0.01239\n",
            "Epoch 41/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0117 - mae: 0.1108 - val_loss: 0.0174 - val_mae: 0.1305\n",
            "\n",
            "Epoch 00041: loss did not improve from 0.01239\n",
            "Epoch 42/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0124 - mae: 0.1122 - val_loss: 0.0169 - val_mae: 0.1269\n",
            "\n",
            "Epoch 00042: loss did not improve from 0.01239\n",
            "Epoch 43/500\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.0130 - mae: 0.1109 - val_loss: 0.0173 - val_mae: 0.1299\n",
            "\n",
            "Epoch 00043: loss did not improve from 0.01239\n",
            "Epoch 44/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0135 - mae: 0.1131 - val_loss: 0.0154 - val_mae: 0.1263\n",
            "\n",
            "Epoch 00044: loss improved from 0.01239 to 0.01231, saving model to poids.hdf5\n",
            "Epoch 45/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0129 - mae: 0.1141 - val_loss: 0.0186 - val_mae: 0.1332\n",
            "\n",
            "Epoch 00045: loss improved from 0.01231 to 0.01227, saving model to poids.hdf5\n",
            "Epoch 46/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0129 - mae: 0.1132 - val_loss: 0.0174 - val_mae: 0.1295\n",
            "\n",
            "Epoch 00046: loss did not improve from 0.01227\n",
            "Epoch 47/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0139 - mae: 0.1149 - val_loss: 0.0177 - val_mae: 0.1299\n",
            "\n",
            "Epoch 00047: loss improved from 0.01227 to 0.01212, saving model to poids.hdf5\n",
            "Epoch 48/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0113 - mae: 0.1108 - val_loss: 0.0169 - val_mae: 0.1284\n",
            "\n",
            "Epoch 00048: loss did not improve from 0.01212\n",
            "Epoch 49/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0140 - mae: 0.1186 - val_loss: 0.0159 - val_mae: 0.1256\n",
            "\n",
            "Epoch 00049: loss did not improve from 0.01212\n",
            "Epoch 50/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0116 - mae: 0.1118 - val_loss: 0.0188 - val_mae: 0.1385\n",
            "\n",
            "Epoch 00050: loss did not improve from 0.01212\n",
            "Epoch 51/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0128 - mae: 0.1185 - val_loss: 0.0170 - val_mae: 0.1282\n",
            "\n",
            "Epoch 00051: loss did not improve from 0.01212\n",
            "Epoch 52/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0115 - mae: 0.1104 - val_loss: 0.0169 - val_mae: 0.1282\n",
            "\n",
            "Epoch 00052: loss did not improve from 0.01212\n",
            "Epoch 53/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0132 - mae: 0.1136 - val_loss: 0.0166 - val_mae: 0.1294\n",
            "\n",
            "Epoch 00053: loss did not improve from 0.01212\n",
            "Epoch 54/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0105 - mae: 0.1080 - val_loss: 0.0165 - val_mae: 0.1276\n",
            "\n",
            "Epoch 00054: loss improved from 0.01212 to 0.01210, saving model to poids.hdf5\n",
            "Epoch 55/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0144 - mae: 0.1247 - val_loss: 0.0169 - val_mae: 0.1273\n",
            "\n",
            "Epoch 00055: loss did not improve from 0.01210\n",
            "Epoch 56/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0118 - mae: 0.1114 - val_loss: 0.0161 - val_mae: 0.1271\n",
            "\n",
            "Epoch 00056: loss did not improve from 0.01210\n",
            "Epoch 57/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0114 - mae: 0.1116 - val_loss: 0.0168 - val_mae: 0.1271\n",
            "\n",
            "Epoch 00057: loss did not improve from 0.01210\n",
            "Epoch 58/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0116 - mae: 0.1106 - val_loss: 0.0161 - val_mae: 0.1240\n",
            "\n",
            "Epoch 00058: loss did not improve from 0.01210\n",
            "Epoch 59/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0136 - mae: 0.1156 - val_loss: 0.0157 - val_mae: 0.1265\n",
            "\n",
            "Epoch 00059: loss improved from 0.01210 to 0.01208, saving model to poids.hdf5\n",
            "Epoch 60/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0116 - mae: 0.1084 - val_loss: 0.0166 - val_mae: 0.1273\n",
            "\n",
            "Epoch 00060: loss did not improve from 0.01208\n",
            "Epoch 61/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0121 - mae: 0.1099 - val_loss: 0.0164 - val_mae: 0.1272\n",
            "\n",
            "Epoch 00061: loss did not improve from 0.01208\n",
            "Epoch 62/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0137 - mae: 0.1186 - val_loss: 0.0176 - val_mae: 0.1303\n",
            "\n",
            "Epoch 00062: loss did not improve from 0.01208\n",
            "Epoch 63/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0116 - mae: 0.1103 - val_loss: 0.0162 - val_mae: 0.1239\n",
            "\n",
            "Epoch 00063: loss did not improve from 0.01208\n",
            "Epoch 64/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0122 - mae: 0.1121 - val_loss: 0.0175 - val_mae: 0.1286\n",
            "\n",
            "Epoch 00064: loss did not improve from 0.01208\n",
            "Epoch 65/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0115 - mae: 0.1115 - val_loss: 0.0163 - val_mae: 0.1264\n",
            "\n",
            "Epoch 00065: loss did not improve from 0.01208\n",
            "Epoch 66/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0116 - mae: 0.1099 - val_loss: 0.0171 - val_mae: 0.1275\n",
            "\n",
            "Epoch 00066: loss improved from 0.01208 to 0.01196, saving model to poids.hdf5\n",
            "Epoch 67/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0118 - mae: 0.1075 - val_loss: 0.0165 - val_mae: 0.1269\n",
            "\n",
            "Epoch 00067: loss improved from 0.01196 to 0.01184, saving model to poids.hdf5\n",
            "Epoch 68/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0130 - mae: 0.1088 - val_loss: 0.0166 - val_mae: 0.1255\n",
            "\n",
            "Epoch 00068: loss did not improve from 0.01184\n",
            "Epoch 69/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0106 - mae: 0.1049 - val_loss: 0.0148 - val_mae: 0.1231\n",
            "\n",
            "Epoch 00069: loss did not improve from 0.01184\n",
            "Epoch 70/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0110 - mae: 0.1055 - val_loss: 0.0166 - val_mae: 0.1277\n",
            "\n",
            "Epoch 00070: loss did not improve from 0.01184\n",
            "Epoch 71/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0128 - mae: 0.1130 - val_loss: 0.0164 - val_mae: 0.1275\n",
            "\n",
            "Epoch 00071: loss did not improve from 0.01184\n",
            "Epoch 72/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0120 - mae: 0.1151 - val_loss: 0.0153 - val_mae: 0.1227\n",
            "\n",
            "Epoch 00072: loss did not improve from 0.01184\n",
            "Epoch 73/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0130 - mae: 0.1147 - val_loss: 0.0154 - val_mae: 0.1268\n",
            "\n",
            "Epoch 00073: loss did not improve from 0.01184\n",
            "Epoch 74/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0117 - mae: 0.1097 - val_loss: 0.0162 - val_mae: 0.1269\n",
            "\n",
            "Epoch 00074: loss improved from 0.01184 to 0.01146, saving model to poids.hdf5\n",
            "Epoch 75/500\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.0117 - mae: 0.1068 - val_loss: 0.0165 - val_mae: 0.1262\n",
            "\n",
            "Epoch 00075: loss did not improve from 0.01146\n",
            "Epoch 76/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0113 - mae: 0.1090 - val_loss: 0.0147 - val_mae: 0.1237\n",
            "\n",
            "Epoch 00076: loss did not improve from 0.01146\n",
            "Epoch 77/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0149 - mae: 0.1202 - val_loss: 0.0163 - val_mae: 0.1268\n",
            "\n",
            "Epoch 00077: loss did not improve from 0.01146\n",
            "Epoch 78/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0115 - mae: 0.1074 - val_loss: 0.0150 - val_mae: 0.1216\n",
            "\n",
            "Epoch 00078: loss did not improve from 0.01146\n",
            "Epoch 79/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0109 - mae: 0.1064 - val_loss: 0.0164 - val_mae: 0.1255\n",
            "\n",
            "Epoch 00079: loss did not improve from 0.01146\n",
            "Epoch 80/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0120 - mae: 0.1096 - val_loss: 0.0146 - val_mae: 0.1216\n",
            "\n",
            "Epoch 00080: loss did not improve from 0.01146\n",
            "Epoch 81/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0117 - mae: 0.1127 - val_loss: 0.0158 - val_mae: 0.1256\n",
            "\n",
            "Epoch 00081: loss did not improve from 0.01146\n",
            "Epoch 82/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0119 - mae: 0.1109 - val_loss: 0.0165 - val_mae: 0.1289\n",
            "\n",
            "Epoch 00082: loss did not improve from 0.01146\n",
            "Epoch 83/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0131 - mae: 0.1136 - val_loss: 0.0158 - val_mae: 0.1234\n",
            "\n",
            "Epoch 00083: loss did not improve from 0.01146\n",
            "Epoch 84/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0117 - mae: 0.1101 - val_loss: 0.0143 - val_mae: 0.1207\n",
            "\n",
            "Epoch 00084: loss did not improve from 0.01146\n",
            "Epoch 85/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0102 - mae: 0.1061 - val_loss: 0.0159 - val_mae: 0.1243\n",
            "\n",
            "Epoch 00085: loss did not improve from 0.01146\n",
            "Epoch 86/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0111 - mae: 0.1085 - val_loss: 0.0156 - val_mae: 0.1236\n",
            "\n",
            "Epoch 00086: loss did not improve from 0.01146\n",
            "Epoch 87/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0114 - mae: 0.1099 - val_loss: 0.0155 - val_mae: 0.1221\n",
            "\n",
            "Epoch 00087: loss did not improve from 0.01146\n",
            "Epoch 88/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0099 - mae: 0.1046 - val_loss: 0.0157 - val_mae: 0.1262\n",
            "\n",
            "Epoch 00088: loss did not improve from 0.01146\n",
            "Epoch 89/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0144 - mae: 0.1185 - val_loss: 0.0164 - val_mae: 0.1256\n",
            "\n",
            "Epoch 00089: loss improved from 0.01146 to 0.01143, saving model to poids.hdf5\n",
            "Epoch 90/500\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.0121 - mae: 0.1119 - val_loss: 0.0160 - val_mae: 0.1241\n",
            "\n",
            "Epoch 00090: loss did not improve from 0.01143\n",
            "Epoch 91/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0118 - mae: 0.1109 - val_loss: 0.0165 - val_mae: 0.1258\n",
            "\n",
            "Epoch 00091: loss did not improve from 0.01143\n",
            "Epoch 92/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0112 - mae: 0.1083 - val_loss: 0.0161 - val_mae: 0.1255\n",
            "\n",
            "Epoch 00092: loss did not improve from 0.01143\n",
            "Epoch 93/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0111 - mae: 0.1088 - val_loss: 0.0161 - val_mae: 0.1252\n",
            "\n",
            "Epoch 00093: loss did not improve from 0.01143\n",
            "Epoch 94/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0107 - mae: 0.1067 - val_loss: 0.0159 - val_mae: 0.1238\n",
            "\n",
            "Epoch 00094: loss did not improve from 0.01143\n",
            "Epoch 95/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0125 - mae: 0.1112 - val_loss: 0.0161 - val_mae: 0.1264\n",
            "\n",
            "Epoch 00095: loss did not improve from 0.01143\n",
            "Epoch 96/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0120 - mae: 0.1114 - val_loss: 0.0151 - val_mae: 0.1244\n",
            "\n",
            "Epoch 00096: loss did not improve from 0.01143\n",
            "Epoch 97/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0098 - mae: 0.1032 - val_loss: 0.0157 - val_mae: 0.1234\n",
            "\n",
            "Epoch 00097: loss did not improve from 0.01143\n",
            "Epoch 98/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0107 - mae: 0.1075 - val_loss: 0.0149 - val_mae: 0.1239\n",
            "\n",
            "Epoch 00098: loss did not improve from 0.01143\n",
            "Epoch 99/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0103 - mae: 0.1074 - val_loss: 0.0150 - val_mae: 0.1242\n",
            "\n",
            "Epoch 00099: loss did not improve from 0.01143\n",
            "Epoch 100/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0101 - mae: 0.1043 - val_loss: 0.0160 - val_mae: 0.1256\n",
            "\n",
            "Epoch 00100: loss did not improve from 0.01143\n",
            "Epoch 101/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0123 - mae: 0.1128 - val_loss: 0.0155 - val_mae: 0.1224\n",
            "\n",
            "Epoch 00101: loss did not improve from 0.01143\n",
            "Epoch 102/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0115 - mae: 0.1099 - val_loss: 0.0159 - val_mae: 0.1250\n",
            "\n",
            "Epoch 00102: loss did not improve from 0.01143\n",
            "Epoch 103/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0118 - mae: 0.1135 - val_loss: 0.0161 - val_mae: 0.1240\n",
            "\n",
            "Epoch 00103: loss did not improve from 0.01143\n",
            "Epoch 104/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0139 - mae: 0.1163 - val_loss: 0.0161 - val_mae: 0.1247\n",
            "\n",
            "Epoch 00104: loss did not improve from 0.01143\n",
            "Epoch 105/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0122 - mae: 0.1124 - val_loss: 0.0157 - val_mae: 0.1237\n",
            "\n",
            "Epoch 00105: loss did not improve from 0.01143\n",
            "Epoch 106/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0129 - mae: 0.1110 - val_loss: 0.0159 - val_mae: 0.1248\n",
            "\n",
            "Epoch 00106: loss did not improve from 0.01143\n",
            "Epoch 107/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0146 - mae: 0.1174 - val_loss: 0.0162 - val_mae: 0.1255\n",
            "\n",
            "Epoch 00107: loss did not improve from 0.01143\n",
            "Epoch 108/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0111 - mae: 0.1084 - val_loss: 0.0147 - val_mae: 0.1219\n",
            "\n",
            "Epoch 00108: loss did not improve from 0.01143\n",
            "Epoch 109/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0121 - mae: 0.1099 - val_loss: 0.0162 - val_mae: 0.1272\n",
            "\n",
            "Epoch 00109: loss did not improve from 0.01143\n",
            "Epoch 110/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0116 - mae: 0.1146 - val_loss: 0.0156 - val_mae: 0.1241\n",
            "\n",
            "Epoch 00110: loss did not improve from 0.01143\n",
            "Epoch 111/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0120 - mae: 0.1093 - val_loss: 0.0149 - val_mae: 0.1227\n",
            "\n",
            "Epoch 00111: loss did not improve from 0.01143\n",
            "Epoch 112/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0136 - mae: 0.1157 - val_loss: 0.0157 - val_mae: 0.1227\n",
            "\n",
            "Epoch 00112: loss did not improve from 0.01143\n",
            "Epoch 113/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0132 - mae: 0.1139 - val_loss: 0.0155 - val_mae: 0.1238\n",
            "\n",
            "Epoch 00113: loss did not improve from 0.01143\n",
            "Epoch 114/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0146 - mae: 0.1157 - val_loss: 0.0158 - val_mae: 0.1239\n",
            "\n",
            "Epoch 00114: loss did not improve from 0.01143\n",
            "Epoch 115/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0098 - mae: 0.1057 - val_loss: 0.0157 - val_mae: 0.1226\n",
            "\n",
            "Epoch 00115: loss did not improve from 0.01143\n",
            "Epoch 116/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0112 - mae: 0.1086 - val_loss: 0.0140 - val_mae: 0.1200\n",
            "\n",
            "Epoch 00116: loss did not improve from 0.01143\n",
            "Epoch 117/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0113 - mae: 0.1109 - val_loss: 0.0158 - val_mae: 0.1252\n",
            "\n",
            "Epoch 00117: loss improved from 0.01143 to 0.01130, saving model to poids.hdf5\n",
            "Epoch 118/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0114 - mae: 0.1066 - val_loss: 0.0160 - val_mae: 0.1241\n",
            "\n",
            "Epoch 00118: loss did not improve from 0.01130\n",
            "Epoch 119/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0143 - mae: 0.1128 - val_loss: 0.0162 - val_mae: 0.1288\n",
            "\n",
            "Epoch 00119: loss did not improve from 0.01130\n",
            "Epoch 120/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0115 - mae: 0.1081 - val_loss: 0.0158 - val_mae: 0.1232\n",
            "\n",
            "Epoch 00120: loss did not improve from 0.01130\n",
            "Epoch 121/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0111 - mae: 0.1114 - val_loss: 0.0155 - val_mae: 0.1230\n",
            "\n",
            "Epoch 00121: loss did not improve from 0.01130\n",
            "Epoch 122/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0148 - mae: 0.1173 - val_loss: 0.0158 - val_mae: 0.1237\n",
            "\n",
            "Epoch 00122: loss did not improve from 0.01130\n",
            "Epoch 123/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0111 - mae: 0.1077 - val_loss: 0.0161 - val_mae: 0.1251\n",
            "\n",
            "Epoch 00123: loss did not improve from 0.01130\n",
            "Epoch 124/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0115 - mae: 0.1121 - val_loss: 0.0147 - val_mae: 0.1225\n",
            "\n",
            "Epoch 00124: loss did not improve from 0.01130\n",
            "Epoch 125/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0102 - mae: 0.1069 - val_loss: 0.0158 - val_mae: 0.1243\n",
            "\n",
            "Epoch 00125: loss improved from 0.01130 to 0.01099, saving model to poids.hdf5\n",
            "Epoch 126/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0140 - mae: 0.1165 - val_loss: 0.0158 - val_mae: 0.1250\n",
            "\n",
            "Epoch 00126: loss did not improve from 0.01099\n",
            "Epoch 127/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0110 - mae: 0.1069 - val_loss: 0.0157 - val_mae: 0.1244\n",
            "\n",
            "Epoch 00127: loss did not improve from 0.01099\n",
            "Epoch 128/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0112 - mae: 0.1081 - val_loss: 0.0143 - val_mae: 0.1202\n",
            "\n",
            "Epoch 00128: loss did not improve from 0.01099\n",
            "Epoch 129/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0106 - mae: 0.1074 - val_loss: 0.0158 - val_mae: 0.1247\n",
            "\n",
            "Epoch 00129: loss did not improve from 0.01099\n",
            "Epoch 130/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0120 - mae: 0.1105 - val_loss: 0.0159 - val_mae: 0.1261\n",
            "\n",
            "Epoch 00130: loss did not improve from 0.01099\n",
            "Epoch 131/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0122 - mae: 0.1125 - val_loss: 0.0153 - val_mae: 0.1210\n",
            "\n",
            "Epoch 00131: loss did not improve from 0.01099\n",
            "Epoch 132/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0121 - mae: 0.1109 - val_loss: 0.0138 - val_mae: 0.1206\n",
            "\n",
            "Epoch 00132: loss did not improve from 0.01099\n",
            "Epoch 133/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0117 - mae: 0.1106 - val_loss: 0.0159 - val_mae: 0.1240\n",
            "\n",
            "Epoch 00133: loss did not improve from 0.01099\n",
            "Epoch 134/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0121 - mae: 0.1103 - val_loss: 0.0156 - val_mae: 0.1232\n",
            "\n",
            "Epoch 00134: loss did not improve from 0.01099\n",
            "Epoch 135/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0139 - mae: 0.1150 - val_loss: 0.0150 - val_mae: 0.1219\n",
            "\n",
            "Epoch 00135: loss did not improve from 0.01099\n",
            "Epoch 136/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0125 - mae: 0.1129 - val_loss: 0.0146 - val_mae: 0.1229\n",
            "\n",
            "Epoch 00136: loss did not improve from 0.01099\n",
            "Epoch 137/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0106 - mae: 0.1099 - val_loss: 0.0147 - val_mae: 0.1211\n",
            "\n",
            "Epoch 00137: loss did not improve from 0.01099\n",
            "Epoch 138/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0103 - mae: 0.1036 - val_loss: 0.0136 - val_mae: 0.1203\n",
            "\n",
            "Epoch 00138: loss did not improve from 0.01099\n",
            "Epoch 139/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0098 - mae: 0.1059 - val_loss: 0.0156 - val_mae: 0.1227\n",
            "\n",
            "Epoch 00139: loss did not improve from 0.01099\n",
            "Epoch 140/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0144 - mae: 0.1181 - val_loss: 0.0153 - val_mae: 0.1217\n",
            "\n",
            "Epoch 00140: loss did not improve from 0.01099\n",
            "Epoch 141/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0110 - mae: 0.1082 - val_loss: 0.0154 - val_mae: 0.1231\n",
            "\n",
            "Epoch 00141: loss did not improve from 0.01099\n",
            "Epoch 142/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0134 - mae: 0.1151 - val_loss: 0.0150 - val_mae: 0.1249\n",
            "\n",
            "Epoch 00142: loss did not improve from 0.01099\n",
            "Epoch 143/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0117 - mae: 0.1094 - val_loss: 0.0153 - val_mae: 0.1226\n",
            "\n",
            "Epoch 00143: loss did not improve from 0.01099\n",
            "Epoch 144/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0107 - mae: 0.1065 - val_loss: 0.0159 - val_mae: 0.1250\n",
            "\n",
            "Epoch 00144: loss did not improve from 0.01099\n",
            "Epoch 145/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0120 - mae: 0.1070 - val_loss: 0.0157 - val_mae: 0.1234\n",
            "\n",
            "Epoch 00145: loss did not improve from 0.01099\n",
            "Epoch 146/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0115 - mae: 0.1081 - val_loss: 0.0155 - val_mae: 0.1239\n",
            "\n",
            "Epoch 00146: loss did not improve from 0.01099\n",
            "Epoch 147/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0114 - mae: 0.1114 - val_loss: 0.0159 - val_mae: 0.1262\n",
            "\n",
            "Epoch 00147: loss did not improve from 0.01099\n",
            "Epoch 148/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0104 - mae: 0.1073 - val_loss: 0.0148 - val_mae: 0.1221\n",
            "\n",
            "Epoch 00148: loss did not improve from 0.01099\n",
            "Epoch 149/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0114 - mae: 0.1081 - val_loss: 0.0153 - val_mae: 0.1225\n",
            "\n",
            "Epoch 00149: loss did not improve from 0.01099\n",
            "Epoch 150/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0094 - mae: 0.1030 - val_loss: 0.0152 - val_mae: 0.1219\n",
            "\n",
            "Epoch 00150: loss did not improve from 0.01099\n",
            "Epoch 151/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0139 - mae: 0.1114 - val_loss: 0.0158 - val_mae: 0.1259\n",
            "\n",
            "Epoch 00151: loss improved from 0.01099 to 0.01096, saving model to poids.hdf5\n",
            "Epoch 152/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0136 - mae: 0.1150 - val_loss: 0.0158 - val_mae: 0.1246\n",
            "\n",
            "Epoch 00152: loss did not improve from 0.01096\n",
            "Epoch 153/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0114 - mae: 0.1086 - val_loss: 0.0157 - val_mae: 0.1245\n",
            "\n",
            "Epoch 00153: loss did not improve from 0.01096\n",
            "Epoch 154/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0124 - mae: 0.1114 - val_loss: 0.0158 - val_mae: 0.1259\n",
            "\n",
            "Epoch 00154: loss did not improve from 0.01096\n",
            "Epoch 155/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0131 - mae: 0.1109 - val_loss: 0.0152 - val_mae: 0.1226\n",
            "\n",
            "Epoch 00155: loss did not improve from 0.01096\n",
            "Epoch 156/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0112 - mae: 0.1072 - val_loss: 0.0157 - val_mae: 0.1248\n",
            "\n",
            "Epoch 00156: loss did not improve from 0.01096\n",
            "Epoch 157/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0099 - mae: 0.1043 - val_loss: 0.0155 - val_mae: 0.1245\n",
            "\n",
            "Epoch 00157: loss did not improve from 0.01096\n",
            "Epoch 158/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0112 - mae: 0.1100 - val_loss: 0.0156 - val_mae: 0.1241\n",
            "\n",
            "Epoch 00158: loss improved from 0.01096 to 0.01083, saving model to poids.hdf5\n",
            "Epoch 159/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0099 - mae: 0.1046 - val_loss: 0.0155 - val_mae: 0.1229\n",
            "\n",
            "Epoch 00159: loss did not improve from 0.01083\n",
            "Epoch 160/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0130 - mae: 0.1111 - val_loss: 0.0157 - val_mae: 0.1261\n",
            "\n",
            "Epoch 00160: loss did not improve from 0.01083\n",
            "Epoch 161/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0129 - mae: 0.1137 - val_loss: 0.0147 - val_mae: 0.1231\n",
            "\n",
            "Epoch 00161: loss did not improve from 0.01083\n",
            "Epoch 162/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0116 - mae: 0.1106 - val_loss: 0.0153 - val_mae: 0.1213\n",
            "\n",
            "Epoch 00162: loss did not improve from 0.01083\n",
            "Epoch 163/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0150 - mae: 0.1177 - val_loss: 0.0154 - val_mae: 0.1239\n",
            "\n",
            "Epoch 00163: loss did not improve from 0.01083\n",
            "Epoch 164/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0113 - mae: 0.1081 - val_loss: 0.0153 - val_mae: 0.1235\n",
            "\n",
            "Epoch 00164: loss did not improve from 0.01083\n",
            "Epoch 165/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0129 - mae: 0.1105 - val_loss: 0.0156 - val_mae: 0.1234\n",
            "\n",
            "Epoch 00165: loss did not improve from 0.01083\n",
            "Epoch 166/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0121 - mae: 0.1125 - val_loss: 0.0159 - val_mae: 0.1259\n",
            "\n",
            "Epoch 00166: loss did not improve from 0.01083\n",
            "Epoch 167/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0119 - mae: 0.1097 - val_loss: 0.0144 - val_mae: 0.1210\n",
            "\n",
            "Epoch 00167: loss did not improve from 0.01083\n",
            "Epoch 168/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0125 - mae: 0.1101 - val_loss: 0.0155 - val_mae: 0.1233\n",
            "\n",
            "Epoch 00168: loss did not improve from 0.01083\n",
            "Epoch 169/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0114 - mae: 0.1066 - val_loss: 0.0154 - val_mae: 0.1256\n",
            "\n",
            "Epoch 00169: loss did not improve from 0.01083\n",
            "Epoch 170/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0114 - mae: 0.1109 - val_loss: 0.0155 - val_mae: 0.1248\n",
            "\n",
            "Epoch 00170: loss did not improve from 0.01083\n",
            "Epoch 171/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0098 - mae: 0.1062 - val_loss: 0.0157 - val_mae: 0.1235\n",
            "\n",
            "Epoch 00171: loss did not improve from 0.01083\n",
            "Epoch 172/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0110 - mae: 0.1087 - val_loss: 0.0154 - val_mae: 0.1232\n",
            "\n",
            "Epoch 00172: loss did not improve from 0.01083\n",
            "Epoch 173/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0140 - mae: 0.1135 - val_loss: 0.0158 - val_mae: 0.1261\n",
            "\n",
            "Epoch 00173: loss did not improve from 0.01083\n",
            "Epoch 174/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0115 - mae: 0.1082 - val_loss: 0.0156 - val_mae: 0.1248\n",
            "\n",
            "Epoch 00174: loss did not improve from 0.01083\n",
            "Epoch 175/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0104 - mae: 0.1088 - val_loss: 0.0147 - val_mae: 0.1205\n",
            "\n",
            "Epoch 00175: loss did not improve from 0.01083\n",
            "Epoch 176/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0109 - mae: 0.1083 - val_loss: 0.0150 - val_mae: 0.1220\n",
            "\n",
            "Epoch 00176: loss did not improve from 0.01083\n",
            "Epoch 177/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0111 - mae: 0.1090 - val_loss: 0.0154 - val_mae: 0.1229\n",
            "\n",
            "Epoch 00177: loss did not improve from 0.01083\n",
            "Epoch 178/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0108 - mae: 0.1072 - val_loss: 0.0157 - val_mae: 0.1248\n",
            "\n",
            "Epoch 00178: loss did not improve from 0.01083\n",
            "Epoch 179/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0115 - mae: 0.1081 - val_loss: 0.0157 - val_mae: 0.1268\n",
            "\n",
            "Epoch 00179: loss did not improve from 0.01083\n",
            "Epoch 180/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0125 - mae: 0.1125 - val_loss: 0.0153 - val_mae: 0.1224\n",
            "\n",
            "Epoch 00180: loss did not improve from 0.01083\n",
            "Epoch 181/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0110 - mae: 0.1048 - val_loss: 0.0157 - val_mae: 0.1251\n",
            "\n",
            "Epoch 00181: loss did not improve from 0.01083\n",
            "Epoch 182/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0105 - mae: 0.1081 - val_loss: 0.0157 - val_mae: 0.1260\n",
            "\n",
            "Epoch 00182: loss did not improve from 0.01083\n",
            "Epoch 183/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0111 - mae: 0.1086 - val_loss: 0.0157 - val_mae: 0.1246\n",
            "\n",
            "Epoch 00183: loss did not improve from 0.01083\n",
            "Epoch 184/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0117 - mae: 0.1085 - val_loss: 0.0146 - val_mae: 0.1208\n",
            "\n",
            "Epoch 00184: loss did not improve from 0.01083\n",
            "Epoch 185/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0133 - mae: 0.1143 - val_loss: 0.0153 - val_mae: 0.1240\n",
            "\n",
            "Epoch 00185: loss did not improve from 0.01083\n",
            "Epoch 186/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0100 - mae: 0.1030 - val_loss: 0.0154 - val_mae: 0.1241\n",
            "\n",
            "Epoch 00186: loss did not improve from 0.01083\n",
            "Epoch 187/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0098 - mae: 0.1036 - val_loss: 0.0155 - val_mae: 0.1245\n",
            "\n",
            "Epoch 00187: loss did not improve from 0.01083\n",
            "Epoch 188/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0103 - mae: 0.1059 - val_loss: 0.0155 - val_mae: 0.1244\n",
            "\n",
            "Epoch 00188: loss did not improve from 0.01083\n",
            "Epoch 189/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0151 - mae: 0.1167 - val_loss: 0.0155 - val_mae: 0.1254\n",
            "\n",
            "Epoch 00189: loss did not improve from 0.01083\n",
            "Epoch 190/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0098 - mae: 0.1058 - val_loss: 0.0144 - val_mae: 0.1223\n",
            "\n",
            "Epoch 00190: loss did not improve from 0.01083\n",
            "Epoch 191/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0096 - mae: 0.1047 - val_loss: 0.0153 - val_mae: 0.1238\n",
            "\n",
            "Epoch 00191: loss did not improve from 0.01083\n",
            "Epoch 192/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0115 - mae: 0.1073 - val_loss: 0.0146 - val_mae: 0.1228\n",
            "\n",
            "Epoch 00192: loss did not improve from 0.01083\n",
            "Epoch 193/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0137 - mae: 0.1130 - val_loss: 0.0159 - val_mae: 0.1262\n",
            "\n",
            "Epoch 00193: loss did not improve from 0.01083\n",
            "Epoch 194/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0116 - mae: 0.1112 - val_loss: 0.0156 - val_mae: 0.1238\n",
            "\n",
            "Epoch 00194: loss did not improve from 0.01083\n",
            "Epoch 195/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0089 - mae: 0.1007 - val_loss: 0.0135 - val_mae: 0.1202\n",
            "\n",
            "Epoch 00195: loss did not improve from 0.01083\n",
            "Epoch 196/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0108 - mae: 0.1087 - val_loss: 0.0153 - val_mae: 0.1236\n",
            "\n",
            "Epoch 00196: loss did not improve from 0.01083\n",
            "Epoch 197/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0131 - mae: 0.1116 - val_loss: 0.0143 - val_mae: 0.1230\n",
            "\n",
            "Epoch 00197: loss did not improve from 0.01083\n",
            "Epoch 198/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0110 - mae: 0.1083 - val_loss: 0.0153 - val_mae: 0.1218\n",
            "\n",
            "Epoch 00198: loss did not improve from 0.01083\n",
            "Epoch 199/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0107 - mae: 0.1080 - val_loss: 0.0152 - val_mae: 0.1246\n",
            "\n",
            "Epoch 00199: loss did not improve from 0.01083\n",
            "Epoch 200/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0097 - mae: 0.1045 - val_loss: 0.0156 - val_mae: 0.1249\n",
            "\n",
            "Epoch 00200: loss did not improve from 0.01083\n",
            "Epoch 201/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0106 - mae: 0.1096 - val_loss: 0.0142 - val_mae: 0.1214\n",
            "\n",
            "Epoch 00201: loss improved from 0.01083 to 0.01074, saving model to poids.hdf5\n",
            "Epoch 202/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0124 - mae: 0.1132 - val_loss: 0.0156 - val_mae: 0.1239\n",
            "\n",
            "Epoch 00202: loss did not improve from 0.01074\n",
            "Epoch 203/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0105 - mae: 0.1089 - val_loss: 0.0145 - val_mae: 0.1237\n",
            "\n",
            "Epoch 00203: loss did not improve from 0.01074\n",
            "Epoch 204/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0103 - mae: 0.1059 - val_loss: 0.0156 - val_mae: 0.1251\n",
            "\n",
            "Epoch 00204: loss did not improve from 0.01074\n",
            "Epoch 205/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0115 - mae: 0.1106 - val_loss: 0.0155 - val_mae: 0.1244\n",
            "\n",
            "Epoch 00205: loss did not improve from 0.01074\n",
            "Epoch 206/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0126 - mae: 0.1119 - val_loss: 0.0155 - val_mae: 0.1246\n",
            "\n",
            "Epoch 00206: loss did not improve from 0.01074\n",
            "Epoch 207/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0102 - mae: 0.1060 - val_loss: 0.0158 - val_mae: 0.1262\n",
            "\n",
            "Epoch 00207: loss did not improve from 0.01074\n",
            "Epoch 208/500\n",
            "30/30 [==============================] - 1s 27ms/step - loss: 0.0101 - mae: 0.1038 - val_loss: 0.0157 - val_mae: 0.1259\n",
            "\n",
            "Epoch 00208: loss did not improve from 0.01074\n",
            "Epoch 209/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0108 - mae: 0.1062 - val_loss: 0.0157 - val_mae: 0.1258\n",
            "\n",
            "Epoch 00209: loss did not improve from 0.01074\n",
            "Epoch 210/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0107 - mae: 0.1068 - val_loss: 0.0154 - val_mae: 0.1254\n",
            "\n",
            "Epoch 00210: loss did not improve from 0.01074\n",
            "Epoch 211/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0104 - mae: 0.1054 - val_loss: 0.0156 - val_mae: 0.1266\n",
            "\n",
            "Epoch 00211: loss did not improve from 0.01074\n",
            "Epoch 212/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0128 - mae: 0.1125 - val_loss: 0.0153 - val_mae: 0.1226\n",
            "\n",
            "Epoch 00212: loss did not improve from 0.01074\n",
            "Epoch 213/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0116 - mae: 0.1074 - val_loss: 0.0156 - val_mae: 0.1237\n",
            "\n",
            "Epoch 00213: loss did not improve from 0.01074\n",
            "Epoch 214/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0123 - mae: 0.1102 - val_loss: 0.0149 - val_mae: 0.1214\n",
            "\n",
            "Epoch 00214: loss did not improve from 0.01074\n",
            "Epoch 215/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0109 - mae: 0.1059 - val_loss: 0.0151 - val_mae: 0.1214\n",
            "\n",
            "Epoch 00215: loss did not improve from 0.01074\n",
            "Epoch 216/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0120 - mae: 0.1084 - val_loss: 0.0156 - val_mae: 0.1242\n",
            "\n",
            "Epoch 00216: loss did not improve from 0.01074\n",
            "Epoch 217/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0119 - mae: 0.1082 - val_loss: 0.0156 - val_mae: 0.1242\n",
            "\n",
            "Epoch 00217: loss did not improve from 0.01074\n",
            "Epoch 218/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0133 - mae: 0.1122 - val_loss: 0.0155 - val_mae: 0.1267\n",
            "\n",
            "Epoch 00218: loss did not improve from 0.01074\n",
            "Epoch 219/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0105 - mae: 0.1082 - val_loss: 0.0152 - val_mae: 0.1241\n",
            "\n",
            "Epoch 00219: loss improved from 0.01074 to 0.01063, saving model to poids.hdf5\n",
            "Epoch 220/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0098 - mae: 0.1070 - val_loss: 0.0142 - val_mae: 0.1228\n",
            "\n",
            "Epoch 00220: loss did not improve from 0.01063\n",
            "Epoch 221/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0103 - mae: 0.1058 - val_loss: 0.0156 - val_mae: 0.1255\n",
            "\n",
            "Epoch 00221: loss did not improve from 0.01063\n",
            "Epoch 222/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0100 - mae: 0.1051 - val_loss: 0.0153 - val_mae: 0.1244\n",
            "\n",
            "Epoch 00222: loss did not improve from 0.01063\n",
            "Epoch 223/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0102 - mae: 0.1077 - val_loss: 0.0146 - val_mae: 0.1243\n",
            "\n",
            "Epoch 00223: loss did not improve from 0.01063\n",
            "Epoch 224/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0105 - mae: 0.1095 - val_loss: 0.0159 - val_mae: 0.1280\n",
            "\n",
            "Epoch 00224: loss did not improve from 0.01063\n",
            "Epoch 225/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0103 - mae: 0.1040 - val_loss: 0.0153 - val_mae: 0.1234\n",
            "\n",
            "Epoch 00225: loss did not improve from 0.01063\n",
            "Epoch 226/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0097 - mae: 0.1035 - val_loss: 0.0157 - val_mae: 0.1256\n",
            "\n",
            "Epoch 00226: loss did not improve from 0.01063\n",
            "Epoch 227/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0108 - mae: 0.1087 - val_loss: 0.0158 - val_mae: 0.1268\n",
            "\n",
            "Epoch 00227: loss did not improve from 0.01063\n",
            "Epoch 228/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0112 - mae: 0.1081 - val_loss: 0.0156 - val_mae: 0.1256\n",
            "\n",
            "Epoch 00228: loss did not improve from 0.01063\n",
            "Epoch 229/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0108 - mae: 0.1067 - val_loss: 0.0154 - val_mae: 0.1237\n",
            "\n",
            "Epoch 00229: loss did not improve from 0.01063\n",
            "Epoch 230/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0107 - mae: 0.1071 - val_loss: 0.0149 - val_mae: 0.1255\n",
            "\n",
            "Epoch 00230: loss did not improve from 0.01063\n",
            "Epoch 231/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0118 - mae: 0.1097 - val_loss: 0.0152 - val_mae: 0.1228\n",
            "\n",
            "Epoch 00231: loss did not improve from 0.01063\n",
            "Epoch 232/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0118 - mae: 0.1125 - val_loss: 0.0155 - val_mae: 0.1264\n",
            "\n",
            "Epoch 00232: loss did not improve from 0.01063\n",
            "Epoch 233/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0109 - mae: 0.1098 - val_loss: 0.0154 - val_mae: 0.1240\n",
            "\n",
            "Epoch 00233: loss did not improve from 0.01063\n",
            "Epoch 234/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0099 - mae: 0.1062 - val_loss: 0.0152 - val_mae: 0.1241\n",
            "\n",
            "Epoch 00234: loss did not improve from 0.01063\n",
            "Epoch 235/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0132 - mae: 0.1114 - val_loss: 0.0155 - val_mae: 0.1246\n",
            "\n",
            "Epoch 00235: loss did not improve from 0.01063\n",
            "Epoch 236/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0099 - mae: 0.1052 - val_loss: 0.0156 - val_mae: 0.1252\n",
            "\n",
            "Epoch 00236: loss did not improve from 0.01063\n",
            "Epoch 237/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0097 - mae: 0.1040 - val_loss: 0.0156 - val_mae: 0.1264\n",
            "\n",
            "Epoch 00237: loss did not improve from 0.01063\n",
            "Epoch 238/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0104 - mae: 0.1104 - val_loss: 0.0158 - val_mae: 0.1278\n",
            "\n",
            "Epoch 00238: loss did not improve from 0.01063\n",
            "Epoch 239/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0112 - mae: 0.1102 - val_loss: 0.0157 - val_mae: 0.1259\n",
            "\n",
            "Epoch 00239: loss did not improve from 0.01063\n",
            "Epoch 240/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0125 - mae: 0.1127 - val_loss: 0.0147 - val_mae: 0.1231\n",
            "\n",
            "Epoch 00240: loss did not improve from 0.01063\n",
            "Epoch 241/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0111 - mae: 0.1075 - val_loss: 0.0154 - val_mae: 0.1237\n",
            "\n",
            "Epoch 00241: loss did not improve from 0.01063\n",
            "Epoch 242/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0122 - mae: 0.1104 - val_loss: 0.0148 - val_mae: 0.1246\n",
            "\n",
            "Epoch 00242: loss did not improve from 0.01063\n",
            "Epoch 243/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0109 - mae: 0.1062 - val_loss: 0.0146 - val_mae: 0.1232\n",
            "\n",
            "Epoch 00243: loss did not improve from 0.01063\n",
            "Epoch 244/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0102 - mae: 0.1071 - val_loss: 0.0156 - val_mae: 0.1246\n",
            "\n",
            "Epoch 00244: loss did not improve from 0.01063\n",
            "Epoch 245/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0104 - mae: 0.1065 - val_loss: 0.0158 - val_mae: 0.1269\n",
            "\n",
            "Epoch 00245: loss did not improve from 0.01063\n",
            "Epoch 246/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0103 - mae: 0.1045 - val_loss: 0.0153 - val_mae: 0.1232\n",
            "\n",
            "Epoch 00246: loss did not improve from 0.01063\n",
            "Epoch 247/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0107 - mae: 0.1057 - val_loss: 0.0158 - val_mae: 0.1263\n",
            "\n",
            "Epoch 00247: loss did not improve from 0.01063\n",
            "Epoch 248/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0099 - mae: 0.1023 - val_loss: 0.0155 - val_mae: 0.1249\n",
            "\n",
            "Epoch 00248: loss did not improve from 0.01063\n",
            "Epoch 249/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0111 - mae: 0.1081 - val_loss: 0.0148 - val_mae: 0.1249\n",
            "\n",
            "Epoch 00249: loss did not improve from 0.01063\n",
            "Epoch 250/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0102 - mae: 0.1043 - val_loss: 0.0153 - val_mae: 0.1242\n",
            "\n",
            "Epoch 00250: loss did not improve from 0.01063\n",
            "Epoch 251/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0104 - mae: 0.1054 - val_loss: 0.0148 - val_mae: 0.1242\n",
            "\n",
            "Epoch 00251: loss did not improve from 0.01063\n",
            "Epoch 252/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0097 - mae: 0.1028 - val_loss: 0.0158 - val_mae: 0.1270\n",
            "\n",
            "Epoch 00252: loss did not improve from 0.01063\n",
            "Epoch 253/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0110 - mae: 0.1082 - val_loss: 0.0153 - val_mae: 0.1244\n",
            "\n",
            "Epoch 00253: loss did not improve from 0.01063\n",
            "Epoch 254/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0108 - mae: 0.1064 - val_loss: 0.0156 - val_mae: 0.1250\n",
            "\n",
            "Epoch 00254: loss did not improve from 0.01063\n",
            "Epoch 255/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0119 - mae: 0.1118 - val_loss: 0.0158 - val_mae: 0.1268\n",
            "\n",
            "Epoch 00255: loss did not improve from 0.01063\n",
            "Epoch 256/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0110 - mae: 0.1104 - val_loss: 0.0156 - val_mae: 0.1250\n",
            "\n",
            "Epoch 00256: loss did not improve from 0.01063\n",
            "Epoch 257/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0107 - mae: 0.1057 - val_loss: 0.0158 - val_mae: 0.1272\n",
            "\n",
            "Epoch 00257: loss did not improve from 0.01063\n",
            "Epoch 258/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0100 - mae: 0.1048 - val_loss: 0.0156 - val_mae: 0.1261\n",
            "\n",
            "Epoch 00258: loss did not improve from 0.01063\n",
            "Epoch 259/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0104 - mae: 0.1062 - val_loss: 0.0156 - val_mae: 0.1260\n",
            "\n",
            "Epoch 00259: loss did not improve from 0.01063\n",
            "Epoch 260/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0114 - mae: 0.1043 - val_loss: 0.0156 - val_mae: 0.1252\n",
            "\n",
            "Epoch 00260: loss did not improve from 0.01063\n",
            "Epoch 261/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0107 - mae: 0.1061 - val_loss: 0.0148 - val_mae: 0.1247\n",
            "\n",
            "Epoch 00261: loss did not improve from 0.01063\n",
            "Epoch 262/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0092 - mae: 0.1021 - val_loss: 0.0157 - val_mae: 0.1263\n",
            "\n",
            "Epoch 00262: loss did not improve from 0.01063\n",
            "Epoch 263/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0113 - mae: 0.1065 - val_loss: 0.0156 - val_mae: 0.1265\n",
            "\n",
            "Epoch 00263: loss did not improve from 0.01063\n",
            "Epoch 264/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0124 - mae: 0.1088 - val_loss: 0.0152 - val_mae: 0.1244\n",
            "\n",
            "Epoch 00264: loss did not improve from 0.01063\n",
            "Epoch 265/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0125 - mae: 0.1149 - val_loss: 0.0157 - val_mae: 0.1263\n",
            "\n",
            "Epoch 00265: loss did not improve from 0.01063\n",
            "Epoch 266/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0116 - mae: 0.1095 - val_loss: 0.0155 - val_mae: 0.1252\n",
            "\n",
            "Epoch 00266: loss did not improve from 0.01063\n",
            "Epoch 267/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0103 - mae: 0.1078 - val_loss: 0.0137 - val_mae: 0.1204\n",
            "\n",
            "Epoch 00267: loss did not improve from 0.01063\n",
            "Epoch 268/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0129 - mae: 0.1117 - val_loss: 0.0156 - val_mae: 0.1258\n",
            "\n",
            "Epoch 00268: loss did not improve from 0.01063\n",
            "Epoch 269/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0112 - mae: 0.1100 - val_loss: 0.0154 - val_mae: 0.1243\n",
            "\n",
            "Epoch 00269: loss did not improve from 0.01063\n",
            "Epoch 270/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0110 - mae: 0.1075 - val_loss: 0.0157 - val_mae: 0.1262\n",
            "\n",
            "Epoch 00270: loss did not improve from 0.01063\n",
            "Epoch 271/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0095 - mae: 0.1050 - val_loss: 0.0148 - val_mae: 0.1245\n",
            "\n",
            "Epoch 00271: loss did not improve from 0.01063\n",
            "Epoch 272/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0099 - mae: 0.1042 - val_loss: 0.0157 - val_mae: 0.1265\n",
            "\n",
            "Epoch 00272: loss did not improve from 0.01063\n",
            "Epoch 273/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0117 - mae: 0.1059 - val_loss: 0.0159 - val_mae: 0.1282\n",
            "\n",
            "Epoch 00273: loss did not improve from 0.01063\n",
            "Epoch 274/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0118 - mae: 0.1100 - val_loss: 0.0152 - val_mae: 0.1241\n",
            "\n",
            "Epoch 00274: loss did not improve from 0.01063\n",
            "Epoch 275/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0105 - mae: 0.1097 - val_loss: 0.0149 - val_mae: 0.1243\n",
            "\n",
            "Epoch 00275: loss did not improve from 0.01063\n",
            "Epoch 276/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0113 - mae: 0.1073 - val_loss: 0.0156 - val_mae: 0.1266\n",
            "\n",
            "Epoch 00276: loss did not improve from 0.01063\n",
            "Epoch 277/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0130 - mae: 0.1109 - val_loss: 0.0149 - val_mae: 0.1256\n",
            "\n",
            "Epoch 00277: loss did not improve from 0.01063\n",
            "Epoch 278/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0101 - mae: 0.1076 - val_loss: 0.0144 - val_mae: 0.1229\n",
            "\n",
            "Epoch 00278: loss did not improve from 0.01063\n",
            "Epoch 279/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0118 - mae: 0.1080 - val_loss: 0.0156 - val_mae: 0.1258\n",
            "\n",
            "Epoch 00279: loss did not improve from 0.01063\n",
            "Epoch 280/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0132 - mae: 0.1134 - val_loss: 0.0150 - val_mae: 0.1261\n",
            "\n",
            "Epoch 00280: loss did not improve from 0.01063\n",
            "Epoch 281/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0115 - mae: 0.1092 - val_loss: 0.0154 - val_mae: 0.1250\n",
            "\n",
            "Epoch 00281: loss did not improve from 0.01063\n",
            "Epoch 282/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0105 - mae: 0.1071 - val_loss: 0.0158 - val_mae: 0.1272\n",
            "\n",
            "Epoch 00282: loss did not improve from 0.01063\n",
            "Epoch 283/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0123 - mae: 0.1134 - val_loss: 0.0152 - val_mae: 0.1249\n",
            "\n",
            "Epoch 00283: loss did not improve from 0.01063\n",
            "Epoch 284/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0097 - mae: 0.1065 - val_loss: 0.0157 - val_mae: 0.1268\n",
            "\n",
            "Epoch 00284: loss did not improve from 0.01063\n",
            "Epoch 285/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0118 - mae: 0.1100 - val_loss: 0.0147 - val_mae: 0.1255\n",
            "\n",
            "Epoch 00285: loss did not improve from 0.01063\n",
            "Epoch 286/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0103 - mae: 0.1098 - val_loss: 0.0159 - val_mae: 0.1284\n",
            "\n",
            "Epoch 00286: loss did not improve from 0.01063\n",
            "Epoch 287/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0106 - mae: 0.1082 - val_loss: 0.0156 - val_mae: 0.1259\n",
            "\n",
            "Epoch 00287: loss did not improve from 0.01063\n",
            "Epoch 288/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0103 - mae: 0.1062 - val_loss: 0.0149 - val_mae: 0.1261\n",
            "\n",
            "Epoch 00288: loss did not improve from 0.01063\n",
            "Epoch 289/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0095 - mae: 0.1052 - val_loss: 0.0155 - val_mae: 0.1257\n",
            "\n",
            "Epoch 00289: loss did not improve from 0.01063\n",
            "Epoch 290/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0100 - mae: 0.1049 - val_loss: 0.0150 - val_mae: 0.1265\n",
            "\n",
            "Epoch 00290: loss did not improve from 0.01063\n",
            "Epoch 291/500\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0117 - mae: 0.1088 - val_loss: 0.0144 - val_mae: 0.1241\n",
            "\n",
            "Epoch 00291: loss did not improve from 0.01063\n",
            "Epoch 292/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0103 - mae: 0.1060 - val_loss: 0.0156 - val_mae: 0.1276\n",
            "\n",
            "Epoch 00292: loss did not improve from 0.01063\n",
            "Epoch 293/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0102 - mae: 0.1056 - val_loss: 0.0157 - val_mae: 0.1270\n",
            "\n",
            "Epoch 00293: loss did not improve from 0.01063\n",
            "Epoch 294/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0111 - mae: 0.1081 - val_loss: 0.0158 - val_mae: 0.1272\n",
            "\n",
            "Epoch 00294: loss did not improve from 0.01063\n",
            "Epoch 295/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0097 - mae: 0.1037 - val_loss: 0.0161 - val_mae: 0.1299\n",
            "\n",
            "Epoch 00295: loss did not improve from 0.01063\n",
            "Epoch 296/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0109 - mae: 0.1091 - val_loss: 0.0155 - val_mae: 0.1253\n",
            "\n",
            "Epoch 00296: loss did not improve from 0.01063\n",
            "Epoch 297/500\n",
            "30/30 [==============================] - 1s 27ms/step - loss: 0.0116 - mae: 0.1070 - val_loss: 0.0144 - val_mae: 0.1230\n",
            "\n",
            "Epoch 00297: loss did not improve from 0.01063\n",
            "Epoch 298/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0107 - mae: 0.1052 - val_loss: 0.0154 - val_mae: 0.1254\n",
            "\n",
            "Epoch 00298: loss did not improve from 0.01063\n",
            "Epoch 299/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0107 - mae: 0.1076 - val_loss: 0.0158 - val_mae: 0.1286\n",
            "\n",
            "Epoch 00299: loss did not improve from 0.01063\n",
            "Epoch 300/500\n",
            "30/30 [==============================] - 1s 27ms/step - loss: 0.0128 - mae: 0.1122 - val_loss: 0.0150 - val_mae: 0.1252\n",
            "\n",
            "Epoch 00300: loss did not improve from 0.01063\n",
            "Epoch 301/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0118 - mae: 0.1074 - val_loss: 0.0158 - val_mae: 0.1281\n",
            "\n",
            "Epoch 00301: loss did not improve from 0.01063\n",
            "Epoch 302/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0134 - mae: 0.1125 - val_loss: 0.0139 - val_mae: 0.1234\n",
            "\n",
            "Epoch 00302: loss did not improve from 0.01063\n",
            "Epoch 303/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0108 - mae: 0.1087 - val_loss: 0.0161 - val_mae: 0.1293\n",
            "\n",
            "Epoch 00303: loss did not improve from 0.01063\n",
            "Epoch 304/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0098 - mae: 0.1046 - val_loss: 0.0149 - val_mae: 0.1272\n",
            "\n",
            "Epoch 00304: loss did not improve from 0.01063\n",
            "Epoch 305/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0105 - mae: 0.1072 - val_loss: 0.0156 - val_mae: 0.1274\n",
            "\n",
            "Epoch 00305: loss did not improve from 0.01063\n",
            "Epoch 306/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0102 - mae: 0.1101 - val_loss: 0.0158 - val_mae: 0.1284\n",
            "\n",
            "Epoch 00306: loss did not improve from 0.01063\n",
            "Epoch 307/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0122 - mae: 0.1099 - val_loss: 0.0159 - val_mae: 0.1277\n",
            "\n",
            "Epoch 00307: loss did not improve from 0.01063\n",
            "Epoch 308/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0100 - mae: 0.1055 - val_loss: 0.0157 - val_mae: 0.1267\n",
            "\n",
            "Epoch 00308: loss did not improve from 0.01063\n",
            "Epoch 309/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0117 - mae: 0.1072 - val_loss: 0.0158 - val_mae: 0.1274\n",
            "\n",
            "Epoch 00309: loss did not improve from 0.01063\n",
            "Epoch 310/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0105 - mae: 0.1061 - val_loss: 0.0157 - val_mae: 0.1264\n",
            "\n",
            "Epoch 00310: loss did not improve from 0.01063\n",
            "Epoch 311/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0110 - mae: 0.1075 - val_loss: 0.0156 - val_mae: 0.1260\n",
            "\n",
            "Epoch 00311: loss did not improve from 0.01063\n",
            "Epoch 312/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0107 - mae: 0.1106 - val_loss: 0.0157 - val_mae: 0.1269\n",
            "\n",
            "Epoch 00312: loss did not improve from 0.01063\n",
            "Epoch 313/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0116 - mae: 0.1076 - val_loss: 0.0158 - val_mae: 0.1294\n",
            "\n",
            "Epoch 00313: loss did not improve from 0.01063\n",
            "Epoch 314/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0110 - mae: 0.1095 - val_loss: 0.0158 - val_mae: 0.1271\n",
            "\n",
            "Epoch 00314: loss improved from 0.01063 to 0.01058, saving model to poids.hdf5\n",
            "Epoch 315/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0106 - mae: 0.1065 - val_loss: 0.0160 - val_mae: 0.1279\n",
            "\n",
            "Epoch 00315: loss did not improve from 0.01058\n",
            "Epoch 316/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0120 - mae: 0.1083 - val_loss: 0.0159 - val_mae: 0.1275\n",
            "\n",
            "Epoch 00316: loss did not improve from 0.01058\n",
            "Epoch 317/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0108 - mae: 0.1089 - val_loss: 0.0156 - val_mae: 0.1256\n",
            "\n",
            "Epoch 00317: loss did not improve from 0.01058\n",
            "Epoch 318/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0106 - mae: 0.1077 - val_loss: 0.0152 - val_mae: 0.1273\n",
            "\n",
            "Epoch 00318: loss did not improve from 0.01058\n",
            "Epoch 319/500\n",
            "30/30 [==============================] - 1s 27ms/step - loss: 0.0088 - mae: 0.1033 - val_loss: 0.0157 - val_mae: 0.1267\n",
            "\n",
            "Epoch 00319: loss improved from 0.01058 to 0.01011, saving model to poids.hdf5\n",
            "Epoch 320/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0115 - mae: 0.1090 - val_loss: 0.0156 - val_mae: 0.1258\n",
            "\n",
            "Epoch 00320: loss did not improve from 0.01011\n",
            "Epoch 321/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0109 - mae: 0.1088 - val_loss: 0.0159 - val_mae: 0.1274\n",
            "\n",
            "Epoch 00321: loss did not improve from 0.01011\n",
            "Epoch 322/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0111 - mae: 0.1091 - val_loss: 0.0158 - val_mae: 0.1273\n",
            "\n",
            "Epoch 00322: loss did not improve from 0.01011\n",
            "Epoch 323/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0115 - mae: 0.1095 - val_loss: 0.0148 - val_mae: 0.1243\n",
            "\n",
            "Epoch 00323: loss did not improve from 0.01011\n",
            "Epoch 324/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0096 - mae: 0.1047 - val_loss: 0.0145 - val_mae: 0.1234\n",
            "\n",
            "Epoch 00324: loss did not improve from 0.01011\n",
            "Epoch 325/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0102 - mae: 0.1083 - val_loss: 0.0156 - val_mae: 0.1261\n",
            "\n",
            "Epoch 00325: loss did not improve from 0.01011\n",
            "Epoch 326/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0114 - mae: 0.1092 - val_loss: 0.0157 - val_mae: 0.1267\n",
            "\n",
            "Epoch 00326: loss did not improve from 0.01011\n",
            "Epoch 327/500\n",
            "30/30 [==============================] - 1s 27ms/step - loss: 0.0120 - mae: 0.1101 - val_loss: 0.0158 - val_mae: 0.1274\n",
            "\n",
            "Epoch 00327: loss did not improve from 0.01011\n",
            "Epoch 328/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0135 - mae: 0.1125 - val_loss: 0.0158 - val_mae: 0.1273\n",
            "\n",
            "Epoch 00328: loss did not improve from 0.01011\n",
            "Epoch 329/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0126 - mae: 0.1102 - val_loss: 0.0145 - val_mae: 0.1258\n",
            "\n",
            "Epoch 00329: loss did not improve from 0.01011\n",
            "Epoch 330/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0096 - mae: 0.1033 - val_loss: 0.0156 - val_mae: 0.1270\n",
            "\n",
            "Epoch 00330: loss did not improve from 0.01011\n",
            "Epoch 331/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0115 - mae: 0.1100 - val_loss: 0.0162 - val_mae: 0.1303\n",
            "\n",
            "Epoch 00331: loss did not improve from 0.01011\n",
            "Epoch 332/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0115 - mae: 0.1069 - val_loss: 0.0157 - val_mae: 0.1263\n",
            "\n",
            "Epoch 00332: loss did not improve from 0.01011\n",
            "Epoch 333/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0092 - mae: 0.1040 - val_loss: 0.0157 - val_mae: 0.1282\n",
            "\n",
            "Epoch 00333: loss did not improve from 0.01011\n",
            "Epoch 334/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0112 - mae: 0.1115 - val_loss: 0.0144 - val_mae: 0.1241\n",
            "\n",
            "Epoch 00334: loss did not improve from 0.01011\n",
            "Epoch 335/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0127 - mae: 0.1150 - val_loss: 0.0157 - val_mae: 0.1272\n",
            "\n",
            "Epoch 00335: loss did not improve from 0.01011\n",
            "Epoch 336/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0106 - mae: 0.1033 - val_loss: 0.0156 - val_mae: 0.1260\n",
            "\n",
            "Epoch 00336: loss did not improve from 0.01011\n",
            "Epoch 337/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0113 - mae: 0.1078 - val_loss: 0.0159 - val_mae: 0.1272\n",
            "\n",
            "Epoch 00337: loss did not improve from 0.01011\n",
            "Epoch 338/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0120 - mae: 0.1101 - val_loss: 0.0153 - val_mae: 0.1242\n",
            "\n",
            "Epoch 00338: loss did not improve from 0.01011\n",
            "Epoch 339/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0115 - mae: 0.1079 - val_loss: 0.0161 - val_mae: 0.1297\n",
            "\n",
            "Epoch 00339: loss did not improve from 0.01011\n",
            "Epoch 340/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0103 - mae: 0.1053 - val_loss: 0.0162 - val_mae: 0.1305\n",
            "\n",
            "Epoch 00340: loss did not improve from 0.01011\n",
            "Epoch 341/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0130 - mae: 0.1146 - val_loss: 0.0161 - val_mae: 0.1294\n",
            "\n",
            "Epoch 00341: loss did not improve from 0.01011\n",
            "Epoch 342/500\n",
            "30/30 [==============================] - 1s 27ms/step - loss: 0.0104 - mae: 0.1080 - val_loss: 0.0157 - val_mae: 0.1272\n",
            "\n",
            "Epoch 00342: loss did not improve from 0.01011\n",
            "Epoch 343/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0101 - mae: 0.1047 - val_loss: 0.0161 - val_mae: 0.1305\n",
            "\n",
            "Epoch 00343: loss did not improve from 0.01011\n",
            "Epoch 344/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0092 - mae: 0.1034 - val_loss: 0.0161 - val_mae: 0.1295\n",
            "\n",
            "Epoch 00344: loss did not improve from 0.01011\n",
            "Epoch 345/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0130 - mae: 0.1147 - val_loss: 0.0149 - val_mae: 0.1255\n",
            "\n",
            "Epoch 00345: loss did not improve from 0.01011\n",
            "Epoch 346/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0110 - mae: 0.1098 - val_loss: 0.0153 - val_mae: 0.1246\n",
            "\n",
            "Epoch 00346: loss did not improve from 0.01011\n",
            "Epoch 347/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0110 - mae: 0.1086 - val_loss: 0.0157 - val_mae: 0.1278\n",
            "\n",
            "Epoch 00347: loss did not improve from 0.01011\n",
            "Epoch 348/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0115 - mae: 0.1102 - val_loss: 0.0155 - val_mae: 0.1274\n",
            "\n",
            "Epoch 00348: loss did not improve from 0.01011\n",
            "Epoch 349/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0131 - mae: 0.1130 - val_loss: 0.0146 - val_mae: 0.1247\n",
            "\n",
            "Epoch 00349: loss did not improve from 0.01011\n",
            "Epoch 350/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0110 - mae: 0.1093 - val_loss: 0.0162 - val_mae: 0.1296\n",
            "\n",
            "Epoch 00350: loss did not improve from 0.01011\n",
            "Epoch 351/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0101 - mae: 0.1045 - val_loss: 0.0159 - val_mae: 0.1284\n",
            "\n",
            "Epoch 00351: loss did not improve from 0.01011\n",
            "Epoch 352/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0101 - mae: 0.1065 - val_loss: 0.0146 - val_mae: 0.1248\n",
            "\n",
            "Epoch 00352: loss did not improve from 0.01011\n",
            "Epoch 353/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0105 - mae: 0.1076 - val_loss: 0.0159 - val_mae: 0.1292\n",
            "\n",
            "Epoch 00353: loss did not improve from 0.01011\n",
            "Epoch 354/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0104 - mae: 0.1061 - val_loss: 0.0158 - val_mae: 0.1275\n",
            "\n",
            "Epoch 00354: loss did not improve from 0.01011\n",
            "Epoch 355/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0098 - mae: 0.1060 - val_loss: 0.0164 - val_mae: 0.1313\n",
            "\n",
            "Epoch 00355: loss did not improve from 0.01011\n",
            "Epoch 356/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0115 - mae: 0.1096 - val_loss: 0.0159 - val_mae: 0.1280\n",
            "\n",
            "Epoch 00356: loss did not improve from 0.01011\n",
            "Epoch 357/500\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0119 - mae: 0.1081 - val_loss: 0.0159 - val_mae: 0.1276\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCTzYW31uWbn"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doz2h-t1uWbo"
      },
      "source": [
        "**4. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "By0U8eN8uWbo"
      },
      "source": [
        "taille_fenetre = 20\n",
        "\n",
        "# Création d'une liste vide pour recevoir les prédictions\n",
        "predictions = []\n",
        "\n",
        "# Calcul des prédiction pour chaque groupe de 20 valeurs consécutives de la série\n",
        "# dans l'intervalle de validation\n",
        "for t in temps[temps_separation:-taille_fenetre]:\n",
        "    X = np.reshape(Serie_Normalisee[t:t+taille_fenetre],(1,taille_fenetre))\n",
        "    predictions.append(model.predict(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ0EFFIcuWbp"
      },
      "source": [
        "# Affiche la série et les prédictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps,serie,label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title('Prédictions avec le modèle GRU + Attention')\n",
        "plt.show()\n",
        "\n",
        "# Zoom sur l'intervalle de validation\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps[temps_separation:],serie[temps_separation:],label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title(\"Prédictions avec le modèle GRU + Attention (zoom sur l'intervalle de validation)\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1gFt22CuWbq"
      },
      "source": [
        "# Calcule de l'erreur quadratique moyenne et de l'erreur absolue moyenne \n",
        "\n",
        "mae = tf.keras.metrics.mean_absolute_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "mse = tf.keras.metrics.mean_squared_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "\n",
        "print(mae)\n",
        "print(mse)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}