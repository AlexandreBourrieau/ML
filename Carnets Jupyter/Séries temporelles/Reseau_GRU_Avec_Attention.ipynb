{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reseau_GRU_Avec_Attention.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/Reseau_GRU_Avec_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubCeIvtF6R4W"
      },
      "source": [
        "Dans ce carnet nous allons mettre en place un modèle à réseau de neurones récurrent de type GRU associé à une couche d'attention pour réaliser des prédictions sur notre série temporelle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRhtHsNn5fc3"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFeah3y_6kif"
      },
      "source": [
        "# Création de la série temporelle et du dataset pour l'entrainement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJfSLtub6sdc"
      },
      "source": [
        "# Fonction permettant d'afficher une série temporelle\n",
        "def affiche_serie(temps, serie, format=\"-\", debut=0, fin=None, label=None):\n",
        "    plt.plot(temps[debut:fin], serie[debut:fin], format, label=label)\n",
        "    plt.xlabel(\"Temps\")\n",
        "    plt.ylabel(\"Valeur\")\n",
        "    if label:\n",
        "        plt.legend(fontsize=14)\n",
        "    plt.grid(True)\n",
        "\n",
        "# Fonction permettant de créer une tendance\n",
        "def tendance(temps, pente=0):\n",
        "    return pente * temps\n",
        "\n",
        "# Fonction permettant de créer un motif\n",
        "def motif_periodique(instants):\n",
        "    return (np.where(instants < 0.4,                            # Si les instants sont < 0.4\n",
        "                    np.cos(instants * 2 * np.pi),               # Alors on retourne la fonction cos(2*pi*t)\n",
        "                    1 / np.exp(3 * instants)))                  # Sinon, on retourne la fonction exp(-3t)\n",
        "\n",
        "# Fonction permettant de créer une saisonnalité avec un motif\n",
        "def saisonnalite(temps, periode, amplitude=1, phase=0):\n",
        "    \"\"\"Répétition du motif sur la même période\"\"\"\n",
        "    instants = ((temps + phase) % periode) / periode            # Mapping du temps =[0 1 2 ... 1460] => instants = [0.0 ... 1.0]\n",
        "    return amplitude * motif_periodique(instants)\n",
        "\n",
        "# Fonction permettant de générer du bruit gaussien N(0,1)\n",
        "def bruit_blanc(temps, niveau_bruit=1, graine=None):\n",
        "    rnd = np.random.RandomState(graine)\n",
        "    return rnd.randn(len(temps)) * niveau_bruit\n",
        "\n",
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "# au format X(X1,X2,...Xn) / Y(Y1,Y2,...,Yn)\n",
        "# X sont les données d'entrées du réseau\n",
        "# Y sont les labels\n",
        "\n",
        "def prepare_dataset_XY(serie, taille_fenetre, batch_size, buffer_melange):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie)\n",
        "  dataset = dataset.window(taille_fenetre+1, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre + 1))\n",
        "  dataset = dataset.shuffle(buffer_melange).map(lambda x: (x[:-1], x[-1:]))\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "# Création de la série temporelle\n",
        "temps = np.arange(4 * 365)                # temps = [0 1 2 .... 4*365] = [0 1 2 .... 1460]\n",
        "amplitude = 40                            # Amplitude de la la saisonnalité\n",
        "niveau_bruit = 5                          # Niveau du bruit\n",
        "offset = 10                               # Offset de la série\n",
        "\n",
        "serie = offset + tendance(temps, 0.1) + saisonnalite(temps, periode=365, amplitude=amplitude) + bruit_blanc(temps,niveau_bruit,graine=40)\n",
        "\n",
        "temps_separation = 1000\n",
        "\n",
        "# Extraction des temps et des données d'entrainement\n",
        "temps_entrainement = temps[:temps_separation]\n",
        "x_entrainement = serie[:temps_separation]\n",
        "\n",
        "# Exctraction des temps et des données de valiadation\n",
        "temps_validation = temps[temps_separation:]\n",
        "x_validation = serie[temps_separation:]\n",
        "\n",
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "taille_fenetre = 20\n",
        "batch_size = 32\n",
        "buffer_melange = 1000\n",
        "\n",
        "# Création du dataset X,Y\n",
        "dataset = prepare_dataset_XY(x_entrainement,taille_fenetre,batch_size,buffer_melange)\n",
        "\n",
        "# Création du dataset X,Y de validation\n",
        "dataset_Val = prepare_dataset_XY(x_validation,taille_fenetre,batch_size,buffer_melange)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyndQsxw0wue"
      },
      "source": [
        "# Calcul de la moyenne et de l'écart type de la série\n",
        "mean = tf.math.reduce_mean(serie)\n",
        "std = tf.math.reduce_std(serie)\n",
        "\n",
        "# Normalise les données\n",
        "Serie_Normalisee = (serie-mean)/std\n",
        "min = tf.math.reduce_min(serie)\n",
        "max = tf.math.reduce_max(serie)\n",
        "\n",
        "# Création des données pour l'entrainement et le test\n",
        "x_entrainement_norm = Serie_Normalisee[:temps_separation]\n",
        "x_validation_norm = Serie_Normalisee[temps_separation:]\n",
        "\n",
        "# Création du dataset X,Y\n",
        "dataset_norm = prepare_dataset_XY(x_entrainement_norm,taille_fenetre,batch_size,buffer_melange)\n",
        "\n",
        "# Création du dataset X,Y de validation\n",
        "dataset_Val_norm = prepare_dataset_XY(x_validation_norm,taille_fenetre,batch_size,buffer_melange)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Yt-EgZ3sgPY"
      },
      "source": [
        "# Création du modèle LSTM avec couche d'attention personnalisée simple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyrcfKcgCsZ7"
      },
      "source": [
        "**1. Création du réseau et adaptation des formats d'entrée et de sortie**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeJyix8HK7Kt"
      },
      "source": [
        "Sous forme de shéma, notre réseau est donc le suivant :\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OZkfsmnBNHY"
      },
      "source": [
        "<img src=\"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/images/attention_11_Attention.png?raw=true\" width=\"1200\"> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kgTrJOQ5DUo"
      },
      "source": [
        "# Remise à zéro de tous les états générés par Keras\n",
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLNIAGDlBizT"
      },
      "source": [
        "On créé une classe dérivée de la classe [Layer](https://keras.io/api/layers/base_layer/#layer-class) de Keras. Les méthodes utilisées sont les suivantes :  \n",
        " - [build](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#build) : Permet de créer les variables utilisées par la couche (commes les poids et les offsets)\n",
        " - [call](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#call) : Permet d'implanter la logique de la couche"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3COaR59t5WzJ"
      },
      "source": [
        "<img src=\"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/images/attention_11_Attention_2.png?raw=true\" width=\"1200\"> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hhk0kmPSgqva"
      },
      "source": [
        "# Classe d'attention simple\n",
        "# Applique les poids d'attention sur les vecteurs de la couche récurrente\n",
        "\n",
        "# Importe le Backend de Keras\n",
        "from keras import backend as K\n",
        "\n",
        "# Définit une nouvelle classe Couche_Attention\n",
        "# Héritée de la classe Layer de Keras\n",
        "\n",
        "class Couche_Attention(tf.keras.layers.Layer):\n",
        "  # Fonction d'initialisation de la classe d'attention\n",
        "  def __init__(self):\n",
        "    super().__init__()          # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.w = self.add_weight(shape=(input_shape[2],1),initializer=\"normal\",name=\"w\")\n",
        "    self.b = self.add_weight(shape=(input_shape[1],1),initializer=\"zeros\",name=\"b\")\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Définit la logique de la couche d'attention\n",
        "  # Arguments :   x : Tenseur d'entrée de dimension (None, nbr_v,dim)\n",
        "  def call(self,x):\n",
        "    e = tf.matmul(x,self.w) + self.b\n",
        "    e = K.tanh(e)\n",
        "    a = tf.keras.activations.softmax(e,axis=1)\n",
        "    xa = tf.multiply(x,a)\n",
        "    sortie = K.sum(xa,axis=1)\n",
        "    return sortie"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tifRxHoVDCiW",
        "outputId": "b0b822b0-721c-4f07-eccc-942e67182a72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Fonction de la couche lambda d'entrée\n",
        "def Traitement_Entrees(x):\n",
        "  return tf.expand_dims(x,axis=-1)\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "# Encodeur\n",
        "model.add(tf.keras.Input(shape=(taille_fenetre,)))\n",
        "model.add(tf.keras.layers.Lambda(Traitement_Entrees))\n",
        "model.add(tf.keras.layers.Dropout(0.05))\n",
        "model.add(tf.keras.layers.LSTM(40,return_sequences=True))\n",
        "\n",
        "# Décodeur\n",
        "model.add(Couche_Attention())\n",
        "model.add(tf.keras.layers.Dense(40,activation=\"tanh\"))\n",
        "\n",
        "# Générateur\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "model.save_weights(\"model_initial.hdf5\")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda_2 (Lambda)            (None, 20, 1)             0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 20, 1)             0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 20, 40)            6720      \n",
            "_________________________________________________________________\n",
            "couche__attention_5 (Couche_ (None, 40)                60        \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 40)                1640      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 41        \n",
            "=================================================================\n",
            "Total params: 8,461\n",
            "Trainable params: 8,461\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUM0-SSXGLIQ"
      },
      "source": [
        "**2. Optimisation du taux d'apprentissage**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jejCBhXVuNQ4",
        "outputId": "eb538a6d-743e-4471-e28b-86c5652ed09d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(lr=1e-8)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur, metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset_norm,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "30/30 [==============================] - 3s 9ms/step - loss: 0.3967 - mae: 0.7808\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.39411, saving model to poids.hdf5\n",
            "Epoch 2/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.4152 - mae: 0.7997\n",
            "\n",
            "Epoch 00002: loss did not improve from 0.39411\n",
            "Epoch 3/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.4116 - mae: 0.7932\n",
            "\n",
            "Epoch 00003: loss did not improve from 0.39411\n",
            "Epoch 4/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.4142 - mae: 0.7898\n",
            "\n",
            "Epoch 00004: loss did not improve from 0.39411\n",
            "Epoch 5/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.3930 - mae: 0.7668\n",
            "\n",
            "Epoch 00005: loss did not improve from 0.39411\n",
            "Epoch 6/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.4022 - mae: 0.7817\n",
            "\n",
            "Epoch 00006: loss did not improve from 0.39411\n",
            "Epoch 7/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.3965 - mae: 0.7736\n",
            "\n",
            "Epoch 00007: loss improved from 0.39411 to 0.39009, saving model to poids.hdf5\n",
            "Epoch 8/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.3885 - mae: 0.7693\n",
            "\n",
            "Epoch 00008: loss did not improve from 0.39009\n",
            "Epoch 9/100\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.4077 - mae: 0.7825\n",
            "\n",
            "Epoch 00009: loss did not improve from 0.39009\n",
            "Epoch 10/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.3843 - mae: 0.7551\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.39009\n",
            "Epoch 11/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.4038 - mae: 0.7847\n",
            "\n",
            "Epoch 00011: loss did not improve from 0.39009\n",
            "Epoch 12/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.4072 - mae: 0.7880\n",
            "\n",
            "Epoch 00012: loss did not improve from 0.39009\n",
            "Epoch 13/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.4101 - mae: 0.7926\n",
            "\n",
            "Epoch 00013: loss did not improve from 0.39009\n",
            "Epoch 14/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.3886 - mae: 0.7616\n",
            "\n",
            "Epoch 00014: loss did not improve from 0.39009\n",
            "Epoch 15/100\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.3666 - mae: 0.7366\n",
            "\n",
            "Epoch 00015: loss did not improve from 0.39009\n",
            "Epoch 16/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.3782 - mae: 0.7534\n",
            "\n",
            "Epoch 00016: loss did not improve from 0.39009\n",
            "Epoch 17/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.3890 - mae: 0.7633\n",
            "\n",
            "Epoch 00017: loss did not improve from 0.39009\n",
            "Epoch 18/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.3936 - mae: 0.7681\n",
            "\n",
            "Epoch 00018: loss did not improve from 0.39009\n",
            "Epoch 19/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.3975 - mae: 0.7763\n",
            "\n",
            "Epoch 00019: loss did not improve from 0.39009\n",
            "Epoch 20/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.4038 - mae: 0.7799\n",
            "\n",
            "Epoch 00020: loss did not improve from 0.39009\n",
            "Epoch 21/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.3913 - mae: 0.7665\n",
            "\n",
            "Epoch 00021: loss did not improve from 0.39009\n",
            "Epoch 22/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.3996 - mae: 0.7808\n",
            "\n",
            "Epoch 00022: loss did not improve from 0.39009\n",
            "Epoch 23/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.3743 - mae: 0.7501\n",
            "\n",
            "Epoch 00023: loss did not improve from 0.39009\n",
            "Epoch 24/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.4044 - mae: 0.7853\n",
            "\n",
            "Epoch 00024: loss did not improve from 0.39009\n",
            "Epoch 25/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.4078 - mae: 0.7910\n",
            "\n",
            "Epoch 00025: loss did not improve from 0.39009\n",
            "Epoch 26/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.3934 - mae: 0.7658\n",
            "\n",
            "Epoch 00026: loss did not improve from 0.39009\n",
            "Epoch 27/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.4047 - mae: 0.7780\n",
            "\n",
            "Epoch 00027: loss did not improve from 0.39009\n",
            "Epoch 28/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.3969 - mae: 0.7761\n",
            "\n",
            "Epoch 00028: loss did not improve from 0.39009\n",
            "Epoch 29/100\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.4124 - mae: 0.7894\n",
            "\n",
            "Epoch 00029: loss did not improve from 0.39009\n",
            "Epoch 30/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.4027 - mae: 0.7870\n",
            "\n",
            "Epoch 00030: loss did not improve from 0.39009\n",
            "Epoch 31/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.3815 - mae: 0.7575\n",
            "\n",
            "Epoch 00031: loss did not improve from 0.39009\n",
            "Epoch 32/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.3968 - mae: 0.7738\n",
            "\n",
            "Epoch 00032: loss improved from 0.39009 to 0.38904, saving model to poids.hdf5\n",
            "Epoch 33/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.4004 - mae: 0.7803\n",
            "\n",
            "Epoch 00033: loss did not improve from 0.38904\n",
            "Epoch 34/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.3786 - mae: 0.7499\n",
            "\n",
            "Epoch 00034: loss did not improve from 0.38904\n",
            "Epoch 35/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.4026 - mae: 0.7808\n",
            "\n",
            "Epoch 00035: loss did not improve from 0.38904\n",
            "Epoch 36/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.3779 - mae: 0.7550\n",
            "\n",
            "Epoch 00036: loss improved from 0.38904 to 0.38757, saving model to poids.hdf5\n",
            "Epoch 37/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.4076 - mae: 0.7854\n",
            "\n",
            "Epoch 00037: loss improved from 0.38757 to 0.38554, saving model to poids.hdf5\n",
            "Epoch 38/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.4023 - mae: 0.7795\n",
            "\n",
            "Epoch 00038: loss did not improve from 0.38554\n",
            "Epoch 39/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.3638 - mae: 0.7316\n",
            "\n",
            "Epoch 00039: loss improved from 0.38554 to 0.38009, saving model to poids.hdf5\n",
            "Epoch 40/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.3856 - mae: 0.7671\n",
            "\n",
            "Epoch 00040: loss improved from 0.38009 to 0.37595, saving model to poids.hdf5\n",
            "Epoch 41/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.3613 - mae: 0.7348\n",
            "\n",
            "Epoch 00041: loss improved from 0.37595 to 0.36915, saving model to poids.hdf5\n",
            "Epoch 42/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.3476 - mae: 0.7136\n",
            "\n",
            "Epoch 00042: loss improved from 0.36915 to 0.36072, saving model to poids.hdf5\n",
            "Epoch 43/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.3636 - mae: 0.7299\n",
            "\n",
            "Epoch 00043: loss improved from 0.36072 to 0.35428, saving model to poids.hdf5\n",
            "Epoch 44/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.3426 - mae: 0.7035\n",
            "\n",
            "Epoch 00044: loss improved from 0.35428 to 0.34416, saving model to poids.hdf5\n",
            "Epoch 45/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.3213 - mae: 0.6809\n",
            "\n",
            "Epoch 00045: loss improved from 0.34416 to 0.32963, saving model to poids.hdf5\n",
            "Epoch 46/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.3377 - mae: 0.7052\n",
            "\n",
            "Epoch 00046: loss improved from 0.32963 to 0.31532, saving model to poids.hdf5\n",
            "Epoch 47/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.3046 - mae: 0.6558\n",
            "\n",
            "Epoch 00047: loss improved from 0.31532 to 0.29668, saving model to poids.hdf5\n",
            "Epoch 48/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.2852 - mae: 0.6329\n",
            "\n",
            "Epoch 00048: loss improved from 0.29668 to 0.27187, saving model to poids.hdf5\n",
            "Epoch 49/100\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.2502 - mae: 0.5883\n",
            "\n",
            "Epoch 00049: loss improved from 0.27187 to 0.24997, saving model to poids.hdf5\n",
            "Epoch 50/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.2255 - mae: 0.5531\n",
            "\n",
            "Epoch 00050: loss improved from 0.24997 to 0.22276, saving model to poids.hdf5\n",
            "Epoch 51/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.2057 - mae: 0.5299\n",
            "\n",
            "Epoch 00051: loss improved from 0.22276 to 0.19588, saving model to poids.hdf5\n",
            "Epoch 52/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.1764 - mae: 0.4935\n",
            "\n",
            "Epoch 00052: loss improved from 0.19588 to 0.16865, saving model to poids.hdf5\n",
            "Epoch 53/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.1565 - mae: 0.4594\n",
            "\n",
            "Epoch 00053: loss improved from 0.16865 to 0.14348, saving model to poids.hdf5\n",
            "Epoch 54/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.1303 - mae: 0.4171\n",
            "\n",
            "Epoch 00054: loss improved from 0.14348 to 0.12075, saving model to poids.hdf5\n",
            "Epoch 55/100\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.1034 - mae: 0.3729\n",
            "\n",
            "Epoch 00055: loss improved from 0.12075 to 0.09878, saving model to poids.hdf5\n",
            "Epoch 56/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0832 - mae: 0.3313\n",
            "\n",
            "Epoch 00056: loss improved from 0.09878 to 0.08127, saving model to poids.hdf5\n",
            "Epoch 57/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0638 - mae: 0.2780\n",
            "\n",
            "Epoch 00057: loss improved from 0.08127 to 0.06508, saving model to poids.hdf5\n",
            "Epoch 58/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0529 - mae: 0.2458\n",
            "\n",
            "Epoch 00058: loss improved from 0.06508 to 0.05138, saving model to poids.hdf5\n",
            "Epoch 59/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0438 - mae: 0.2139\n",
            "\n",
            "Epoch 00059: loss improved from 0.05138 to 0.04271, saving model to poids.hdf5\n",
            "Epoch 60/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0348 - mae: 0.1846\n",
            "\n",
            "Epoch 00060: loss improved from 0.04271 to 0.03600, saving model to poids.hdf5\n",
            "Epoch 61/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0319 - mae: 0.1767\n",
            "\n",
            "Epoch 00061: loss improved from 0.03600 to 0.03309, saving model to poids.hdf5\n",
            "Epoch 62/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0356 - mae: 0.1803\n",
            "\n",
            "Epoch 00062: loss improved from 0.03309 to 0.03138, saving model to poids.hdf5\n",
            "Epoch 63/100\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.0270 - mae: 0.1612\n",
            "\n",
            "Epoch 00063: loss did not improve from 0.03138\n",
            "Epoch 64/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0307 - mae: 0.1740\n",
            "\n",
            "Epoch 00064: loss did not improve from 0.03138\n",
            "Epoch 65/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0310 - mae: 0.1757\n",
            "\n",
            "Epoch 00065: loss improved from 0.03138 to 0.03052, saving model to poids.hdf5\n",
            "Epoch 66/100\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.0341 - mae: 0.1849\n",
            "\n",
            "Epoch 00066: loss did not improve from 0.03052\n",
            "Epoch 67/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0295 - mae: 0.1687\n",
            "\n",
            "Epoch 00067: loss did not improve from 0.03052\n",
            "Epoch 68/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0316 - mae: 0.1781\n",
            "\n",
            "Epoch 00068: loss did not improve from 0.03052\n",
            "Epoch 69/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0296 - mae: 0.1666\n",
            "\n",
            "Epoch 00069: loss improved from 0.03052 to 0.02952, saving model to poids.hdf5\n",
            "Epoch 70/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0316 - mae: 0.1772\n",
            "\n",
            "Epoch 00070: loss did not improve from 0.02952\n",
            "Epoch 71/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0331 - mae: 0.1788\n",
            "\n",
            "Epoch 00071: loss did not improve from 0.02952\n",
            "Epoch 72/100\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.0321 - mae: 0.1770\n",
            "\n",
            "Epoch 00072: loss did not improve from 0.02952\n",
            "Epoch 73/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0309 - mae: 0.1732\n",
            "\n",
            "Epoch 00073: loss did not improve from 0.02952\n",
            "Epoch 74/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0286 - mae: 0.1709\n",
            "\n",
            "Epoch 00074: loss did not improve from 0.02952\n",
            "Epoch 75/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0506 - mae: 0.2335\n",
            "\n",
            "Epoch 00075: loss did not improve from 0.02952\n",
            "Epoch 76/100\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.0384 - mae: 0.2043\n",
            "\n",
            "Epoch 00076: loss did not improve from 0.02952\n",
            "Epoch 77/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0453 - mae: 0.2269\n",
            "\n",
            "Epoch 00077: loss did not improve from 0.02952\n",
            "Epoch 78/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0557 - mae: 0.2591\n",
            "\n",
            "Epoch 00078: loss did not improve from 0.02952\n",
            "Epoch 79/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0662 - mae: 0.2797\n",
            "\n",
            "Epoch 00079: loss did not improve from 0.02952\n",
            "Epoch 80/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0515 - mae: 0.2334\n",
            "\n",
            "Epoch 00080: loss did not improve from 0.02952\n",
            "Epoch 81/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0490 - mae: 0.2361\n",
            "\n",
            "Epoch 00081: loss did not improve from 0.02952\n",
            "Epoch 82/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0649 - mae: 0.2999\n",
            "\n",
            "Epoch 00082: loss did not improve from 0.02952\n",
            "Epoch 83/100\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 3.3281 - mae: 3.6532\n",
            "\n",
            "Epoch 00083: loss did not improve from 0.02952\n",
            "Epoch 84/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 39.4765 - mae: 39.9765\n",
            "\n",
            "Epoch 00084: loss did not improve from 0.02952\n",
            "Epoch 85/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 48.9192 - mae: 49.4192\n",
            "\n",
            "Epoch 00085: loss did not improve from 0.02952\n",
            "Epoch 86/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 61.8626 - mae: 62.3626\n",
            "\n",
            "Epoch 00086: loss did not improve from 0.02952\n",
            "Epoch 87/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 77.3171 - mae: 77.8171\n",
            "\n",
            "Epoch 00087: loss did not improve from 0.02952\n",
            "Epoch 88/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 96.9350 - mae: 97.4350\n",
            "\n",
            "Epoch 00088: loss did not improve from 0.02952\n",
            "Epoch 89/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 120.6087 - mae: 121.1087\n",
            "\n",
            "Epoch 00089: loss did not improve from 0.02952\n",
            "Epoch 90/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 160.3661 - mae: 160.8661\n",
            "\n",
            "Epoch 00090: loss did not improve from 0.02952\n",
            "Epoch 91/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 202.1001 - mae: 202.6001\n",
            "\n",
            "Epoch 00091: loss did not improve from 0.02952\n",
            "Epoch 92/100\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 252.7273 - mae: 253.2273\n",
            "\n",
            "Epoch 00092: loss did not improve from 0.02952\n",
            "Epoch 93/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 314.1323 - mae: 314.6323\n",
            "\n",
            "Epoch 00093: loss did not improve from 0.02952\n",
            "Epoch 94/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 392.9132 - mae: 393.4132\n",
            "\n",
            "Epoch 00094: loss did not improve from 0.02952\n",
            "Epoch 95/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 491.5053 - mae: 492.0053\n",
            "\n",
            "Epoch 00095: loss did not improve from 0.02952\n",
            "Epoch 96/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 611.3630 - mae: 611.8630\n",
            "\n",
            "Epoch 00096: loss did not improve from 0.02952\n",
            "Epoch 97/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 777.9938 - mae: 778.4938\n",
            "\n",
            "Epoch 00097: loss did not improve from 0.02952\n",
            "Epoch 98/100\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 973.3876 - mae: 973.8876\n",
            "\n",
            "Epoch 00098: loss did not improve from 0.02952\n",
            "Epoch 99/100\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 1219.3496 - mae: 1219.8496\n",
            "\n",
            "Epoch 00099: loss did not improve from 0.02952\n",
            "Epoch 100/100\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 1529.0379 - mae: 1529.5379\n",
            "\n",
            "Epoch 00100: loss did not improve from 0.02952\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1_WMNlzu2B4",
        "outputId": "736f4868-d9e1-47b7-e93b-370e7dc80840",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 1])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, \"Evolution de l'erreur en fonction du taux d'apprentissage\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XdXh_b0GP_F"
      },
      "source": [
        "**3. Entrainement du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80pEtJ10wIfY"
      },
      "source": [
        "# Charge les meilleurs poids\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16ujUiELwR33",
        "outputId": "c4e7ba37-dbdc-40c9-b9d7-1545f40eb1c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "class TimingCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, logs={}):\n",
        "        self.n_steps = 0\n",
        "        self.t_step = 0\n",
        "        self.n_batch = 0\n",
        "        self.total_batch = 0\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.starttime = timer()\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.t_step = self.t_step  + timer()-self.starttime\n",
        "        self.n_steps = self.n_steps + 1\n",
        "        if (self.total_batch == 0):\n",
        "          self.total_batch=self.n_batch - 1\n",
        "    def on_train_batch_begin(self,batch,logs=None):\n",
        "      self.n_batch= self.n_batch + 1\n",
        "    def GetInfos(self):\n",
        "      return([self.t_step/(self.n_steps*self.total_batch), self.t_step, self.total_batch])\n",
        "\n",
        "cb = TimingCallback()\n",
        "\n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.1,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0.05)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(learning_rate=lr_schedule,momentum=0.9)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_entrainement.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur,metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle\n",
        "historique = model.fit(dataset_norm,validation_data=dataset_Val_norm, epochs=500,verbose=1, callbacks=[CheckPoint,cb])\n",
        "\n",
        "# Affiche quelques informations sur les timings\n",
        "infos = cb.GetInfos()\n",
        "print(\"Step time : %.3f\" %infos[0])\n",
        "print(\"Total time : %.3f\" %infos[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "30/30 [==============================] - 3s 30ms/step - loss: 0.0318 - mae: 0.1818 - val_loss: 0.0635 - val_mae: 0.2875\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.03301, saving model to poids_entrainement.hdf5\n",
            "Epoch 2/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0318 - mae: 0.1725 - val_loss: 0.0351 - val_mae: 0.1974\n",
            "\n",
            "Epoch 00002: loss improved from 0.03301 to 0.02972, saving model to poids_entrainement.hdf5\n",
            "Epoch 3/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0305 - mae: 0.1705 - val_loss: 0.0279 - val_mae: 0.1723\n",
            "\n",
            "Epoch 00003: loss improved from 0.02972 to 0.02888, saving model to poids_entrainement.hdf5\n",
            "Epoch 4/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0269 - mae: 0.1640 - val_loss: 0.0297 - val_mae: 0.1805\n",
            "\n",
            "Epoch 00004: loss improved from 0.02888 to 0.02812, saving model to poids_entrainement.hdf5\n",
            "Epoch 5/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0306 - mae: 0.1734 - val_loss: 0.0354 - val_mae: 0.1990\n",
            "\n",
            "Epoch 00005: loss did not improve from 0.02812\n",
            "Epoch 6/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0271 - mae: 0.1590 - val_loss: 0.0229 - val_mae: 0.1521\n",
            "\n",
            "Epoch 00006: loss improved from 0.02812 to 0.02652, saving model to poids_entrainement.hdf5\n",
            "Epoch 7/500\n",
            "30/30 [==============================] - 1s 13ms/step - loss: 0.0235 - mae: 0.1520 - val_loss: 0.0408 - val_mae: 0.2225\n",
            "\n",
            "Epoch 00007: loss improved from 0.02652 to 0.02542, saving model to poids_entrainement.hdf5\n",
            "Epoch 8/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0250 - mae: 0.1587 - val_loss: 0.0330 - val_mae: 0.1899\n",
            "\n",
            "Epoch 00008: loss improved from 0.02542 to 0.02449, saving model to poids_entrainement.hdf5\n",
            "Epoch 9/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0261 - mae: 0.1576 - val_loss: 0.0282 - val_mae: 0.1685\n",
            "\n",
            "Epoch 00009: loss improved from 0.02449 to 0.02357, saving model to poids_entrainement.hdf5\n",
            "Epoch 10/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0217 - mae: 0.1509 - val_loss: 0.0319 - val_mae: 0.1855\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.02357\n",
            "Epoch 11/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0217 - mae: 0.1519 - val_loss: 0.0216 - val_mae: 0.1407\n",
            "\n",
            "Epoch 00011: loss improved from 0.02357 to 0.02177, saving model to poids_entrainement.hdf5\n",
            "Epoch 12/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0264 - mae: 0.1608 - val_loss: 0.0189 - val_mae: 0.1358\n",
            "\n",
            "Epoch 00012: loss did not improve from 0.02177\n",
            "Epoch 13/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0219 - mae: 0.1462 - val_loss: 0.0200 - val_mae: 0.1359\n",
            "\n",
            "Epoch 00013: loss improved from 0.02177 to 0.02112, saving model to poids_entrainement.hdf5\n",
            "Epoch 14/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0215 - mae: 0.1476 - val_loss: 0.0189 - val_mae: 0.1332\n",
            "\n",
            "Epoch 00014: loss improved from 0.02112 to 0.01985, saving model to poids_entrainement.hdf5\n",
            "Epoch 15/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0180 - mae: 0.1380 - val_loss: 0.0190 - val_mae: 0.1329\n",
            "\n",
            "Epoch 00015: loss did not improve from 0.01985\n",
            "Epoch 16/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0196 - mae: 0.1417 - val_loss: 0.0205 - val_mae: 0.1385\n",
            "\n",
            "Epoch 00016: loss did not improve from 0.01985\n",
            "Epoch 17/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0228 - mae: 0.1517 - val_loss: 0.0202 - val_mae: 0.1323\n",
            "\n",
            "Epoch 00017: loss did not improve from 0.01985\n",
            "Epoch 18/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0214 - mae: 0.1430 - val_loss: 0.0204 - val_mae: 0.1366\n",
            "\n",
            "Epoch 00018: loss did not improve from 0.01985\n",
            "Epoch 19/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0184 - mae: 0.1405 - val_loss: 0.0203 - val_mae: 0.1384\n",
            "\n",
            "Epoch 00019: loss did not improve from 0.01985\n",
            "Epoch 20/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0221 - mae: 0.1445 - val_loss: 0.0202 - val_mae: 0.1331\n",
            "\n",
            "Epoch 00020: loss improved from 0.01985 to 0.01923, saving model to poids_entrainement.hdf5\n",
            "Epoch 21/500\n",
            "30/30 [==============================] - 1s 13ms/step - loss: 0.0226 - mae: 0.1488 - val_loss: 0.0222 - val_mae: 0.1421\n",
            "\n",
            "Epoch 00021: loss did not improve from 0.01923\n",
            "Epoch 22/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0210 - mae: 0.1454 - val_loss: 0.0201 - val_mae: 0.1338\n",
            "\n",
            "Epoch 00022: loss did not improve from 0.01923\n",
            "Epoch 23/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0212 - mae: 0.1440 - val_loss: 0.0200 - val_mae: 0.1388\n",
            "\n",
            "Epoch 00023: loss improved from 0.01923 to 0.01917, saving model to poids_entrainement.hdf5\n",
            "Epoch 24/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0192 - mae: 0.1368 - val_loss: 0.0189 - val_mae: 0.1309\n",
            "\n",
            "Epoch 00024: loss improved from 0.01917 to 0.01915, saving model to poids_entrainement.hdf5\n",
            "Epoch 25/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0151 - mae: 0.1229 - val_loss: 0.0237 - val_mae: 0.1468\n",
            "\n",
            "Epoch 00025: loss improved from 0.01915 to 0.01855, saving model to poids_entrainement.hdf5\n",
            "Epoch 26/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0199 - mae: 0.1444 - val_loss: 0.0197 - val_mae: 0.1315\n",
            "\n",
            "Epoch 00026: loss did not improve from 0.01855\n",
            "Epoch 27/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0177 - mae: 0.1338 - val_loss: 0.0241 - val_mae: 0.1638\n",
            "\n",
            "Epoch 00027: loss did not improve from 0.01855\n",
            "Epoch 28/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0191 - mae: 0.1385 - val_loss: 0.0255 - val_mae: 0.1763\n",
            "\n",
            "Epoch 00028: loss did not improve from 0.01855\n",
            "Epoch 29/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0183 - mae: 0.1378 - val_loss: 0.0212 - val_mae: 0.1488\n",
            "\n",
            "Epoch 00029: loss did not improve from 0.01855\n",
            "Epoch 30/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0183 - mae: 0.1351 - val_loss: 0.0196 - val_mae: 0.1366\n",
            "\n",
            "Epoch 00030: loss improved from 0.01855 to 0.01804, saving model to poids_entrainement.hdf5\n",
            "Epoch 31/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0170 - mae: 0.1305 - val_loss: 0.0194 - val_mae: 0.1333\n",
            "\n",
            "Epoch 00031: loss did not improve from 0.01804\n",
            "Epoch 32/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0180 - mae: 0.1293 - val_loss: 0.0188 - val_mae: 0.1323\n",
            "\n",
            "Epoch 00032: loss did not improve from 0.01804\n",
            "Epoch 33/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0178 - mae: 0.1331 - val_loss: 0.0202 - val_mae: 0.1413\n",
            "\n",
            "Epoch 00033: loss did not improve from 0.01804\n",
            "Epoch 34/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0193 - mae: 0.1397 - val_loss: 0.0214 - val_mae: 0.1463\n",
            "\n",
            "Epoch 00034: loss did not improve from 0.01804\n",
            "Epoch 35/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0159 - mae: 0.1322 - val_loss: 0.0216 - val_mae: 0.1480\n",
            "\n",
            "Epoch 00035: loss did not improve from 0.01804\n",
            "Epoch 36/500\n",
            "30/30 [==============================] - 1s 13ms/step - loss: 0.0171 - mae: 0.1305 - val_loss: 0.0182 - val_mae: 0.1322\n",
            "\n",
            "Epoch 00036: loss improved from 0.01804 to 0.01802, saving model to poids_entrainement.hdf5\n",
            "Epoch 37/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0207 - mae: 0.1393 - val_loss: 0.0208 - val_mae: 0.1502\n",
            "\n",
            "Epoch 00037: loss did not improve from 0.01802\n",
            "Epoch 38/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0171 - mae: 0.1320 - val_loss: 0.0194 - val_mae: 0.1406\n",
            "\n",
            "Epoch 00038: loss improved from 0.01802 to 0.01716, saving model to poids_entrainement.hdf5\n",
            "Epoch 39/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0182 - mae: 0.1316 - val_loss: 0.0184 - val_mae: 0.1358\n",
            "\n",
            "Epoch 00039: loss did not improve from 0.01716\n",
            "Epoch 40/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0182 - mae: 0.1328 - val_loss: 0.0181 - val_mae: 0.1297\n",
            "\n",
            "Epoch 00040: loss did not improve from 0.01716\n",
            "Epoch 41/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0176 - mae: 0.1309 - val_loss: 0.0200 - val_mae: 0.1425\n",
            "\n",
            "Epoch 00041: loss improved from 0.01716 to 0.01701, saving model to poids_entrainement.hdf5\n",
            "Epoch 42/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0171 - mae: 0.1293 - val_loss: 0.0210 - val_mae: 0.1428\n",
            "\n",
            "Epoch 00042: loss did not improve from 0.01701\n",
            "Epoch 43/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0168 - mae: 0.1295 - val_loss: 0.0220 - val_mae: 0.1569\n",
            "\n",
            "Epoch 00043: loss did not improve from 0.01701\n",
            "Epoch 44/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0198 - mae: 0.1389 - val_loss: 0.0194 - val_mae: 0.1364\n",
            "\n",
            "Epoch 00044: loss did not improve from 0.01701\n",
            "Epoch 45/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0175 - mae: 0.1276 - val_loss: 0.0234 - val_mae: 0.1612\n",
            "\n",
            "Epoch 00045: loss did not improve from 0.01701\n",
            "Epoch 46/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0178 - mae: 0.1359 - val_loss: 0.0228 - val_mae: 0.1558\n",
            "\n",
            "Epoch 00046: loss did not improve from 0.01701\n",
            "Epoch 47/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0195 - mae: 0.1357 - val_loss: 0.0195 - val_mae: 0.1390\n",
            "\n",
            "Epoch 00047: loss did not improve from 0.01701\n",
            "Epoch 48/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0169 - mae: 0.1256 - val_loss: 0.0210 - val_mae: 0.1464\n",
            "\n",
            "Epoch 00048: loss did not improve from 0.01701\n",
            "Epoch 49/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0175 - mae: 0.1314 - val_loss: 0.0230 - val_mae: 0.1594\n",
            "\n",
            "Epoch 00049: loss did not improve from 0.01701\n",
            "Epoch 50/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0182 - mae: 0.1319 - val_loss: 0.0186 - val_mae: 0.1356\n",
            "\n",
            "Epoch 00050: loss did not improve from 0.01701\n",
            "Epoch 51/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0173 - mae: 0.1322 - val_loss: 0.0193 - val_mae: 0.1368\n",
            "\n",
            "Epoch 00051: loss did not improve from 0.01701\n",
            "Epoch 52/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0175 - mae: 0.1339 - val_loss: 0.0195 - val_mae: 0.1364\n",
            "\n",
            "Epoch 00052: loss did not improve from 0.01701\n",
            "Epoch 53/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0195 - mae: 0.1306 - val_loss: 0.0186 - val_mae: 0.1326\n",
            "\n",
            "Epoch 00053: loss did not improve from 0.01701\n",
            "Epoch 54/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0179 - mae: 0.1333 - val_loss: 0.0204 - val_mae: 0.1408\n",
            "\n",
            "Epoch 00054: loss did not improve from 0.01701\n",
            "Epoch 55/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0180 - mae: 0.1337 - val_loss: 0.0199 - val_mae: 0.1385\n",
            "\n",
            "Epoch 00055: loss did not improve from 0.01701\n",
            "Epoch 56/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0172 - mae: 0.1296 - val_loss: 0.0257 - val_mae: 0.1747\n",
            "\n",
            "Epoch 00056: loss did not improve from 0.01701\n",
            "Epoch 57/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0152 - mae: 0.1265 - val_loss: 0.0201 - val_mae: 0.1393\n",
            "\n",
            "Epoch 00057: loss did not improve from 0.01701\n",
            "Epoch 58/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0181 - mae: 0.1304 - val_loss: 0.0199 - val_mae: 0.1426\n",
            "\n",
            "Epoch 00058: loss did not improve from 0.01701\n",
            "Epoch 59/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0187 - mae: 0.1332 - val_loss: 0.0192 - val_mae: 0.1368\n",
            "\n",
            "Epoch 00059: loss did not improve from 0.01701\n",
            "Epoch 60/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0177 - mae: 0.1324 - val_loss: 0.0219 - val_mae: 0.1534\n",
            "\n",
            "Epoch 00060: loss did not improve from 0.01701\n",
            "Epoch 61/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0196 - mae: 0.1337 - val_loss: 0.0203 - val_mae: 0.1453\n",
            "\n",
            "Epoch 00061: loss did not improve from 0.01701\n",
            "Epoch 62/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0165 - mae: 0.1245 - val_loss: 0.0212 - val_mae: 0.1496\n",
            "\n",
            "Epoch 00062: loss did not improve from 0.01701\n",
            "Epoch 63/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0172 - mae: 0.1300 - val_loss: 0.0224 - val_mae: 0.1571\n",
            "\n",
            "Epoch 00063: loss did not improve from 0.01701\n",
            "Epoch 64/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0156 - mae: 0.1262 - val_loss: 0.0193 - val_mae: 0.1347\n",
            "\n",
            "Epoch 00064: loss improved from 0.01701 to 0.01646, saving model to poids_entrainement.hdf5\n",
            "Epoch 65/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0146 - mae: 0.1242 - val_loss: 0.0197 - val_mae: 0.1394\n",
            "\n",
            "Epoch 00065: loss did not improve from 0.01646\n",
            "Epoch 66/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0182 - mae: 0.1316 - val_loss: 0.0189 - val_mae: 0.1343\n",
            "\n",
            "Epoch 00066: loss did not improve from 0.01646\n",
            "Epoch 67/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0166 - mae: 0.1330 - val_loss: 0.0184 - val_mae: 0.1381\n",
            "\n",
            "Epoch 00067: loss did not improve from 0.01646\n",
            "Epoch 68/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0209 - mae: 0.1460 - val_loss: 0.0205 - val_mae: 0.1455\n",
            "\n",
            "Epoch 00068: loss did not improve from 0.01646\n",
            "Epoch 69/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0157 - mae: 0.1241 - val_loss: 0.0211 - val_mae: 0.1493\n",
            "\n",
            "Epoch 00069: loss did not improve from 0.01646\n",
            "Epoch 70/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0174 - mae: 0.1303 - val_loss: 0.0196 - val_mae: 0.1391\n",
            "\n",
            "Epoch 00070: loss did not improve from 0.01646\n",
            "Epoch 71/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0161 - mae: 0.1267 - val_loss: 0.0181 - val_mae: 0.1311\n",
            "\n",
            "Epoch 00071: loss did not improve from 0.01646\n",
            "Epoch 72/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0201 - mae: 0.1316 - val_loss: 0.0206 - val_mae: 0.1445\n",
            "\n",
            "Epoch 00072: loss did not improve from 0.01646\n",
            "Epoch 73/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0158 - mae: 0.1268 - val_loss: 0.0196 - val_mae: 0.1442\n",
            "\n",
            "Epoch 00073: loss improved from 0.01646 to 0.01606, saving model to poids_entrainement.hdf5\n",
            "Epoch 74/500\n",
            "30/30 [==============================] - 1s 13ms/step - loss: 0.0141 - mae: 0.1166 - val_loss: 0.0201 - val_mae: 0.1420\n",
            "\n",
            "Epoch 00074: loss did not improve from 0.01606\n",
            "Epoch 75/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0186 - mae: 0.1345 - val_loss: 0.0245 - val_mae: 0.1698\n",
            "\n",
            "Epoch 00075: loss did not improve from 0.01606\n",
            "Epoch 76/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0140 - mae: 0.1194 - val_loss: 0.0194 - val_mae: 0.1384\n",
            "\n",
            "Epoch 00076: loss did not improve from 0.01606\n",
            "Epoch 77/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0165 - mae: 0.1279 - val_loss: 0.0192 - val_mae: 0.1360\n",
            "\n",
            "Epoch 00077: loss did not improve from 0.01606\n",
            "Epoch 78/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0154 - mae: 0.1279 - val_loss: 0.0193 - val_mae: 0.1376\n",
            "\n",
            "Epoch 00078: loss did not improve from 0.01606\n",
            "Epoch 79/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0159 - mae: 0.1246 - val_loss: 0.0201 - val_mae: 0.1473\n",
            "\n",
            "Epoch 00079: loss did not improve from 0.01606\n",
            "Epoch 80/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0175 - mae: 0.1336 - val_loss: 0.0222 - val_mae: 0.1555\n",
            "\n",
            "Epoch 00080: loss did not improve from 0.01606\n",
            "Epoch 81/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0180 - mae: 0.1323 - val_loss: 0.0196 - val_mae: 0.1397\n",
            "\n",
            "Epoch 00081: loss did not improve from 0.01606\n",
            "Epoch 82/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0170 - mae: 0.1295 - val_loss: 0.0202 - val_mae: 0.1434\n",
            "\n",
            "Epoch 00082: loss did not improve from 0.01606\n",
            "Epoch 83/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0182 - mae: 0.1343 - val_loss: 0.0196 - val_mae: 0.1373\n",
            "\n",
            "Epoch 00083: loss did not improve from 0.01606\n",
            "Epoch 84/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0191 - mae: 0.1350 - val_loss: 0.0247 - val_mae: 0.1682\n",
            "\n",
            "Epoch 00084: loss did not improve from 0.01606\n",
            "Epoch 85/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0152 - mae: 0.1269 - val_loss: 0.0216 - val_mae: 0.1501\n",
            "\n",
            "Epoch 00085: loss did not improve from 0.01606\n",
            "Epoch 86/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0178 - mae: 0.1280 - val_loss: 0.0189 - val_mae: 0.1387\n",
            "\n",
            "Epoch 00086: loss did not improve from 0.01606\n",
            "Epoch 87/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0152 - mae: 0.1259 - val_loss: 0.0201 - val_mae: 0.1398\n",
            "\n",
            "Epoch 00087: loss did not improve from 0.01606\n",
            "Epoch 88/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0154 - mae: 0.1202 - val_loss: 0.0172 - val_mae: 0.1307\n",
            "\n",
            "Epoch 00088: loss did not improve from 0.01606\n",
            "Epoch 89/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0173 - mae: 0.1322 - val_loss: 0.0202 - val_mae: 0.1485\n",
            "\n",
            "Epoch 00089: loss did not improve from 0.01606\n",
            "Epoch 90/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0150 - mae: 0.1227 - val_loss: 0.0175 - val_mae: 0.1317\n",
            "\n",
            "Epoch 00090: loss improved from 0.01606 to 0.01603, saving model to poids_entrainement.hdf5\n",
            "Epoch 91/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0162 - mae: 0.1296 - val_loss: 0.0224 - val_mae: 0.1579\n",
            "\n",
            "Epoch 00091: loss did not improve from 0.01603\n",
            "Epoch 92/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0152 - mae: 0.1269 - val_loss: 0.0196 - val_mae: 0.1413\n",
            "\n",
            "Epoch 00092: loss did not improve from 0.01603\n",
            "Epoch 93/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0157 - mae: 0.1204 - val_loss: 0.0172 - val_mae: 0.1307\n",
            "\n",
            "Epoch 00093: loss improved from 0.01603 to 0.01570, saving model to poids_entrainement.hdf5\n",
            "Epoch 94/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0158 - mae: 0.1272 - val_loss: 0.0191 - val_mae: 0.1383\n",
            "\n",
            "Epoch 00094: loss did not improve from 0.01570\n",
            "Epoch 95/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0176 - mae: 0.1310 - val_loss: 0.0187 - val_mae: 0.1347\n",
            "\n",
            "Epoch 00095: loss did not improve from 0.01570\n",
            "Epoch 96/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0163 - mae: 0.1247 - val_loss: 0.0200 - val_mae: 0.1433\n",
            "\n",
            "Epoch 00096: loss did not improve from 0.01570\n",
            "Epoch 97/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0171 - mae: 0.1300 - val_loss: 0.0165 - val_mae: 0.1277\n",
            "\n",
            "Epoch 00097: loss did not improve from 0.01570\n",
            "Epoch 98/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0170 - mae: 0.1305 - val_loss: 0.0203 - val_mae: 0.1437\n",
            "\n",
            "Epoch 00098: loss did not improve from 0.01570\n",
            "Epoch 99/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0179 - mae: 0.1318 - val_loss: 0.0217 - val_mae: 0.1520\n",
            "\n",
            "Epoch 00099: loss did not improve from 0.01570\n",
            "Epoch 100/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0145 - mae: 0.1231 - val_loss: 0.0203 - val_mae: 0.1440\n",
            "\n",
            "Epoch 00100: loss did not improve from 0.01570\n",
            "Epoch 101/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0177 - mae: 0.1319 - val_loss: 0.0188 - val_mae: 0.1345\n",
            "\n",
            "Epoch 00101: loss did not improve from 0.01570\n",
            "Epoch 102/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0187 - mae: 0.1302 - val_loss: 0.0207 - val_mae: 0.1454\n",
            "\n",
            "Epoch 00102: loss did not improve from 0.01570\n",
            "Epoch 103/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0177 - mae: 0.1274 - val_loss: 0.0222 - val_mae: 0.1540\n",
            "\n",
            "Epoch 00103: loss did not improve from 0.01570\n",
            "Epoch 104/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0144 - mae: 0.1233 - val_loss: 0.0204 - val_mae: 0.1437\n",
            "\n",
            "Epoch 00104: loss did not improve from 0.01570\n",
            "Epoch 105/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0188 - mae: 0.1330 - val_loss: 0.0199 - val_mae: 0.1430\n",
            "\n",
            "Epoch 00105: loss did not improve from 0.01570\n",
            "Epoch 106/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0171 - mae: 0.1312 - val_loss: 0.0195 - val_mae: 0.1374\n",
            "\n",
            "Epoch 00106: loss did not improve from 0.01570\n",
            "Epoch 107/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0133 - mae: 0.1188 - val_loss: 0.0203 - val_mae: 0.1434\n",
            "\n",
            "Epoch 00107: loss did not improve from 0.01570\n",
            "Epoch 108/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0155 - mae: 0.1242 - val_loss: 0.0195 - val_mae: 0.1399\n",
            "\n",
            "Epoch 00108: loss did not improve from 0.01570\n",
            "Epoch 109/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0155 - mae: 0.1287 - val_loss: 0.0202 - val_mae: 0.1422\n",
            "\n",
            "Epoch 00109: loss did not improve from 0.01570\n",
            "Epoch 110/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0151 - mae: 0.1229 - val_loss: 0.0199 - val_mae: 0.1439\n",
            "\n",
            "Epoch 00110: loss did not improve from 0.01570\n",
            "Epoch 111/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0149 - mae: 0.1247 - val_loss: 0.0191 - val_mae: 0.1410\n",
            "\n",
            "Epoch 00111: loss did not improve from 0.01570\n",
            "Epoch 112/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0168 - mae: 0.1288 - val_loss: 0.0191 - val_mae: 0.1373\n",
            "\n",
            "Epoch 00112: loss did not improve from 0.01570\n",
            "Epoch 113/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0155 - mae: 0.1241 - val_loss: 0.0204 - val_mae: 0.1455\n",
            "\n",
            "Epoch 00113: loss did not improve from 0.01570\n",
            "Epoch 114/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0151 - mae: 0.1230 - val_loss: 0.0170 - val_mae: 0.1293\n",
            "\n",
            "Epoch 00114: loss did not improve from 0.01570\n",
            "Epoch 115/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0171 - mae: 0.1245 - val_loss: 0.0215 - val_mae: 0.1516\n",
            "\n",
            "Epoch 00115: loss did not improve from 0.01570\n",
            "Epoch 116/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0159 - mae: 0.1252 - val_loss: 0.0192 - val_mae: 0.1371\n",
            "\n",
            "Epoch 00116: loss improved from 0.01570 to 0.01524, saving model to poids_entrainement.hdf5\n",
            "Epoch 117/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0140 - mae: 0.1202 - val_loss: 0.0210 - val_mae: 0.1484\n",
            "\n",
            "Epoch 00117: loss did not improve from 0.01524\n",
            "Epoch 118/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0159 - mae: 0.1256 - val_loss: 0.0210 - val_mae: 0.1479\n",
            "\n",
            "Epoch 00118: loss did not improve from 0.01524\n",
            "Epoch 119/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0175 - mae: 0.1318 - val_loss: 0.0194 - val_mae: 0.1397\n",
            "\n",
            "Epoch 00119: loss did not improve from 0.01524\n",
            "Epoch 120/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0177 - mae: 0.1333 - val_loss: 0.0170 - val_mae: 0.1343\n",
            "\n",
            "Epoch 00120: loss did not improve from 0.01524\n",
            "Epoch 121/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0183 - mae: 0.1314 - val_loss: 0.0211 - val_mae: 0.1479\n",
            "\n",
            "Epoch 00121: loss did not improve from 0.01524\n",
            "Epoch 122/500\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.0153 - mae: 0.1272 - val_loss: 0.0205 - val_mae: 0.1440\n",
            "\n",
            "Epoch 00122: loss did not improve from 0.01524\n",
            "Epoch 123/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0136 - mae: 0.1215 - val_loss: 0.0194 - val_mae: 0.1441\n",
            "\n",
            "Epoch 00123: loss did not improve from 0.01524\n",
            "Epoch 124/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0170 - mae: 0.1307 - val_loss: 0.0190 - val_mae: 0.1357\n",
            "\n",
            "Epoch 00124: loss did not improve from 0.01524\n",
            "Epoch 125/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0182 - mae: 0.1347 - val_loss: 0.0198 - val_mae: 0.1406\n",
            "\n",
            "Epoch 00125: loss did not improve from 0.01524\n",
            "Epoch 126/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0164 - mae: 0.1252 - val_loss: 0.0195 - val_mae: 0.1378\n",
            "\n",
            "Epoch 00126: loss did not improve from 0.01524\n",
            "Epoch 127/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0209 - mae: 0.1370 - val_loss: 0.0203 - val_mae: 0.1438\n",
            "\n",
            "Epoch 00127: loss did not improve from 0.01524\n",
            "Epoch 128/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0142 - mae: 0.1236 - val_loss: 0.0204 - val_mae: 0.1478\n",
            "\n",
            "Epoch 00128: loss did not improve from 0.01524\n",
            "Epoch 129/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0170 - mae: 0.1299 - val_loss: 0.0193 - val_mae: 0.1374\n",
            "\n",
            "Epoch 00129: loss did not improve from 0.01524\n",
            "Epoch 130/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0167 - mae: 0.1295 - val_loss: 0.0200 - val_mae: 0.1421\n",
            "\n",
            "Epoch 00130: loss did not improve from 0.01524\n",
            "Epoch 131/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0167 - mae: 0.1276 - val_loss: 0.0190 - val_mae: 0.1366\n",
            "\n",
            "Epoch 00131: loss did not improve from 0.01524\n",
            "Epoch 132/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0147 - mae: 0.1221 - val_loss: 0.0194 - val_mae: 0.1392\n",
            "\n",
            "Epoch 00132: loss did not improve from 0.01524\n",
            "Epoch 133/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0180 - mae: 0.1339 - val_loss: 0.0184 - val_mae: 0.1327\n",
            "\n",
            "Epoch 00133: loss did not improve from 0.01524\n",
            "Epoch 134/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0138 - mae: 0.1223 - val_loss: 0.0195 - val_mae: 0.1443\n",
            "\n",
            "Epoch 00134: loss did not improve from 0.01524\n",
            "Epoch 135/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0158 - mae: 0.1228 - val_loss: 0.0177 - val_mae: 0.1367\n",
            "\n",
            "Epoch 00135: loss did not improve from 0.01524\n",
            "Epoch 136/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0154 - mae: 0.1255 - val_loss: 0.0188 - val_mae: 0.1454\n",
            "\n",
            "Epoch 00136: loss did not improve from 0.01524\n",
            "Epoch 137/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0158 - mae: 0.1295 - val_loss: 0.0193 - val_mae: 0.1390\n",
            "\n",
            "Epoch 00137: loss did not improve from 0.01524\n",
            "Epoch 138/500\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.0140 - mae: 0.1190 - val_loss: 0.0203 - val_mae: 0.1485\n",
            "\n",
            "Epoch 00138: loss improved from 0.01524 to 0.01496, saving model to poids_entrainement.hdf5\n",
            "Epoch 139/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0181 - mae: 0.1321 - val_loss: 0.0212 - val_mae: 0.1480\n",
            "\n",
            "Epoch 00139: loss did not improve from 0.01496\n",
            "Epoch 140/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0171 - mae: 0.1306 - val_loss: 0.0188 - val_mae: 0.1391\n",
            "\n",
            "Epoch 00140: loss did not improve from 0.01496\n",
            "Epoch 141/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0146 - mae: 0.1211 - val_loss: 0.0208 - val_mae: 0.1473\n",
            "\n",
            "Epoch 00141: loss did not improve from 0.01496\n",
            "Epoch 142/500\n",
            "30/30 [==============================] - 1s 15ms/step - loss: 0.0157 - mae: 0.1253 - val_loss: 0.0178 - val_mae: 0.1338\n",
            "\n",
            "Epoch 00142: loss did not improve from 0.01496\n",
            "Epoch 143/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0163 - mae: 0.1319 - val_loss: 0.0200 - val_mae: 0.1429\n",
            "\n",
            "Epoch 00143: loss did not improve from 0.01496\n",
            "Epoch 144/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0158 - mae: 0.1248 - val_loss: 0.0198 - val_mae: 0.1417\n",
            "\n",
            "Epoch 00144: loss did not improve from 0.01496\n",
            "Epoch 145/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0176 - mae: 0.1311 - val_loss: 0.0194 - val_mae: 0.1395\n",
            "\n",
            "Epoch 00145: loss did not improve from 0.01496\n",
            "Epoch 146/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0150 - mae: 0.1200 - val_loss: 0.0204 - val_mae: 0.1440\n",
            "\n",
            "Epoch 00146: loss did not improve from 0.01496\n",
            "Epoch 147/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0147 - mae: 0.1226 - val_loss: 0.0202 - val_mae: 0.1438\n",
            "\n",
            "Epoch 00147: loss did not improve from 0.01496\n",
            "Epoch 148/500\n",
            "30/30 [==============================] - 1s 13ms/step - loss: 0.0161 - mae: 0.1235 - val_loss: 0.0191 - val_mae: 0.1378\n",
            "\n",
            "Epoch 00148: loss did not improve from 0.01496\n",
            "Epoch 149/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0175 - mae: 0.1302 - val_loss: 0.0194 - val_mae: 0.1382\n",
            "\n",
            "Epoch 00149: loss did not improve from 0.01496\n",
            "Epoch 150/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0173 - mae: 0.1270 - val_loss: 0.0189 - val_mae: 0.1419\n",
            "\n",
            "Epoch 00150: loss did not improve from 0.01496\n",
            "Epoch 151/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0137 - mae: 0.1194 - val_loss: 0.0214 - val_mae: 0.1511\n",
            "\n",
            "Epoch 00151: loss did not improve from 0.01496\n",
            "Epoch 152/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0159 - mae: 0.1245 - val_loss: 0.0196 - val_mae: 0.1410\n",
            "\n",
            "Epoch 00152: loss did not improve from 0.01496\n",
            "Epoch 153/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0188 - mae: 0.1338 - val_loss: 0.0208 - val_mae: 0.1484\n",
            "\n",
            "Epoch 00153: loss did not improve from 0.01496\n",
            "Epoch 154/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0160 - mae: 0.1240 - val_loss: 0.0186 - val_mae: 0.1343\n",
            "\n",
            "Epoch 00154: loss did not improve from 0.01496\n",
            "Epoch 155/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0172 - mae: 0.1316 - val_loss: 0.0193 - val_mae: 0.1413\n",
            "\n",
            "Epoch 00155: loss did not improve from 0.01496\n",
            "Epoch 156/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0165 - mae: 0.1278 - val_loss: 0.0189 - val_mae: 0.1389\n",
            "\n",
            "Epoch 00156: loss did not improve from 0.01496\n",
            "Epoch 157/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0143 - mae: 0.1243 - val_loss: 0.0189 - val_mae: 0.1411\n",
            "\n",
            "Epoch 00157: loss did not improve from 0.01496\n",
            "Epoch 158/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0162 - mae: 0.1289 - val_loss: 0.0194 - val_mae: 0.1419\n",
            "\n",
            "Epoch 00158: loss did not improve from 0.01496\n",
            "Epoch 159/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0186 - mae: 0.1324 - val_loss: 0.0207 - val_mae: 0.1467\n",
            "\n",
            "Epoch 00159: loss did not improve from 0.01496\n",
            "Epoch 160/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0162 - mae: 0.1290 - val_loss: 0.0206 - val_mae: 0.1458\n",
            "\n",
            "Epoch 00160: loss did not improve from 0.01496\n",
            "Epoch 161/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0173 - mae: 0.1288 - val_loss: 0.0202 - val_mae: 0.1439\n",
            "\n",
            "Epoch 00161: loss did not improve from 0.01496\n",
            "Epoch 162/500\n",
            "30/30 [==============================] - 1s 13ms/step - loss: 0.0181 - mae: 0.1289 - val_loss: 0.0191 - val_mae: 0.1379\n",
            "\n",
            "Epoch 00162: loss did not improve from 0.01496\n",
            "Epoch 163/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0166 - mae: 0.1278 - val_loss: 0.0202 - val_mae: 0.1436\n",
            "\n",
            "Epoch 00163: loss did not improve from 0.01496\n",
            "Epoch 164/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0183 - mae: 0.1294 - val_loss: 0.0198 - val_mae: 0.1421\n",
            "\n",
            "Epoch 00164: loss did not improve from 0.01496\n",
            "Epoch 165/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0174 - mae: 0.1282 - val_loss: 0.0193 - val_mae: 0.1397\n",
            "\n",
            "Epoch 00165: loss did not improve from 0.01496\n",
            "Epoch 166/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0133 - mae: 0.1184 - val_loss: 0.0187 - val_mae: 0.1366\n",
            "\n",
            "Epoch 00166: loss did not improve from 0.01496\n",
            "Epoch 167/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0163 - mae: 0.1284 - val_loss: 0.0202 - val_mae: 0.1455\n",
            "\n",
            "Epoch 00167: loss did not improve from 0.01496\n",
            "Epoch 168/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0150 - mae: 0.1241 - val_loss: 0.0197 - val_mae: 0.1416\n",
            "\n",
            "Epoch 00168: loss did not improve from 0.01496\n",
            "Epoch 169/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0170 - mae: 0.1280 - val_loss: 0.0195 - val_mae: 0.1423\n",
            "\n",
            "Epoch 00169: loss did not improve from 0.01496\n",
            "Epoch 170/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0150 - mae: 0.1245 - val_loss: 0.0194 - val_mae: 0.1392\n",
            "\n",
            "Epoch 00170: loss did not improve from 0.01496\n",
            "Epoch 171/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0143 - mae: 0.1207 - val_loss: 0.0209 - val_mae: 0.1494\n",
            "\n",
            "Epoch 00171: loss improved from 0.01496 to 0.01487, saving model to poids_entrainement.hdf5\n",
            "Epoch 172/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0131 - mae: 0.1185 - val_loss: 0.0186 - val_mae: 0.1353\n",
            "\n",
            "Epoch 00172: loss did not improve from 0.01487\n",
            "Epoch 173/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0156 - mae: 0.1222 - val_loss: 0.0199 - val_mae: 0.1440\n",
            "\n",
            "Epoch 00173: loss did not improve from 0.01487\n",
            "Epoch 174/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0179 - mae: 0.1375 - val_loss: 0.0194 - val_mae: 0.1442\n",
            "\n",
            "Epoch 00174: loss did not improve from 0.01487\n",
            "Epoch 175/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0177 - mae: 0.1329 - val_loss: 0.0194 - val_mae: 0.1437\n",
            "\n",
            "Epoch 00175: loss did not improve from 0.01487\n",
            "Epoch 176/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0142 - mae: 0.1189 - val_loss: 0.0182 - val_mae: 0.1378\n",
            "\n",
            "Epoch 00176: loss did not improve from 0.01487\n",
            "Epoch 177/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0168 - mae: 0.1247 - val_loss: 0.0205 - val_mae: 0.1467\n",
            "\n",
            "Epoch 00177: loss did not improve from 0.01487\n",
            "Epoch 178/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0162 - mae: 0.1295 - val_loss: 0.0201 - val_mae: 0.1464\n",
            "\n",
            "Epoch 00178: loss did not improve from 0.01487\n",
            "Epoch 179/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0194 - mae: 0.1294 - val_loss: 0.0194 - val_mae: 0.1400\n",
            "\n",
            "Epoch 00179: loss did not improve from 0.01487\n",
            "Epoch 180/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0139 - mae: 0.1236 - val_loss: 0.0189 - val_mae: 0.1353\n",
            "\n",
            "Epoch 00180: loss improved from 0.01487 to 0.01484, saving model to poids_entrainement.hdf5\n",
            "Epoch 181/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0176 - mae: 0.1287 - val_loss: 0.0210 - val_mae: 0.1495\n",
            "\n",
            "Epoch 00181: loss did not improve from 0.01484\n",
            "Epoch 182/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0173 - mae: 0.1258 - val_loss: 0.0202 - val_mae: 0.1442\n",
            "\n",
            "Epoch 00182: loss did not improve from 0.01484\n",
            "Epoch 183/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0146 - mae: 0.1254 - val_loss: 0.0194 - val_mae: 0.1383\n",
            "\n",
            "Epoch 00183: loss did not improve from 0.01484\n",
            "Epoch 184/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0161 - mae: 0.1200 - val_loss: 0.0191 - val_mae: 0.1399\n",
            "\n",
            "Epoch 00184: loss did not improve from 0.01484\n",
            "Epoch 185/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0149 - mae: 0.1213 - val_loss: 0.0185 - val_mae: 0.1369\n",
            "\n",
            "Epoch 00185: loss did not improve from 0.01484\n",
            "Epoch 186/500\n",
            "30/30 [==============================] - 1s 13ms/step - loss: 0.0153 - mae: 0.1227 - val_loss: 0.0191 - val_mae: 0.1405\n",
            "\n",
            "Epoch 00186: loss did not improve from 0.01484\n",
            "Epoch 187/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0142 - mae: 0.1202 - val_loss: 0.0194 - val_mae: 0.1396\n",
            "\n",
            "Epoch 00187: loss did not improve from 0.01484\n",
            "Epoch 188/500\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0139 - mae: 0.1196 - val_loss: 0.0199 - val_mae: 0.1438\n",
            "\n",
            "Epoch 00188: loss did not improve from 0.01484\n",
            "Epoch 189/500\n",
            "30/30 [==============================] - 1s 14ms/step - loss: 0.0153 - mae: 0.1234 - val_loss: 0.0197 - val_mae: 0.1419\n",
            "\n",
            "Epoch 00189: loss did not improve from 0.01484\n",
            "Epoch 190/500\n",
            " 1/30 [>.............................] - ETA: 2s - loss: 0.0201 - mae: 0.1142"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COP9u4yitvmw"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o2zh6N9t1b7"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement[400:500])),erreur_entrainement[400:500], label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement[400:500])),erreur_validation[400:500], label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6Gq2CkeGR_1"
      },
      "source": [
        "**4. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRxXtHRXGXfF"
      },
      "source": [
        "taille_fenetre = 20\n",
        "\n",
        "# Création d'une liste vide pour recevoir les prédictions\n",
        "predictions = []\n",
        "\n",
        "# Calcul des prédiction pour chaque groupe de 20 valeurs consécutives de la série\n",
        "# dans l'intervalle de validation\n",
        "for t in temps[temps_separation:-taille_fenetre]:\n",
        "    X = np.reshape(Serie_Normalisee[t:t+taille_fenetre],(1,taille_fenetre))\n",
        "    predictions.append(model.predict(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd2nfDcgG8EO"
      },
      "source": [
        "# Affiche la série et les prédictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps,serie,label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title('Prédictions avec le modèle LSTM + Attention')\n",
        "plt.show()\n",
        "\n",
        "# Zoom sur l'intervalle de validation\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps[temps_separation:],serie[temps_separation:],label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title(\"Prédictions avec le modèle LSTM + Attention (zoom sur l'intervalle de validation)\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDS7BJvZG_e1"
      },
      "source": [
        "# Calcule de l'erreur quadratique moyenne et de l'erreur absolue moyenne \n",
        "\n",
        "mae = tf.keras.metrics.mean_absolute_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "mse = tf.keras.metrics.mean_squared_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "\n",
        "print(mae)\n",
        "print(mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL-yxXMkGss-"
      },
      "source": [
        "# Création du modèle GRU avec couche d'attention personnalisée simple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O658a3pTGstP"
      },
      "source": [
        "**1. Création du réseau et adaptation des formats d'entrée et de sortie**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgKfH7PwGstR"
      },
      "source": [
        "# Fonction de la couche lambda d'entrée\n",
        "def Traitement_Entrees(x):\n",
        "  return tf.expand_dims(x,axis=-1)\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "# Encodeur\n",
        "model.add(tf.keras.Input(shape=(taille_fenetre,)))\n",
        "model.add(tf.keras.layers.Lambda(Traitement_Entrees))\n",
        "model.add(tf.keras.layers.Dropout(0.05))\n",
        "model.add(tf.keras.layers.GRU(40,return_sequences=True))\n",
        "\n",
        "# Décodeur\n",
        "model.add(Couche_Attention())\n",
        "model.add(tf.keras.layers.Dense(40,activation=\"tanh\"))\n",
        "\n",
        "# Générateur\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "model.save_weights(\"model_initial.hdf5\")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F14BUeNPGstS"
      },
      "source": [
        "**2. Optimisation du taux d'apprentissage**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-6gQ2R7GstS"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(lr=1e-8)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur, metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset_norm,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXpSPz_0GstT"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 1])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hioDaFAGstT"
      },
      "source": [
        "**3. Entrainement du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qPr4aieGstU"
      },
      "source": [
        "# Charge les meilleurs poids\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvQOopfBGstU"
      },
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "class TimingCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, logs={}):\n",
        "        self.n_steps = 0\n",
        "        self.t_step = 0\n",
        "        self.n_batch = 0\n",
        "        self.total_batch = 0\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.starttime = timer()\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.t_step = self.t_step  + timer()-self.starttime\n",
        "        self.n_steps = self.n_steps + 1\n",
        "        if (self.total_batch == 0):\n",
        "          self.total_batch=self.n_batch - 1\n",
        "    def on_train_batch_begin(self,batch,logs=None):\n",
        "      self.n_batch= self.n_batch + 1\n",
        "    def GetInfos(self):\n",
        "      return([self.t_step/(self.n_steps*self.total_batch), self.t_step, self.total_batch])\n",
        "\n",
        "cb = TimingCallback()\n",
        "\n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.1,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0.05)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(learning_rate=lr_schedule,momentum=0.9)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_entrainement.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur,metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle\n",
        "historique = model.fit(dataset_norm,validation_data=dataset_Val_norm, epochs=500,verbose=1, callbacks=[CheckPoint,cb])\n",
        "\n",
        "# Affiche quelques informations sur les timings\n",
        "infos = cb.GetInfos()\n",
        "print(\"Step time : %.3f\" %infos[0])\n",
        "print(\"Total time : %.3f\" %infos[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALLfIbIGGstV"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3ezkxYHGstV"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement[400:500])),erreur_entrainement[400:500], label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement[400:500])),erreur_validation[400:500], label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jz0Ivk7hGstV"
      },
      "source": [
        "**4. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_93zkImGstW"
      },
      "source": [
        "taille_fenetre = 20\n",
        "\n",
        "# Création d'une liste vide pour recevoir les prédictions\n",
        "predictions = []\n",
        "\n",
        "# Calcul des prédiction pour chaque groupe de 20 valeurs consécutives de la série\n",
        "# dans l'intervalle de validation\n",
        "for t in temps[temps_separation:-taille_fenetre]:\n",
        "    X = np.reshape(Serie_Normalisee[t:t+taille_fenetre],(1,taille_fenetre))\n",
        "    predictions.append(model.predict(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UU4DAZXDGstW"
      },
      "source": [
        "# Affiche la série et les prédictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps,serie,label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title('Prédictions avec le modèle GRU + Attention')\n",
        "plt.show()\n",
        "\n",
        "# Zoom sur l'intervalle de validation\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps[temps_separation:],serie[temps_separation:],label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title(\"Prédictions avec le modèle GRU + Attention (zoom sur l'intervalle de validation)\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9mvuqfLGstW"
      },
      "source": [
        "# Calcule de l'erreur quadratique moyenne et de l'erreur absolue moyenne \n",
        "\n",
        "mae = tf.keras.metrics.mean_absolute_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "mse = tf.keras.metrics.mean_squared_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "\n",
        "print(mae)\n",
        "print(mse)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}