{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bitcoin.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/Bitcoin/Bitcoin_Create_Features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVuCzMYKDp3z"
      },
      "source": [
        "# Chargement des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3nLyKtnliCW"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eo2qVS1lrzuX"
      },
      "source": [
        "On télécharge un script depuis Github permettant de télécharger un fichier stocké sur GoogleDrive, puis on utilise ce script écrit en Python pour télécharger le fichier `bitcoin.zip`. Enfin, on décompresse les données pour obtenir le fichier `bitstampUSD_1-min_data_2012-01-01_to_2021-03-31.csv` :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sOdetggHqKE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c98c0b45-28e0-4596-a972-211ed0ce6312"
      },
      "source": [
        "# Récupération des données au format .csv\n",
        "\n",
        "!git clone https://github.com/chentinghao/download_google_drive.git\n",
        "!python download_google_drive/download_gdrive.py \"1FZsEdpBm-AQ2L9n_pMnm6336-O_IVo7z\" \"/content/bitcoin.zip\"\n",
        "!unzip bitcoin.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'download_google_drive'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Total 16 (delta 0), reused 0 (delta 0), pack-reused 16\u001b[K\n",
            "Unpacking objects: 100% (16/16), done.\n",
            "100MB [00:01, 54.9MB/s] \n",
            "Archive:  bitcoin.zip\n",
            "  inflating: bitstampUSD_1-min_data_2012-01-01_to_2021-03-31.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPJ6nnrRsUBm"
      },
      "source": [
        "Charge la série sous Pandas et affiche les informations du fichier :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEB_JjihEYTP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "e08640ae-06bf-4fd8-842d-3776185c7c7d"
      },
      "source": [
        "# Création de la série sous Pandas\n",
        "serie = pd.read_csv(\"bitstampUSD_1-min_data_2012-01-01_to_2021-03-31.csv\")\n",
        "serie"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Timestamp</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume_(BTC)</th>\n",
              "      <th>Volume_(Currency)</th>\n",
              "      <th>Weighted_Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1325317920</td>\n",
              "      <td>4.39</td>\n",
              "      <td>4.39</td>\n",
              "      <td>4.39</td>\n",
              "      <td>4.39</td>\n",
              "      <td>0.455581</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.390000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1325317980</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1325318040</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1325318100</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1325318160</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4857372</th>\n",
              "      <td>1617148560</td>\n",
              "      <td>58714.31</td>\n",
              "      <td>58714.31</td>\n",
              "      <td>58686.00</td>\n",
              "      <td>58686.00</td>\n",
              "      <td>1.384487</td>\n",
              "      <td>81259.372187</td>\n",
              "      <td>58692.753339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4857373</th>\n",
              "      <td>1617148620</td>\n",
              "      <td>58683.97</td>\n",
              "      <td>58693.43</td>\n",
              "      <td>58683.97</td>\n",
              "      <td>58685.81</td>\n",
              "      <td>7.294848</td>\n",
              "      <td>428158.146640</td>\n",
              "      <td>58693.226508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4857374</th>\n",
              "      <td>1617148680</td>\n",
              "      <td>58693.43</td>\n",
              "      <td>58723.84</td>\n",
              "      <td>58693.43</td>\n",
              "      <td>58723.84</td>\n",
              "      <td>1.705682</td>\n",
              "      <td>100117.070370</td>\n",
              "      <td>58696.198496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4857375</th>\n",
              "      <td>1617148740</td>\n",
              "      <td>58742.18</td>\n",
              "      <td>58770.38</td>\n",
              "      <td>58742.18</td>\n",
              "      <td>58760.59</td>\n",
              "      <td>0.720415</td>\n",
              "      <td>42332.958633</td>\n",
              "      <td>58761.866202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4857376</th>\n",
              "      <td>1617148800</td>\n",
              "      <td>58767.75</td>\n",
              "      <td>58778.18</td>\n",
              "      <td>58755.97</td>\n",
              "      <td>58778.18</td>\n",
              "      <td>2.712831</td>\n",
              "      <td>159417.751000</td>\n",
              "      <td>58764.349363</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4857377 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          Timestamp      Open  ...  Volume_(Currency)  Weighted_Price\n",
              "0        1325317920      4.39  ...           2.000000        4.390000\n",
              "1        1325317980       NaN  ...                NaN             NaN\n",
              "2        1325318040       NaN  ...                NaN             NaN\n",
              "3        1325318100       NaN  ...                NaN             NaN\n",
              "4        1325318160       NaN  ...                NaN             NaN\n",
              "...             ...       ...  ...                ...             ...\n",
              "4857372  1617148560  58714.31  ...       81259.372187    58692.753339\n",
              "4857373  1617148620  58683.97  ...      428158.146640    58693.226508\n",
              "4857374  1617148680  58693.43  ...      100117.070370    58696.198496\n",
              "4857375  1617148740  58742.18  ...       42332.958633    58761.866202\n",
              "4857376  1617148800  58767.75  ...      159417.751000    58764.349363\n",
              "\n",
              "[4857377 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF62FumBty9H"
      },
      "source": [
        "# Pré-traitement des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2oeGZgr1UUP"
      },
      "source": [
        "**1. Recherche des erreurs dans les données**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2d3HD1YwmIS"
      },
      "source": [
        "On commence par vérifier qu'il ne manque pas de dates. Pour cela, on vérifie qu'il y a bien 60 secondes entre deux Timestamp. Si on trouve un décalage non cohérent, on enregistre les informations dans une liste."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCXSGY9Swltw"
      },
      "source": [
        "# Fonction permettant de vérifier si chaque intervalle est bien de 60s\n",
        "def recherche_erreur(fenetre):\n",
        "  if fenetre.values[1] - fenetre.values[0] != 60:\n",
        "    Timestamp_Errors.append(fenetre.values)\n",
        "  return 0\n",
        "\n",
        "# Définit une liste pour sauvegarder le résultat des recherches\n",
        "Timestamp_Errors = []\n",
        "\n",
        "# Applique la fonction sur une fenêtre glissante des données\n",
        "serie.Timestamp.rolling(2).apply(recherche_erreur)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l1fTgb_1O8b"
      },
      "source": [
        "On affiche les erreurs trouvées :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZERkJS40sHP"
      },
      "source": [
        "# Affiche les informations sur les erreurs trouvées\n",
        "\n",
        "for erreur in Timestamp_Errors:\n",
        "  print (pd.to_datetime(Timestamp_Errors[0],unit=\"s\"))\n",
        "  print((Timestamp_Errors[0][1] - Timestamp_Errors[0][0])/60 - 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHOSRv8_1aYa"
      },
      "source": [
        "On observe qu'il manque des données entre le 5 janvier 2015 à 9:12:00 et le 9 janvier 2015 à 21:05:00, soit 6472 données."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tjbmho3h2nB7"
      },
      "source": [
        "Recherchons maintenant le nombre de données manquantes :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUzjd-qN2s-T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0335f436-23cd-4856-cfa4-75cfbe31a16b"
      },
      "source": [
        "# Affichage du nombre total de données manquantes\n",
        "\n",
        "data_manquantes = sum(np.isnan(serie['Open']))\n",
        "print (\"Nombre de données manquantes : %s\" %data_manquantes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nombre de données manquantes : 1243608\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D41XKoGE3O60"
      },
      "source": [
        "On a donc en tout : 6472 + 1243608 = 1250080 données manquantes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GefH2TaM3a0S"
      },
      "source": [
        "**2. Identification des erreurs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XivWgphwsdIm"
      },
      "source": [
        "On convertit maintenant les `Timestamp` (mesure de temps exprimé en seconde écoulé depuis le 01/01/1970 - 00:00:00 UTC) en format plus standard :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdLSr2IWSOnX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "c3181bf5-a55d-442f-b879-4740e474805c"
      },
      "source": [
        "# Conversion des timestamp en date\n",
        "serie.Timestamp = pd.to_datetime(serie['Timestamp'], unit=\"s\")\n",
        "serie"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Timestamp</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume_(BTC)</th>\n",
              "      <th>Volume_(Currency)</th>\n",
              "      <th>Weighted_Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2011-12-31 07:52:00</td>\n",
              "      <td>4.39</td>\n",
              "      <td>4.39</td>\n",
              "      <td>4.39</td>\n",
              "      <td>4.39</td>\n",
              "      <td>0.455581</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.390000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2011-12-31 07:53:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2011-12-31 07:54:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2011-12-31 07:55:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2011-12-31 07:56:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4857372</th>\n",
              "      <td>2021-03-30 23:56:00</td>\n",
              "      <td>58714.31</td>\n",
              "      <td>58714.31</td>\n",
              "      <td>58686.00</td>\n",
              "      <td>58686.00</td>\n",
              "      <td>1.384487</td>\n",
              "      <td>81259.372187</td>\n",
              "      <td>58692.753339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4857373</th>\n",
              "      <td>2021-03-30 23:57:00</td>\n",
              "      <td>58683.97</td>\n",
              "      <td>58693.43</td>\n",
              "      <td>58683.97</td>\n",
              "      <td>58685.81</td>\n",
              "      <td>7.294848</td>\n",
              "      <td>428158.146640</td>\n",
              "      <td>58693.226508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4857374</th>\n",
              "      <td>2021-03-30 23:58:00</td>\n",
              "      <td>58693.43</td>\n",
              "      <td>58723.84</td>\n",
              "      <td>58693.43</td>\n",
              "      <td>58723.84</td>\n",
              "      <td>1.705682</td>\n",
              "      <td>100117.070370</td>\n",
              "      <td>58696.198496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4857375</th>\n",
              "      <td>2021-03-30 23:59:00</td>\n",
              "      <td>58742.18</td>\n",
              "      <td>58770.38</td>\n",
              "      <td>58742.18</td>\n",
              "      <td>58760.59</td>\n",
              "      <td>0.720415</td>\n",
              "      <td>42332.958633</td>\n",
              "      <td>58761.866202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4857376</th>\n",
              "      <td>2021-03-31 00:00:00</td>\n",
              "      <td>58767.75</td>\n",
              "      <td>58778.18</td>\n",
              "      <td>58755.97</td>\n",
              "      <td>58778.18</td>\n",
              "      <td>2.712831</td>\n",
              "      <td>159417.751000</td>\n",
              "      <td>58764.349363</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4857377 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Timestamp      Open  ...  Volume_(Currency)  Weighted_Price\n",
              "0       2011-12-31 07:52:00      4.39  ...           2.000000        4.390000\n",
              "1       2011-12-31 07:53:00       NaN  ...                NaN             NaN\n",
              "2       2011-12-31 07:54:00       NaN  ...                NaN             NaN\n",
              "3       2011-12-31 07:55:00       NaN  ...                NaN             NaN\n",
              "4       2011-12-31 07:56:00       NaN  ...                NaN             NaN\n",
              "...                     ...       ...  ...                ...             ...\n",
              "4857372 2021-03-30 23:56:00  58714.31  ...       81259.372187    58692.753339\n",
              "4857373 2021-03-30 23:57:00  58683.97  ...      428158.146640    58693.226508\n",
              "4857374 2021-03-30 23:58:00  58693.43  ...      100117.070370    58696.198496\n",
              "4857375 2021-03-30 23:59:00  58742.18  ...       42332.958633    58761.866202\n",
              "4857376 2021-03-31 00:00:00  58767.75  ...      159417.751000    58764.349363\n",
              "\n",
              "[4857377 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgHcZyd13gCa"
      },
      "source": [
        "On demande maintenant à échantillonner les données sur 60 secondes :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVNlzo9QWNmP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "6666f480-ccb2-457c-da8e-95fe471b8073"
      },
      "source": [
        "# Echantillonnage de la série sur 1min\n",
        "serie_minute = serie.set_index('Timestamp').resample('60s').asfreq()\n",
        "\n",
        "# Récupère le nombre de données sans valeurs numériques\n",
        "data_manquantes = sum(np.isnan(serie_minute['Open']))\n",
        "\n",
        "# Affiche le nombre de données manquantes et la série sur 1min \n",
        "print (\"Nombre de données manquantes : %s\" %data_manquantes)\n",
        "serie_minute"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nombre de données manquantes : 1250080\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume_(BTC)</th>\n",
              "      <th>Volume_(Currency)</th>\n",
              "      <th>Weighted_Price</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Timestamp</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2011-12-31 07:52:00</th>\n",
              "      <td>4.39</td>\n",
              "      <td>4.39</td>\n",
              "      <td>4.39</td>\n",
              "      <td>4.39</td>\n",
              "      <td>0.455581</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.390000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2011-12-31 07:53:00</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2011-12-31 07:54:00</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2011-12-31 07:55:00</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2011-12-31 07:56:00</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-30 23:56:00</th>\n",
              "      <td>58714.31</td>\n",
              "      <td>58714.31</td>\n",
              "      <td>58686.00</td>\n",
              "      <td>58686.00</td>\n",
              "      <td>1.384487</td>\n",
              "      <td>81259.372187</td>\n",
              "      <td>58692.753339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-30 23:57:00</th>\n",
              "      <td>58683.97</td>\n",
              "      <td>58693.43</td>\n",
              "      <td>58683.97</td>\n",
              "      <td>58685.81</td>\n",
              "      <td>7.294848</td>\n",
              "      <td>428158.146640</td>\n",
              "      <td>58693.226508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-30 23:58:00</th>\n",
              "      <td>58693.43</td>\n",
              "      <td>58723.84</td>\n",
              "      <td>58693.43</td>\n",
              "      <td>58723.84</td>\n",
              "      <td>1.705682</td>\n",
              "      <td>100117.070370</td>\n",
              "      <td>58696.198496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-30 23:59:00</th>\n",
              "      <td>58742.18</td>\n",
              "      <td>58770.38</td>\n",
              "      <td>58742.18</td>\n",
              "      <td>58760.59</td>\n",
              "      <td>0.720415</td>\n",
              "      <td>42332.958633</td>\n",
              "      <td>58761.866202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-31 00:00:00</th>\n",
              "      <td>58767.75</td>\n",
              "      <td>58778.18</td>\n",
              "      <td>58755.97</td>\n",
              "      <td>58778.18</td>\n",
              "      <td>2.712831</td>\n",
              "      <td>159417.751000</td>\n",
              "      <td>58764.349363</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4863849 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                         Open      High  ...  Volume_(Currency)  Weighted_Price\n",
              "Timestamp                                ...                                   \n",
              "2011-12-31 07:52:00      4.39      4.39  ...           2.000000        4.390000\n",
              "2011-12-31 07:53:00       NaN       NaN  ...                NaN             NaN\n",
              "2011-12-31 07:54:00       NaN       NaN  ...                NaN             NaN\n",
              "2011-12-31 07:55:00       NaN       NaN  ...                NaN             NaN\n",
              "2011-12-31 07:56:00       NaN       NaN  ...                NaN             NaN\n",
              "...                       ...       ...  ...                ...             ...\n",
              "2021-03-30 23:56:00  58714.31  58714.31  ...       81259.372187    58692.753339\n",
              "2021-03-30 23:57:00  58683.97  58693.43  ...      428158.146640    58693.226508\n",
              "2021-03-30 23:58:00  58693.43  58723.84  ...      100117.070370    58696.198496\n",
              "2021-03-30 23:59:00  58742.18  58770.38  ...       42332.958633    58761.866202\n",
              "2021-03-31 00:00:00  58767.75  58778.18  ...      159417.751000    58764.349363\n",
              "\n",
              "[4863849 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqFOnDlU3tey"
      },
      "source": [
        "On obtient en tout 4863849 données après échantillonnage, soit (4863849-4857377) =  6472 données supplémentaires. Ceci est cohérent avec ce qu'on avait trouvé avant. Il manque 1250080 données. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tkbi7uuBnUD3"
      },
      "source": [
        "**3. Correction des données**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IQuSqAknduG"
      },
      "source": [
        "Pour corriger les données, on va tout simplement utiliser la fonction [fillna](https://pandas.pydata.org/docs/reference/api/pandas.Series.fillna.html) de Pandas avec la fonctionnalité de type `backfill` :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2Oav6gin5aP"
      },
      "source": [
        "# Applique la fonction de remplissage automatique des données non numérique avec l'option backfill\n",
        "serie_minute = serie_minute.interpolate(method=\"slinear\")\n",
        "serie_minute = serie_minute.fillna(method=\"backfill\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfOvQ-BKn80v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "4a3ba5db-c072-49da-dac6-a312a4ef1245"
      },
      "source": [
        "# Récupère le nombre de données non numériques et affiche les informations\n",
        "\n",
        "data_manquantes = sum(np.isnan(serie_minute['Open']))\n",
        "print (\"Nombre de données manquantes : %s\" %data_manquantes)\n",
        "serie_minute"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nombre de données manquantes : 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume_(BTC)</th>\n",
              "      <th>Volume_(Currency)</th>\n",
              "      <th>Weighted_Price</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Timestamp</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2011-12-31 07:52:00</th>\n",
              "      <td>4.39</td>\n",
              "      <td>4.39</td>\n",
              "      <td>4.39</td>\n",
              "      <td>4.39</td>\n",
              "      <td>0.455581</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.390000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2011-12-31 07:53:00</th>\n",
              "      <td>4.39</td>\n",
              "      <td>4.39</td>\n",
              "      <td>4.39</td>\n",
              "      <td>4.39</td>\n",
              "      <td>0.555046</td>\n",
              "      <td>2.436653</td>\n",
              "      <td>4.390000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2011-12-31 07:54:00</th>\n",
              "      <td>4.39</td>\n",
              "      <td>4.39</td>\n",
              "      <td>4.39</td>\n",
              "      <td>4.39</td>\n",
              "      <td>0.654511</td>\n",
              "      <td>2.873305</td>\n",
              "      <td>4.390000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2011-12-31 07:55:00</th>\n",
              "      <td>4.39</td>\n",
              "      <td>4.39</td>\n",
              "      <td>4.39</td>\n",
              "      <td>4.39</td>\n",
              "      <td>0.753977</td>\n",
              "      <td>3.309958</td>\n",
              "      <td>4.390000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2011-12-31 07:56:00</th>\n",
              "      <td>4.39</td>\n",
              "      <td>4.39</td>\n",
              "      <td>4.39</td>\n",
              "      <td>4.39</td>\n",
              "      <td>0.853442</td>\n",
              "      <td>3.746611</td>\n",
              "      <td>4.390000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-30 23:56:00</th>\n",
              "      <td>58714.31</td>\n",
              "      <td>58714.31</td>\n",
              "      <td>58686.00</td>\n",
              "      <td>58686.00</td>\n",
              "      <td>1.384487</td>\n",
              "      <td>81259.372187</td>\n",
              "      <td>58692.753339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-30 23:57:00</th>\n",
              "      <td>58683.97</td>\n",
              "      <td>58693.43</td>\n",
              "      <td>58683.97</td>\n",
              "      <td>58685.81</td>\n",
              "      <td>7.294848</td>\n",
              "      <td>428158.146640</td>\n",
              "      <td>58693.226508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-30 23:58:00</th>\n",
              "      <td>58693.43</td>\n",
              "      <td>58723.84</td>\n",
              "      <td>58693.43</td>\n",
              "      <td>58723.84</td>\n",
              "      <td>1.705682</td>\n",
              "      <td>100117.070370</td>\n",
              "      <td>58696.198496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-30 23:59:00</th>\n",
              "      <td>58742.18</td>\n",
              "      <td>58770.38</td>\n",
              "      <td>58742.18</td>\n",
              "      <td>58760.59</td>\n",
              "      <td>0.720415</td>\n",
              "      <td>42332.958633</td>\n",
              "      <td>58761.866202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-31 00:00:00</th>\n",
              "      <td>58767.75</td>\n",
              "      <td>58778.18</td>\n",
              "      <td>58755.97</td>\n",
              "      <td>58778.18</td>\n",
              "      <td>2.712831</td>\n",
              "      <td>159417.751000</td>\n",
              "      <td>58764.349363</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4863849 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                         Open      High  ...  Volume_(Currency)  Weighted_Price\n",
              "Timestamp                                ...                                   \n",
              "2011-12-31 07:52:00      4.39      4.39  ...           2.000000        4.390000\n",
              "2011-12-31 07:53:00      4.39      4.39  ...           2.436653        4.390000\n",
              "2011-12-31 07:54:00      4.39      4.39  ...           2.873305        4.390000\n",
              "2011-12-31 07:55:00      4.39      4.39  ...           3.309958        4.390000\n",
              "2011-12-31 07:56:00      4.39      4.39  ...           3.746611        4.390000\n",
              "...                       ...       ...  ...                ...             ...\n",
              "2021-03-30 23:56:00  58714.31  58714.31  ...       81259.372187    58692.753339\n",
              "2021-03-30 23:57:00  58683.97  58693.43  ...      428158.146640    58693.226508\n",
              "2021-03-30 23:58:00  58693.43  58723.84  ...      100117.070370    58696.198496\n",
              "2021-03-30 23:59:00  58742.18  58770.38  ...       42332.958633    58761.866202\n",
              "2021-03-31 00:00:00  58767.75  58778.18  ...      159417.751000    58764.349363\n",
              "\n",
              "[4863849 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v05rWWccJI26"
      },
      "source": [
        "**4. Affichage des données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1QKMBThNQni"
      },
      "source": [
        "# Affiche la série\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(serie_minute.index, serie_minute.Open)\n",
        "plt.title(\"Evolution du prix du BTC\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU7u1mA1E6jk"
      },
      "source": [
        "# Préparation des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhIs2GS7vu8k"
      },
      "source": [
        "Nous allons réaliser des modélisations sur la série journalière, et pour une période allant du 1er avril 2013 au 31 mars 2021."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeKrafzHv9gd"
      },
      "source": [
        "**1. Création de la série horaire pour la modélisation globale**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUlNO0pswBpr"
      },
      "source": [
        "On va réaliser des prédictions à l'aide d'une série à fréquence journalière. On commence par tenter d'estimer les données manquantes à l'aide d'une interpolation linéaire à l'aide de la fonction [interpolate](https://pandas.pydata.org/docs/reference/api/pandas.Series.interpolate.html#pandas.Series.interpolate) de Pandas, puis on complète avec la méthode `backfill` si nécessaire."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir4BkaUtuiXX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "b4467f9e-479e-4d98-d0ed-4601dd91c163"
      },
      "source": [
        "# Echantillonne la série sur 1 heure\n",
        "serie_heure = serie.set_index('Timestamp').resample('1H').asfreq()\n",
        "\n",
        "# Remplissage des données non numériques par interpolation linéraire\n",
        "serie_heure = serie_heure.interpolate(method=\"slinear\")\n",
        "\n",
        "# Remplissage des données non numériques restantes par backfill\n",
        "serie_heure = serie_heure.fillna(method=\"backfill\")\n",
        "\n",
        "# Affiche les informations\n",
        "data_manquantes = sum(np.isnan(serie_heure['Open']))\n",
        "print (\"Nombre de données manquantes : %s\" %data_manquantes)\n",
        "serie_heure"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nombre de données manquantes : 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume_(BTC)</th>\n",
              "      <th>Volume_(Currency)</th>\n",
              "      <th>Weighted_Price</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Timestamp</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2011-12-31 07:00:00</th>\n",
              "      <td>4.58</td>\n",
              "      <td>4.58</td>\n",
              "      <td>4.58</td>\n",
              "      <td>4.58</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>41.220000</td>\n",
              "      <td>4.580000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2011-12-31 08:00:00</th>\n",
              "      <td>4.58</td>\n",
              "      <td>4.58</td>\n",
              "      <td>4.58</td>\n",
              "      <td>4.58</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>41.220000</td>\n",
              "      <td>4.580000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2011-12-31 09:00:00</th>\n",
              "      <td>4.58</td>\n",
              "      <td>4.58</td>\n",
              "      <td>4.58</td>\n",
              "      <td>4.58</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>41.220000</td>\n",
              "      <td>4.580000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2011-12-31 10:00:00</th>\n",
              "      <td>4.58</td>\n",
              "      <td>4.58</td>\n",
              "      <td>4.58</td>\n",
              "      <td>4.58</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>41.220000</td>\n",
              "      <td>4.580000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2011-12-31 11:00:00</th>\n",
              "      <td>4.58</td>\n",
              "      <td>4.58</td>\n",
              "      <td>4.58</td>\n",
              "      <td>4.58</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>41.220000</td>\n",
              "      <td>4.580000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-30 20:00:00</th>\n",
              "      <td>59118.91</td>\n",
              "      <td>59135.00</td>\n",
              "      <td>59023.96</td>\n",
              "      <td>59037.79</td>\n",
              "      <td>2.684344</td>\n",
              "      <td>158619.612840</td>\n",
              "      <td>59090.651748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-30 21:00:00</th>\n",
              "      <td>58644.04</td>\n",
              "      <td>58670.91</td>\n",
              "      <td>58591.06</td>\n",
              "      <td>58591.06</td>\n",
              "      <td>9.979587</td>\n",
              "      <td>585005.680240</td>\n",
              "      <td>58620.228970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-30 22:00:00</th>\n",
              "      <td>58758.44</td>\n",
              "      <td>58762.56</td>\n",
              "      <td>58758.44</td>\n",
              "      <td>58762.56</td>\n",
              "      <td>0.573484</td>\n",
              "      <td>33699.187441</td>\n",
              "      <td>58762.179612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-30 23:00:00</th>\n",
              "      <td>58699.43</td>\n",
              "      <td>58699.43</td>\n",
              "      <td>58643.16</td>\n",
              "      <td>58657.09</td>\n",
              "      <td>0.281702</td>\n",
              "      <td>16521.438155</td>\n",
              "      <td>58648.607194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-31 00:00:00</th>\n",
              "      <td>58767.75</td>\n",
              "      <td>58778.18</td>\n",
              "      <td>58755.97</td>\n",
              "      <td>58778.18</td>\n",
              "      <td>2.712831</td>\n",
              "      <td>159417.751000</td>\n",
              "      <td>58764.349363</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>81066 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                         Open      High  ...  Volume_(Currency)  Weighted_Price\n",
              "Timestamp                                ...                                   \n",
              "2011-12-31 07:00:00      4.58      4.58  ...          41.220000        4.580000\n",
              "2011-12-31 08:00:00      4.58      4.58  ...          41.220000        4.580000\n",
              "2011-12-31 09:00:00      4.58      4.58  ...          41.220000        4.580000\n",
              "2011-12-31 10:00:00      4.58      4.58  ...          41.220000        4.580000\n",
              "2011-12-31 11:00:00      4.58      4.58  ...          41.220000        4.580000\n",
              "...                       ...       ...  ...                ...             ...\n",
              "2021-03-30 20:00:00  59118.91  59135.00  ...      158619.612840    59090.651748\n",
              "2021-03-30 21:00:00  58644.04  58670.91  ...      585005.680240    58620.228970\n",
              "2021-03-30 22:00:00  58758.44  58762.56  ...       33699.187441    58762.179612\n",
              "2021-03-30 23:00:00  58699.43  58699.43  ...       16521.438155    58648.607194\n",
              "2021-03-31 00:00:00  58767.75  58778.18  ...      159417.751000    58764.349363\n",
              "\n",
              "[81066 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epIrgQTvca6p"
      },
      "source": [
        "# Affiche la série\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(serie_heure.index, serie_heure.Open)\n",
        "plt.title(\"Evolution du prix du BTC\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hsbej4XvckFD"
      },
      "source": [
        "On construit une nouvelle série avec les dates retenues pour le début et la fin :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMiDRe1ZcqMR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "outputId": "ee64b506-53cc-408c-d19a-f2a20e631d5b"
      },
      "source": [
        "# Définition des dates de début et de fin\n",
        "\n",
        "date_debut = \"2013-04-01 00:00:00\"\n",
        "date_fin = \"2021-03-31 00:00:00\"\n",
        "\n",
        "serie_etude = serie_heure.loc[date_debut:date_fin].copy()\n",
        "serie_etude"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume_(BTC)</th>\n",
              "      <th>Volume_(Currency)</th>\n",
              "      <th>Weighted_Price</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Timestamp</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2013-04-01 00:00:00</th>\n",
              "      <td>95.373684</td>\n",
              "      <td>95.373684</td>\n",
              "      <td>95.373684</td>\n",
              "      <td>95.373684</td>\n",
              "      <td>2.154868</td>\n",
              "      <td>206.698263</td>\n",
              "      <td>95.373684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-01 01:00:00</th>\n",
              "      <td>95.463158</td>\n",
              "      <td>95.463158</td>\n",
              "      <td>95.463158</td>\n",
              "      <td>95.463158</td>\n",
              "      <td>2.311941</td>\n",
              "      <td>221.801368</td>\n",
              "      <td>95.463158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-01 02:00:00</th>\n",
              "      <td>95.552632</td>\n",
              "      <td>95.552632</td>\n",
              "      <td>95.552632</td>\n",
              "      <td>95.552632</td>\n",
              "      <td>2.469013</td>\n",
              "      <td>236.904474</td>\n",
              "      <td>95.552632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-01 03:00:00</th>\n",
              "      <td>95.642105</td>\n",
              "      <td>95.642105</td>\n",
              "      <td>95.642105</td>\n",
              "      <td>95.642105</td>\n",
              "      <td>2.626086</td>\n",
              "      <td>252.007579</td>\n",
              "      <td>95.642105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-01 04:00:00</th>\n",
              "      <td>95.731579</td>\n",
              "      <td>95.731579</td>\n",
              "      <td>95.731579</td>\n",
              "      <td>95.731579</td>\n",
              "      <td>2.783158</td>\n",
              "      <td>267.110684</td>\n",
              "      <td>95.731579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-30 20:00:00</th>\n",
              "      <td>59118.910000</td>\n",
              "      <td>59135.000000</td>\n",
              "      <td>59023.960000</td>\n",
              "      <td>59037.790000</td>\n",
              "      <td>2.684344</td>\n",
              "      <td>158619.612840</td>\n",
              "      <td>59090.651748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-30 21:00:00</th>\n",
              "      <td>58644.040000</td>\n",
              "      <td>58670.910000</td>\n",
              "      <td>58591.060000</td>\n",
              "      <td>58591.060000</td>\n",
              "      <td>9.979587</td>\n",
              "      <td>585005.680240</td>\n",
              "      <td>58620.228970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-30 22:00:00</th>\n",
              "      <td>58758.440000</td>\n",
              "      <td>58762.560000</td>\n",
              "      <td>58758.440000</td>\n",
              "      <td>58762.560000</td>\n",
              "      <td>0.573484</td>\n",
              "      <td>33699.187441</td>\n",
              "      <td>58762.179612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-30 23:00:00</th>\n",
              "      <td>58699.430000</td>\n",
              "      <td>58699.430000</td>\n",
              "      <td>58643.160000</td>\n",
              "      <td>58657.090000</td>\n",
              "      <td>0.281702</td>\n",
              "      <td>16521.438155</td>\n",
              "      <td>58648.607194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-31 00:00:00</th>\n",
              "      <td>58767.750000</td>\n",
              "      <td>58778.180000</td>\n",
              "      <td>58755.970000</td>\n",
              "      <td>58778.180000</td>\n",
              "      <td>2.712831</td>\n",
              "      <td>159417.751000</td>\n",
              "      <td>58764.349363</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>70105 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                             Open  ...  Weighted_Price\n",
              "Timestamp                          ...                \n",
              "2013-04-01 00:00:00     95.373684  ...       95.373684\n",
              "2013-04-01 01:00:00     95.463158  ...       95.463158\n",
              "2013-04-01 02:00:00     95.552632  ...       95.552632\n",
              "2013-04-01 03:00:00     95.642105  ...       95.642105\n",
              "2013-04-01 04:00:00     95.731579  ...       95.731579\n",
              "...                           ...  ...             ...\n",
              "2021-03-30 20:00:00  59118.910000  ...    59090.651748\n",
              "2021-03-30 21:00:00  58644.040000  ...    58620.228970\n",
              "2021-03-30 22:00:00  58758.440000  ...    58762.179612\n",
              "2021-03-30 23:00:00  58699.430000  ...    58648.607194\n",
              "2021-03-31 00:00:00  58767.750000  ...    58764.349363\n",
              "\n",
              "[70105 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUD1yl1wxkla",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "outputId": "93022374-0edb-47c4-ca99-19ea5971e24d"
      },
      "source": [
        "serie_etude['x'] = np.linspace(0,serie_etude.index.size-1,serie_etude.index.size)\n",
        "serie_etude"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume_(BTC)</th>\n",
              "      <th>Volume_(Currency)</th>\n",
              "      <th>Weighted_Price</th>\n",
              "      <th>x</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Timestamp</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2013-04-01 00:00:00</th>\n",
              "      <td>95.373684</td>\n",
              "      <td>95.373684</td>\n",
              "      <td>95.373684</td>\n",
              "      <td>95.373684</td>\n",
              "      <td>2.154868</td>\n",
              "      <td>206.698263</td>\n",
              "      <td>95.373684</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-01 01:00:00</th>\n",
              "      <td>95.463158</td>\n",
              "      <td>95.463158</td>\n",
              "      <td>95.463158</td>\n",
              "      <td>95.463158</td>\n",
              "      <td>2.311941</td>\n",
              "      <td>221.801368</td>\n",
              "      <td>95.463158</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-01 02:00:00</th>\n",
              "      <td>95.552632</td>\n",
              "      <td>95.552632</td>\n",
              "      <td>95.552632</td>\n",
              "      <td>95.552632</td>\n",
              "      <td>2.469013</td>\n",
              "      <td>236.904474</td>\n",
              "      <td>95.552632</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-01 03:00:00</th>\n",
              "      <td>95.642105</td>\n",
              "      <td>95.642105</td>\n",
              "      <td>95.642105</td>\n",
              "      <td>95.642105</td>\n",
              "      <td>2.626086</td>\n",
              "      <td>252.007579</td>\n",
              "      <td>95.642105</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-01 04:00:00</th>\n",
              "      <td>95.731579</td>\n",
              "      <td>95.731579</td>\n",
              "      <td>95.731579</td>\n",
              "      <td>95.731579</td>\n",
              "      <td>2.783158</td>\n",
              "      <td>267.110684</td>\n",
              "      <td>95.731579</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-30 20:00:00</th>\n",
              "      <td>59118.910000</td>\n",
              "      <td>59135.000000</td>\n",
              "      <td>59023.960000</td>\n",
              "      <td>59037.790000</td>\n",
              "      <td>2.684344</td>\n",
              "      <td>158619.612840</td>\n",
              "      <td>59090.651748</td>\n",
              "      <td>70100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-30 21:00:00</th>\n",
              "      <td>58644.040000</td>\n",
              "      <td>58670.910000</td>\n",
              "      <td>58591.060000</td>\n",
              "      <td>58591.060000</td>\n",
              "      <td>9.979587</td>\n",
              "      <td>585005.680240</td>\n",
              "      <td>58620.228970</td>\n",
              "      <td>70101.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-30 22:00:00</th>\n",
              "      <td>58758.440000</td>\n",
              "      <td>58762.560000</td>\n",
              "      <td>58758.440000</td>\n",
              "      <td>58762.560000</td>\n",
              "      <td>0.573484</td>\n",
              "      <td>33699.187441</td>\n",
              "      <td>58762.179612</td>\n",
              "      <td>70102.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-30 23:00:00</th>\n",
              "      <td>58699.430000</td>\n",
              "      <td>58699.430000</td>\n",
              "      <td>58643.160000</td>\n",
              "      <td>58657.090000</td>\n",
              "      <td>0.281702</td>\n",
              "      <td>16521.438155</td>\n",
              "      <td>58648.607194</td>\n",
              "      <td>70103.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-31 00:00:00</th>\n",
              "      <td>58767.750000</td>\n",
              "      <td>58778.180000</td>\n",
              "      <td>58755.970000</td>\n",
              "      <td>58778.180000</td>\n",
              "      <td>2.712831</td>\n",
              "      <td>159417.751000</td>\n",
              "      <td>58764.349363</td>\n",
              "      <td>70104.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>70105 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                             Open          High  ...  Weighted_Price        x\n",
              "Timestamp                                        ...                         \n",
              "2013-04-01 00:00:00     95.373684     95.373684  ...       95.373684      0.0\n",
              "2013-04-01 01:00:00     95.463158     95.463158  ...       95.463158      1.0\n",
              "2013-04-01 02:00:00     95.552632     95.552632  ...       95.552632      2.0\n",
              "2013-04-01 03:00:00     95.642105     95.642105  ...       95.642105      3.0\n",
              "2013-04-01 04:00:00     95.731579     95.731579  ...       95.731579      4.0\n",
              "...                           ...           ...  ...             ...      ...\n",
              "2021-03-30 20:00:00  59118.910000  59135.000000  ...    59090.651748  70100.0\n",
              "2021-03-30 21:00:00  58644.040000  58670.910000  ...    58620.228970  70101.0\n",
              "2021-03-30 22:00:00  58758.440000  58762.560000  ...    58762.179612  70102.0\n",
              "2021-03-30 23:00:00  58699.430000  58699.430000  ...    58648.607194  70103.0\n",
              "2021-03-31 00:00:00  58767.750000  58778.180000  ...    58764.349363  70104.0\n",
              "\n",
              "[70105 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwOeFLtLSPnv"
      },
      "source": [
        "**2. Détection des anomalies dans la série \"horaire\"**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1joYv2Kd7Js"
      },
      "source": [
        "Les anomalies sont fréquentes dans les séries temporelles, et la performance des prédictions est souvent améliorée lorsque ces anomalies sont traitées.  \n",
        "Pour avoir un apperçu de ces éventuelles anomalies, nous allons utiliser la méthode [\"Isolation Forest\"](https://scikit-learn.org/stable/modules/outlier_detection.html#isolation-forest) disponnible dans Scikit-learn.  \n",
        "\n",
        "Les paramètres utilisés sont les suivants :\n",
        " - **n_estimators** : C'est le nombre de sous-groupes d'échantillons à utiliser. Une valeur de 128 ou 256 est préconnisée dans le document de recherche.\n",
        " - **max_samples** : C'est le nombre d'échantillons maximum à utiliser. Nous utiliserons l'ensemble des échantillons.\n",
        " - **max_features** :  C'est le nombre de motifs aléatoirement choisis sur chaque noeud de l'arbre. Nous choisirons un seul motif.\n",
        " - **contamination** : C'est le pourcentage estimé d'anomalies dans les données. Ce paramètre permet de régler la sensibilité de l'algorithme. On va commencer avec 5% et affiner si nécessaire par la suite."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHag65S4dH7x"
      },
      "source": [
        "# Initialise le modèle\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "clf = IsolationForest(n_estimators=256,max_samples=serie_etude['Open'].size, contamination=0.05,max_features=1, verbose=1)\n",
        "clf.fit(serie_etude['Open'].values.reshape(-1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAPFfAaffb4h"
      },
      "source": [
        "# Réalise les prédictions\n",
        "pred = clf.predict(serie_etude['Open'].values.reshape(-1,1))\n",
        "pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU0TN1UEBqR2"
      },
      "source": [
        "On ajoute maintenant ces informations dans la série journalière et on affiche les informations :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWg0uUb9G5Ws"
      },
      "source": [
        "# Ajoute une colonne \"Anomalie\" dans la série\n",
        "serie_etude['Anomalies']=pred\n",
        "serie_etude['Anomalies'] = serie_etude['Anomalies'].apply(lambda x: 1 if (x==-1) else 0)\n",
        "serie_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "105qNoy1EwWd"
      },
      "source": [
        "# Affiche les informations sur les anomalies\n",
        "print(serie_etude['Anomalies'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaV_MfJyFXkF"
      },
      "source": [
        "**3. Affichage des anomalies sur le graphique**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2idYKYImFh8v"
      },
      "source": [
        "# Affiche la série\n",
        "\n",
        "fig = px.line(x=serie_etude.index,y=serie_etude['Open'],title=\"Evolution du prix du BTC\")\n",
        "fig.add_trace(px.scatter(x=serie_etude.index,y=serie_etude['Anomalies']*serie_etude['Open'],color=serie_etude['Anomalies'].astype(np.bool)).data[0])\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVAWVQioe-kS"
      },
      "source": [
        "Comme les anomalies détectées ne sembles pas cohérentes, nous n'allons pas les traiter..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vDYEK-cxE0C"
      },
      "source": [
        "# Analyse de la série"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGY4fCB3xdUx"
      },
      "source": [
        "serie_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBCbjPSNxWXy"
      },
      "source": [
        "**1. ACF & PACF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CleN4htOxYqZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "16feb3f1-1a24-48e0-fb93-2e8ceafede31"
      },
      "source": [
        "# ACF & PACF du bruit blanc\n",
        "\n",
        "serie = serie_etude['Open']\n",
        "\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "f1, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "f1.subplots_adjust(hspace=0.3,wspace=0.2)\n",
        "\n",
        "plot_acf(serie, ax=ax1, lags = range(0,50))\n",
        "ax1.set_title(\"Autocorrélation du bruit blanc\")\n",
        "\n",
        "plot_pacf(serie, ax=ax2, lags = range(0, 50))\n",
        "ax2.set_title(\"Autocorrélation partielle du bruit blanc\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning:\n",
            "\n",
            "pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Autocorrélation partielle du bruit blanc')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o66MqHZTyHX5"
      },
      "source": [
        "**2. Test de Dickey-Fuller**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm5VXMhkyLSN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa4e35d9-39c0-425a-c199-7380d48d6261"
      },
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "serie_test = serie\n",
        "\n",
        "adf, p, usedlag, nobs, cvs, aic = sm.tsa.stattools.adfuller(serie_test)\n",
        "\n",
        "adf_results_string = 'ADF: {}\\np-value: {},\\nN: {}, \\ncritical values: {}'\n",
        "print(adf_results_string.format(adf, p, nobs, cvs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ADF: 5.319872512211358\n",
            "p-value: 1.0,\n",
            "N: 70044, \n",
            "critical values: {'1%': -3.430443363309713, '5%': -2.8615812649257024, '10%': -2.566791963909876}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moHyQgGfyTi4"
      },
      "source": [
        "**3. Suppression de la tendance non linéaire et test de sationnarité**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43AcGds6y0pI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "b88fe525-de41-4852-f185-701a92506933"
      },
      "source": [
        "from scipy.stats import boxcox\n",
        "\n",
        "serie_log, lam = boxcox(serie)\n",
        "\n",
        "f1, (ax1,ax2) = plt.subplots(2, 1, figsize=(15, 5))\n",
        "ax1.plot(serie_etude.index,serie_log)\n",
        "ax2.plot(serie_etude.index,serie)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f1511cc7210>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fRPXCkO0DUh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79c9ecfb-2647-44f2-d0fa-a95d85eba0dd"
      },
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "serie_test = serie_log\n",
        "\n",
        "adf, p, usedlag, nobs, cvs, aic = sm.tsa.stattools.adfuller(serie_test)\n",
        "\n",
        "adf_results_string = 'ADF: {}\\np-value: {},\\nN: {}, \\ncritical values: {}'\n",
        "print(adf_results_string.format(adf, p, nobs, cvs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ADF: -0.007269082976458623\n",
            "p-value: 0.9579267936566662,\n",
            "N: 70043, \n",
            "critical values: {'1%': -3.430443364642705, '5%': -2.8615812655148516, '10%': -2.566791964223462}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI8sp_Rlz6GT"
      },
      "source": [
        "***4. Suppression de la tendance linéaire et test de stationnarité***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iHrAWJH0TdT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "393de21f-8c27-46df-af62-0c3d31ccdb04"
      },
      "source": [
        "f1, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 5))\n",
        "\n",
        "# Calcul des coefficients\n",
        "x = np.linspace(0,len(serie_log),len(serie_log))\n",
        "coefs = np.polyfit(x,serie_log,1)\n",
        "\n",
        "# Calcul de la tendance non linéaire\n",
        "trend = coefs[0]*np.power(x,1) + coefs[1]\n",
        "\n",
        "# Calcul de la série sans tendance\n",
        "serie_log_detrend = serie_log - trend\n",
        "\n",
        "# Affiche les résultats\n",
        "ax1.plot(trend)\n",
        "ax1.plot(serie_log)\n",
        "ax1.set_title(\"Série originale et tendance non linéaire\")\n",
        "\n",
        "ax2.plot(serie_log_detrend)\n",
        "ax2.set_title(\"Série avec tendance non linéaire supprimée\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Série avec tendance non linéaire supprimée')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNXs9Fm--Kcl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "04f3e13c-4b01-47ed-db22-fa67cc5cdb5f"
      },
      "source": [
        "# ACF & PACF du bruit blanc\n",
        "\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "f1, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "f1.subplots_adjust(hspace=0.3,wspace=0.2)\n",
        "\n",
        "plot_acf(serie_log_detrend, ax=ax1, lags = range(0,50))\n",
        "ax1.set_title(\"Autocorrélation du bruit blanc\")\n",
        "\n",
        "plot_pacf(serie_log_detrend, ax=ax2, lags = range(0, 50))\n",
        "ax2.set_title(\"Autocorrélation partielle du bruit blanc\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Autocorrélation partielle du bruit blanc')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_r01cDgq0xaJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d319333-5468-4013-a2cd-245c35f5e9f0"
      },
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "serie_test = serie_log_detrend\n",
        "\n",
        "adf, p, usedlag, nobs, cvs, aic = sm.tsa.stattools.adfuller(serie_test)\n",
        "\n",
        "adf_results_string = 'ADF: {}\\np-value: {},\\nN: {}, \\ncritical values: {}'\n",
        "print(adf_results_string.format(adf, p, nobs, cvs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ADF: -1.6379667449120026\n",
            "p-value: 0.46332342415130157,\n",
            "N: 70043, \n",
            "critical values: {'1%': -3.430443364642705, '5%': -2.8615812655148516, '10%': -2.566791964223462}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ychdf1RxMPDD"
      },
      "source": [
        "**5. Différentiation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qaHkgtQMOqJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "2d8fb633-a894-4950-e39f-c5d6c5f4f43c"
      },
      "source": [
        "# Différenciation d'odre 1 et saisonnale à l'odre 1 et de période 12\n",
        "\n",
        "from statsmodels.tsa.statespace.tools import diff\n",
        "\n",
        "serie_log_detrend_diff1 = diff(serie_log_detrend,1)       # diff=1 ; diff_saison=1 ; periode = 12\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(serie_log_detrend_diff1)\n",
        "plt.title(\"Signal différencié d'ordre 1 + saisonalité\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFLlzv0JMks5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0767b81d-c518-401d-90b4-ef5d7bfdcf27"
      },
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "serie_test = serie_log_detrend_diff1\n",
        "\n",
        "adf, p, usedlag, nobs, cvs, aic = sm.tsa.stattools.adfuller(serie_test)\n",
        "\n",
        "adf_results_string = 'ADF: {}\\np-value: {},\\nN: {}, \\ncritical values: {}'\n",
        "print(adf_results_string.format(adf, p, nobs, cvs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ADF: -35.69789536061268\n",
            "p-value: 0.0,\n",
            "N: 70041, \n",
            "critical values: {'1%': -3.430443367308802, '5%': -2.861581266693201, '10%': -2.56679196485066}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0Oqd_7XMqaZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "6be6167f-838b-44b1-a04f-fb29244201b8"
      },
      "source": [
        "# ACF & PACF du bruit blanc\n",
        "\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "f1, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "f1.subplots_adjust(hspace=0.3,wspace=0.2)\n",
        "\n",
        "plot_acf(serie_log_detrend_diff1, ax=ax1, lags = range(0,50))\n",
        "ax1.set_title(\"Autocorrélation du bruit blanc\")\n",
        "\n",
        "plot_pacf(serie_log_detrend_diff1, ax=ax2, lags = range(0, 50))\n",
        "ax2.set_title(\"Autocorrélation partielle du bruit blanc\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Autocorrélation partielle du bruit blanc')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5oIY2Yl5Tlt"
      },
      "source": [
        "**5. Enregistrement des données dans le dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjFSWhdeM4KM"
      },
      "source": [
        "serie_log_detrend_diff1 = np.insert(serie_log_detrend_diff1,0,0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ele3kFOp5TTW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "outputId": "a2758688-45d0-4f9d-c406-e9dc9274fe20"
      },
      "source": [
        "serie_etude['diff'] = serie_log_detrend_diff1\n",
        "serie_etude['diff'][0] = \"Nan\"\n",
        "serie_etude"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume_(BTC)</th>\n",
              "      <th>Volume_(Currency)</th>\n",
              "      <th>Weighted_Price</th>\n",
              "      <th>x</th>\n",
              "      <th>diff</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Timestamp</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2013-04-01 00:00:00</th>\n",
              "      <td>95.373684</td>\n",
              "      <td>95.373684</td>\n",
              "      <td>95.373684</td>\n",
              "      <td>95.373684</td>\n",
              "      <td>2.154868</td>\n",
              "      <td>206.698263</td>\n",
              "      <td>95.373684</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-01 01:00:00</th>\n",
              "      <td>95.463158</td>\n",
              "      <td>95.463158</td>\n",
              "      <td>95.463158</td>\n",
              "      <td>95.463158</td>\n",
              "      <td>2.311941</td>\n",
              "      <td>221.801368</td>\n",
              "      <td>95.463158</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-01 02:00:00</th>\n",
              "      <td>95.552632</td>\n",
              "      <td>95.552632</td>\n",
              "      <td>95.552632</td>\n",
              "      <td>95.552632</td>\n",
              "      <td>2.469013</td>\n",
              "      <td>236.904474</td>\n",
              "      <td>95.552632</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.000813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-01 03:00:00</th>\n",
              "      <td>95.642105</td>\n",
              "      <td>95.642105</td>\n",
              "      <td>95.642105</td>\n",
              "      <td>95.642105</td>\n",
              "      <td>2.626086</td>\n",
              "      <td>252.007579</td>\n",
              "      <td>95.642105</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.000812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-04-01 04:00:00</th>\n",
              "      <td>95.731579</td>\n",
              "      <td>95.731579</td>\n",
              "      <td>95.731579</td>\n",
              "      <td>95.731579</td>\n",
              "      <td>2.783158</td>\n",
              "      <td>267.110684</td>\n",
              "      <td>95.731579</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-30 20:00:00</th>\n",
              "      <td>59118.910000</td>\n",
              "      <td>59135.000000</td>\n",
              "      <td>59023.960000</td>\n",
              "      <td>59037.790000</td>\n",
              "      <td>2.684344</td>\n",
              "      <td>158619.612840</td>\n",
              "      <td>59090.651748</td>\n",
              "      <td>70100.0</td>\n",
              "      <td>0.003735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-30 21:00:00</th>\n",
              "      <td>58644.040000</td>\n",
              "      <td>58670.910000</td>\n",
              "      <td>58591.060000</td>\n",
              "      <td>58591.060000</td>\n",
              "      <td>9.979587</td>\n",
              "      <td>585005.680240</td>\n",
              "      <td>58620.228970</td>\n",
              "      <td>70101.0</td>\n",
              "      <td>-0.007035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-30 22:00:00</th>\n",
              "      <td>58758.440000</td>\n",
              "      <td>58762.560000</td>\n",
              "      <td>58758.440000</td>\n",
              "      <td>58762.560000</td>\n",
              "      <td>0.573484</td>\n",
              "      <td>33699.187441</td>\n",
              "      <td>58762.179612</td>\n",
              "      <td>70102.0</td>\n",
              "      <td>0.001615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-30 23:00:00</th>\n",
              "      <td>58699.430000</td>\n",
              "      <td>58699.430000</td>\n",
              "      <td>58643.160000</td>\n",
              "      <td>58657.090000</td>\n",
              "      <td>0.281702</td>\n",
              "      <td>16521.438155</td>\n",
              "      <td>58648.607194</td>\n",
              "      <td>70103.0</td>\n",
              "      <td>-0.000936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-31 00:00:00</th>\n",
              "      <td>58767.750000</td>\n",
              "      <td>58778.180000</td>\n",
              "      <td>58755.970000</td>\n",
              "      <td>58778.180000</td>\n",
              "      <td>2.712831</td>\n",
              "      <td>159417.751000</td>\n",
              "      <td>58764.349363</td>\n",
              "      <td>70104.0</td>\n",
              "      <td>0.000936</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>70105 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                             Open          High  ...        x      diff\n",
              "Timestamp                                        ...                   \n",
              "2013-04-01 00:00:00     95.373684     95.373684  ...      0.0       NaN\n",
              "2013-04-01 01:00:00     95.463158     95.463158  ...      1.0  0.000814\n",
              "2013-04-01 02:00:00     95.552632     95.552632  ...      2.0  0.000813\n",
              "2013-04-01 03:00:00     95.642105     95.642105  ...      3.0  0.000812\n",
              "2013-04-01 04:00:00     95.731579     95.731579  ...      4.0  0.000811\n",
              "...                           ...           ...  ...      ...       ...\n",
              "2021-03-30 20:00:00  59118.910000  59135.000000  ...  70100.0  0.003735\n",
              "2021-03-30 21:00:00  58644.040000  58670.910000  ...  70101.0 -0.007035\n",
              "2021-03-30 22:00:00  58758.440000  58762.560000  ...  70102.0  0.001615\n",
              "2021-03-30 23:00:00  58699.430000  58699.430000  ...  70103.0 -0.000936\n",
              "2021-03-31 00:00:00  58767.750000  58778.180000  ...  70104.0  0.000936\n",
              "\n",
              "[70105 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3q8CAozN-Gt"
      },
      "source": [
        "# Prépartion des datasets diff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWrUXYyFN-Gu"
      },
      "source": [
        "serie_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp724KcgN-Gw"
      },
      "source": [
        "**1. Séparation des données en données pour l'entrainement et la validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIWQZUvVN-Gw"
      },
      "source": [
        "On réserve 20% des données pour l'entrainement et le reste pour la validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZaGwL4YN-Gx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a994dcf3-b882-4f74-e3e5-e6e16193154f"
      },
      "source": [
        "# Sépare les données en entrainement et tests\n",
        "pourcentage = 0.8\n",
        "temps_separation = int(len(serie_etude) * pourcentage)\n",
        "date_separation = serie_etude.index[temps_separation]\n",
        "\n",
        "serie_entrainement = serie_etude['diff'].iloc[1:temps_separation]\n",
        "serie_test = serie_etude['diff'].iloc[temps_separation:]\n",
        "\n",
        "print(\"Taille de l'entrainement : %d\" %len(serie_entrainement))\n",
        "print(\"Taille de la validation : %d\" %len(serie_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Taille de l'entrainement : 56083\n",
            "Taille de la validation : 14021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-YXKb5TN-Gy"
      },
      "source": [
        "On normalise les données :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcszCbQQN-Gy"
      },
      "source": [
        "# Calcul de la moyenne et de l'écart type de la série\n",
        "mean = tf.math.reduce_mean(np.asarray(serie_entrainement))\n",
        "std = tf.math.reduce_std(np.asarray((serie_entrainement)))\n",
        "\n",
        "# Normalisation des données\n",
        "serie_entrainement = (serie_entrainement-mean)/std\n",
        "serie_test = (serie_test-mean)/std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixzH25StN-Gz"
      },
      "source": [
        "# Affiche la série\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "ax.plot(serie_entrainement, label=\"Entrainement\")\n",
        "ax.plot(serie_test,label=\"Validation\")\n",
        "\n",
        "ax.set_title(\"Evolution du prix du BTC\")\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsIJ_4OON-Gz"
      },
      "source": [
        "**2. Création des datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlwKgWqiN-G0"
      },
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "\n",
        "def prepare_dataset_XY(serie, taille_fenetre, horizon, batch_size):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie)\n",
        "  dataset = dataset.window(taille_fenetre+horizon, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre + horizon))\n",
        "  dataset = dataset.map(lambda x: (tf.expand_dims(x[0:taille_fenetre],axis=1),x[-1:]))\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9qP2YSBN-G1"
      },
      "source": [
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "taille_fenetre = 500\n",
        "horizon = 1\n",
        "batch_size = 100\n",
        "\n",
        "# Création du dataset\n",
        "dataset = prepare_dataset_XY(serie_entrainement,taille_fenetre,horizon,batch_size)\n",
        "dataset_val = prepare_dataset_XY(serie_test,taille_fenetre,horizon,batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFRs-cLHN-HA"
      },
      "source": [
        "print(len(list(dataset.as_numpy_iterator())))\n",
        "for element in dataset.take(1):\n",
        "  print(element[0].shape)\n",
        "  print(element[1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ti9mbzgN-HA"
      },
      "source": [
        "On extrait maintenant les deux tenseurs (X,Y) pour l'entrainement :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIwGFoF7N-HB"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "#56x((1000,4,1),(1000,1)) => (56*1000,4,1) ; (56*1000,1)\n",
        "\n",
        "x,y = tuple(zip(*dataset))\n",
        "\n",
        "# Recombine les données\n",
        "# (56,1000,4,1) => (56*128,4,1)\n",
        "# (56,1000,1) => (56*128,1)\n",
        "x_train = np.asarray(tf.reshape(np.asarray(x,dtype=np.float32),shape=(np.asarray(x).shape[0]*np.asarray(x).shape[1],taille_fenetre,1)))\n",
        "y_train = np.asarray(tf.reshape(np.asarray(y,dtype=np.float32),shape=(np.asarray(y).shape[0]*np.asarray(y).shape[1])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4-eznwiN-HC"
      },
      "source": [
        "Puis la même chose pour les données de validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e25gJ8ymN-HC"
      },
      "source": [
        "# Extrait les X,Y du dataset_val\n",
        "\n",
        "x,y = tuple(zip(*dataset_val))\n",
        "\n",
        "# Recombine les données\n",
        "\n",
        "x_val = np.asarray(tf.reshape(np.asarray(x,dtype=np.float32),shape=(np.asarray(x).shape[0]*np.asarray(x).shape[1],taille_fenetre,1)))\n",
        "y_val = np.asarray(tf.reshape(np.asarray(y,dtype=np.float32),shape=(np.asarray(y).shape[0]*np.asarray(y).shape[1])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG5Fyx5O5oe5"
      },
      "source": [
        "# Prépartion des datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tShkj2wRIp6a"
      },
      "source": [
        "serie_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hVq4bTVIhYN"
      },
      "source": [
        "serie_etude['Open'] = serie_etude['Open']*serie_etude['Volume_(BTC)']\n",
        "serie_etude['Open'] = serie_etude.interpolate(method=\"slinear\")\n",
        "serie_etude['Open'] = serie_etude.fillna(method=\"backfill\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7cGUeWb5oe7"
      },
      "source": [
        "**1. Séparation des données en données pour l'entrainement et la validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob-EAw_j5oe8"
      },
      "source": [
        "On réserve 20% des données pour l'entrainement et le reste pour la validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5AWeK_Z5oe8"
      },
      "source": [
        "# Sépare les données en entrainement et tests\n",
        "pourcentage = 0.8\n",
        "temps_separation = int(len(serie_etude) * pourcentage)\n",
        "date_separation = serie_etude.index[temps_separation]\n",
        "\n",
        "serie_entrainement = serie_etude['Open'].iloc[:temps_separation]\n",
        "serie_test = serie_etude['Open'].iloc[temps_separation:]\n",
        "\n",
        "print(\"Taille de l'entrainement : %d\" %len(serie_entrainement))\n",
        "print(\"Taille de la validation : %d\" %len(serie_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZUMMMro5oe9"
      },
      "source": [
        "On normalise les données :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu_YxoSI5oe9"
      },
      "source": [
        "# Calcul de la moyenne et de l'écart type de la série\n",
        "mean = tf.math.reduce_mean(np.asarray(serie_entrainement))\n",
        "std = tf.math.reduce_std(np.asarray((serie_entrainement)))\n",
        "\n",
        "# Normalisation des données\n",
        "serie_entrainement = (serie_entrainement-mean)/std\n",
        "serie_test = (serie_test-mean)/std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_4OZJ-p5oe9"
      },
      "source": [
        "# Affiche la série\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "ax.plot(serie_entrainement, label=\"Entrainement\")\n",
        "ax.plot(serie_test,label=\"Validation\")\n",
        "\n",
        "ax.set_title(\"Evolution du prix du BTC\")\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-bANnT35oe-"
      },
      "source": [
        "**2. Création des datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_EweLDJ5oe-"
      },
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "\n",
        "def prepare_dataset_XY(serie, taille_fenetre, horizon, batch_size):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie)\n",
        "  dataset = dataset.window(taille_fenetre+horizon, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre + horizon))\n",
        "  dataset = dataset.map(lambda x: (tf.expand_dims(x[0:taille_fenetre],axis=1),x[-1:]))\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Jh1RZYo5oe_"
      },
      "source": [
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "taille_fenetre = 50\n",
        "horizon = 1\n",
        "batch_size = 1000\n",
        "\n",
        "# Création du dataset\n",
        "dataset = prepare_dataset_XY(serie_entrainement,taille_fenetre,horizon,batch_size)\n",
        "dataset_val = prepare_dataset_XY(serie_test,taille_fenetre,horizon,batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr2pbMox5oe_"
      },
      "source": [
        "print(len(list(dataset.as_numpy_iterator())))\n",
        "for element in dataset.take(1):\n",
        "  print(element[0].shape)\n",
        "  print(element[1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHCcYn6i5oe_"
      },
      "source": [
        "On extrait maintenant les deux tenseurs (X,Y) pour l'entrainement :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3ZhLIK15ofA"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "#56x((1000,4,1),(1000,1)) => (56*1000,4,1) ; (56*1000,1)\n",
        "\n",
        "x,y = tuple(zip(*dataset))\n",
        "\n",
        "# Recombine les données\n",
        "# (56,1000,4,1) => (56*128,4,1)\n",
        "# (56,1000,1) => (56*128,1)\n",
        "x_train = np.asarray(tf.reshape(np.asarray(x,dtype=np.float32),shape=(np.asarray(x).shape[0]*np.asarray(x).shape[1],taille_fenetre,1)))\n",
        "y_train = np.asarray(tf.reshape(np.asarray(y,dtype=np.float32),shape=(np.asarray(y).shape[0]*np.asarray(y).shape[1])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llnKyLvl5ofA"
      },
      "source": [
        "Puis la même chose pour les données de validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hadrKVrZ5ofB"
      },
      "source": [
        "# Extrait les X,Y du dataset_val\n",
        "\n",
        "x,y = tuple(zip(*dataset_val))\n",
        "\n",
        "# Recombine les données\n",
        "\n",
        "x_val = np.asarray(tf.reshape(np.asarray(x,dtype=np.float32),shape=(np.asarray(x).shape[0]*np.asarray(x).shape[1],taille_fenetre,1)))\n",
        "y_val = np.asarray(tf.reshape(np.asarray(y,dtype=np.float32),shape=(np.asarray(y).shape[0]*np.asarray(y).shape[1])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Htl_PyCdqKMK"
      },
      "source": [
        "# Prépartion des datasets X/Y(log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RqAZOY-WM3Y"
      },
      "source": [
        "**1. Séparation des données en données pour l'entrainement et la validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QL65znv4N1l2"
      },
      "source": [
        "On réserve 20% des données pour l'entrainement et le reste pour la validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouWe7AKpNi6m"
      },
      "source": [
        "# Sépare les données en entrainement et tests\n",
        "pourcentage = 0.8\n",
        "temps_separation = int(len(serie_etude) * pourcentage)\n",
        "date_separation = serie_etude.index[temps_separation]\n",
        "\n",
        "serie_entrainement_x = serie_etude['Open'].iloc[:temps_separation]\n",
        "serie_entrainement_y = serie_etude['Open_log'].iloc[:temps_separation]\n",
        "\n",
        "serie_test_x = serie_etude['Open'].iloc[temps_separation:]\n",
        "serie_test_y = serie_etude['Open_log'].iloc[temps_separation:]\n",
        "\n",
        "print(\"Taille de l'entrainement : %d\" %len(serie_entrainement_x))\n",
        "print(\"Taille de la validation : %d\" %len(serie_test_x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFPjmI7-N3FH"
      },
      "source": [
        "On normalise les labels :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJI0xJlQNsTW"
      },
      "source": [
        "# Calcul de la moyenne et de l'écart type de la série\n",
        "mean = tf.math.reduce_mean(np.asarray(serie_entrainement_y))\n",
        "std = tf.math.reduce_std(np.asarray((serie_entrainement_y)))\n",
        "\n",
        "# Normalisation des données\n",
        "serie_entrainement_y = (serie_entrainement_y-mean)/std\n",
        "serie_test_y = (serie_test_y-mean)/std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXcAk4BjNyCp"
      },
      "source": [
        "# Affiche la série\n",
        "f1, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 5))\n",
        "\n",
        "ax1.plot(serie_entrainement_x, label=\"Entrainement\")\n",
        "ax1.plot(serie_test_x,label=\"Validation\")\n",
        "\n",
        "ax2.plot(serie_entrainement_y, label=\"Entrainement (log)\")\n",
        "ax2.plot(serie_test_y,label=\"Validation (log)\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Gewbn6bOPTH"
      },
      "source": [
        "**2. Création des datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hen7VrB260G_"
      },
      "source": [
        "def prepare_dataset_XY(serie_x,serie_y, taille_fenetre, horizon, batch_size):\n",
        "  dataset_x = tf.data.Dataset.from_tensor_slices(serie_x)\n",
        "  dataset_x = dataset_x.window(taille_fenetre, shift=1, drop_remainder=True)\n",
        "  dataset_x = dataset_x.flat_map(lambda x: x.batch(taille_fenetre))\n",
        "  dataset_x = dataset_x.map(lambda x: tf.expand_dims(x[0:taille_fenetre],axis=1))\n",
        "\n",
        "  dataset_y = tf.data.Dataset.from_tensor_slices(serie_y)\n",
        "  dataset_y = dataset_y.window(taille_fenetre+horizon, shift=1, drop_remainder=True)\n",
        "  dataset_y = dataset_y.flat_map(lambda x: x.batch(taille_fenetre+horizon))\n",
        "  dataset_y = dataset_y.map(lambda x: (x[-1:]))\n",
        "\n",
        "  dataset = tf.data.Dataset.zip((dataset_x,dataset_y))\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXDZCZF9-0_V"
      },
      "source": [
        "  dataset = dataset.map(lambda x: (tf.expand_dims(x[0:taille_fenetre],axis=1),x[-1:]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oltK9vwW7B1W"
      },
      "source": [
        "x = np.linspace(0,1000,1001)\n",
        "y = np.linspace(1001,2000,1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdh-lOfb7fcG"
      },
      "source": [
        "toto = prepare_dataset_XY(x,x,10,1,5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjZmE59v7lJG"
      },
      "source": [
        "print(len(list(toto.as_numpy_iterator())))\n",
        "for element in toto.take(1):\n",
        "  print(element)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BDjHf5NN--b"
      },
      "source": [
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "taille_fenetre = 50\n",
        "horizon = 1\n",
        "batch_size = 1000\n",
        "\n",
        "# Création du dataset\n",
        "dataset = prepare_dataset_XY(serie_entrainement_x,serie_entrainement_y,taille_fenetre,horizon,batch_size)\n",
        "dataset_val = prepare_dataset_XY(serie_test_x,serie_test_y,taille_fenetre,horizon,batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c_BOKBJQksv"
      },
      "source": [
        "print(len(list(dataset.as_numpy_iterator())))\n",
        "for element in dataset.take(1):\n",
        "  print(element[0].shape)\n",
        "  print(element[1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VpY_YzzQRy9"
      },
      "source": [
        "On extrait maintenant les deux tenseurs (X,Y) pour l'entrainement :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwEhjWtWOdFf"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "#56x((1000,50,1),(1000,1)) => (56*1000,50,1) ; (56*1000,1)\n",
        "\n",
        "x,y = tuple(zip(*dataset))\n",
        "\n",
        "# Recombine les données\n",
        "# (56,1000,50,1) => (56*1000,50,1)\n",
        "# (56,1000,1) => (56*1000,1)\n",
        "x_train = np.asarray(tf.reshape(np.asarray(x,dtype=np.float32),shape=(np.asarray(x).shape[0]*np.asarray(x).shape[1],taille_fenetre,1)))\n",
        "y_train = np.asarray(tf.reshape(np.asarray(y,dtype=np.float32),shape=(np.asarray(y).shape[0]*np.asarray(y).shape[1])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T70G6av-jV1r"
      },
      "source": [
        "Puis la même chose pour les données de validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNIUOBLJjVQr"
      },
      "source": [
        "# Extrait les X,Y du dataset_val\n",
        "\n",
        "x,y = tuple(zip(*dataset_val))\n",
        "\n",
        "# Recombine les données\n",
        "\n",
        "x_val = np.asarray(tf.reshape(np.asarray(x,dtype=np.float32),shape=(np.asarray(x).shape[0]*np.asarray(x).shape[1],taille_fenetre,1)))\n",
        "y_val = np.asarray(tf.reshape(np.asarray(y,dtype=np.float32),shape=(np.asarray(y).shape[0]*np.asarray(y).shape[1])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "083QISTMM3AM"
      },
      "source": [
        "# Optimisation des hyperparamètres"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VzGM7ODMf8e"
      },
      "source": [
        "**1. Création de la série horaire pour l'optimisation des hyperparamètres**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKW0cGrbMl7u"
      },
      "source": [
        "# Définition des dates de début et de fin\n",
        "\n",
        "date_debut = \"2020-01-01 00:00:00\"\n",
        "date_fin = \"2021-03-31 00:00:00\"\n",
        "\n",
        "serie_opti = serie_etude['Open'].loc[date_debut:date_fin].copy()\n",
        "serie_opti"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJy04evcNBST"
      },
      "source": [
        "# Affiche la série\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(serie_opti)\n",
        "plt.title(\"Evolution du prix du BTC\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K5rZtb0Nc8-"
      },
      "source": [
        "**1. Préparation des données**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP9rTvLRhrsq"
      },
      "source": [
        "On normalise les données :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYoSqpbuhFHQ"
      },
      "source": [
        "# Calcul de la moyenne et de l'écart type de la série\n",
        "mean = tf.math.reduce_mean(np.asarray(serie_opti))\n",
        "std = tf.math.reduce_std(np.asarray((serie_opti)))\n",
        "\n",
        "# Normalisation des données\n",
        "serie_opti = (serie_opti-mean)/std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AB4yOw4Ohwvw"
      },
      "source": [
        "**2. Création du dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCBTKbgkhzL-"
      },
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "\n",
        "def prepare_dataset_XY(serie, taille_fenetre, horizon, batch_size):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie)\n",
        "  dataset = dataset.window(taille_fenetre+horizon, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre + horizon))\n",
        "  dataset = dataset.map(lambda x: (tf.expand_dims(x[0:taille_fenetre],axis=1),x[-1:]))\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "  return dataset\n",
        "\n",
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "taille_fenetre = 4\n",
        "horizon = 1\n",
        "batch_size = 32\n",
        "\n",
        "# Création du dataset\n",
        "dataset = prepare_dataset_XY(serie_entrainement,taille_fenetre,horizon,batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyzkXx5eh_To"
      },
      "source": [
        "print(len(list(dataset.as_numpy_iterator())))\n",
        "for element in dataset.take(1):\n",
        "  print(element[0].shape)\n",
        "  print(element[1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt06fAi0iz0m"
      },
      "source": [
        "On extriat maintenant les données X et les labels Y du dataset :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qt19cKylizLA"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "# 272x((32,4,1),(32,1)) => (272*32,4,1) ; (272*32,1)\n",
        "\n",
        "x,y = tuple(zip(*dataset))\n",
        "\n",
        "# Recombine les données\n",
        "# (272,32,4,1) => (272*32,4,1)\n",
        "# (272,32,1) => (272*32,1)\n",
        "x_train = np.asarray(tf.reshape(np.asarray(x,dtype=np.float32),shape=(np.asarray(x).shape[0]*np.asarray(x).shape[1],taille_fenetre,1)))\n",
        "y_train = np.asarray(tf.reshape(np.asarray(y,dtype=np.float32),shape=(np.asarray(y).shape[0]*np.asarray(y).shape[1],1)))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUpvJmTQiNk-"
      },
      "source": [
        "**3. Définition du modèle**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuAyb8pciUle"
      },
      "source": [
        "Dans le modèle, les paramètres dim_LSTM, l1_reg, l2_reg seront optimisés :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCFNM5i2iP0Q"
      },
      "source": [
        "def ModelLSTM(dim_LSTM = 10, l1_reg=0, l2_reg=0):\n",
        "\n",
        "  entrees = tf.keras.layers.Input(shape=(taille_fenetre,1))\n",
        "\n",
        "  # Encodeur\n",
        "  s_encodeur = tf.keras.layers.LSTM(dim_LSTM, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg,l2=l2_reg))(entrees)\n",
        "  \n",
        "  # Générateur\n",
        "  sortie = tf.keras.layers.Dense(1,kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg,l2=l2_reg))(s_encodeur)\n",
        "\n",
        "  # Construction du modèle\n",
        "  model = tf.keras.Model(entrees,sortie)\n",
        "  model.compile(loss='mse', optimizer='adam')\n",
        "  return(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYTuqrcYii9w"
      },
      "source": [
        "**4. Cross-validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVCNBdgcihuv"
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import KFold, TimeSeriesSplit, GridSearchCV\n",
        "\n",
        "# Définitions des paramètres\n",
        "dim_LSTM = [5,10,15,20,30,40]\n",
        "l1_reg = [0,0.001,0.01,0.1]\n",
        "l2_reg = [0,0.001,0.01,0.1]\n",
        "batch_size = [32]\n",
        "\n",
        "param_grid = {'dim_LSTM': dim_LSTM, 'l1_reg': l1_reg, 'l2_reg': l2_reg, 'batch_size': batch_size}\n",
        "param_grid = {'dim_LSTM': dim_LSTM, 'batch_size': batch_size}\n",
        "\n",
        "max_periodes = 5\n",
        "\n",
        "# Surveillance de l'entrainement\n",
        "es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=50, min_delta=1e-7, restore_best_weights=True)\n",
        "\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits = 5)\n",
        "model = KerasRegressor(build_fn=ModelLSTM, epochs=max_periodes, verbose=2)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=tscv, n_jobs=-1, verbose=3)\n",
        "\n",
        "grid_result = grid.fit(x_train, y_train,callbacks=[es])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWTWBh8DjJpP"
      },
      "source": [
        "# Affiche les résultats\n",
        "print(\"Meilleur résultat : %f avec %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params_ = grid_result.cv_results_['params']\n",
        "for mean, stdev, param_ in zip(means, stds, params_):\n",
        "  print(\"%f (%f) with %r\" % (mean, stdev, param_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHfiXLFZhrPs"
      },
      "source": [
        "# Création du modèle LSTM de type encodeur-décodeur"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd9AzWFtjnjs"
      },
      "source": [
        "**1. Création du réseau**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9OCzL7UjAhL"
      },
      "source": [
        "Par défaut, la dimension des vecteurs cachés est de 10 et aucune régularisation n'est utilisée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnFw_FPPhxiE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21184422-dc81-4c44-cd81-7bb90d83b026"
      },
      "source": [
        "dim_LSTM = 500\n",
        "l1_reg = 0.0\n",
        "l2_reg = 0.0\n",
        "\n",
        "# Définition de l'entrée du modèle\n",
        "entrees = tf.keras.layers.Input(shape=(taille_fenetre,1),batch_size=batch_size)\n",
        "\n",
        "# Encodeur\n",
        "s_encodeur = tf.keras.layers.LSTM(dim_LSTM, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg,l2=l2_reg),stateful=True)(entrees)\n",
        "\n",
        "# Décodeur\n",
        "s_decodeur = tf.keras.layers.Dense(dim_LSTM,activation=\"tanh\",kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg,l2=l2_reg))(s_encodeur)\n",
        "s_decodeur = tf.keras.layers.Concatenate()([s_decodeur,s_encodeur])\n",
        "\n",
        "# Générateur\n",
        "sortie = tf.keras.layers.Dense(1,kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg,l2=l2_reg))(s_decodeur)\n",
        "\n",
        "# Construction du modèle\n",
        "model = tf.keras.Model(entrees,sortie)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            [(100, 500, 1)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_5 (LSTM)                   (100, 500)           1004000     input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (100, 500)           250500      lstm_5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (100, 1000)          0           dense_8[0][0]                    \n",
            "                                                                 lstm_5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (100, 1)             1001        concatenate_4[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 1,255,501\n",
            "Trainable params: 1,255,501\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_azfJaeUo2nU"
      },
      "source": [
        "**2. Optimisation de l'apprentissage**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf3lwaQBnjxL"
      },
      "source": [
        "Pour accélérer le traitement des données, nous n'allons pas utiliser l'intégralité des données pendant la mise à jour du gradient, comme cela a été fait jusqu'à présent (en utilisant le dataset).  \n",
        "Cette fois-ci, nous allons forcer les mises à jour du gradient à se produire de manière moins fréquente en attribuant la valeur du batch_size à prendre en compte lors de la regression du modèle.  \n",
        "Pour cela, on utilise l'argument \"batch_size\" dans la méthode fit. En précisant un batch_size=1000, cela signifie que :\n",
        " - Sur notre total de 56000 échantillons, 56 seront utilisés pour les calculs du gradient\n",
        " - Il y aura également 56 itérations à chaque période.\n",
        "  \n",
        "    \n",
        "    \n",
        "Si nous avions pris le dataset comme entrée, nous aurions eu :\n",
        "- Un total de 56000 échantillons également\n",
        "- Chaque période aurait également pris 56 itérations pour se compléter\n",
        "- Mais 1000 échantillons auraient été utilisés pour le calcul du gradient, au lieu de 56 avec la méthode utilisée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6Z35rNWj5SA"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD()\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"mse\", optimizer=optimiseur, metrics=\"mse\")\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(x=x_train,y=y_train,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint],batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2aP9J3TkNGG"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 1])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZxCgpuYkQ2Q"
      },
      "source": [
        "# Chargement des poids sauvegardés\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmdbo23qkTKE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7abaa392-6ea9-46ff-bd82-960e6d111a08"
      },
      "source": [
        "max_periodes = 10000\n",
        "\n",
        "# Classe permettant d'arrêter l'entrainement si la variation\n",
        "# devient plus petite qu'une valeur à choisir sur un nombre\n",
        "# de périodes à choisir\n",
        "class StopTrain(keras.callbacks.Callback):\n",
        "    def __init__(self, delta=0.01,periodes=100, term=\"loss\", logs={}):\n",
        "      self.n_periodes = 0\n",
        "      self.periodes = periodes\n",
        "      self.loss_1 = 100\n",
        "      self.delta = delta\n",
        "      self.term = term\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "      diff_loss = abs(self.loss_1 - logs[self.term])\n",
        "      self.loss_1 = logs[self.term]\n",
        "      if (diff_loss < self.delta):\n",
        "        self.n_periodes = self.n_periodes + 1\n",
        "      else:\n",
        "        self.n_periodes = 0\n",
        "      if (self.n_periodes == self.periodes):\n",
        "        print(\"Arrêt de l'entrainement...\")\n",
        "        self.model.stop_training = True\n",
        "\n",
        "def  My_MSE(y_true,y_pred):\n",
        "  return(tf.keras.metrics.mse(y_true,y_pred)*std.numpy()+mean.numpy())\n",
        "  \n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.001,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0.01)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"mse\", optimizer=optimiseur, metrics=[\"mse\",My_MSE])\n",
        "\n",
        "# Entraine le modèle, avec une réduction des calculs du gradient\n",
        "#historique = model.fit(x=x_train,y=y_train,validation_data=(x_val,y_val), epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=1e-7,periodes = 10, term=\"My_MSE\")],batch_size=batch_size)\n",
        "\n",
        "# Entraine le modèle sans réduction de calculs\n",
        "historique = model.fit(dataset,validation_data=dataset_val, epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=1e-8,periodes = 10, term=\"val_My_MSE\")])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10000\n",
            "      6/Unknown - 2s 104ms/step - loss: 2.9279 - mse: 2.9279 - My_MSE: 0.0288WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0428s vs `on_train_batch_end` time: 0.0616s). Check your callbacks.\n",
            "555/555 [==============================] - 63s 111ms/step - loss: 1.0617 - mse: 1.0617 - My_MSE: 0.0105 - val_loss: 0.5896 - val_mse: 0.5896 - val_My_MSE: 0.0058\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.77471, saving model to poids_train.hdf5\n",
            "Epoch 2/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 1.0507 - mse: 1.0507 - My_MSE: 0.0103 - val_loss: 0.5893 - val_mse: 0.5893 - val_My_MSE: 0.0058\n",
            "\n",
            "Epoch 00002: loss improved from 0.77471 to 0.77075, saving model to poids_train.hdf5\n",
            "Epoch 3/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 1.0451 - mse: 1.0451 - My_MSE: 0.0103 - val_loss: 0.5879 - val_mse: 0.5879 - val_My_MSE: 0.0058\n",
            "\n",
            "Epoch 00003: loss improved from 0.77075 to 0.76795, saving model to poids_train.hdf5\n",
            "Epoch 4/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 1.0389 - mse: 1.0389 - My_MSE: 0.0102 - val_loss: 0.5871 - val_mse: 0.5871 - val_My_MSE: 0.0058\n",
            "\n",
            "Epoch 00004: loss improved from 0.76795 to 0.76564, saving model to poids_train.hdf5\n",
            "Epoch 5/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 1.0549 - mse: 1.0549 - My_MSE: 0.0104 - val_loss: 0.5903 - val_mse: 0.5903 - val_My_MSE: 0.0058\n",
            "\n",
            "Epoch 00005: loss did not improve from 0.76564\n",
            "Epoch 6/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 1.0327 - mse: 1.0327 - My_MSE: 0.0102 - val_loss: 0.5928 - val_mse: 0.5928 - val_My_MSE: 0.0058\n",
            "\n",
            "Epoch 00006: loss improved from 0.76564 to 0.76222, saving model to poids_train.hdf5\n",
            "Epoch 7/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 1.0252 - mse: 1.0252 - My_MSE: 0.0101 - val_loss: 0.5964 - val_mse: 0.5964 - val_My_MSE: 0.0059\n",
            "\n",
            "Epoch 00007: loss improved from 0.76222 to 0.75873, saving model to poids_train.hdf5\n",
            "Epoch 8/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 1.0201 - mse: 1.0201 - My_MSE: 0.0100 - val_loss: 0.5996 - val_mse: 0.5996 - val_My_MSE: 0.0059\n",
            "\n",
            "Epoch 00008: loss improved from 0.75873 to 0.75622, saving model to poids_train.hdf5\n",
            "Epoch 9/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 1.0157 - mse: 1.0157 - My_MSE: 0.0100 - val_loss: 0.6021 - val_mse: 0.6021 - val_My_MSE: 0.0059\n",
            "\n",
            "Epoch 00009: loss improved from 0.75622 to 0.75396, saving model to poids_train.hdf5\n",
            "Epoch 10/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 1.0114 - mse: 1.0114 - My_MSE: 0.0100 - val_loss: 0.6030 - val_mse: 0.6030 - val_My_MSE: 0.0059\n",
            "\n",
            "Epoch 00010: loss improved from 0.75396 to 0.75124, saving model to poids_train.hdf5\n",
            "Epoch 11/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 1.0082 - mse: 1.0082 - My_MSE: 0.0099 - val_loss: 0.6117 - val_mse: 0.6117 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00011: loss improved from 0.75124 to 0.74897, saving model to poids_train.hdf5\n",
            "Epoch 12/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.9997 - mse: 0.9997 - My_MSE: 0.0098 - val_loss: 0.6349 - val_mse: 0.6349 - val_My_MSE: 0.0063\n",
            "\n",
            "Epoch 00012: loss improved from 0.74897 to 0.74546, saving model to poids_train.hdf5\n",
            "Epoch 13/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.9938 - mse: 0.9938 - My_MSE: 0.0098 - val_loss: 0.6122 - val_mse: 0.6122 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00013: loss improved from 0.74546 to 0.74350, saving model to poids_train.hdf5\n",
            "Epoch 14/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.9887 - mse: 0.9887 - My_MSE: 0.0097 - val_loss: 0.6105 - val_mse: 0.6105 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00014: loss improved from 0.74350 to 0.74084, saving model to poids_train.hdf5\n",
            "Epoch 15/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.9785 - mse: 0.9785 - My_MSE: 0.0096 - val_loss: 0.6274 - val_mse: 0.6274 - val_My_MSE: 0.0062\n",
            "\n",
            "Epoch 00015: loss improved from 0.74084 to 0.73674, saving model to poids_train.hdf5\n",
            "Epoch 16/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.9788 - mse: 0.9788 - My_MSE: 0.0096 - val_loss: 0.6180 - val_mse: 0.6180 - val_My_MSE: 0.0061\n",
            "\n",
            "Epoch 00016: loss improved from 0.73674 to 0.73574, saving model to poids_train.hdf5\n",
            "Epoch 17/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.9655 - mse: 0.9655 - My_MSE: 0.0095 - val_loss: 0.6044 - val_mse: 0.6044 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00017: loss improved from 0.73574 to 0.73103, saving model to poids_train.hdf5\n",
            "Epoch 18/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.9673 - mse: 0.9673 - My_MSE: 0.0095 - val_loss: 0.6059 - val_mse: 0.6059 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00018: loss did not improve from 0.73103\n",
            "Epoch 19/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.9522 - mse: 0.9522 - My_MSE: 0.0094 - val_loss: 0.6264 - val_mse: 0.6264 - val_My_MSE: 0.0062\n",
            "\n",
            "Epoch 00019: loss improved from 0.73103 to 0.72426, saving model to poids_train.hdf5\n",
            "Epoch 20/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.9498 - mse: 0.9498 - My_MSE: 0.0094 - val_loss: 0.6189 - val_mse: 0.6189 - val_My_MSE: 0.0061\n",
            "\n",
            "Epoch 00020: loss improved from 0.72426 to 0.72330, saving model to poids_train.hdf5\n",
            "Epoch 21/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.9397 - mse: 0.9397 - My_MSE: 0.0093 - val_loss: 0.6259 - val_mse: 0.6259 - val_My_MSE: 0.0062\n",
            "\n",
            "Epoch 00021: loss improved from 0.72330 to 0.71952, saving model to poids_train.hdf5\n",
            "Epoch 22/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.9417 - mse: 0.9417 - My_MSE: 0.0093 - val_loss: 0.5954 - val_mse: 0.5954 - val_My_MSE: 0.0059\n",
            "\n",
            "Epoch 00022: loss did not improve from 0.71952\n",
            "Epoch 23/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.9322 - mse: 0.9322 - My_MSE: 0.0092 - val_loss: 0.6088 - val_mse: 0.6088 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00023: loss improved from 0.71952 to 0.71649, saving model to poids_train.hdf5\n",
            "Epoch 24/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.9377 - mse: 0.9377 - My_MSE: 0.0092 - val_loss: 0.6179 - val_mse: 0.6179 - val_My_MSE: 0.0061\n",
            "\n",
            "Epoch 00024: loss did not improve from 0.71649\n",
            "Epoch 25/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.9176 - mse: 0.9176 - My_MSE: 0.0090 - val_loss: 0.6116 - val_mse: 0.6116 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00025: loss improved from 0.71649 to 0.70962, saving model to poids_train.hdf5\n",
            "Epoch 26/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.9278 - mse: 0.9278 - My_MSE: 0.0091 - val_loss: 0.6061 - val_mse: 0.6061 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00026: loss did not improve from 0.70962\n",
            "Epoch 27/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.9106 - mse: 0.9106 - My_MSE: 0.0090 - val_loss: 0.6034 - val_mse: 0.6034 - val_My_MSE: 0.0059\n",
            "\n",
            "Epoch 00027: loss improved from 0.70962 to 0.70758, saving model to poids_train.hdf5\n",
            "Epoch 28/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.9005 - mse: 0.9005 - My_MSE: 0.0089 - val_loss: 0.6140 - val_mse: 0.6140 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00028: loss improved from 0.70758 to 0.70311, saving model to poids_train.hdf5\n",
            "Epoch 29/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.9023 - mse: 0.9023 - My_MSE: 0.0089 - val_loss: 0.5908 - val_mse: 0.5908 - val_My_MSE: 0.0058\n",
            "\n",
            "Epoch 00029: loss did not improve from 0.70311\n",
            "Epoch 30/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.8983 - mse: 0.8983 - My_MSE: 0.0088 - val_loss: 0.6052 - val_mse: 0.6052 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00030: loss improved from 0.70311 to 0.70163, saving model to poids_train.hdf5\n",
            "Epoch 31/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.8965 - mse: 0.8965 - My_MSE: 0.0088 - val_loss: 0.6038 - val_mse: 0.6038 - val_My_MSE: 0.0059\n",
            "\n",
            "Epoch 00031: loss did not improve from 0.70163\n",
            "Epoch 32/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.8925 - mse: 0.8925 - My_MSE: 0.0088 - val_loss: 0.6034 - val_mse: 0.6034 - val_My_MSE: 0.0059\n",
            "\n",
            "Epoch 00032: loss improved from 0.70163 to 0.70006, saving model to poids_train.hdf5\n",
            "Epoch 33/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.8811 - mse: 0.8811 - My_MSE: 0.0087 - val_loss: 0.6041 - val_mse: 0.6041 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00033: loss improved from 0.70006 to 0.69594, saving model to poids_train.hdf5\n",
            "Epoch 34/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.8846 - mse: 0.8846 - My_MSE: 0.0087 - val_loss: 0.6058 - val_mse: 0.6058 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00034: loss did not improve from 0.69594\n",
            "Epoch 35/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.8760 - mse: 0.8760 - My_MSE: 0.0086 - val_loss: 0.6089 - val_mse: 0.6089 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00035: loss improved from 0.69594 to 0.69275, saving model to poids_train.hdf5\n",
            "Epoch 36/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.8827 - mse: 0.8827 - My_MSE: 0.0087 - val_loss: 0.6074 - val_mse: 0.6074 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00036: loss did not improve from 0.69275\n",
            "Epoch 37/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.8651 - mse: 0.8651 - My_MSE: 0.0085 - val_loss: 0.6128 - val_mse: 0.6128 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00037: loss improved from 0.69275 to 0.68814, saving model to poids_train.hdf5\n",
            "Epoch 38/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.8672 - mse: 0.8672 - My_MSE: 0.0085 - val_loss: 0.6059 - val_mse: 0.6059 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00038: loss did not improve from 0.68814\n",
            "Epoch 39/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.8829 - mse: 0.8829 - My_MSE: 0.0087 - val_loss: 0.6052 - val_mse: 0.6052 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00039: loss did not improve from 0.68814\n",
            "Epoch 40/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.8582 - mse: 0.8582 - My_MSE: 0.0085 - val_loss: 0.6064 - val_mse: 0.6064 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00040: loss improved from 0.68814 to 0.68475, saving model to poids_train.hdf5\n",
            "Epoch 41/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.8517 - mse: 0.8517 - My_MSE: 0.0084 - val_loss: 0.6089 - val_mse: 0.6089 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00041: loss improved from 0.68475 to 0.68304, saving model to poids_train.hdf5\n",
            "Epoch 42/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.8423 - mse: 0.8423 - My_MSE: 0.0083 - val_loss: 0.6109 - val_mse: 0.6109 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00042: loss improved from 0.68304 to 0.67958, saving model to poids_train.hdf5\n",
            "Epoch 43/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.8497 - mse: 0.8497 - My_MSE: 0.0084 - val_loss: 0.6121 - val_mse: 0.6121 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00043: loss did not improve from 0.67958\n",
            "Epoch 44/10000\n",
            "555/555 [==============================] - 61s 110ms/step - loss: 0.8457 - mse: 0.8457 - My_MSE: 0.0083 - val_loss: 0.6084 - val_mse: 0.6084 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00044: loss did not improve from 0.67958\n",
            "Epoch 45/10000\n",
            "555/555 [==============================] - 61s 110ms/step - loss: 0.8395 - mse: 0.8395 - My_MSE: 0.0083 - val_loss: 0.6166 - val_mse: 0.6166 - val_My_MSE: 0.0061\n",
            "\n",
            "Epoch 00045: loss improved from 0.67958 to 0.67596, saving model to poids_train.hdf5\n",
            "Epoch 46/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.8345 - mse: 0.8345 - My_MSE: 0.0082 - val_loss: 0.6049 - val_mse: 0.6049 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00046: loss improved from 0.67596 to 0.67339, saving model to poids_train.hdf5\n",
            "Epoch 47/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.8344 - mse: 0.8344 - My_MSE: 0.0082 - val_loss: 0.6119 - val_mse: 0.6119 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00047: loss did not improve from 0.67339\n",
            "Epoch 48/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.8269 - mse: 0.8269 - My_MSE: 0.0081 - val_loss: 0.6069 - val_mse: 0.6069 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00048: loss improved from 0.67339 to 0.67062, saving model to poids_train.hdf5\n",
            "Epoch 49/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.8290 - mse: 0.8290 - My_MSE: 0.0082 - val_loss: 0.6083 - val_mse: 0.6083 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00049: loss did not improve from 0.67062\n",
            "Epoch 50/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.8194 - mse: 0.8194 - My_MSE: 0.0081 - val_loss: 0.6192 - val_mse: 0.6192 - val_My_MSE: 0.0061\n",
            "\n",
            "Epoch 00050: loss improved from 0.67062 to 0.66656, saving model to poids_train.hdf5\n",
            "Epoch 51/10000\n",
            "555/555 [==============================] - 61s 110ms/step - loss: 0.8242 - mse: 0.8242 - My_MSE: 0.0081 - val_loss: 0.6096 - val_mse: 0.6096 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00051: loss did not improve from 0.66656\n",
            "Epoch 52/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.8326 - mse: 0.8326 - My_MSE: 0.0082 - val_loss: 0.6142 - val_mse: 0.6142 - val_My_MSE: 0.0061\n",
            "\n",
            "Epoch 00052: loss did not improve from 0.66656\n",
            "Epoch 53/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.8165 - mse: 0.8165 - My_MSE: 0.0080 - val_loss: 0.6090 - val_mse: 0.6090 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00053: loss improved from 0.66656 to 0.66411, saving model to poids_train.hdf5\n",
            "Epoch 54/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.8098 - mse: 0.8098 - My_MSE: 0.0080 - val_loss: 0.6161 - val_mse: 0.6161 - val_My_MSE: 0.0061\n",
            "\n",
            "Epoch 00054: loss improved from 0.66411 to 0.66158, saving model to poids_train.hdf5\n",
            "Epoch 55/10000\n",
            "555/555 [==============================] - 61s 110ms/step - loss: 0.8111 - mse: 0.8111 - My_MSE: 0.0080 - val_loss: 0.6102 - val_mse: 0.6102 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00055: loss did not improve from 0.66158\n",
            "Epoch 56/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.8078 - mse: 0.8078 - My_MSE: 0.0080 - val_loss: 0.6099 - val_mse: 0.6099 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00056: loss improved from 0.66158 to 0.66073, saving model to poids_train.hdf5\n",
            "Epoch 57/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.8136 - mse: 0.8136 - My_MSE: 0.0080 - val_loss: 0.6229 - val_mse: 0.6229 - val_My_MSE: 0.0061\n",
            "\n",
            "Epoch 00057: loss did not improve from 0.66073\n",
            "Epoch 58/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.8108 - mse: 0.8108 - My_MSE: 0.0080 - val_loss: 0.6213 - val_mse: 0.6213 - val_My_MSE: 0.0061\n",
            "\n",
            "Epoch 00058: loss improved from 0.66073 to 0.66057, saving model to poids_train.hdf5\n",
            "Epoch 59/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.8035 - mse: 0.8035 - My_MSE: 0.0079 - val_loss: 0.6143 - val_mse: 0.6143 - val_My_MSE: 0.0061\n",
            "\n",
            "Epoch 00059: loss improved from 0.66057 to 0.65762, saving model to poids_train.hdf5\n",
            "Epoch 60/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.8015 - mse: 0.8015 - My_MSE: 0.0079 - val_loss: 0.6139 - val_mse: 0.6139 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00060: loss improved from 0.65762 to 0.65649, saving model to poids_train.hdf5\n",
            "Epoch 61/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.8035 - mse: 0.8035 - My_MSE: 0.0079 - val_loss: 0.6220 - val_mse: 0.6220 - val_My_MSE: 0.0061\n",
            "\n",
            "Epoch 00061: loss did not improve from 0.65649\n",
            "Epoch 62/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.8021 - mse: 0.8021 - My_MSE: 0.0079 - val_loss: 0.6056 - val_mse: 0.6056 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00062: loss improved from 0.65649 to 0.65644, saving model to poids_train.hdf5\n",
            "Epoch 63/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.8054 - mse: 0.8054 - My_MSE: 0.0079 - val_loss: 0.6197 - val_mse: 0.6197 - val_My_MSE: 0.0061\n",
            "\n",
            "Epoch 00063: loss did not improve from 0.65644\n",
            "Epoch 64/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.8004 - mse: 0.8004 - My_MSE: 0.0079 - val_loss: 0.6237 - val_mse: 0.6237 - val_My_MSE: 0.0061\n",
            "\n",
            "Epoch 00064: loss improved from 0.65644 to 0.65456, saving model to poids_train.hdf5\n",
            "Epoch 65/10000\n",
            "555/555 [==============================] - 61s 110ms/step - loss: 0.8034 - mse: 0.8034 - My_MSE: 0.0079 - val_loss: 0.6183 - val_mse: 0.6183 - val_My_MSE: 0.0061\n",
            "\n",
            "Epoch 00065: loss did not improve from 0.65456\n",
            "Epoch 66/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.8050 - mse: 0.8050 - My_MSE: 0.0079 - val_loss: 0.6118 - val_mse: 0.6118 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00066: loss did not improve from 0.65456\n",
            "Epoch 67/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7994 - mse: 0.7994 - My_MSE: 0.0079 - val_loss: 0.6155 - val_mse: 0.6155 - val_My_MSE: 0.0061\n",
            "\n",
            "Epoch 00067: loss improved from 0.65456 to 0.65404, saving model to poids_train.hdf5\n",
            "Epoch 68/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7923 - mse: 0.7923 - My_MSE: 0.0078 - val_loss: 0.6175 - val_mse: 0.6175 - val_My_MSE: 0.0061\n",
            "\n",
            "Epoch 00068: loss improved from 0.65404 to 0.65014, saving model to poids_train.hdf5\n",
            "Epoch 69/10000\n",
            "555/555 [==============================] - 61s 110ms/step - loss: 0.7861 - mse: 0.7861 - My_MSE: 0.0077 - val_loss: 0.6141 - val_mse: 0.6141 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00069: loss improved from 0.65014 to 0.64812, saving model to poids_train.hdf5\n",
            "Epoch 70/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7874 - mse: 0.7874 - My_MSE: 0.0078 - val_loss: 0.6170 - val_mse: 0.6170 - val_My_MSE: 0.0061\n",
            "\n",
            "Epoch 00070: loss did not improve from 0.64812\n",
            "Epoch 71/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7938 - mse: 0.7938 - My_MSE: 0.0078 - val_loss: 0.6133 - val_mse: 0.6133 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00071: loss did not improve from 0.64812\n",
            "Epoch 72/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.7966 - mse: 0.7966 - My_MSE: 0.0078 - val_loss: 0.6100 - val_mse: 0.6100 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00072: loss did not improve from 0.64812\n",
            "Epoch 73/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.7893 - mse: 0.7893 - My_MSE: 0.0078 - val_loss: 0.6219 - val_mse: 0.6219 - val_My_MSE: 0.0061\n",
            "\n",
            "Epoch 00073: loss did not improve from 0.64812\n",
            "Epoch 74/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.7857 - mse: 0.7857 - My_MSE: 0.0077 - val_loss: 0.6180 - val_mse: 0.6180 - val_My_MSE: 0.0061\n",
            "\n",
            "Epoch 00074: loss improved from 0.64812 to 0.64686, saving model to poids_train.hdf5\n",
            "Epoch 75/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7806 - mse: 0.7806 - My_MSE: 0.0077 - val_loss: 0.6144 - val_mse: 0.6144 - val_My_MSE: 0.0061\n",
            "\n",
            "Epoch 00075: loss improved from 0.64686 to 0.64469, saving model to poids_train.hdf5\n",
            "Epoch 76/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.7816 - mse: 0.7816 - My_MSE: 0.0077 - val_loss: 0.6161 - val_mse: 0.6161 - val_My_MSE: 0.0061\n",
            "\n",
            "Epoch 00076: loss did not improve from 0.64469\n",
            "Epoch 77/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7787 - mse: 0.7787 - My_MSE: 0.0077 - val_loss: 0.6092 - val_mse: 0.6092 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00077: loss improved from 0.64469 to 0.64417, saving model to poids_train.hdf5\n",
            "Epoch 78/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7848 - mse: 0.7848 - My_MSE: 0.0077 - val_loss: 0.6244 - val_mse: 0.6244 - val_My_MSE: 0.0062\n",
            "\n",
            "Epoch 00078: loss did not improve from 0.64417\n",
            "Epoch 79/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7787 - mse: 0.7787 - My_MSE: 0.0077 - val_loss: 0.6138 - val_mse: 0.6138 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00079: loss improved from 0.64417 to 0.64331, saving model to poids_train.hdf5\n",
            "Epoch 80/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.7821 - mse: 0.7821 - My_MSE: 0.0077 - val_loss: 0.6187 - val_mse: 0.6187 - val_My_MSE: 0.0061\n",
            "\n",
            "Epoch 00080: loss improved from 0.64331 to 0.64330, saving model to poids_train.hdf5\n",
            "Epoch 81/10000\n",
            "555/555 [==============================] - 61s 110ms/step - loss: 0.7736 - mse: 0.7736 - My_MSE: 0.0076 - val_loss: 0.6156 - val_mse: 0.6156 - val_My_MSE: 0.0061\n",
            "\n",
            "Epoch 00081: loss improved from 0.64330 to 0.64003, saving model to poids_train.hdf5\n",
            "Epoch 82/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7715 - mse: 0.7715 - My_MSE: 0.0076 - val_loss: 0.6134 - val_mse: 0.6134 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00082: loss did not improve from 0.64003\n",
            "Epoch 83/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7757 - mse: 0.7757 - My_MSE: 0.0076 - val_loss: 0.6231 - val_mse: 0.6231 - val_My_MSE: 0.0061\n",
            "\n",
            "Epoch 00083: loss did not improve from 0.64003\n",
            "Epoch 84/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7726 - mse: 0.7726 - My_MSE: 0.0076 - val_loss: 0.6128 - val_mse: 0.6128 - val_My_MSE: 0.0060\n",
            "\n",
            "Epoch 00084: loss improved from 0.64003 to 0.63942, saving model to poids_train.hdf5\n",
            "Epoch 85/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.7709 - mse: 0.7709 - My_MSE: 0.0076 - val_loss: 0.6222 - val_mse: 0.6222 - val_My_MSE: 0.0061\n",
            "\n",
            "Epoch 00085: loss improved from 0.63942 to 0.63881, saving model to poids_train.hdf5\n",
            "Epoch 86/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.7688 - mse: 0.7688 - My_MSE: 0.0076 - val_loss: 0.6186 - val_mse: 0.6186 - val_My_MSE: 0.0061\n",
            "\n",
            "Epoch 00086: loss improved from 0.63881 to 0.63753, saving model to poids_train.hdf5\n",
            "Epoch 87/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7678 - mse: 0.7678 - My_MSE: 0.0076 - val_loss: 0.6170 - val_mse: 0.6170 - val_My_MSE: 0.0061\n",
            "\n",
            "Epoch 00087: loss improved from 0.63753 to 0.63662, saving model to poids_train.hdf5\n",
            "Epoch 88/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.7652 - mse: 0.7652 - My_MSE: 0.0075 - val_loss: 0.6292 - val_mse: 0.6292 - val_My_MSE: 0.0062\n",
            "\n",
            "Epoch 00088: loss improved from 0.63662 to 0.63629, saving model to poids_train.hdf5\n",
            "Epoch 89/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7637 - mse: 0.7637 - My_MSE: 0.0075 - val_loss: 0.6158 - val_mse: 0.6158 - val_My_MSE: 0.0061\n",
            "\n",
            "Epoch 00089: loss improved from 0.63629 to 0.63402, saving model to poids_train.hdf5\n",
            "Epoch 90/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7652 - mse: 0.7652 - My_MSE: 0.0075 - val_loss: 0.6211 - val_mse: 0.6211 - val_My_MSE: 0.0061\n",
            "\n",
            "Epoch 00090: loss did not improve from 0.63402\n",
            "Epoch 91/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7622 - mse: 0.7622 - My_MSE: 0.0075 - val_loss: 0.6235 - val_mse: 0.6235 - val_My_MSE: 0.0061\n",
            "\n",
            "Epoch 00091: loss improved from 0.63402 to 0.63355, saving model to poids_train.hdf5\n",
            "Epoch 92/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7650 - mse: 0.7650 - My_MSE: 0.0075 - val_loss: 0.6143 - val_mse: 0.6143 - val_My_MSE: 0.0061\n",
            "\n",
            "Epoch 00092: loss did not improve from 0.63355\n",
            "Epoch 93/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7675 - mse: 0.7675 - My_MSE: 0.0076 - val_loss: 0.6339 - val_mse: 0.6339 - val_My_MSE: 0.0062\n",
            "\n",
            "Epoch 00093: loss did not improve from 0.63355\n",
            "Epoch 94/10000\n",
            "555/555 [==============================] - 61s 110ms/step - loss: 0.7630 - mse: 0.7630 - My_MSE: 0.0075 - val_loss: 0.6165 - val_mse: 0.6165 - val_My_MSE: 0.0061\n",
            "\n",
            "Epoch 00094: loss improved from 0.63355 to 0.63341, saving model to poids_train.hdf5\n",
            "Epoch 95/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7584 - mse: 0.7584 - My_MSE: 0.0075 - val_loss: 0.6175 - val_mse: 0.6175 - val_My_MSE: 0.0061\n",
            "\n",
            "Epoch 00095: loss improved from 0.63341 to 0.63172, saving model to poids_train.hdf5\n",
            "Epoch 96/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7576 - mse: 0.7576 - My_MSE: 0.0075 - val_loss: 0.6350 - val_mse: 0.6350 - val_My_MSE: 0.0063\n",
            "\n",
            "Epoch 00096: loss improved from 0.63172 to 0.63111, saving model to poids_train.hdf5\n",
            "Epoch 97/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7563 - mse: 0.7563 - My_MSE: 0.0074 - val_loss: 0.6156 - val_mse: 0.6156 - val_My_MSE: 0.0061\n",
            "\n",
            "Epoch 00097: loss improved from 0.63111 to 0.62948, saving model to poids_train.hdf5\n",
            "Epoch 98/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7534 - mse: 0.7534 - My_MSE: 0.0074 - val_loss: 0.6279 - val_mse: 0.6279 - val_My_MSE: 0.0062\n",
            "\n",
            "Epoch 00098: loss improved from 0.62948 to 0.62833, saving model to poids_train.hdf5\n",
            "Epoch 99/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7529 - mse: 0.7529 - My_MSE: 0.0074 - val_loss: 0.6329 - val_mse: 0.6329 - val_My_MSE: 0.0062\n",
            "\n",
            "Epoch 00099: loss improved from 0.62833 to 0.62799, saving model to poids_train.hdf5\n",
            "Epoch 100/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7531 - mse: 0.7531 - My_MSE: 0.0074 - val_loss: 0.6417 - val_mse: 0.6417 - val_My_MSE: 0.0063\n",
            "\n",
            "Epoch 00100: loss did not improve from 0.62799\n",
            "Epoch 101/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7525 - mse: 0.7525 - My_MSE: 0.0074 - val_loss: 0.6256 - val_mse: 0.6256 - val_My_MSE: 0.0062\n",
            "\n",
            "Epoch 00101: loss improved from 0.62799 to 0.62727, saving model to poids_train.hdf5\n",
            "Epoch 102/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.7556 - mse: 0.7556 - My_MSE: 0.0074 - val_loss: 0.6376 - val_mse: 0.6376 - val_My_MSE: 0.0063\n",
            "\n",
            "Epoch 00102: loss did not improve from 0.62727\n",
            "Epoch 103/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7537 - mse: 0.7537 - My_MSE: 0.0074 - val_loss: 0.6339 - val_mse: 0.6339 - val_My_MSE: 0.0062\n",
            "\n",
            "Epoch 00103: loss did not improve from 0.62727\n",
            "Epoch 104/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7491 - mse: 0.7491 - My_MSE: 0.0074 - val_loss: 0.6515 - val_mse: 0.6515 - val_My_MSE: 0.0064\n",
            "\n",
            "Epoch 00104: loss improved from 0.62727 to 0.62472, saving model to poids_train.hdf5\n",
            "Epoch 105/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7480 - mse: 0.7480 - My_MSE: 0.0074 - val_loss: 0.6459 - val_mse: 0.6459 - val_My_MSE: 0.0064\n",
            "\n",
            "Epoch 00105: loss improved from 0.62472 to 0.62414, saving model to poids_train.hdf5\n",
            "Epoch 106/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7482 - mse: 0.7482 - My_MSE: 0.0074 - val_loss: 0.6283 - val_mse: 0.6283 - val_My_MSE: 0.0062\n",
            "\n",
            "Epoch 00106: loss did not improve from 0.62414\n",
            "Epoch 107/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.7502 - mse: 0.7502 - My_MSE: 0.0074 - val_loss: 0.6644 - val_mse: 0.6644 - val_My_MSE: 0.0065\n",
            "\n",
            "Epoch 00107: loss did not improve from 0.62414\n",
            "Epoch 108/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7495 - mse: 0.7495 - My_MSE: 0.0074 - val_loss: 0.6425 - val_mse: 0.6425 - val_My_MSE: 0.0063\n",
            "\n",
            "Epoch 00108: loss did not improve from 0.62414\n",
            "Epoch 109/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7496 - mse: 0.7496 - My_MSE: 0.0074 - val_loss: 0.6542 - val_mse: 0.6542 - val_My_MSE: 0.0064\n",
            "\n",
            "Epoch 00109: loss improved from 0.62414 to 0.62369, saving model to poids_train.hdf5\n",
            "Epoch 110/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7430 - mse: 0.7430 - My_MSE: 0.0073 - val_loss: 0.6517 - val_mse: 0.6517 - val_My_MSE: 0.0064\n",
            "\n",
            "Epoch 00110: loss improved from 0.62369 to 0.62177, saving model to poids_train.hdf5\n",
            "Epoch 111/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7404 - mse: 0.7404 - My_MSE: 0.0073 - val_loss: 0.6441 - val_mse: 0.6441 - val_My_MSE: 0.0063\n",
            "\n",
            "Epoch 00111: loss improved from 0.62177 to 0.62008, saving model to poids_train.hdf5\n",
            "Epoch 112/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7409 - mse: 0.7409 - My_MSE: 0.0073 - val_loss: 0.6390 - val_mse: 0.6390 - val_My_MSE: 0.0063\n",
            "\n",
            "Epoch 00112: loss did not improve from 0.62008\n",
            "Epoch 113/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7418 - mse: 0.7418 - My_MSE: 0.0073 - val_loss: 0.6635 - val_mse: 0.6635 - val_My_MSE: 0.0065\n",
            "\n",
            "Epoch 00113: loss did not improve from 0.62008\n",
            "Epoch 114/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7443 - mse: 0.7443 - My_MSE: 0.0073 - val_loss: 0.6364 - val_mse: 0.6364 - val_My_MSE: 0.0063\n",
            "\n",
            "Epoch 00114: loss did not improve from 0.62008\n",
            "Epoch 115/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7412 - mse: 0.7412 - My_MSE: 0.0073 - val_loss: 0.6618 - val_mse: 0.6618 - val_My_MSE: 0.0065\n",
            "\n",
            "Epoch 00115: loss did not improve from 0.62008\n",
            "Epoch 116/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7405 - mse: 0.7405 - My_MSE: 0.0073 - val_loss: 0.6514 - val_mse: 0.6514 - val_My_MSE: 0.0064\n",
            "\n",
            "Epoch 00116: loss improved from 0.62008 to 0.61811, saving model to poids_train.hdf5\n",
            "Epoch 117/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7355 - mse: 0.7355 - My_MSE: 0.0072 - val_loss: 0.6500 - val_mse: 0.6500 - val_My_MSE: 0.0064\n",
            "\n",
            "Epoch 00117: loss improved from 0.61811 to 0.61685, saving model to poids_train.hdf5\n",
            "Epoch 118/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7351 - mse: 0.7351 - My_MSE: 0.0072 - val_loss: 0.6582 - val_mse: 0.6582 - val_My_MSE: 0.0065\n",
            "\n",
            "Epoch 00118: loss did not improve from 0.61685\n",
            "Epoch 119/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7339 - mse: 0.7339 - My_MSE: 0.0072 - val_loss: 0.6558 - val_mse: 0.6558 - val_My_MSE: 0.0065\n",
            "\n",
            "Epoch 00119: loss improved from 0.61685 to 0.61670, saving model to poids_train.hdf5\n",
            "Epoch 120/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7343 - mse: 0.7343 - My_MSE: 0.0072 - val_loss: 0.6500 - val_mse: 0.6500 - val_My_MSE: 0.0064\n",
            "\n",
            "Epoch 00120: loss improved from 0.61670 to 0.61621, saving model to poids_train.hdf5\n",
            "Epoch 121/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7351 - mse: 0.7351 - My_MSE: 0.0072 - val_loss: 0.6716 - val_mse: 0.6716 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00121: loss improved from 0.61621 to 0.61586, saving model to poids_train.hdf5\n",
            "Epoch 122/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7373 - mse: 0.7373 - My_MSE: 0.0073 - val_loss: 0.6573 - val_mse: 0.6573 - val_My_MSE: 0.0065\n",
            "\n",
            "Epoch 00122: loss did not improve from 0.61586\n",
            "Epoch 123/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7340 - mse: 0.7340 - My_MSE: 0.0072 - val_loss: 0.6604 - val_mse: 0.6604 - val_My_MSE: 0.0065\n",
            "\n",
            "Epoch 00123: loss improved from 0.61586 to 0.61526, saving model to poids_train.hdf5\n",
            "Epoch 124/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.7321 - mse: 0.7321 - My_MSE: 0.0072 - val_loss: 0.6677 - val_mse: 0.6677 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00124: loss improved from 0.61526 to 0.61444, saving model to poids_train.hdf5\n",
            "Epoch 125/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7317 - mse: 0.7317 - My_MSE: 0.0072 - val_loss: 0.6510 - val_mse: 0.6510 - val_My_MSE: 0.0064\n",
            "\n",
            "Epoch 00125: loss did not improve from 0.61444\n",
            "Epoch 126/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7398 - mse: 0.7398 - My_MSE: 0.0073 - val_loss: 0.6755 - val_mse: 0.6755 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00126: loss did not improve from 0.61444\n",
            "Epoch 127/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7315 - mse: 0.7315 - My_MSE: 0.0072 - val_loss: 0.6530 - val_mse: 0.6530 - val_My_MSE: 0.0064\n",
            "\n",
            "Epoch 00127: loss improved from 0.61444 to 0.61357, saving model to poids_train.hdf5\n",
            "Epoch 128/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7285 - mse: 0.7285 - My_MSE: 0.0072 - val_loss: 0.6718 - val_mse: 0.6718 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00128: loss improved from 0.61357 to 0.61218, saving model to poids_train.hdf5\n",
            "Epoch 129/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7321 - mse: 0.7321 - My_MSE: 0.0072 - val_loss: 0.6561 - val_mse: 0.6561 - val_My_MSE: 0.0065\n",
            "\n",
            "Epoch 00129: loss did not improve from 0.61218\n",
            "Epoch 130/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7305 - mse: 0.7305 - My_MSE: 0.0072 - val_loss: 0.6741 - val_mse: 0.6741 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00130: loss improved from 0.61218 to 0.61164, saving model to poids_train.hdf5\n",
            "Epoch 131/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7272 - mse: 0.7272 - My_MSE: 0.0072 - val_loss: 0.6626 - val_mse: 0.6626 - val_My_MSE: 0.0065\n",
            "\n",
            "Epoch 00131: loss improved from 0.61164 to 0.61072, saving model to poids_train.hdf5\n",
            "Epoch 132/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7238 - mse: 0.7238 - My_MSE: 0.0071 - val_loss: 0.6651 - val_mse: 0.6651 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00132: loss improved from 0.61072 to 0.60933, saving model to poids_train.hdf5\n",
            "Epoch 133/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7284 - mse: 0.7284 - My_MSE: 0.0072 - val_loss: 0.6618 - val_mse: 0.6618 - val_My_MSE: 0.0065\n",
            "\n",
            "Epoch 00133: loss did not improve from 0.60933\n",
            "Epoch 134/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7308 - mse: 0.7308 - My_MSE: 0.0072 - val_loss: 0.6682 - val_mse: 0.6682 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00134: loss did not improve from 0.60933\n",
            "Epoch 135/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7274 - mse: 0.7274 - My_MSE: 0.0072 - val_loss: 0.6685 - val_mse: 0.6685 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00135: loss did not improve from 0.60933\n",
            "Epoch 136/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7218 - mse: 0.7218 - My_MSE: 0.0071 - val_loss: 0.6771 - val_mse: 0.6771 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00136: loss improved from 0.60933 to 0.60782, saving model to poids_train.hdf5\n",
            "Epoch 137/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7246 - mse: 0.7246 - My_MSE: 0.0071 - val_loss: 0.6663 - val_mse: 0.6663 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00137: loss did not improve from 0.60782\n",
            "Epoch 138/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7204 - mse: 0.7204 - My_MSE: 0.0071 - val_loss: 0.6719 - val_mse: 0.6719 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00138: loss improved from 0.60782 to 0.60643, saving model to poids_train.hdf5\n",
            "Epoch 139/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7198 - mse: 0.7198 - My_MSE: 0.0071 - val_loss: 0.6773 - val_mse: 0.6773 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00139: loss improved from 0.60643 to 0.60617, saving model to poids_train.hdf5\n",
            "Epoch 140/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7225 - mse: 0.7225 - My_MSE: 0.0071 - val_loss: 0.6633 - val_mse: 0.6633 - val_My_MSE: 0.0065\n",
            "\n",
            "Epoch 00140: loss did not improve from 0.60617\n",
            "Epoch 141/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.7264 - mse: 0.7264 - My_MSE: 0.0072 - val_loss: 0.6840 - val_mse: 0.6840 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00141: loss did not improve from 0.60617\n",
            "Epoch 142/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7212 - mse: 0.7212 - My_MSE: 0.0071 - val_loss: 0.6646 - val_mse: 0.6646 - val_My_MSE: 0.0065\n",
            "\n",
            "Epoch 00142: loss did not improve from 0.60617\n",
            "Epoch 143/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7168 - mse: 0.7168 - My_MSE: 0.0071 - val_loss: 0.6662 - val_mse: 0.6662 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00143: loss improved from 0.60617 to 0.60441, saving model to poids_train.hdf5\n",
            "Epoch 144/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7175 - mse: 0.7175 - My_MSE: 0.0071 - val_loss: 0.6854 - val_mse: 0.6854 - val_My_MSE: 0.0068\n",
            "\n",
            "Epoch 00144: loss did not improve from 0.60441\n",
            "Epoch 145/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7194 - mse: 0.7194 - My_MSE: 0.0071 - val_loss: 0.6742 - val_mse: 0.6742 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00145: loss did not improve from 0.60441\n",
            "Epoch 146/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7182 - mse: 0.7182 - My_MSE: 0.0071 - val_loss: 0.6690 - val_mse: 0.6690 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00146: loss did not improve from 0.60441\n",
            "Epoch 147/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7164 - mse: 0.7164 - My_MSE: 0.0071 - val_loss: 0.6778 - val_mse: 0.6778 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00147: loss did not improve from 0.60441\n",
            "Epoch 148/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7132 - mse: 0.7132 - My_MSE: 0.0070 - val_loss: 0.6662 - val_mse: 0.6662 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00148: loss improved from 0.60441 to 0.60219, saving model to poids_train.hdf5\n",
            "Epoch 149/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.7139 - mse: 0.7139 - My_MSE: 0.0070 - val_loss: 0.6769 - val_mse: 0.6769 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00149: loss improved from 0.60219 to 0.60170, saving model to poids_train.hdf5\n",
            "Epoch 150/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7124 - mse: 0.7124 - My_MSE: 0.0070 - val_loss: 0.6841 - val_mse: 0.6841 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00150: loss did not improve from 0.60170\n",
            "Epoch 151/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7170 - mse: 0.7170 - My_MSE: 0.0071 - val_loss: 0.6652 - val_mse: 0.6652 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00151: loss did not improve from 0.60170\n",
            "Epoch 152/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7145 - mse: 0.7145 - My_MSE: 0.0070 - val_loss: 0.6816 - val_mse: 0.6816 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00152: loss did not improve from 0.60170\n",
            "Epoch 153/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.7103 - mse: 0.7103 - My_MSE: 0.0070 - val_loss: 0.6625 - val_mse: 0.6625 - val_My_MSE: 0.0065\n",
            "\n",
            "Epoch 00153: loss improved from 0.60170 to 0.60026, saving model to poids_train.hdf5\n",
            "Epoch 154/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7105 - mse: 0.7105 - My_MSE: 0.0070 - val_loss: 0.6842 - val_mse: 0.6842 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00154: loss did not improve from 0.60026\n",
            "Epoch 155/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7115 - mse: 0.7115 - My_MSE: 0.0070 - val_loss: 0.6723 - val_mse: 0.6723 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00155: loss did not improve from 0.60026\n",
            "Epoch 156/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.7116 - mse: 0.7116 - My_MSE: 0.0070 - val_loss: 0.6766 - val_mse: 0.6766 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00156: loss improved from 0.60026 to 0.59990, saving model to poids_train.hdf5\n",
            "Epoch 157/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7096 - mse: 0.7096 - My_MSE: 0.0070 - val_loss: 0.6711 - val_mse: 0.6711 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00157: loss improved from 0.59990 to 0.59918, saving model to poids_train.hdf5\n",
            "Epoch 158/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.7103 - mse: 0.7103 - My_MSE: 0.0070 - val_loss: 0.6793 - val_mse: 0.6793 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00158: loss improved from 0.59918 to 0.59884, saving model to poids_train.hdf5\n",
            "Epoch 159/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7083 - mse: 0.7083 - My_MSE: 0.0070 - val_loss: 0.6691 - val_mse: 0.6691 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00159: loss improved from 0.59884 to 0.59797, saving model to poids_train.hdf5\n",
            "Epoch 160/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7062 - mse: 0.7062 - My_MSE: 0.0070 - val_loss: 0.6765 - val_mse: 0.6765 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00160: loss improved from 0.59797 to 0.59715, saving model to poids_train.hdf5\n",
            "Epoch 161/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.7053 - mse: 0.7053 - My_MSE: 0.0069 - val_loss: 0.6712 - val_mse: 0.6712 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00161: loss improved from 0.59715 to 0.59707, saving model to poids_train.hdf5\n",
            "Epoch 162/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7088 - mse: 0.7088 - My_MSE: 0.0070 - val_loss: 0.6842 - val_mse: 0.6842 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00162: loss did not improve from 0.59707\n",
            "Epoch 163/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7157 - mse: 0.7157 - My_MSE: 0.0071 - val_loss: 0.6659 - val_mse: 0.6659 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00163: loss did not improve from 0.59707\n",
            "Epoch 164/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7089 - mse: 0.7089 - My_MSE: 0.0070 - val_loss: 0.6775 - val_mse: 0.6775 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00164: loss improved from 0.59707 to 0.59698, saving model to poids_train.hdf5\n",
            "Epoch 165/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7044 - mse: 0.7044 - My_MSE: 0.0069 - val_loss: 0.6718 - val_mse: 0.6718 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00165: loss improved from 0.59698 to 0.59503, saving model to poids_train.hdf5\n",
            "Epoch 166/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7017 - mse: 0.7017 - My_MSE: 0.0069 - val_loss: 0.6788 - val_mse: 0.6788 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00166: loss improved from 0.59503 to 0.59417, saving model to poids_train.hdf5\n",
            "Epoch 167/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7022 - mse: 0.7022 - My_MSE: 0.0069 - val_loss: 0.6821 - val_mse: 0.6821 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00167: loss did not improve from 0.59417\n",
            "Epoch 168/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7026 - mse: 0.7026 - My_MSE: 0.0069 - val_loss: 0.6725 - val_mse: 0.6725 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00168: loss did not improve from 0.59417\n",
            "Epoch 169/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7043 - mse: 0.7043 - My_MSE: 0.0069 - val_loss: 0.6729 - val_mse: 0.6729 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00169: loss did not improve from 0.59417\n",
            "Epoch 170/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7080 - mse: 0.7080 - My_MSE: 0.0070 - val_loss: 0.6750 - val_mse: 0.6750 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00170: loss did not improve from 0.59417\n",
            "Epoch 171/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7028 - mse: 0.7028 - My_MSE: 0.0069 - val_loss: 0.6776 - val_mse: 0.6776 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00171: loss improved from 0.59417 to 0.59374, saving model to poids_train.hdf5\n",
            "Epoch 172/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6992 - mse: 0.6992 - My_MSE: 0.0069 - val_loss: 0.6705 - val_mse: 0.6705 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00172: loss improved from 0.59374 to 0.59207, saving model to poids_train.hdf5\n",
            "Epoch 173/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6990 - mse: 0.6990 - My_MSE: 0.0069 - val_loss: 0.6817 - val_mse: 0.6817 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00173: loss did not improve from 0.59207\n",
            "Epoch 174/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7006 - mse: 0.7006 - My_MSE: 0.0069 - val_loss: 0.6689 - val_mse: 0.6689 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00174: loss did not improve from 0.59207\n",
            "Epoch 175/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7017 - mse: 0.7017 - My_MSE: 0.0069 - val_loss: 0.6771 - val_mse: 0.6771 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00175: loss did not improve from 0.59207\n",
            "Epoch 176/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7010 - mse: 0.7010 - My_MSE: 0.0069 - val_loss: 0.6769 - val_mse: 0.6769 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00176: loss did not improve from 0.59207\n",
            "Epoch 177/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.7014 - mse: 0.7014 - My_MSE: 0.0069 - val_loss: 0.6757 - val_mse: 0.6757 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00177: loss did not improve from 0.59207\n",
            "Epoch 178/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6991 - mse: 0.6991 - My_MSE: 0.0069 - val_loss: 0.6729 - val_mse: 0.6729 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00178: loss improved from 0.59207 to 0.59174, saving model to poids_train.hdf5\n",
            "Epoch 179/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.6968 - mse: 0.6968 - My_MSE: 0.0069 - val_loss: 0.6794 - val_mse: 0.6794 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00179: loss improved from 0.59174 to 0.59002, saving model to poids_train.hdf5\n",
            "Epoch 180/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6965 - mse: 0.6965 - My_MSE: 0.0069 - val_loss: 0.6683 - val_mse: 0.6683 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00180: loss improved from 0.59002 to 0.58947, saving model to poids_train.hdf5\n",
            "Epoch 181/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6956 - mse: 0.6956 - My_MSE: 0.0069 - val_loss: 0.6812 - val_mse: 0.6812 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00181: loss improved from 0.58947 to 0.58944, saving model to poids_train.hdf5\n",
            "Epoch 182/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6969 - mse: 0.6969 - My_MSE: 0.0069 - val_loss: 0.6682 - val_mse: 0.6682 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00182: loss did not improve from 0.58944\n",
            "Epoch 183/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6990 - mse: 0.6990 - My_MSE: 0.0069 - val_loss: 0.6739 - val_mse: 0.6739 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00183: loss did not improve from 0.58944\n",
            "Epoch 184/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6971 - mse: 0.6971 - My_MSE: 0.0069 - val_loss: 0.6835 - val_mse: 0.6835 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00184: loss improved from 0.58944 to 0.58898, saving model to poids_train.hdf5\n",
            "Epoch 185/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6963 - mse: 0.6963 - My_MSE: 0.0069 - val_loss: 0.6785 - val_mse: 0.6785 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00185: loss improved from 0.58898 to 0.58809, saving model to poids_train.hdf5\n",
            "Epoch 186/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6965 - mse: 0.6965 - My_MSE: 0.0069 - val_loss: 0.6794 - val_mse: 0.6794 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00186: loss did not improve from 0.58809\n",
            "Epoch 187/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.6932 - mse: 0.6932 - My_MSE: 0.0068 - val_loss: 0.6758 - val_mse: 0.6758 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00187: loss improved from 0.58809 to 0.58795, saving model to poids_train.hdf5\n",
            "Epoch 188/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6947 - mse: 0.6947 - My_MSE: 0.0068 - val_loss: 0.6784 - val_mse: 0.6784 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00188: loss improved from 0.58795 to 0.58794, saving model to poids_train.hdf5\n",
            "Epoch 189/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6932 - mse: 0.6932 - My_MSE: 0.0068 - val_loss: 0.6719 - val_mse: 0.6719 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00189: loss improved from 0.58794 to 0.58674, saving model to poids_train.hdf5\n",
            "Epoch 190/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6924 - mse: 0.6924 - My_MSE: 0.0068 - val_loss: 0.6870 - val_mse: 0.6870 - val_My_MSE: 0.0068\n",
            "\n",
            "Epoch 00190: loss improved from 0.58674 to 0.58636, saving model to poids_train.hdf5\n",
            "Epoch 191/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6900 - mse: 0.6900 - My_MSE: 0.0068 - val_loss: 0.6748 - val_mse: 0.6748 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00191: loss improved from 0.58636 to 0.58544, saving model to poids_train.hdf5\n",
            "Epoch 192/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6924 - mse: 0.6924 - My_MSE: 0.0068 - val_loss: 0.6672 - val_mse: 0.6672 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00192: loss did not improve from 0.58544\n",
            "Epoch 193/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6916 - mse: 0.6916 - My_MSE: 0.0068 - val_loss: 0.6828 - val_mse: 0.6828 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00193: loss improved from 0.58544 to 0.58534, saving model to poids_train.hdf5\n",
            "Epoch 194/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6927 - mse: 0.6927 - My_MSE: 0.0068 - val_loss: 0.6798 - val_mse: 0.6798 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00194: loss did not improve from 0.58534\n",
            "Epoch 195/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6935 - mse: 0.6935 - My_MSE: 0.0068 - val_loss: 0.6621 - val_mse: 0.6621 - val_My_MSE: 0.0065\n",
            "\n",
            "Epoch 00195: loss did not improve from 0.58534\n",
            "Epoch 196/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6958 - mse: 0.6958 - My_MSE: 0.0069 - val_loss: 0.6827 - val_mse: 0.6827 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00196: loss did not improve from 0.58534\n",
            "Epoch 197/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6885 - mse: 0.6885 - My_MSE: 0.0068 - val_loss: 0.6895 - val_mse: 0.6895 - val_My_MSE: 0.0068\n",
            "\n",
            "Epoch 00197: loss improved from 0.58534 to 0.58393, saving model to poids_train.hdf5\n",
            "Epoch 198/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6878 - mse: 0.6878 - My_MSE: 0.0068 - val_loss: 0.6770 - val_mse: 0.6770 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00198: loss improved from 0.58393 to 0.58347, saving model to poids_train.hdf5\n",
            "Epoch 199/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6885 - mse: 0.6885 - My_MSE: 0.0068 - val_loss: 0.6744 - val_mse: 0.6744 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00199: loss improved from 0.58347 to 0.58343, saving model to poids_train.hdf5\n",
            "Epoch 200/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6889 - mse: 0.6889 - My_MSE: 0.0068 - val_loss: 0.6796 - val_mse: 0.6796 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00200: loss improved from 0.58343 to 0.58319, saving model to poids_train.hdf5\n",
            "Epoch 201/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.6875 - mse: 0.6875 - My_MSE: 0.0068 - val_loss: 0.6907 - val_mse: 0.6907 - val_My_MSE: 0.0068\n",
            "\n",
            "Epoch 00201: loss improved from 0.58319 to 0.58278, saving model to poids_train.hdf5\n",
            "Epoch 202/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6914 - mse: 0.6914 - My_MSE: 0.0068 - val_loss: 0.6841 - val_mse: 0.6841 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00202: loss did not improve from 0.58278\n",
            "Epoch 203/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6889 - mse: 0.6889 - My_MSE: 0.0068 - val_loss: 0.6705 - val_mse: 0.6705 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00203: loss did not improve from 0.58278\n",
            "Epoch 204/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6864 - mse: 0.6864 - My_MSE: 0.0068 - val_loss: 0.6806 - val_mse: 0.6806 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00204: loss improved from 0.58278 to 0.58160, saving model to poids_train.hdf5\n",
            "Epoch 205/10000\n",
            "555/555 [==============================] - 61s 110ms/step - loss: 0.6846 - mse: 0.6846 - My_MSE: 0.0067 - val_loss: 0.6824 - val_mse: 0.6824 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00205: loss improved from 0.58160 to 0.58084, saving model to poids_train.hdf5\n",
            "Epoch 206/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6854 - mse: 0.6854 - My_MSE: 0.0068 - val_loss: 0.6874 - val_mse: 0.6874 - val_My_MSE: 0.0068\n",
            "\n",
            "Epoch 00206: loss did not improve from 0.58084\n",
            "Epoch 207/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6897 - mse: 0.6897 - My_MSE: 0.0068 - val_loss: 0.6781 - val_mse: 0.6781 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00207: loss did not improve from 0.58084\n",
            "Epoch 208/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.6878 - mse: 0.6878 - My_MSE: 0.0068 - val_loss: 0.6739 - val_mse: 0.6739 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00208: loss did not improve from 0.58084\n",
            "Epoch 209/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6844 - mse: 0.6844 - My_MSE: 0.0067 - val_loss: 0.6871 - val_mse: 0.6871 - val_My_MSE: 0.0068\n",
            "\n",
            "Epoch 00209: loss improved from 0.58084 to 0.58009, saving model to poids_train.hdf5\n",
            "Epoch 210/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.6842 - mse: 0.6842 - My_MSE: 0.0067 - val_loss: 0.6819 - val_mse: 0.6819 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00210: loss did not improve from 0.58009\n",
            "Epoch 211/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6846 - mse: 0.6846 - My_MSE: 0.0067 - val_loss: 0.6908 - val_mse: 0.6908 - val_My_MSE: 0.0068\n",
            "\n",
            "Epoch 00211: loss improved from 0.58009 to 0.57995, saving model to poids_train.hdf5\n",
            "Epoch 212/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6841 - mse: 0.6841 - My_MSE: 0.0067 - val_loss: 0.6672 - val_mse: 0.6672 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00212: loss did not improve from 0.57995\n",
            "Epoch 213/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6853 - mse: 0.6853 - My_MSE: 0.0068 - val_loss: 0.6832 - val_mse: 0.6832 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00213: loss did not improve from 0.57995\n",
            "Epoch 214/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6834 - mse: 0.6834 - My_MSE: 0.0067 - val_loss: 0.6875 - val_mse: 0.6875 - val_My_MSE: 0.0068\n",
            "\n",
            "Epoch 00214: loss improved from 0.57995 to 0.57992, saving model to poids_train.hdf5\n",
            "Epoch 215/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6837 - mse: 0.6837 - My_MSE: 0.0067 - val_loss: 0.6833 - val_mse: 0.6833 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00215: loss improved from 0.57992 to 0.57901, saving model to poids_train.hdf5\n",
            "Epoch 216/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6835 - mse: 0.6835 - My_MSE: 0.0067 - val_loss: 0.6830 - val_mse: 0.6830 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00216: loss improved from 0.57901 to 0.57900, saving model to poids_train.hdf5\n",
            "Epoch 217/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6834 - mse: 0.6834 - My_MSE: 0.0067 - val_loss: 0.6888 - val_mse: 0.6888 - val_My_MSE: 0.0068\n",
            "\n",
            "Epoch 00217: loss improved from 0.57900 to 0.57835, saving model to poids_train.hdf5\n",
            "Epoch 218/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6801 - mse: 0.6801 - My_MSE: 0.0067 - val_loss: 0.6916 - val_mse: 0.6916 - val_My_MSE: 0.0068\n",
            "\n",
            "Epoch 00218: loss improved from 0.57835 to 0.57703, saving model to poids_train.hdf5\n",
            "Epoch 219/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6799 - mse: 0.6799 - My_MSE: 0.0067 - val_loss: 0.6763 - val_mse: 0.6763 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00219: loss improved from 0.57703 to 0.57702, saving model to poids_train.hdf5\n",
            "Epoch 220/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6809 - mse: 0.6809 - My_MSE: 0.0067 - val_loss: 0.6938 - val_mse: 0.6938 - val_My_MSE: 0.0068\n",
            "\n",
            "Epoch 00220: loss improved from 0.57702 to 0.57683, saving model to poids_train.hdf5\n",
            "Epoch 221/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6823 - mse: 0.6823 - My_MSE: 0.0067 - val_loss: 0.6820 - val_mse: 0.6820 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00221: loss did not improve from 0.57683\n",
            "Epoch 222/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6819 - mse: 0.6819 - My_MSE: 0.0067 - val_loss: 0.6749 - val_mse: 0.6749 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00222: loss did not improve from 0.57683\n",
            "Epoch 223/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6781 - mse: 0.6781 - My_MSE: 0.0067 - val_loss: 0.6933 - val_mse: 0.6933 - val_My_MSE: 0.0068\n",
            "\n",
            "Epoch 00223: loss improved from 0.57683 to 0.57520, saving model to poids_train.hdf5\n",
            "Epoch 224/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6770 - mse: 0.6770 - My_MSE: 0.0067 - val_loss: 0.6843 - val_mse: 0.6843 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00224: loss improved from 0.57520 to 0.57473, saving model to poids_train.hdf5\n",
            "Epoch 225/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6795 - mse: 0.6795 - My_MSE: 0.0067 - val_loss: 0.6901 - val_mse: 0.6901 - val_My_MSE: 0.0068\n",
            "\n",
            "Epoch 00225: loss did not improve from 0.57473\n",
            "Epoch 226/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6794 - mse: 0.6794 - My_MSE: 0.0067 - val_loss: 0.6822 - val_mse: 0.6822 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00226: loss did not improve from 0.57473\n",
            "Epoch 227/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6784 - mse: 0.6784 - My_MSE: 0.0067 - val_loss: 0.6929 - val_mse: 0.6929 - val_My_MSE: 0.0068\n",
            "\n",
            "Epoch 00227: loss did not improve from 0.57473\n",
            "Epoch 228/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6772 - mse: 0.6772 - My_MSE: 0.0067 - val_loss: 0.6897 - val_mse: 0.6897 - val_My_MSE: 0.0068\n",
            "\n",
            "Epoch 00228: loss improved from 0.57473 to 0.57446, saving model to poids_train.hdf5\n",
            "Epoch 229/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6772 - mse: 0.6772 - My_MSE: 0.0067 - val_loss: 0.7052 - val_mse: 0.7052 - val_My_MSE: 0.0069\n",
            "\n",
            "Epoch 00229: loss improved from 0.57446 to 0.57375, saving model to poids_train.hdf5\n",
            "Epoch 230/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6776 - mse: 0.6776 - My_MSE: 0.0067 - val_loss: 0.6940 - val_mse: 0.6940 - val_My_MSE: 0.0068\n",
            "\n",
            "Epoch 00230: loss improved from 0.57375 to 0.57365, saving model to poids_train.hdf5\n",
            "Epoch 231/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6781 - mse: 0.6781 - My_MSE: 0.0067 - val_loss: 0.6663 - val_mse: 0.6663 - val_My_MSE: 0.0066\n",
            "\n",
            "Epoch 00231: loss did not improve from 0.57365\n",
            "Epoch 232/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6796 - mse: 0.6796 - My_MSE: 0.0067 - val_loss: 0.6964 - val_mse: 0.6964 - val_My_MSE: 0.0069\n",
            "\n",
            "Epoch 00232: loss did not improve from 0.57365\n",
            "Epoch 233/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6755 - mse: 0.6755 - My_MSE: 0.0067 - val_loss: 0.6893 - val_mse: 0.6893 - val_My_MSE: 0.0068\n",
            "\n",
            "Epoch 00233: loss improved from 0.57365 to 0.57272, saving model to poids_train.hdf5\n",
            "Epoch 234/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6750 - mse: 0.6750 - My_MSE: 0.0066 - val_loss: 0.6825 - val_mse: 0.6825 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00234: loss improved from 0.57272 to 0.57246, saving model to poids_train.hdf5\n",
            "Epoch 235/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6748 - mse: 0.6748 - My_MSE: 0.0066 - val_loss: 0.7064 - val_mse: 0.7064 - val_My_MSE: 0.0070\n",
            "\n",
            "Epoch 00235: loss did not improve from 0.57246\n",
            "Epoch 236/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6802 - mse: 0.6802 - My_MSE: 0.0067 - val_loss: 0.6960 - val_mse: 0.6960 - val_My_MSE: 0.0069\n",
            "\n",
            "Epoch 00236: loss did not improve from 0.57246\n",
            "Epoch 237/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6779 - mse: 0.6779 - My_MSE: 0.0067 - val_loss: 0.6975 - val_mse: 0.6975 - val_My_MSE: 0.0069\n",
            "\n",
            "Epoch 00237: loss did not improve from 0.57246\n",
            "Epoch 238/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6733 - mse: 0.6733 - My_MSE: 0.0066 - val_loss: 0.6980 - val_mse: 0.6980 - val_My_MSE: 0.0069\n",
            "\n",
            "Epoch 00238: loss improved from 0.57246 to 0.57066, saving model to poids_train.hdf5\n",
            "Epoch 239/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6730 - mse: 0.6730 - My_MSE: 0.0066 - val_loss: 0.6916 - val_mse: 0.6916 - val_My_MSE: 0.0068\n",
            "\n",
            "Epoch 00239: loss improved from 0.57066 to 0.57042, saving model to poids_train.hdf5\n",
            "Epoch 240/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6729 - mse: 0.6729 - My_MSE: 0.0066 - val_loss: 0.7170 - val_mse: 0.7170 - val_My_MSE: 0.0071\n",
            "\n",
            "Epoch 00240: loss did not improve from 0.57042\n",
            "Epoch 241/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6743 - mse: 0.6743 - My_MSE: 0.0066 - val_loss: 0.6756 - val_mse: 0.6756 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00241: loss did not improve from 0.57042\n",
            "Epoch 242/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6739 - mse: 0.6739 - My_MSE: 0.0066 - val_loss: 0.6906 - val_mse: 0.6906 - val_My_MSE: 0.0068\n",
            "\n",
            "Epoch 00242: loss did not improve from 0.57042\n",
            "Epoch 243/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6711 - mse: 0.6711 - My_MSE: 0.0066 - val_loss: 0.6982 - val_mse: 0.6982 - val_My_MSE: 0.0069\n",
            "\n",
            "Epoch 00243: loss improved from 0.57042 to 0.56991, saving model to poids_train.hdf5\n",
            "Epoch 244/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6718 - mse: 0.6718 - My_MSE: 0.0066 - val_loss: 0.7101 - val_mse: 0.7101 - val_My_MSE: 0.0070\n",
            "\n",
            "Epoch 00244: loss improved from 0.56991 to 0.56945, saving model to poids_train.hdf5\n",
            "Epoch 245/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6717 - mse: 0.6717 - My_MSE: 0.0066 - val_loss: 0.6996 - val_mse: 0.6996 - val_My_MSE: 0.0069\n",
            "\n",
            "Epoch 00245: loss improved from 0.56945 to 0.56907, saving model to poids_train.hdf5\n",
            "Epoch 246/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6710 - mse: 0.6710 - My_MSE: 0.0066 - val_loss: 0.6921 - val_mse: 0.6921 - val_My_MSE: 0.0068\n",
            "\n",
            "Epoch 00246: loss did not improve from 0.56907\n",
            "Epoch 247/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6722 - mse: 0.6722 - My_MSE: 0.0066 - val_loss: 0.6836 - val_mse: 0.6836 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00247: loss did not improve from 0.56907\n",
            "Epoch 248/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6731 - mse: 0.6731 - My_MSE: 0.0066 - val_loss: 0.7020 - val_mse: 0.7020 - val_My_MSE: 0.0069\n",
            "\n",
            "Epoch 00248: loss did not improve from 0.56907\n",
            "Epoch 249/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6732 - mse: 0.6732 - My_MSE: 0.0066 - val_loss: 0.6970 - val_mse: 0.6970 - val_My_MSE: 0.0069\n",
            "\n",
            "Epoch 00249: loss did not improve from 0.56907\n",
            "Epoch 250/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6706 - mse: 0.6706 - My_MSE: 0.0066 - val_loss: 0.7046 - val_mse: 0.7046 - val_My_MSE: 0.0069\n",
            "\n",
            "Epoch 00250: loss improved from 0.56907 to 0.56795, saving model to poids_train.hdf5\n",
            "Epoch 251/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6679 - mse: 0.6679 - My_MSE: 0.0066 - val_loss: 0.7150 - val_mse: 0.7150 - val_My_MSE: 0.0070\n",
            "\n",
            "Epoch 00251: loss improved from 0.56795 to 0.56671, saving model to poids_train.hdf5\n",
            "Epoch 252/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6677 - mse: 0.6677 - My_MSE: 0.0066 - val_loss: 0.7019 - val_mse: 0.7019 - val_My_MSE: 0.0069\n",
            "\n",
            "Epoch 00252: loss improved from 0.56671 to 0.56651, saving model to poids_train.hdf5\n",
            "Epoch 253/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6691 - mse: 0.6691 - My_MSE: 0.0066 - val_loss: 0.7051 - val_mse: 0.7051 - val_My_MSE: 0.0069\n",
            "\n",
            "Epoch 00253: loss did not improve from 0.56651\n",
            "Epoch 254/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6723 - mse: 0.6723 - My_MSE: 0.0066 - val_loss: 0.7093 - val_mse: 0.7093 - val_My_MSE: 0.0070\n",
            "\n",
            "Epoch 00254: loss did not improve from 0.56651\n",
            "Epoch 255/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6730 - mse: 0.6730 - My_MSE: 0.0066 - val_loss: 0.6853 - val_mse: 0.6853 - val_My_MSE: 0.0068\n",
            "\n",
            "Epoch 00255: loss did not improve from 0.56651\n",
            "Epoch 256/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6701 - mse: 0.6701 - My_MSE: 0.0066 - val_loss: 0.6926 - val_mse: 0.6926 - val_My_MSE: 0.0068\n",
            "\n",
            "Epoch 00256: loss did not improve from 0.56651\n",
            "Epoch 257/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6673 - mse: 0.6673 - My_MSE: 0.0066 - val_loss: 0.7154 - val_mse: 0.7154 - val_My_MSE: 0.0070\n",
            "\n",
            "Epoch 00257: loss improved from 0.56651 to 0.56528, saving model to poids_train.hdf5\n",
            "Epoch 258/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6667 - mse: 0.6667 - My_MSE: 0.0066 - val_loss: 0.7205 - val_mse: 0.7205 - val_My_MSE: 0.0071\n",
            "\n",
            "Epoch 00258: loss did not improve from 0.56528\n",
            "Epoch 259/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6683 - mse: 0.6683 - My_MSE: 0.0066 - val_loss: 0.6971 - val_mse: 0.6971 - val_My_MSE: 0.0069\n",
            "\n",
            "Epoch 00259: loss did not improve from 0.56528\n",
            "Epoch 260/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6683 - mse: 0.6683 - My_MSE: 0.0066 - val_loss: 0.7318 - val_mse: 0.7318 - val_My_MSE: 0.0072\n",
            "\n",
            "Epoch 00260: loss did not improve from 0.56528\n",
            "Epoch 261/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6680 - mse: 0.6680 - My_MSE: 0.0066 - val_loss: 0.6901 - val_mse: 0.6901 - val_My_MSE: 0.0068\n",
            "\n",
            "Epoch 00261: loss did not improve from 0.56528\n",
            "Epoch 262/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6680 - mse: 0.6680 - My_MSE: 0.0066 - val_loss: 0.7134 - val_mse: 0.7134 - val_My_MSE: 0.0070\n",
            "\n",
            "Epoch 00262: loss improved from 0.56528 to 0.56497, saving model to poids_train.hdf5\n",
            "Epoch 263/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6634 - mse: 0.6634 - My_MSE: 0.0065 - val_loss: 0.7152 - val_mse: 0.7152 - val_My_MSE: 0.0070\n",
            "\n",
            "Epoch 00263: loss improved from 0.56497 to 0.56300, saving model to poids_train.hdf5\n",
            "Epoch 264/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6641 - mse: 0.6641 - My_MSE: 0.0065 - val_loss: 0.7113 - val_mse: 0.7113 - val_My_MSE: 0.0070\n",
            "\n",
            "Epoch 00264: loss did not improve from 0.56300\n",
            "Epoch 265/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6645 - mse: 0.6645 - My_MSE: 0.0065 - val_loss: 0.6835 - val_mse: 0.6835 - val_My_MSE: 0.0067\n",
            "\n",
            "Epoch 00265: loss did not improve from 0.56300\n",
            "Epoch 266/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6698 - mse: 0.6698 - My_MSE: 0.0066 - val_loss: 0.7512 - val_mse: 0.7512 - val_My_MSE: 0.0074\n",
            "\n",
            "Epoch 00266: loss did not improve from 0.56300\n",
            "Epoch 267/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6660 - mse: 0.6660 - My_MSE: 0.0066 - val_loss: 0.7142 - val_mse: 0.7142 - val_My_MSE: 0.0070\n",
            "\n",
            "Epoch 00267: loss did not improve from 0.56300\n",
            "Epoch 268/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6650 - mse: 0.6650 - My_MSE: 0.0066 - val_loss: 0.7093 - val_mse: 0.7093 - val_My_MSE: 0.0070\n",
            "\n",
            "Epoch 00268: loss improved from 0.56300 to 0.56297, saving model to poids_train.hdf5\n",
            "Epoch 269/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6643 - mse: 0.6643 - My_MSE: 0.0065 - val_loss: 0.6917 - val_mse: 0.6917 - val_My_MSE: 0.0068\n",
            "\n",
            "Epoch 00269: loss did not improve from 0.56297\n",
            "Epoch 270/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6636 - mse: 0.6636 - My_MSE: 0.0065 - val_loss: 0.7315 - val_mse: 0.7315 - val_My_MSE: 0.0072\n",
            "\n",
            "Epoch 00270: loss improved from 0.56297 to 0.56213, saving model to poids_train.hdf5\n",
            "Epoch 271/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6618 - mse: 0.6618 - My_MSE: 0.0065 - val_loss: 0.7169 - val_mse: 0.7169 - val_My_MSE: 0.0071\n",
            "\n",
            "Epoch 00271: loss improved from 0.56213 to 0.56115, saving model to poids_train.hdf5\n",
            "Epoch 272/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6616 - mse: 0.6616 - My_MSE: 0.0065 - val_loss: 0.7345 - val_mse: 0.7345 - val_My_MSE: 0.0072\n",
            "\n",
            "Epoch 00272: loss did not improve from 0.56115\n",
            "Epoch 273/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6621 - mse: 0.6621 - My_MSE: 0.0065 - val_loss: 0.6939 - val_mse: 0.6939 - val_My_MSE: 0.0068\n",
            "\n",
            "Epoch 00273: loss did not improve from 0.56115\n",
            "Epoch 274/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6644 - mse: 0.6644 - My_MSE: 0.0065 - val_loss: 0.7451 - val_mse: 0.7451 - val_My_MSE: 0.0073\n",
            "\n",
            "Epoch 00274: loss did not improve from 0.56115\n",
            "Epoch 275/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6628 - mse: 0.6628 - My_MSE: 0.0065 - val_loss: 0.6976 - val_mse: 0.6976 - val_My_MSE: 0.0069\n",
            "\n",
            "Epoch 00275: loss did not improve from 0.56115\n",
            "Epoch 276/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6648 - mse: 0.6648 - My_MSE: 0.0065 - val_loss: 0.7361 - val_mse: 0.7361 - val_My_MSE: 0.0073\n",
            "\n",
            "Epoch 00276: loss did not improve from 0.56115\n",
            "Epoch 277/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6599 - mse: 0.6599 - My_MSE: 0.0065 - val_loss: 0.7258 - val_mse: 0.7258 - val_My_MSE: 0.0071\n",
            "\n",
            "Epoch 00277: loss improved from 0.56115 to 0.55920, saving model to poids_train.hdf5\n",
            "Epoch 278/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6598 - mse: 0.6598 - My_MSE: 0.0065 - val_loss: 0.7342 - val_mse: 0.7342 - val_My_MSE: 0.0072\n",
            "\n",
            "Epoch 00278: loss did not improve from 0.55920\n",
            "Epoch 279/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6608 - mse: 0.6608 - My_MSE: 0.0065 - val_loss: 0.6876 - val_mse: 0.6876 - val_My_MSE: 0.0068\n",
            "\n",
            "Epoch 00279: loss did not improve from 0.55920\n",
            "Epoch 280/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6635 - mse: 0.6635 - My_MSE: 0.0065 - val_loss: 0.7452 - val_mse: 0.7452 - val_My_MSE: 0.0073\n",
            "\n",
            "Epoch 00280: loss did not improve from 0.55920\n",
            "Epoch 281/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6585 - mse: 0.6585 - My_MSE: 0.0065 - val_loss: 0.7168 - val_mse: 0.7168 - val_My_MSE: 0.0071\n",
            "\n",
            "Epoch 00281: loss improved from 0.55920 to 0.55906, saving model to poids_train.hdf5\n",
            "Epoch 282/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6584 - mse: 0.6584 - My_MSE: 0.0065 - val_loss: 0.7550 - val_mse: 0.7550 - val_My_MSE: 0.0074\n",
            "\n",
            "Epoch 00282: loss did not improve from 0.55906\n",
            "Epoch 283/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6576 - mse: 0.6576 - My_MSE: 0.0065 - val_loss: 0.7008 - val_mse: 0.7008 - val_My_MSE: 0.0069\n",
            "\n",
            "Epoch 00283: loss improved from 0.55906 to 0.55812, saving model to poids_train.hdf5\n",
            "Epoch 284/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6580 - mse: 0.6580 - My_MSE: 0.0065 - val_loss: 0.7436 - val_mse: 0.7436 - val_My_MSE: 0.0073\n",
            "\n",
            "Epoch 00284: loss improved from 0.55812 to 0.55783, saving model to poids_train.hdf5\n",
            "Epoch 285/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6570 - mse: 0.6570 - My_MSE: 0.0065 - val_loss: 0.6999 - val_mse: 0.6999 - val_My_MSE: 0.0069\n",
            "\n",
            "Epoch 00285: loss did not improve from 0.55783\n",
            "Epoch 286/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6617 - mse: 0.6617 - My_MSE: 0.0065 - val_loss: 0.7580 - val_mse: 0.7580 - val_My_MSE: 0.0075\n",
            "\n",
            "Epoch 00286: loss did not improve from 0.55783\n",
            "Epoch 287/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6571 - mse: 0.6571 - My_MSE: 0.0065 - val_loss: 0.7289 - val_mse: 0.7289 - val_My_MSE: 0.0072\n",
            "\n",
            "Epoch 00287: loss improved from 0.55783 to 0.55695, saving model to poids_train.hdf5\n",
            "Epoch 288/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6566 - mse: 0.6566 - My_MSE: 0.0065 - val_loss: 0.7497 - val_mse: 0.7497 - val_My_MSE: 0.0074\n",
            "\n",
            "Epoch 00288: loss did not improve from 0.55695\n",
            "Epoch 289/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6569 - mse: 0.6569 - My_MSE: 0.0065 - val_loss: 0.7162 - val_mse: 0.7162 - val_My_MSE: 0.0071\n",
            "\n",
            "Epoch 00289: loss did not improve from 0.55695\n",
            "Epoch 290/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6586 - mse: 0.6586 - My_MSE: 0.0065 - val_loss: 0.7199 - val_mse: 0.7199 - val_My_MSE: 0.0071\n",
            "\n",
            "Epoch 00290: loss did not improve from 0.55695\n",
            "Epoch 291/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6590 - mse: 0.6590 - My_MSE: 0.0065 - val_loss: 0.7530 - val_mse: 0.7530 - val_My_MSE: 0.0074\n",
            "\n",
            "Epoch 00291: loss did not improve from 0.55695\n",
            "Epoch 292/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6560 - mse: 0.6560 - My_MSE: 0.0065 - val_loss: 0.7285 - val_mse: 0.7285 - val_My_MSE: 0.0072\n",
            "\n",
            "Epoch 00292: loss did not improve from 0.55695\n",
            "Epoch 293/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6564 - mse: 0.6564 - My_MSE: 0.0065 - val_loss: 0.7241 - val_mse: 0.7241 - val_My_MSE: 0.0071\n",
            "\n",
            "Epoch 00293: loss improved from 0.55695 to 0.55688, saving model to poids_train.hdf5\n",
            "Epoch 294/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6544 - mse: 0.6544 - My_MSE: 0.0064 - val_loss: 0.7538 - val_mse: 0.7538 - val_My_MSE: 0.0074\n",
            "\n",
            "Epoch 00294: loss improved from 0.55688 to 0.55572, saving model to poids_train.hdf5\n",
            "Epoch 295/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6554 - mse: 0.6554 - My_MSE: 0.0065 - val_loss: 0.7558 - val_mse: 0.7558 - val_My_MSE: 0.0074\n",
            "\n",
            "Epoch 00295: loss improved from 0.55572 to 0.55551, saving model to poids_train.hdf5\n",
            "Epoch 296/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6545 - mse: 0.6545 - My_MSE: 0.0064 - val_loss: 0.7546 - val_mse: 0.7546 - val_My_MSE: 0.0074\n",
            "\n",
            "Epoch 00296: loss improved from 0.55551 to 0.55484, saving model to poids_train.hdf5\n",
            "Epoch 297/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6526 - mse: 0.6526 - My_MSE: 0.0064 - val_loss: 0.7056 - val_mse: 0.7056 - val_My_MSE: 0.0070\n",
            "\n",
            "Epoch 00297: loss did not improve from 0.55484\n",
            "Epoch 298/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6550 - mse: 0.6550 - My_MSE: 0.0065 - val_loss: 0.7565 - val_mse: 0.7565 - val_My_MSE: 0.0075\n",
            "\n",
            "Epoch 00298: loss did not improve from 0.55484\n",
            "Epoch 299/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6540 - mse: 0.6540 - My_MSE: 0.0064 - val_loss: 0.7276 - val_mse: 0.7276 - val_My_MSE: 0.0072\n",
            "\n",
            "Epoch 00299: loss did not improve from 0.55484\n",
            "Epoch 300/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6547 - mse: 0.6547 - My_MSE: 0.0064 - val_loss: 0.7612 - val_mse: 0.7612 - val_My_MSE: 0.0075\n",
            "\n",
            "Epoch 00300: loss did not improve from 0.55484\n",
            "Epoch 301/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6536 - mse: 0.6536 - My_MSE: 0.0064 - val_loss: 0.7537 - val_mse: 0.7537 - val_My_MSE: 0.0074\n",
            "\n",
            "Epoch 00301: loss did not improve from 0.55484\n",
            "Epoch 302/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6536 - mse: 0.6536 - My_MSE: 0.0064 - val_loss: 0.7093 - val_mse: 0.7093 - val_My_MSE: 0.0070\n",
            "\n",
            "Epoch 00302: loss did not improve from 0.55484\n",
            "Epoch 303/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6539 - mse: 0.6539 - My_MSE: 0.0064 - val_loss: 0.7554 - val_mse: 0.7554 - val_My_MSE: 0.0074\n",
            "\n",
            "Epoch 00303: loss improved from 0.55484 to 0.55411, saving model to poids_train.hdf5\n",
            "Epoch 304/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6509 - mse: 0.6509 - My_MSE: 0.0064 - val_loss: 0.7413 - val_mse: 0.7413 - val_My_MSE: 0.0073\n",
            "\n",
            "Epoch 00304: loss improved from 0.55411 to 0.55225, saving model to poids_train.hdf5\n",
            "Epoch 305/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6504 - mse: 0.6504 - My_MSE: 0.0064 - val_loss: 0.7792 - val_mse: 0.7792 - val_My_MSE: 0.0077\n",
            "\n",
            "Epoch 00305: loss did not improve from 0.55225\n",
            "Epoch 306/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6509 - mse: 0.6509 - My_MSE: 0.0064 - val_loss: 0.7222 - val_mse: 0.7222 - val_My_MSE: 0.0071\n",
            "\n",
            "Epoch 00306: loss did not improve from 0.55225\n",
            "Epoch 307/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6523 - mse: 0.6523 - My_MSE: 0.0064 - val_loss: 0.7644 - val_mse: 0.7644 - val_My_MSE: 0.0075\n",
            "\n",
            "Epoch 00307: loss did not improve from 0.55225\n",
            "Epoch 308/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6520 - mse: 0.6520 - My_MSE: 0.0064 - val_loss: 0.7149 - val_mse: 0.7149 - val_My_MSE: 0.0070\n",
            "\n",
            "Epoch 00308: loss did not improve from 0.55225\n",
            "Epoch 309/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6529 - mse: 0.6529 - My_MSE: 0.0064 - val_loss: 0.7559 - val_mse: 0.7559 - val_My_MSE: 0.0074\n",
            "\n",
            "Epoch 00309: loss did not improve from 0.55225\n",
            "Epoch 310/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6496 - mse: 0.6496 - My_MSE: 0.0064 - val_loss: 0.7548 - val_mse: 0.7548 - val_My_MSE: 0.0074\n",
            "\n",
            "Epoch 00310: loss improved from 0.55225 to 0.55118, saving model to poids_train.hdf5\n",
            "Epoch 311/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6488 - mse: 0.6488 - My_MSE: 0.0064 - val_loss: 0.7535 - val_mse: 0.7535 - val_My_MSE: 0.0074\n",
            "\n",
            "Epoch 00311: loss improved from 0.55118 to 0.55072, saving model to poids_train.hdf5\n",
            "Epoch 312/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6491 - mse: 0.6491 - My_MSE: 0.0064 - val_loss: 0.7341 - val_mse: 0.7341 - val_My_MSE: 0.0072\n",
            "\n",
            "Epoch 00312: loss did not improve from 0.55072\n",
            "Epoch 313/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6498 - mse: 0.6498 - My_MSE: 0.0064 - val_loss: 0.7768 - val_mse: 0.7768 - val_My_MSE: 0.0077\n",
            "\n",
            "Epoch 00313: loss did not improve from 0.55072\n",
            "Epoch 314/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6489 - mse: 0.6489 - My_MSE: 0.0064 - val_loss: 0.7419 - val_mse: 0.7419 - val_My_MSE: 0.0073\n",
            "\n",
            "Epoch 00314: loss did not improve from 0.55072\n",
            "Epoch 315/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6486 - mse: 0.6486 - My_MSE: 0.0064 - val_loss: 0.7680 - val_mse: 0.7680 - val_My_MSE: 0.0076\n",
            "\n",
            "Epoch 00315: loss did not improve from 0.55072\n",
            "Epoch 316/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6473 - mse: 0.6473 - My_MSE: 0.0064 - val_loss: 0.7335 - val_mse: 0.7335 - val_My_MSE: 0.0072\n",
            "\n",
            "Epoch 00316: loss improved from 0.55072 to 0.54983, saving model to poids_train.hdf5\n",
            "Epoch 317/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6481 - mse: 0.6481 - My_MSE: 0.0064 - val_loss: 0.7411 - val_mse: 0.7411 - val_My_MSE: 0.0073\n",
            "\n",
            "Epoch 00317: loss did not improve from 0.54983\n",
            "Epoch 318/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6489 - mse: 0.6489 - My_MSE: 0.0064 - val_loss: 0.7769 - val_mse: 0.7769 - val_My_MSE: 0.0077\n",
            "\n",
            "Epoch 00318: loss did not improve from 0.54983\n",
            "Epoch 319/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6489 - mse: 0.6489 - My_MSE: 0.0064 - val_loss: 0.7621 - val_mse: 0.7621 - val_My_MSE: 0.0075\n",
            "\n",
            "Epoch 00319: loss did not improve from 0.54983\n",
            "Epoch 320/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6474 - mse: 0.6474 - My_MSE: 0.0064 - val_loss: 0.7706 - val_mse: 0.7706 - val_My_MSE: 0.0076\n",
            "\n",
            "Epoch 00320: loss improved from 0.54983 to 0.54939, saving model to poids_train.hdf5\n",
            "Epoch 321/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6465 - mse: 0.6465 - My_MSE: 0.0064 - val_loss: 0.7545 - val_mse: 0.7545 - val_My_MSE: 0.0074\n",
            "\n",
            "Epoch 00321: loss did not improve from 0.54939\n",
            "Epoch 322/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6464 - mse: 0.6464 - My_MSE: 0.0064 - val_loss: 0.7574 - val_mse: 0.7574 - val_My_MSE: 0.0075\n",
            "\n",
            "Epoch 00322: loss improved from 0.54939 to 0.54922, saving model to poids_train.hdf5\n",
            "Epoch 323/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6459 - mse: 0.6459 - My_MSE: 0.0064 - val_loss: 0.7598 - val_mse: 0.7598 - val_My_MSE: 0.0075\n",
            "\n",
            "Epoch 00323: loss did not improve from 0.54922\n",
            "Epoch 324/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6465 - mse: 0.6465 - My_MSE: 0.0064 - val_loss: 0.7682 - val_mse: 0.7682 - val_My_MSE: 0.0076\n",
            "\n",
            "Epoch 00324: loss improved from 0.54922 to 0.54913, saving model to poids_train.hdf5\n",
            "Epoch 325/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6461 - mse: 0.6461 - My_MSE: 0.0064 - val_loss: 0.7640 - val_mse: 0.7640 - val_My_MSE: 0.0075\n",
            "\n",
            "Epoch 00325: loss did not improve from 0.54913\n",
            "Epoch 326/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6448 - mse: 0.6448 - My_MSE: 0.0064 - val_loss: 0.7474 - val_mse: 0.7474 - val_My_MSE: 0.0074\n",
            "\n",
            "Epoch 00326: loss improved from 0.54913 to 0.54854, saving model to poids_train.hdf5\n",
            "Epoch 327/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6443 - mse: 0.6443 - My_MSE: 0.0063 - val_loss: 0.7758 - val_mse: 0.7758 - val_My_MSE: 0.0076\n",
            "\n",
            "Epoch 00327: loss improved from 0.54854 to 0.54747, saving model to poids_train.hdf5\n",
            "Epoch 328/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6446 - mse: 0.6446 - My_MSE: 0.0063 - val_loss: 0.7479 - val_mse: 0.7479 - val_My_MSE: 0.0074\n",
            "\n",
            "Epoch 00328: loss did not improve from 0.54747\n",
            "Epoch 329/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6452 - mse: 0.6452 - My_MSE: 0.0064 - val_loss: 0.7630 - val_mse: 0.7630 - val_My_MSE: 0.0075\n",
            "\n",
            "Epoch 00329: loss did not improve from 0.54747\n",
            "Epoch 330/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6453 - mse: 0.6453 - My_MSE: 0.0064 - val_loss: 0.7825 - val_mse: 0.7825 - val_My_MSE: 0.0077\n",
            "\n",
            "Epoch 00330: loss did not improve from 0.54747\n",
            "Epoch 331/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6440 - mse: 0.6440 - My_MSE: 0.0063 - val_loss: 0.7760 - val_mse: 0.7760 - val_My_MSE: 0.0076\n",
            "\n",
            "Epoch 00331: loss did not improve from 0.54747\n",
            "Epoch 332/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6424 - mse: 0.6424 - My_MSE: 0.0063 - val_loss: 0.7533 - val_mse: 0.7533 - val_My_MSE: 0.0074\n",
            "\n",
            "Epoch 00332: loss improved from 0.54747 to 0.54651, saving model to poids_train.hdf5\n",
            "Epoch 333/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6426 - mse: 0.6426 - My_MSE: 0.0063 - val_loss: 0.7605 - val_mse: 0.7605 - val_My_MSE: 0.0075\n",
            "\n",
            "Epoch 00333: loss improved from 0.54651 to 0.54643, saving model to poids_train.hdf5\n",
            "Epoch 334/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6421 - mse: 0.6421 - My_MSE: 0.0063 - val_loss: 0.7608 - val_mse: 0.7608 - val_My_MSE: 0.0075\n",
            "\n",
            "Epoch 00334: loss improved from 0.54643 to 0.54592, saving model to poids_train.hdf5\n",
            "Epoch 335/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6418 - mse: 0.6418 - My_MSE: 0.0063 - val_loss: 0.8103 - val_mse: 0.8103 - val_My_MSE: 0.0080\n",
            "\n",
            "Epoch 00335: loss improved from 0.54592 to 0.54557, saving model to poids_train.hdf5\n",
            "Epoch 336/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6428 - mse: 0.6428 - My_MSE: 0.0063 - val_loss: 0.7656 - val_mse: 0.7656 - val_My_MSE: 0.0075\n",
            "\n",
            "Epoch 00336: loss did not improve from 0.54557\n",
            "Epoch 337/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6420 - mse: 0.6420 - My_MSE: 0.0063 - val_loss: 0.7724 - val_mse: 0.7724 - val_My_MSE: 0.0076\n",
            "\n",
            "Epoch 00337: loss did not improve from 0.54557\n",
            "Epoch 338/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6427 - mse: 0.6427 - My_MSE: 0.0063 - val_loss: 0.7655 - val_mse: 0.7655 - val_My_MSE: 0.0075\n",
            "\n",
            "Epoch 00338: loss did not improve from 0.54557\n",
            "Epoch 339/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6412 - mse: 0.6412 - My_MSE: 0.0063 - val_loss: 0.7970 - val_mse: 0.7970 - val_My_MSE: 0.0079\n",
            "\n",
            "Epoch 00339: loss improved from 0.54557 to 0.54497, saving model to poids_train.hdf5\n",
            "Epoch 340/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6422 - mse: 0.6422 - My_MSE: 0.0063 - val_loss: 0.7837 - val_mse: 0.7837 - val_My_MSE: 0.0077\n",
            "\n",
            "Epoch 00340: loss did not improve from 0.54497\n",
            "Epoch 341/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6407 - mse: 0.6407 - My_MSE: 0.0063 - val_loss: 0.7597 - val_mse: 0.7597 - val_My_MSE: 0.0075\n",
            "\n",
            "Epoch 00341: loss improved from 0.54497 to 0.54494, saving model to poids_train.hdf5\n",
            "Epoch 342/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6406 - mse: 0.6406 - My_MSE: 0.0063 - val_loss: 0.7831 - val_mse: 0.7831 - val_My_MSE: 0.0077\n",
            "\n",
            "Epoch 00342: loss improved from 0.54494 to 0.54439, saving model to poids_train.hdf5\n",
            "Epoch 343/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6399 - mse: 0.6399 - My_MSE: 0.0063 - val_loss: 0.7775 - val_mse: 0.7775 - val_My_MSE: 0.0077\n",
            "\n",
            "Epoch 00343: loss improved from 0.54439 to 0.54368, saving model to poids_train.hdf5\n",
            "Epoch 344/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6408 - mse: 0.6408 - My_MSE: 0.0063 - val_loss: 0.7960 - val_mse: 0.7960 - val_My_MSE: 0.0078\n",
            "\n",
            "Epoch 00344: loss did not improve from 0.54368\n",
            "Epoch 345/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6400 - mse: 0.6400 - My_MSE: 0.0063 - val_loss: 0.7492 - val_mse: 0.7492 - val_My_MSE: 0.0074\n",
            "\n",
            "Epoch 00345: loss did not improve from 0.54368\n",
            "Epoch 346/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6433 - mse: 0.6433 - My_MSE: 0.0063 - val_loss: 0.7900 - val_mse: 0.7900 - val_My_MSE: 0.0078\n",
            "\n",
            "Epoch 00346: loss did not improve from 0.54368\n",
            "Epoch 347/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6395 - mse: 0.6395 - My_MSE: 0.0063 - val_loss: 0.7877 - val_mse: 0.7877 - val_My_MSE: 0.0078\n",
            "\n",
            "Epoch 00347: loss improved from 0.54368 to 0.54275, saving model to poids_train.hdf5\n",
            "Epoch 348/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6382 - mse: 0.6382 - My_MSE: 0.0063 - val_loss: 0.7942 - val_mse: 0.7942 - val_My_MSE: 0.0078\n",
            "\n",
            "Epoch 00348: loss improved from 0.54275 to 0.54255, saving model to poids_train.hdf5\n",
            "Epoch 349/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6381 - mse: 0.6381 - My_MSE: 0.0063 - val_loss: 0.7754 - val_mse: 0.7754 - val_My_MSE: 0.0076\n",
            "\n",
            "Epoch 00349: loss improved from 0.54255 to 0.54249, saving model to poids_train.hdf5\n",
            "Epoch 350/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6376 - mse: 0.6376 - My_MSE: 0.0063 - val_loss: 0.7727 - val_mse: 0.7727 - val_My_MSE: 0.0076\n",
            "\n",
            "Epoch 00350: loss did not improve from 0.54249\n",
            "Epoch 351/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6403 - mse: 0.6403 - My_MSE: 0.0063 - val_loss: 0.8014 - val_mse: 0.8014 - val_My_MSE: 0.0079\n",
            "\n",
            "Epoch 00351: loss did not improve from 0.54249\n",
            "Epoch 352/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6379 - mse: 0.6379 - My_MSE: 0.0063 - val_loss: 0.7969 - val_mse: 0.7969 - val_My_MSE: 0.0078\n",
            "\n",
            "Epoch 00352: loss improved from 0.54249 to 0.54222, saving model to poids_train.hdf5\n",
            "Epoch 353/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6368 - mse: 0.6368 - My_MSE: 0.0063 - val_loss: 0.7871 - val_mse: 0.7871 - val_My_MSE: 0.0078\n",
            "\n",
            "Epoch 00353: loss improved from 0.54222 to 0.54134, saving model to poids_train.hdf5\n",
            "Epoch 354/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6367 - mse: 0.6367 - My_MSE: 0.0063 - val_loss: 0.7843 - val_mse: 0.7843 - val_My_MSE: 0.0077\n",
            "\n",
            "Epoch 00354: loss did not improve from 0.54134\n",
            "Epoch 355/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6400 - mse: 0.6400 - My_MSE: 0.0063 - val_loss: 0.7825 - val_mse: 0.7825 - val_My_MSE: 0.0077\n",
            "\n",
            "Epoch 00355: loss did not improve from 0.54134\n",
            "Epoch 356/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6379 - mse: 0.6379 - My_MSE: 0.0063 - val_loss: 0.8052 - val_mse: 0.8052 - val_My_MSE: 0.0079\n",
            "\n",
            "Epoch 00356: loss did not improve from 0.54134\n",
            "Epoch 357/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6359 - mse: 0.6359 - My_MSE: 0.0063 - val_loss: 0.8076 - val_mse: 0.8076 - val_My_MSE: 0.0080\n",
            "\n",
            "Epoch 00357: loss improved from 0.54134 to 0.54045, saving model to poids_train.hdf5\n",
            "Epoch 358/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6350 - mse: 0.6350 - My_MSE: 0.0063 - val_loss: 0.8089 - val_mse: 0.8089 - val_My_MSE: 0.0080\n",
            "\n",
            "Epoch 00358: loss improved from 0.54045 to 0.53968, saving model to poids_train.hdf5\n",
            "Epoch 359/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6349 - mse: 0.6349 - My_MSE: 0.0063 - val_loss: 0.7944 - val_mse: 0.7944 - val_My_MSE: 0.0078\n",
            "\n",
            "Epoch 00359: loss did not improve from 0.53968\n",
            "Epoch 360/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6370 - mse: 0.6370 - My_MSE: 0.0063 - val_loss: 0.7744 - val_mse: 0.7744 - val_My_MSE: 0.0076\n",
            "\n",
            "Epoch 00360: loss did not improve from 0.53968\n",
            "Epoch 361/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6373 - mse: 0.6373 - My_MSE: 0.0063 - val_loss: 0.8125 - val_mse: 0.8125 - val_My_MSE: 0.0080\n",
            "\n",
            "Epoch 00361: loss did not improve from 0.53968\n",
            "Epoch 362/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6353 - mse: 0.6353 - My_MSE: 0.0063 - val_loss: 0.7932 - val_mse: 0.7932 - val_My_MSE: 0.0078\n",
            "\n",
            "Epoch 00362: loss improved from 0.53968 to 0.53949, saving model to poids_train.hdf5\n",
            "Epoch 363/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6330 - mse: 0.6330 - My_MSE: 0.0062 - val_loss: 0.8169 - val_mse: 0.8169 - val_My_MSE: 0.0080\n",
            "\n",
            "Epoch 00363: loss improved from 0.53949 to 0.53859, saving model to poids_train.hdf5\n",
            "Epoch 364/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6346 - mse: 0.6346 - My_MSE: 0.0063 - val_loss: 0.7921 - val_mse: 0.7921 - val_My_MSE: 0.0078\n",
            "\n",
            "Epoch 00364: loss did not improve from 0.53859\n",
            "Epoch 365/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6364 - mse: 0.6364 - My_MSE: 0.0063 - val_loss: 0.8065 - val_mse: 0.8065 - val_My_MSE: 0.0079\n",
            "\n",
            "Epoch 00365: loss did not improve from 0.53859\n",
            "Epoch 366/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6348 - mse: 0.6348 - My_MSE: 0.0063 - val_loss: 0.8280 - val_mse: 0.8280 - val_My_MSE: 0.0082\n",
            "\n",
            "Epoch 00366: loss did not improve from 0.53859\n",
            "Epoch 367/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6333 - mse: 0.6333 - My_MSE: 0.0062 - val_loss: 0.8158 - val_mse: 0.8158 - val_My_MSE: 0.0080\n",
            "\n",
            "Epoch 00367: loss improved from 0.53859 to 0.53842, saving model to poids_train.hdf5\n",
            "Epoch 368/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6330 - mse: 0.6330 - My_MSE: 0.0062 - val_loss: 0.8181 - val_mse: 0.8181 - val_My_MSE: 0.0081\n",
            "\n",
            "Epoch 00368: loss did not improve from 0.53842\n",
            "Epoch 369/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6331 - mse: 0.6331 - My_MSE: 0.0062 - val_loss: 0.7832 - val_mse: 0.7832 - val_My_MSE: 0.0077\n",
            "\n",
            "Epoch 00369: loss did not improve from 0.53842\n",
            "Epoch 370/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6337 - mse: 0.6337 - My_MSE: 0.0062 - val_loss: 0.8046 - val_mse: 0.8046 - val_My_MSE: 0.0079\n",
            "\n",
            "Epoch 00370: loss did not improve from 0.53842\n",
            "Epoch 371/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6339 - mse: 0.6339 - My_MSE: 0.0062 - val_loss: 0.8010 - val_mse: 0.8010 - val_My_MSE: 0.0079\n",
            "\n",
            "Epoch 00371: loss did not improve from 0.53842\n",
            "Epoch 372/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6346 - mse: 0.6346 - My_MSE: 0.0063 - val_loss: 0.8063 - val_mse: 0.8063 - val_My_MSE: 0.0079\n",
            "\n",
            "Epoch 00372: loss did not improve from 0.53842\n",
            "Epoch 373/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6323 - mse: 0.6323 - My_MSE: 0.0062 - val_loss: 0.8282 - val_mse: 0.8282 - val_My_MSE: 0.0082\n",
            "\n",
            "Epoch 00373: loss improved from 0.53842 to 0.53800, saving model to poids_train.hdf5\n",
            "Epoch 374/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6306 - mse: 0.6306 - My_MSE: 0.0062 - val_loss: 0.8141 - val_mse: 0.8141 - val_My_MSE: 0.0080\n",
            "\n",
            "Epoch 00374: loss improved from 0.53800 to 0.53664, saving model to poids_train.hdf5\n",
            "Epoch 375/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6299 - mse: 0.6299 - My_MSE: 0.0062 - val_loss: 0.8070 - val_mse: 0.8070 - val_My_MSE: 0.0079\n",
            "\n",
            "Epoch 00375: loss improved from 0.53664 to 0.53627, saving model to poids_train.hdf5\n",
            "Epoch 376/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6308 - mse: 0.6308 - My_MSE: 0.0062 - val_loss: 0.8248 - val_mse: 0.8248 - val_My_MSE: 0.0081\n",
            "\n",
            "Epoch 00376: loss did not improve from 0.53627\n",
            "Epoch 377/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6304 - mse: 0.6304 - My_MSE: 0.0062 - val_loss: 0.8065 - val_mse: 0.8065 - val_My_MSE: 0.0079\n",
            "\n",
            "Epoch 00377: loss did not improve from 0.53627\n",
            "Epoch 378/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6348 - mse: 0.6348 - My_MSE: 0.0063 - val_loss: 0.8331 - val_mse: 0.8331 - val_My_MSE: 0.0082\n",
            "\n",
            "Epoch 00378: loss did not improve from 0.53627\n",
            "Epoch 379/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6329 - mse: 0.6329 - My_MSE: 0.0062 - val_loss: 0.8089 - val_mse: 0.8089 - val_My_MSE: 0.0080\n",
            "\n",
            "Epoch 00379: loss did not improve from 0.53627\n",
            "Epoch 380/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6291 - mse: 0.6291 - My_MSE: 0.0062 - val_loss: 0.8490 - val_mse: 0.8490 - val_My_MSE: 0.0084\n",
            "\n",
            "Epoch 00380: loss improved from 0.53627 to 0.53541, saving model to poids_train.hdf5\n",
            "Epoch 381/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6300 - mse: 0.6300 - My_MSE: 0.0062 - val_loss: 0.8168 - val_mse: 0.8168 - val_My_MSE: 0.0080\n",
            "\n",
            "Epoch 00381: loss did not improve from 0.53541\n",
            "Epoch 382/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6309 - mse: 0.6309 - My_MSE: 0.0062 - val_loss: 0.8209 - val_mse: 0.8209 - val_My_MSE: 0.0081\n",
            "\n",
            "Epoch 00382: loss did not improve from 0.53541\n",
            "Epoch 383/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6322 - mse: 0.6322 - My_MSE: 0.0062 - val_loss: 0.8392 - val_mse: 0.8392 - val_My_MSE: 0.0083\n",
            "\n",
            "Epoch 00383: loss did not improve from 0.53541\n",
            "Epoch 384/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6298 - mse: 0.6298 - My_MSE: 0.0062 - val_loss: 0.8186 - val_mse: 0.8186 - val_My_MSE: 0.0081\n",
            "\n",
            "Epoch 00384: loss did not improve from 0.53541\n",
            "Epoch 385/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6286 - mse: 0.6286 - My_MSE: 0.0062 - val_loss: 0.8390 - val_mse: 0.8390 - val_My_MSE: 0.0083\n",
            "\n",
            "Epoch 00385: loss improved from 0.53541 to 0.53519, saving model to poids_train.hdf5\n",
            "Epoch 386/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6272 - mse: 0.6272 - My_MSE: 0.0062 - val_loss: 0.8250 - val_mse: 0.8250 - val_My_MSE: 0.0081\n",
            "\n",
            "Epoch 00386: loss improved from 0.53519 to 0.53398, saving model to poids_train.hdf5\n",
            "Epoch 387/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6267 - mse: 0.6267 - My_MSE: 0.0062 - val_loss: 0.8294 - val_mse: 0.8294 - val_My_MSE: 0.0082\n",
            "\n",
            "Epoch 00387: loss did not improve from 0.53398\n",
            "Epoch 388/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6301 - mse: 0.6301 - My_MSE: 0.0062 - val_loss: 0.7971 - val_mse: 0.7971 - val_My_MSE: 0.0079\n",
            "\n",
            "Epoch 00388: loss did not improve from 0.53398\n",
            "Epoch 389/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6405 - mse: 0.6405 - My_MSE: 0.0063 - val_loss: 0.8075 - val_mse: 0.8075 - val_My_MSE: 0.0080\n",
            "\n",
            "Epoch 00389: loss did not improve from 0.53398\n",
            "Epoch 390/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6305 - mse: 0.6305 - My_MSE: 0.0062 - val_loss: 0.8436 - val_mse: 0.8436 - val_My_MSE: 0.0083\n",
            "\n",
            "Epoch 00390: loss did not improve from 0.53398\n",
            "Epoch 391/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6265 - mse: 0.6265 - My_MSE: 0.0062 - val_loss: 0.8440 - val_mse: 0.8440 - val_My_MSE: 0.0083\n",
            "\n",
            "Epoch 00391: loss improved from 0.53398 to 0.53361, saving model to poids_train.hdf5\n",
            "Epoch 392/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6251 - mse: 0.6251 - My_MSE: 0.0062 - val_loss: 0.8468 - val_mse: 0.8468 - val_My_MSE: 0.0083\n",
            "\n",
            "Epoch 00392: loss improved from 0.53361 to 0.53258, saving model to poids_train.hdf5\n",
            "Epoch 393/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6270 - mse: 0.6270 - My_MSE: 0.0062 - val_loss: 0.8464 - val_mse: 0.8464 - val_My_MSE: 0.0083\n",
            "\n",
            "Epoch 00393: loss did not improve from 0.53258\n",
            "Epoch 394/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6266 - mse: 0.6266 - My_MSE: 0.0062 - val_loss: 0.8355 - val_mse: 0.8355 - val_My_MSE: 0.0082\n",
            "\n",
            "Epoch 00394: loss did not improve from 0.53258\n",
            "Epoch 395/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6306 - mse: 0.6306 - My_MSE: 0.0062 - val_loss: 0.8285 - val_mse: 0.8285 - val_My_MSE: 0.0082\n",
            "\n",
            "Epoch 00395: loss did not improve from 0.53258\n",
            "Epoch 396/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6278 - mse: 0.6278 - My_MSE: 0.0062 - val_loss: 0.8311 - val_mse: 0.8311 - val_My_MSE: 0.0082\n",
            "\n",
            "Epoch 00396: loss did not improve from 0.53258\n",
            "Epoch 397/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6257 - mse: 0.6257 - My_MSE: 0.0062 - val_loss: 0.8367 - val_mse: 0.8367 - val_My_MSE: 0.0082\n",
            "\n",
            "Epoch 00397: loss did not improve from 0.53258\n",
            "Epoch 398/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6244 - mse: 0.6244 - My_MSE: 0.0062 - val_loss: 0.8573 - val_mse: 0.8573 - val_My_MSE: 0.0084\n",
            "\n",
            "Epoch 00398: loss improved from 0.53258 to 0.53231, saving model to poids_train.hdf5\n",
            "Epoch 399/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6248 - mse: 0.6248 - My_MSE: 0.0062 - val_loss: 0.8507 - val_mse: 0.8507 - val_My_MSE: 0.0084\n",
            "\n",
            "Epoch 00399: loss improved from 0.53231 to 0.53178, saving model to poids_train.hdf5\n",
            "Epoch 400/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6237 - mse: 0.6237 - My_MSE: 0.0061 - val_loss: 0.8388 - val_mse: 0.8388 - val_My_MSE: 0.0083\n",
            "\n",
            "Epoch 00400: loss did not improve from 0.53178\n",
            "Epoch 401/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6274 - mse: 0.6274 - My_MSE: 0.0062 - val_loss: 0.8458 - val_mse: 0.8458 - val_My_MSE: 0.0083\n",
            "\n",
            "Epoch 00401: loss did not improve from 0.53178\n",
            "Epoch 402/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6271 - mse: 0.6271 - My_MSE: 0.0062 - val_loss: 0.8309 - val_mse: 0.8309 - val_My_MSE: 0.0082\n",
            "\n",
            "Epoch 00402: loss did not improve from 0.53178\n",
            "Epoch 403/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6260 - mse: 0.6260 - My_MSE: 0.0062 - val_loss: 0.8341 - val_mse: 0.8341 - val_My_MSE: 0.0082\n",
            "\n",
            "Epoch 00403: loss did not improve from 0.53178\n",
            "Epoch 404/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6261 - mse: 0.6261 - My_MSE: 0.0062 - val_loss: 0.8388 - val_mse: 0.8388 - val_My_MSE: 0.0083\n",
            "\n",
            "Epoch 00404: loss did not improve from 0.53178\n",
            "Epoch 405/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6263 - mse: 0.6263 - My_MSE: 0.0062 - val_loss: 0.8602 - val_mse: 0.8602 - val_My_MSE: 0.0085\n",
            "\n",
            "Epoch 00405: loss did not improve from 0.53178\n",
            "Epoch 406/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6237 - mse: 0.6237 - My_MSE: 0.0061 - val_loss: 0.8434 - val_mse: 0.8434 - val_My_MSE: 0.0083\n",
            "\n",
            "Epoch 00406: loss improved from 0.53178 to 0.53146, saving model to poids_train.hdf5\n",
            "Epoch 407/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6226 - mse: 0.6226 - My_MSE: 0.0061 - val_loss: 0.8636 - val_mse: 0.8636 - val_My_MSE: 0.0085\n",
            "\n",
            "Epoch 00407: loss improved from 0.53146 to 0.53065, saving model to poids_train.hdf5\n",
            "Epoch 408/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6209 - mse: 0.6209 - My_MSE: 0.0061 - val_loss: 0.8442 - val_mse: 0.8442 - val_My_MSE: 0.0083\n",
            "\n",
            "Epoch 00408: loss improved from 0.53065 to 0.53011, saving model to poids_train.hdf5\n",
            "Epoch 409/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6221 - mse: 0.6221 - My_MSE: 0.0061 - val_loss: 0.8614 - val_mse: 0.8614 - val_My_MSE: 0.0085\n",
            "\n",
            "Epoch 00409: loss did not improve from 0.53011\n",
            "Epoch 410/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6252 - mse: 0.6252 - My_MSE: 0.0062 - val_loss: 0.8316 - val_mse: 0.8316 - val_My_MSE: 0.0082\n",
            "\n",
            "Epoch 00410: loss did not improve from 0.53011\n",
            "Epoch 411/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6275 - mse: 0.6275 - My_MSE: 0.0062 - val_loss: 0.8539 - val_mse: 0.8539 - val_My_MSE: 0.0084\n",
            "\n",
            "Epoch 00411: loss did not improve from 0.53011\n",
            "Epoch 412/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6224 - mse: 0.6224 - My_MSE: 0.0061 - val_loss: 0.8587 - val_mse: 0.8587 - val_My_MSE: 0.0085\n",
            "\n",
            "Epoch 00412: loss did not improve from 0.53011\n",
            "Epoch 413/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6248 - mse: 0.6248 - My_MSE: 0.0062 - val_loss: 0.8545 - val_mse: 0.8545 - val_My_MSE: 0.0084\n",
            "\n",
            "Epoch 00413: loss did not improve from 0.53011\n",
            "Epoch 414/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6237 - mse: 0.6237 - My_MSE: 0.0061 - val_loss: 0.8550 - val_mse: 0.8550 - val_My_MSE: 0.0084\n",
            "\n",
            "Epoch 00414: loss did not improve from 0.53011\n",
            "Epoch 415/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6211 - mse: 0.6211 - My_MSE: 0.0061 - val_loss: 0.8663 - val_mse: 0.8663 - val_My_MSE: 0.0085\n",
            "\n",
            "Epoch 00415: loss improved from 0.53011 to 0.52987, saving model to poids_train.hdf5\n",
            "Epoch 416/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6212 - mse: 0.6212 - My_MSE: 0.0061 - val_loss: 0.8504 - val_mse: 0.8504 - val_My_MSE: 0.0084\n",
            "\n",
            "Epoch 00416: loss improved from 0.52987 to 0.52950, saving model to poids_train.hdf5\n",
            "Epoch 417/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6199 - mse: 0.6199 - My_MSE: 0.0061 - val_loss: 0.8674 - val_mse: 0.8674 - val_My_MSE: 0.0085\n",
            "\n",
            "Epoch 00417: loss improved from 0.52950 to 0.52899, saving model to poids_train.hdf5\n",
            "Epoch 418/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6209 - mse: 0.6209 - My_MSE: 0.0061 - val_loss: 0.8589 - val_mse: 0.8589 - val_My_MSE: 0.0085\n",
            "\n",
            "Epoch 00418: loss did not improve from 0.52899\n",
            "Epoch 419/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6229 - mse: 0.6229 - My_MSE: 0.0061 - val_loss: 0.8439 - val_mse: 0.8439 - val_My_MSE: 0.0083\n",
            "\n",
            "Epoch 00419: loss did not improve from 0.52899\n",
            "Epoch 420/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6226 - mse: 0.6226 - My_MSE: 0.0061 - val_loss: 0.8595 - val_mse: 0.8595 - val_My_MSE: 0.0085\n",
            "\n",
            "Epoch 00420: loss did not improve from 0.52899\n",
            "Epoch 421/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6196 - mse: 0.6196 - My_MSE: 0.0061 - val_loss: 0.8601 - val_mse: 0.8601 - val_My_MSE: 0.0085\n",
            "\n",
            "Epoch 00421: loss did not improve from 0.52899\n",
            "Epoch 422/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6214 - mse: 0.6214 - My_MSE: 0.0061 - val_loss: 0.8629 - val_mse: 0.8629 - val_My_MSE: 0.0085\n",
            "\n",
            "Epoch 00422: loss did not improve from 0.52899\n",
            "Epoch 423/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6207 - mse: 0.6207 - My_MSE: 0.0061 - val_loss: 0.8702 - val_mse: 0.8702 - val_My_MSE: 0.0086\n",
            "\n",
            "Epoch 00423: loss improved from 0.52899 to 0.52881, saving model to poids_train.hdf5\n",
            "Epoch 424/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6205 - mse: 0.6205 - My_MSE: 0.0061 - val_loss: 0.8678 - val_mse: 0.8678 - val_My_MSE: 0.0085\n",
            "\n",
            "Epoch 00424: loss did not improve from 0.52881\n",
            "Epoch 425/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6183 - mse: 0.6183 - My_MSE: 0.0061 - val_loss: 0.8689 - val_mse: 0.8689 - val_My_MSE: 0.0086\n",
            "\n",
            "Epoch 00425: loss improved from 0.52881 to 0.52846, saving model to poids_train.hdf5\n",
            "Epoch 426/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6192 - mse: 0.6192 - My_MSE: 0.0061 - val_loss: 0.8575 - val_mse: 0.8575 - val_My_MSE: 0.0084\n",
            "\n",
            "Epoch 00426: loss did not improve from 0.52846\n",
            "Epoch 427/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6198 - mse: 0.6198 - My_MSE: 0.0061 - val_loss: 0.8799 - val_mse: 0.8799 - val_My_MSE: 0.0087\n",
            "\n",
            "Epoch 00427: loss did not improve from 0.52846\n",
            "Epoch 428/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6196 - mse: 0.6196 - My_MSE: 0.0061 - val_loss: 0.8511 - val_mse: 0.8511 - val_My_MSE: 0.0084\n",
            "\n",
            "Epoch 00428: loss did not improve from 0.52846\n",
            "Epoch 429/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6198 - mse: 0.6198 - My_MSE: 0.0061 - val_loss: 0.8638 - val_mse: 0.8638 - val_My_MSE: 0.0085\n",
            "\n",
            "Epoch 00429: loss did not improve from 0.52846\n",
            "Epoch 430/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6189 - mse: 0.6189 - My_MSE: 0.0061 - val_loss: 0.8785 - val_mse: 0.8785 - val_My_MSE: 0.0087\n",
            "\n",
            "Epoch 00430: loss improved from 0.52846 to 0.52769, saving model to poids_train.hdf5\n",
            "Epoch 431/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6171 - mse: 0.6171 - My_MSE: 0.0061 - val_loss: 0.8688 - val_mse: 0.8688 - val_My_MSE: 0.0086\n",
            "\n",
            "Epoch 00431: loss improved from 0.52769 to 0.52714, saving model to poids_train.hdf5\n",
            "Epoch 432/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6189 - mse: 0.6189 - My_MSE: 0.0061 - val_loss: 0.8884 - val_mse: 0.8884 - val_My_MSE: 0.0087\n",
            "\n",
            "Epoch 00432: loss did not improve from 0.52714\n",
            "Epoch 433/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6180 - mse: 0.6180 - My_MSE: 0.0061 - val_loss: 0.8530 - val_mse: 0.8530 - val_My_MSE: 0.0084\n",
            "\n",
            "Epoch 00433: loss did not improve from 0.52714\n",
            "Epoch 434/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6185 - mse: 0.6185 - My_MSE: 0.0061 - val_loss: 0.8873 - val_mse: 0.8873 - val_My_MSE: 0.0087\n",
            "\n",
            "Epoch 00434: loss did not improve from 0.52714\n",
            "Epoch 435/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6169 - mse: 0.6169 - My_MSE: 0.0061 - val_loss: 0.8725 - val_mse: 0.8725 - val_My_MSE: 0.0086\n",
            "\n",
            "Epoch 00435: loss improved from 0.52714 to 0.52665, saving model to poids_train.hdf5\n",
            "Epoch 436/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6202 - mse: 0.6202 - My_MSE: 0.0061 - val_loss: 0.8758 - val_mse: 0.8758 - val_My_MSE: 0.0086\n",
            "\n",
            "Epoch 00436: loss did not improve from 0.52665\n",
            "Epoch 437/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6180 - mse: 0.6180 - My_MSE: 0.0061 - val_loss: 0.8709 - val_mse: 0.8709 - val_My_MSE: 0.0086\n",
            "\n",
            "Epoch 00437: loss did not improve from 0.52665\n",
            "Epoch 438/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6158 - mse: 0.6158 - My_MSE: 0.0061 - val_loss: 0.8680 - val_mse: 0.8680 - val_My_MSE: 0.0085\n",
            "\n",
            "Epoch 00438: loss improved from 0.52665 to 0.52615, saving model to poids_train.hdf5\n",
            "Epoch 439/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6174 - mse: 0.6174 - My_MSE: 0.0061 - val_loss: 0.8863 - val_mse: 0.8863 - val_My_MSE: 0.0087\n",
            "\n",
            "Epoch 00439: loss did not improve from 0.52615\n",
            "Epoch 440/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6180 - mse: 0.6180 - My_MSE: 0.0061 - val_loss: 0.8588 - val_mse: 0.8588 - val_My_MSE: 0.0085\n",
            "\n",
            "Epoch 00440: loss did not improve from 0.52615\n",
            "Epoch 441/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6180 - mse: 0.6180 - My_MSE: 0.0061 - val_loss: 0.8914 - val_mse: 0.8914 - val_My_MSE: 0.0088\n",
            "\n",
            "Epoch 00441: loss did not improve from 0.52615\n",
            "Epoch 442/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6155 - mse: 0.6155 - My_MSE: 0.0061 - val_loss: 0.8873 - val_mse: 0.8873 - val_My_MSE: 0.0087\n",
            "\n",
            "Epoch 00442: loss improved from 0.52615 to 0.52534, saving model to poids_train.hdf5\n",
            "Epoch 443/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6144 - mse: 0.6144 - My_MSE: 0.0061 - val_loss: 0.8954 - val_mse: 0.8954 - val_My_MSE: 0.0088\n",
            "\n",
            "Epoch 00443: loss improved from 0.52534 to 0.52478, saving model to poids_train.hdf5\n",
            "Epoch 444/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6140 - mse: 0.6140 - My_MSE: 0.0060 - val_loss: 0.8531 - val_mse: 0.8531 - val_My_MSE: 0.0084\n",
            "\n",
            "Epoch 00444: loss did not improve from 0.52478\n",
            "Epoch 445/10000\n",
            "555/555 [==============================] - 62s 113ms/step - loss: 0.6170 - mse: 0.6170 - My_MSE: 0.0061 - val_loss: 0.8660 - val_mse: 0.8660 - val_My_MSE: 0.0085\n",
            "\n",
            "Epoch 00445: loss did not improve from 0.52478\n",
            "Epoch 446/10000\n",
            "555/555 [==============================] - 63s 113ms/step - loss: 0.6149 - mse: 0.6149 - My_MSE: 0.0061 - val_loss: 0.8923 - val_mse: 0.8923 - val_My_MSE: 0.0088\n",
            "\n",
            "Epoch 00446: loss did not improve from 0.52478\n",
            "Epoch 447/10000\n",
            "555/555 [==============================] - 63s 113ms/step - loss: 0.6142 - mse: 0.6142 - My_MSE: 0.0061 - val_loss: 0.9009 - val_mse: 0.9009 - val_My_MSE: 0.0089\n",
            "\n",
            "Epoch 00447: loss did not improve from 0.52478\n",
            "Epoch 448/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6155 - mse: 0.6155 - My_MSE: 0.0061 - val_loss: 0.8657 - val_mse: 0.8657 - val_My_MSE: 0.0085\n",
            "\n",
            "Epoch 00448: loss did not improve from 0.52478\n",
            "Epoch 449/10000\n",
            "555/555 [==============================] - 62s 113ms/step - loss: 0.6159 - mse: 0.6159 - My_MSE: 0.0061 - val_loss: 0.8747 - val_mse: 0.8747 - val_My_MSE: 0.0086\n",
            "\n",
            "Epoch 00449: loss did not improve from 0.52478\n",
            "Epoch 450/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6160 - mse: 0.6160 - My_MSE: 0.0061 - val_loss: 0.8934 - val_mse: 0.8934 - val_My_MSE: 0.0088\n",
            "\n",
            "Epoch 00450: loss did not improve from 0.52478\n",
            "Epoch 451/10000\n",
            "555/555 [==============================] - 62s 113ms/step - loss: 0.6141 - mse: 0.6141 - My_MSE: 0.0060 - val_loss: 0.8922 - val_mse: 0.8922 - val_My_MSE: 0.0088\n",
            "\n",
            "Epoch 00451: loss improved from 0.52478 to 0.52409, saving model to poids_train.hdf5\n",
            "Epoch 452/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6123 - mse: 0.6123 - My_MSE: 0.0060 - val_loss: 0.8909 - val_mse: 0.8909 - val_My_MSE: 0.0088\n",
            "\n",
            "Epoch 00452: loss improved from 0.52409 to 0.52365, saving model to poids_train.hdf5\n",
            "Epoch 453/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6130 - mse: 0.6130 - My_MSE: 0.0060 - val_loss: 0.8853 - val_mse: 0.8853 - val_My_MSE: 0.0087\n",
            "\n",
            "Epoch 00453: loss did not improve from 0.52365\n",
            "Epoch 454/10000\n",
            "555/555 [==============================] - 63s 113ms/step - loss: 0.6160 - mse: 0.6160 - My_MSE: 0.0061 - val_loss: 0.8920 - val_mse: 0.8920 - val_My_MSE: 0.0088\n",
            "\n",
            "Epoch 00454: loss did not improve from 0.52365\n",
            "Epoch 455/10000\n",
            "555/555 [==============================] - 63s 113ms/step - loss: 0.6187 - mse: 0.6187 - My_MSE: 0.0061 - val_loss: 0.8988 - val_mse: 0.8988 - val_My_MSE: 0.0089\n",
            "\n",
            "Epoch 00455: loss did not improve from 0.52365\n",
            "Epoch 456/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6129 - mse: 0.6129 - My_MSE: 0.0060 - val_loss: 0.8725 - val_mse: 0.8725 - val_My_MSE: 0.0086\n",
            "\n",
            "Epoch 00456: loss did not improve from 0.52365\n",
            "Epoch 457/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6129 - mse: 0.6129 - My_MSE: 0.0060 - val_loss: 0.9081 - val_mse: 0.9081 - val_My_MSE: 0.0089\n",
            "\n",
            "Epoch 00457: loss did not improve from 0.52365\n",
            "Epoch 458/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6132 - mse: 0.6132 - My_MSE: 0.0060 - val_loss: 0.8896 - val_mse: 0.8896 - val_My_MSE: 0.0088\n",
            "\n",
            "Epoch 00458: loss improved from 0.52365 to 0.52355, saving model to poids_train.hdf5\n",
            "Epoch 459/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6124 - mse: 0.6124 - My_MSE: 0.0060 - val_loss: 0.8892 - val_mse: 0.8892 - val_My_MSE: 0.0088\n",
            "\n",
            "Epoch 00459: loss improved from 0.52355 to 0.52345, saving model to poids_train.hdf5\n",
            "Epoch 460/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6140 - mse: 0.6140 - My_MSE: 0.0060 - val_loss: 0.8917 - val_mse: 0.8917 - val_My_MSE: 0.0088\n",
            "\n",
            "Epoch 00460: loss did not improve from 0.52345\n",
            "Epoch 461/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6143 - mse: 0.6143 - My_MSE: 0.0061 - val_loss: 0.8770 - val_mse: 0.8770 - val_My_MSE: 0.0086\n",
            "\n",
            "Epoch 00461: loss did not improve from 0.52345\n",
            "Epoch 462/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6121 - mse: 0.6121 - My_MSE: 0.0060 - val_loss: 0.9170 - val_mse: 0.9170 - val_My_MSE: 0.0090\n",
            "\n",
            "Epoch 00462: loss improved from 0.52345 to 0.52289, saving model to poids_train.hdf5\n",
            "Epoch 463/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6119 - mse: 0.6119 - My_MSE: 0.0060 - val_loss: 0.8932 - val_mse: 0.8932 - val_My_MSE: 0.0088\n",
            "\n",
            "Epoch 00463: loss improved from 0.52289 to 0.52274, saving model to poids_train.hdf5\n",
            "Epoch 464/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6164 - mse: 0.6164 - My_MSE: 0.0061 - val_loss: 0.9105 - val_mse: 0.9105 - val_My_MSE: 0.0090\n",
            "\n",
            "Epoch 00464: loss did not improve from 0.52274\n",
            "Epoch 465/10000\n",
            "555/555 [==============================] - 62s 113ms/step - loss: 0.6118 - mse: 0.6118 - My_MSE: 0.0060 - val_loss: 0.8831 - val_mse: 0.8831 - val_My_MSE: 0.0087\n",
            "\n",
            "Epoch 00465: loss did not improve from 0.52274\n",
            "Epoch 466/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6118 - mse: 0.6118 - My_MSE: 0.0060 - val_loss: 0.8784 - val_mse: 0.8784 - val_My_MSE: 0.0087\n",
            "\n",
            "Epoch 00466: loss did not improve from 0.52274\n",
            "Epoch 467/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6134 - mse: 0.6134 - My_MSE: 0.0060 - val_loss: 0.8928 - val_mse: 0.8928 - val_My_MSE: 0.0088\n",
            "\n",
            "Epoch 00467: loss did not improve from 0.52274\n",
            "Epoch 468/10000\n",
            "555/555 [==============================] - 63s 113ms/step - loss: 0.6088 - mse: 0.6088 - My_MSE: 0.0060 - val_loss: 0.9282 - val_mse: 0.9282 - val_My_MSE: 0.0091\n",
            "\n",
            "Epoch 00468: loss improved from 0.52274 to 0.52121, saving model to poids_train.hdf5\n",
            "Epoch 469/10000\n",
            "555/555 [==============================] - 63s 113ms/step - loss: 0.6095 - mse: 0.6095 - My_MSE: 0.0060 - val_loss: 0.9002 - val_mse: 0.9002 - val_My_MSE: 0.0089\n",
            "\n",
            "Epoch 00469: loss did not improve from 0.52121\n",
            "Epoch 470/10000\n",
            "555/555 [==============================] - 63s 113ms/step - loss: 0.6097 - mse: 0.6097 - My_MSE: 0.0060 - val_loss: 0.8914 - val_mse: 0.8914 - val_My_MSE: 0.0088\n",
            "\n",
            "Epoch 00470: loss did not improve from 0.52121\n",
            "Epoch 471/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6107 - mse: 0.6107 - My_MSE: 0.0060 - val_loss: 0.8814 - val_mse: 0.8814 - val_My_MSE: 0.0087\n",
            "\n",
            "Epoch 00471: loss did not improve from 0.52121\n",
            "Epoch 472/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6118 - mse: 0.6118 - My_MSE: 0.0060 - val_loss: 0.9278 - val_mse: 0.9278 - val_My_MSE: 0.0091\n",
            "\n",
            "Epoch 00472: loss did not improve from 0.52121\n",
            "Epoch 473/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6128 - mse: 0.6128 - My_MSE: 0.0060 - val_loss: 0.8996 - val_mse: 0.8996 - val_My_MSE: 0.0089\n",
            "\n",
            "Epoch 00473: loss did not improve from 0.52121\n",
            "Epoch 474/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6099 - mse: 0.6099 - My_MSE: 0.0060 - val_loss: 0.9234 - val_mse: 0.9234 - val_My_MSE: 0.0091\n",
            "\n",
            "Epoch 00474: loss did not improve from 0.52121\n",
            "Epoch 475/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6089 - mse: 0.6089 - My_MSE: 0.0060 - val_loss: 0.8937 - val_mse: 0.8937 - val_My_MSE: 0.0088\n",
            "\n",
            "Epoch 00475: loss did not improve from 0.52121\n",
            "Epoch 476/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6086 - mse: 0.6086 - My_MSE: 0.0060 - val_loss: 0.9191 - val_mse: 0.9191 - val_My_MSE: 0.0091\n",
            "\n",
            "Epoch 00476: loss improved from 0.52121 to 0.52090, saving model to poids_train.hdf5\n",
            "Epoch 477/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6074 - mse: 0.6074 - My_MSE: 0.0060 - val_loss: 0.9186 - val_mse: 0.9186 - val_My_MSE: 0.0090\n",
            "\n",
            "Epoch 00477: loss improved from 0.52090 to 0.51990, saving model to poids_train.hdf5\n",
            "Epoch 478/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6096 - mse: 0.6096 - My_MSE: 0.0060 - val_loss: 0.9169 - val_mse: 0.9169 - val_My_MSE: 0.0090\n",
            "\n",
            "Epoch 00478: loss did not improve from 0.51990\n",
            "Epoch 479/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6087 - mse: 0.6087 - My_MSE: 0.0060 - val_loss: 0.8935 - val_mse: 0.8935 - val_My_MSE: 0.0088\n",
            "\n",
            "Epoch 00479: loss did not improve from 0.51990\n",
            "Epoch 480/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6093 - mse: 0.6093 - My_MSE: 0.0060 - val_loss: 0.9049 - val_mse: 0.9049 - val_My_MSE: 0.0089\n",
            "\n",
            "Epoch 00480: loss did not improve from 0.51990\n",
            "Epoch 481/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6083 - mse: 0.6083 - My_MSE: 0.0060 - val_loss: 0.9069 - val_mse: 0.9069 - val_My_MSE: 0.0089\n",
            "\n",
            "Epoch 00481: loss did not improve from 0.51990\n",
            "Epoch 482/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6074 - mse: 0.6074 - My_MSE: 0.0060 - val_loss: 0.9077 - val_mse: 0.9077 - val_My_MSE: 0.0089\n",
            "\n",
            "Epoch 00482: loss did not improve from 0.51990\n",
            "Epoch 483/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6087 - mse: 0.6087 - My_MSE: 0.0060 - val_loss: 0.9212 - val_mse: 0.9212 - val_My_MSE: 0.0091\n",
            "\n",
            "Epoch 00483: loss did not improve from 0.51990\n",
            "Epoch 484/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6085 - mse: 0.6085 - My_MSE: 0.0060 - val_loss: 0.9089 - val_mse: 0.9089 - val_My_MSE: 0.0090\n",
            "\n",
            "Epoch 00484: loss did not improve from 0.51990\n",
            "Epoch 485/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6070 - mse: 0.6070 - My_MSE: 0.0060 - val_loss: 0.8979 - val_mse: 0.8979 - val_My_MSE: 0.0088\n",
            "\n",
            "Epoch 00485: loss did not improve from 0.51990\n",
            "Epoch 486/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6072 - mse: 0.6072 - My_MSE: 0.0060 - val_loss: 0.9335 - val_mse: 0.9335 - val_My_MSE: 0.0092\n",
            "\n",
            "Epoch 00486: loss did not improve from 0.51990\n",
            "Epoch 487/10000\n",
            "555/555 [==============================] - 63s 113ms/step - loss: 0.6088 - mse: 0.6088 - My_MSE: 0.0060 - val_loss: 0.9122 - val_mse: 0.9122 - val_My_MSE: 0.0090\n",
            "\n",
            "Epoch 00487: loss did not improve from 0.51990\n",
            "Epoch 488/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6063 - mse: 0.6063 - My_MSE: 0.0060 - val_loss: 0.9244 - val_mse: 0.9244 - val_My_MSE: 0.0091\n",
            "\n",
            "Epoch 00488: loss improved from 0.51990 to 0.51931, saving model to poids_train.hdf5\n",
            "Epoch 489/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6054 - mse: 0.6054 - My_MSE: 0.0060 - val_loss: 0.9211 - val_mse: 0.9211 - val_My_MSE: 0.0091\n",
            "\n",
            "Epoch 00489: loss improved from 0.51931 to 0.51923, saving model to poids_train.hdf5\n",
            "Epoch 490/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6070 - mse: 0.6070 - My_MSE: 0.0060 - val_loss: 0.8704 - val_mse: 0.8704 - val_My_MSE: 0.0086\n",
            "\n",
            "Epoch 00490: loss did not improve from 0.51923\n",
            "Epoch 491/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6135 - mse: 0.6135 - My_MSE: 0.0060 - val_loss: 0.9204 - val_mse: 0.9204 - val_My_MSE: 0.0091\n",
            "\n",
            "Epoch 00491: loss did not improve from 0.51923\n",
            "Epoch 492/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6070 - mse: 0.6070 - My_MSE: 0.0060 - val_loss: 0.9481 - val_mse: 0.9481 - val_My_MSE: 0.0093\n",
            "\n",
            "Epoch 00492: loss did not improve from 0.51923\n",
            "Epoch 493/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6047 - mse: 0.6047 - My_MSE: 0.0060 - val_loss: 0.9103 - val_mse: 0.9103 - val_My_MSE: 0.0090\n",
            "\n",
            "Epoch 00493: loss improved from 0.51923 to 0.51825, saving model to poids_train.hdf5\n",
            "Epoch 494/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6039 - mse: 0.6039 - My_MSE: 0.0059 - val_loss: 0.9440 - val_mse: 0.9440 - val_My_MSE: 0.0093\n",
            "\n",
            "Epoch 00494: loss improved from 0.51825 to 0.51707, saving model to poids_train.hdf5\n",
            "Epoch 495/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6039 - mse: 0.6039 - My_MSE: 0.0060 - val_loss: 0.9155 - val_mse: 0.9155 - val_My_MSE: 0.0090\n",
            "\n",
            "Epoch 00495: loss did not improve from 0.51707\n",
            "Epoch 496/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6043 - mse: 0.6043 - My_MSE: 0.0060 - val_loss: 0.9101 - val_mse: 0.9101 - val_My_MSE: 0.0090\n",
            "\n",
            "Epoch 00496: loss did not improve from 0.51707\n",
            "Epoch 497/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6074 - mse: 0.6074 - My_MSE: 0.0060 - val_loss: 0.8985 - val_mse: 0.8985 - val_My_MSE: 0.0088\n",
            "\n",
            "Epoch 00497: loss did not improve from 0.51707\n",
            "Epoch 498/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6049 - mse: 0.6049 - My_MSE: 0.0060 - val_loss: 0.9536 - val_mse: 0.9536 - val_My_MSE: 0.0094\n",
            "\n",
            "Epoch 00498: loss did not improve from 0.51707\n",
            "Epoch 499/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6044 - mse: 0.6044 - My_MSE: 0.0060 - val_loss: 0.9537 - val_mse: 0.9537 - val_My_MSE: 0.0094\n",
            "\n",
            "Epoch 00499: loss did not improve from 0.51707\n",
            "Epoch 500/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6034 - mse: 0.6034 - My_MSE: 0.0059 - val_loss: 0.9288 - val_mse: 0.9288 - val_My_MSE: 0.0091\n",
            "\n",
            "Epoch 00500: loss did not improve from 0.51707\n",
            "Epoch 501/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6046 - mse: 0.6046 - My_MSE: 0.0060 - val_loss: 0.9106 - val_mse: 0.9106 - val_My_MSE: 0.0090\n",
            "\n",
            "Epoch 00501: loss did not improve from 0.51707\n",
            "Epoch 502/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6047 - mse: 0.6047 - My_MSE: 0.0060 - val_loss: 0.9202 - val_mse: 0.9202 - val_My_MSE: 0.0091\n",
            "\n",
            "Epoch 00502: loss did not improve from 0.51707\n",
            "Epoch 503/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6047 - mse: 0.6047 - My_MSE: 0.0060 - val_loss: 0.9210 - val_mse: 0.9210 - val_My_MSE: 0.0091\n",
            "\n",
            "Epoch 00503: loss did not improve from 0.51707\n",
            "Epoch 504/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6075 - mse: 0.6075 - My_MSE: 0.0060 - val_loss: 0.9364 - val_mse: 0.9364 - val_My_MSE: 0.0092\n",
            "\n",
            "Epoch 00504: loss did not improve from 0.51707\n",
            "Epoch 505/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6042 - mse: 0.6042 - My_MSE: 0.0060 - val_loss: 0.9275 - val_mse: 0.9275 - val_My_MSE: 0.0091\n",
            "\n",
            "Epoch 00505: loss did not improve from 0.51707\n",
            "Epoch 506/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6009 - mse: 0.6009 - My_MSE: 0.0059 - val_loss: 0.9539 - val_mse: 0.9539 - val_My_MSE: 0.0094\n",
            "\n",
            "Epoch 00506: loss improved from 0.51707 to 0.51561, saving model to poids_train.hdf5\n",
            "Epoch 507/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6024 - mse: 0.6024 - My_MSE: 0.0059 - val_loss: 0.9418 - val_mse: 0.9418 - val_My_MSE: 0.0093\n",
            "\n",
            "Epoch 00507: loss did not improve from 0.51561\n",
            "Epoch 508/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6030 - mse: 0.6030 - My_MSE: 0.0059 - val_loss: 0.9206 - val_mse: 0.9206 - val_My_MSE: 0.0091\n",
            "\n",
            "Epoch 00508: loss did not improve from 0.51561\n",
            "Epoch 509/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6033 - mse: 0.6033 - My_MSE: 0.0059 - val_loss: 0.9238 - val_mse: 0.9238 - val_My_MSE: 0.0091\n",
            "\n",
            "Epoch 00509: loss did not improve from 0.51561\n",
            "Epoch 510/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6033 - mse: 0.6033 - My_MSE: 0.0059 - val_loss: 0.9169 - val_mse: 0.9169 - val_My_MSE: 0.0090\n",
            "\n",
            "Epoch 00510: loss did not improve from 0.51561\n",
            "Epoch 511/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6024 - mse: 0.6024 - My_MSE: 0.0059 - val_loss: 0.9443 - val_mse: 0.9443 - val_My_MSE: 0.0093\n",
            "\n",
            "Epoch 00511: loss did not improve from 0.51561\n",
            "Epoch 512/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6036 - mse: 0.6036 - My_MSE: 0.0059 - val_loss: 0.9502 - val_mse: 0.9502 - val_My_MSE: 0.0094\n",
            "\n",
            "Epoch 00512: loss did not improve from 0.51561\n",
            "Epoch 513/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6045 - mse: 0.6045 - My_MSE: 0.0060 - val_loss: 0.9326 - val_mse: 0.9326 - val_My_MSE: 0.0092\n",
            "\n",
            "Epoch 00513: loss did not improve from 0.51561\n",
            "Epoch 514/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6016 - mse: 0.6016 - My_MSE: 0.0059 - val_loss: 0.9573 - val_mse: 0.9573 - val_My_MSE: 0.0094\n",
            "\n",
            "Epoch 00514: loss did not improve from 0.51561\n",
            "Epoch 515/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6022 - mse: 0.6022 - My_MSE: 0.0059 - val_loss: 0.9357 - val_mse: 0.9357 - val_My_MSE: 0.0092\n",
            "\n",
            "Epoch 00515: loss did not improve from 0.51561\n",
            "Epoch 516/10000\n",
            "555/555 [==============================] - 62s 112ms/step - loss: 0.6022 - mse: 0.6022 - My_MSE: 0.0059 - val_loss: 0.9531 - val_mse: 0.9531 - val_My_MSE: 0.0094\n",
            "\n",
            "Epoch 00516: loss did not improve from 0.51561\n",
            "Epoch 517/10000\n",
            "555/555 [==============================] - 61s 110ms/step - loss: 0.6009 - mse: 0.6009 - My_MSE: 0.0059 - val_loss: 0.9486 - val_mse: 0.9486 - val_My_MSE: 0.0093\n",
            "\n",
            "Epoch 00517: loss improved from 0.51561 to 0.51552, saving model to poids_train.hdf5\n",
            "Epoch 518/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.5995 - mse: 0.5995 - My_MSE: 0.0059 - val_loss: 0.9664 - val_mse: 0.9664 - val_My_MSE: 0.0095\n",
            "\n",
            "Epoch 00518: loss improved from 0.51552 to 0.51496, saving model to poids_train.hdf5\n",
            "Epoch 519/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.5997 - mse: 0.5997 - My_MSE: 0.0059 - val_loss: 0.9343 - val_mse: 0.9343 - val_My_MSE: 0.0092\n",
            "\n",
            "Epoch 00519: loss did not improve from 0.51496\n",
            "Epoch 520/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.6017 - mse: 0.6017 - My_MSE: 0.0059 - val_loss: 0.9257 - val_mse: 0.9257 - val_My_MSE: 0.0091\n",
            "\n",
            "Epoch 00520: loss did not improve from 0.51496\n",
            "Epoch 521/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6019 - mse: 0.6019 - My_MSE: 0.0059 - val_loss: 0.9677 - val_mse: 0.9677 - val_My_MSE: 0.0095\n",
            "\n",
            "Epoch 00521: loss did not improve from 0.51496\n",
            "Epoch 522/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.6006 - mse: 0.6006 - My_MSE: 0.0059 - val_loss: 0.9682 - val_mse: 0.9682 - val_My_MSE: 0.0095\n",
            "\n",
            "Epoch 00522: loss did not improve from 0.51496\n",
            "Epoch 523/10000\n",
            "555/555 [==============================] - 61s 110ms/step - loss: 0.5998 - mse: 0.5998 - My_MSE: 0.0059 - val_loss: 0.9408 - val_mse: 0.9408 - val_My_MSE: 0.0093\n",
            "\n",
            "Epoch 00523: loss improved from 0.51496 to 0.51489, saving model to poids_train.hdf5\n",
            "Epoch 524/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.6002 - mse: 0.6002 - My_MSE: 0.0059 - val_loss: 0.9268 - val_mse: 0.9268 - val_My_MSE: 0.0091\n",
            "\n",
            "Epoch 00524: loss did not improve from 0.51489\n",
            "Epoch 525/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.6024 - mse: 0.6024 - My_MSE: 0.0059 - val_loss: 0.9325 - val_mse: 0.9325 - val_My_MSE: 0.0092\n",
            "\n",
            "Epoch 00525: loss did not improve from 0.51489\n",
            "Epoch 526/10000\n",
            "555/555 [==============================] - 61s 110ms/step - loss: 0.6014 - mse: 0.6014 - My_MSE: 0.0059 - val_loss: 0.9351 - val_mse: 0.9351 - val_My_MSE: 0.0092\n",
            "\n",
            "Epoch 00526: loss did not improve from 0.51489\n",
            "Epoch 527/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.5992 - mse: 0.5992 - My_MSE: 0.0059 - val_loss: 0.9454 - val_mse: 0.9454 - val_My_MSE: 0.0093\n",
            "\n",
            "Epoch 00527: loss improved from 0.51489 to 0.51436, saving model to poids_train.hdf5\n",
            "Epoch 528/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.5978 - mse: 0.5978 - My_MSE: 0.0059 - val_loss: 0.9485 - val_mse: 0.9485 - val_My_MSE: 0.0093\n",
            "\n",
            "Epoch 00528: loss did not improve from 0.51436\n",
            "Epoch 529/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.5994 - mse: 0.5994 - My_MSE: 0.0059 - val_loss: 0.9459 - val_mse: 0.9459 - val_My_MSE: 0.0093\n",
            "\n",
            "Epoch 00529: loss did not improve from 0.51436\n",
            "Epoch 530/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.6009 - mse: 0.6009 - My_MSE: 0.0059 - val_loss: 0.9431 - val_mse: 0.9431 - val_My_MSE: 0.0093\n",
            "\n",
            "Epoch 00530: loss did not improve from 0.51436\n",
            "Epoch 531/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.5985 - mse: 0.5985 - My_MSE: 0.0059 - val_loss: 0.9650 - val_mse: 0.9650 - val_My_MSE: 0.0095\n",
            "\n",
            "Epoch 00531: loss improved from 0.51436 to 0.51402, saving model to poids_train.hdf5\n",
            "Epoch 532/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.5974 - mse: 0.5974 - My_MSE: 0.0059 - val_loss: 0.9824 - val_mse: 0.9824 - val_My_MSE: 0.0097\n",
            "\n",
            "Epoch 00532: loss improved from 0.51402 to 0.51347, saving model to poids_train.hdf5\n",
            "Epoch 533/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.5989 - mse: 0.5989 - My_MSE: 0.0059 - val_loss: 0.9229 - val_mse: 0.9229 - val_My_MSE: 0.0091\n",
            "\n",
            "Epoch 00533: loss did not improve from 0.51347\n",
            "Epoch 534/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.6000 - mse: 0.6000 - My_MSE: 0.0059 - val_loss: 0.9629 - val_mse: 0.9629 - val_My_MSE: 0.0095\n",
            "\n",
            "Epoch 00534: loss did not improve from 0.51347\n",
            "Epoch 535/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.5997 - mse: 0.5997 - My_MSE: 0.0059 - val_loss: 0.9810 - val_mse: 0.9810 - val_My_MSE: 0.0097\n",
            "\n",
            "Epoch 00535: loss did not improve from 0.51347\n",
            "Epoch 536/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.5973 - mse: 0.5973 - My_MSE: 0.0059 - val_loss: 0.9418 - val_mse: 0.9418 - val_My_MSE: 0.0093\n",
            "\n",
            "Epoch 00536: loss did not improve from 0.51347\n",
            "Epoch 537/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.5975 - mse: 0.5975 - My_MSE: 0.0059 - val_loss: 0.9704 - val_mse: 0.9704 - val_My_MSE: 0.0096\n",
            "\n",
            "Epoch 00537: loss did not improve from 0.51347\n",
            "Epoch 538/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.5962 - mse: 0.5962 - My_MSE: 0.0059 - val_loss: 0.9749 - val_mse: 0.9749 - val_My_MSE: 0.0096\n",
            "\n",
            "Epoch 00538: loss improved from 0.51347 to 0.51245, saving model to poids_train.hdf5\n",
            "Epoch 539/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.5953 - mse: 0.5953 - My_MSE: 0.0059 - val_loss: 0.9350 - val_mse: 0.9350 - val_My_MSE: 0.0092\n",
            "\n",
            "Epoch 00539: loss did not improve from 0.51245\n",
            "Epoch 540/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.5973 - mse: 0.5973 - My_MSE: 0.0059 - val_loss: 0.9594 - val_mse: 0.9594 - val_My_MSE: 0.0094\n",
            "\n",
            "Epoch 00540: loss did not improve from 0.51245\n",
            "Epoch 541/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.5992 - mse: 0.5992 - My_MSE: 0.0059 - val_loss: 0.9457 - val_mse: 0.9457 - val_My_MSE: 0.0093\n",
            "\n",
            "Epoch 00541: loss did not improve from 0.51245\n",
            "Epoch 542/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.5974 - mse: 0.5974 - My_MSE: 0.0059 - val_loss: 0.9656 - val_mse: 0.9656 - val_My_MSE: 0.0095\n",
            "\n",
            "Epoch 00542: loss did not improve from 0.51245\n",
            "Epoch 543/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.5952 - mse: 0.5952 - My_MSE: 0.0059 - val_loss: 0.9717 - val_mse: 0.9717 - val_My_MSE: 0.0096\n",
            "\n",
            "Epoch 00543: loss improved from 0.51245 to 0.51208, saving model to poids_train.hdf5\n",
            "Epoch 544/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.5952 - mse: 0.5952 - My_MSE: 0.0059 - val_loss: 0.9793 - val_mse: 0.9793 - val_My_MSE: 0.0096\n",
            "\n",
            "Epoch 00544: loss did not improve from 0.51208\n",
            "Epoch 545/10000\n",
            "555/555 [==============================] - 61s 111ms/step - loss: 0.5950 - mse: 0.5950 - My_MSE: 0.0059 - val_loss: 0.9656 - val_mse: 0.9656 - val_My_MSE: 0.0095\n",
            "\n",
            "Epoch 00545: loss did not improve from 0.51208\n",
            "Epoch 546/10000\n",
            "555/555 [==============================] - 62s 111ms/step - loss: 0.5964 - mse: 0.5964 - My_MSE: 0.0059 - val_loss: 0.9834 - val_mse: 0.9834 - val_My_MSE: 0.0097\n",
            "\n",
            "Epoch 00546: loss did not improve from 0.51208\n",
            "Epoch 547/10000\n",
            "295/555 [==============>...............] - ETA: 26s - loss: 0.6869 - mse: 0.6869 - My_MSE: 0.0068"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-65fe0ab70d6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# Entraine le modèle sans réduction de calculs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mhistorique\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_periodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCheckPoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mStopTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mperiodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val_My_MSE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1103\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \"\"\"\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    294\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_supports_tf_logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1083\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_finalize_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values, finalize)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m       \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m       \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0;31m# and give a timeout to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m                     \u001b[0;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                     \u001b[0;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfbomV0LS9LD"
      },
      "source": [
        "model.load_weights(\"poids_train.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MDY8O1-l6kN"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEuSDQ6vZnBm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc08fd55-495f-4dc8-b92c-13f4404afe27"
      },
      "source": [
        "# Evaluation du modèle\n",
        "\n",
        "model.evaluate(dataset)\n",
        "model.evaluate(dataset_val)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "555/555 [==============================] - 22s 39ms/step - loss: 0.5131 - mse: 0.5131 - My_MSE: 0.0051\n",
            "135/135 [==============================] - 5s 40ms/step - loss: 0.9717 - mse: 0.9717 - My_MSE: 0.0096\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9716706275939941, 0.9716708064079285, 0.009568632580339909]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOrzs53kvrIw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "fca188eb-234e-4341-d64a-dbdde602191e"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('poids_train.hdf5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_1bba8a02-0c36-4c3c-aaac-f6ae5fc66158\", \"poids_train.hdf5\", 5040456)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbzro22hgt4b"
      },
      "source": [
        "**3. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMmVn1e5zEAm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "348fc04d-c8cb-461c-8707-08ae4b1f27ec"
      },
      "source": [
        "# Création des instants d'entrainement et de validation\n",
        "y_train_timing = serie_entrainement.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_train)]\n",
        "y_val_timing = serie_test.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_val)]\n",
        "\n",
        "# Calcul des prédictions\n",
        "pred_ent = model.predict(x_train, verbose=1)\n",
        "pred_val = model.predict(x_val, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-6e28f18428f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Création des instants d'entrainement et de validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_train_timing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mserie_entrainement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtaille_fenetre\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhorizon\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtaille_fenetre\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhorizon\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_val_timing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mserie_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtaille_fenetre\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhorizon\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtaille_fenetre\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhorizon\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Calcul des prédictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsT_RRS6iFhA"
      },
      "source": [
        "**4.Affichage sur une période de 1 heure**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoGhoCJWHOuj"
      },
      "source": [
        "Création d'une série contenant les valeurs originales et les prédictions, synchronisées dans le temps :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBWDeZVBfynP"
      },
      "source": [
        "serie_btc_ent_ori = serie_entrainement*std+mean\n",
        "serie_btc_val_ori = serie_test*std+mean\n",
        "\n",
        "serie_btc_ent_pred = pd.Series(data=(pred_ent[:,0]*std+mean),index=y_train_timing)\n",
        "serie_btc_val_pred = pd.Series(data=(pred_val[:,0]*std+mean),index=y_val_timing)\n",
        "\n",
        "serie_btc_ori = pd.concat([serie_btc_ent_ori,serie_btc_val_ori])\n",
        "serie_btc_pred = pd.concat([serie_btc_ent_pred,serie_btc_val_pred])\n",
        "\n",
        "serie_btc_ori = serie_btc_ori.fillna(method=\"backfill\")\n",
        "serie_btc_pred = serie_btc_pred.fillna(method=\"backfill\")\n",
        "\n",
        "serie_btc_ent_ori = serie_btc_ent_ori.fillna(method=\"backfill\")\n",
        "serie_btc_val_ori = serie_btc_val_ori.fillna(method=\"backfill\")\n",
        "serie_btc_ent_pred = serie_btc_ent_pred.fillna(method=\"backfill\")\n",
        "serie_btc_val_pred = serie_btc_val_pred.fillna(method=\"backfill\")\n",
        "\n",
        "frame = {'BTC_ALL' : serie_btc_ori, 'BTC_ENT': serie_btc_ent_ori, 'BTC_VAL' : serie_btc_val_ori, 'BTC_PRED' : serie_btc_pred, 'BTC_PRED_ENT' : serie_btc_ent_pred, 'BTC_PRED_VAL':serie_btc_val_pred}\n",
        "df_resultats = pd.DataFrame(frame)\n",
        "df_resultats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pFnvAVy-3lM"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbe originale\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_ALL'],line=dict(color='blue', width=1),name=\"Prix BTC\"))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_ENT'],line=dict(color='green', width=1),name=\"Entrainement\"))\n",
        "\n",
        "# Courbe de validation\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_VAL'],line=dict(color='red', width=1),name=\"Validation\"))\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNUg-0NAR0gv"
      },
      "source": [
        "serie_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0SkDEbHS2my"
      },
      "source": [
        "from scipy.special import inv_boxcox"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Nv-2zsjTTJ6"
      },
      "source": [
        "from scipy.integrate import cumtrapz\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def f(x):\n",
        "    return serie_etude['Open'][0:taille_fenetre+1]\n",
        "\n",
        "f = np.vectorize(f)\n",
        "\n",
        "X = np.linspace(0,taille_fenetre+1,taille_fenetre+2)\n",
        "\n",
        "fv = f(X)\n",
        "plt.plot(fv)\n",
        "\n",
        "F = cumtrapz(fv, x=X, initial=0)\n",
        "plt.plot(F);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_C5qDpsRkrP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "4bc4bfb7-a155-48b1-d66b-3d536043e00b"
      },
      "source": [
        "from statsmodels.tsa.statespace.tools import diff\n",
        "\n",
        "predictions=[]\n",
        "\n",
        "for t in range(0,200):\n",
        "  data_to_predict = serie_etude['Open'][t:t+taille_fenetre+1]\n",
        "  data_to_predict = boxcox(data_to_predict,lam)\n",
        "  data_to_predict = data_to_predict - trend[t:t+taille_fenetre+1]\n",
        "  data_to_predict = diff(data_to_predict,1)\n",
        "  data_to_predict = np.insert(data_to_predict,0,0)\n",
        "\n",
        "  data_to_predict = tf.expand_dims(data_to_predict,axis=1)\n",
        "  data_to_predict = tf.expand_dims(data_to_predict,axis=0)\n",
        "  pred = model.predict(data_to_predict[:,1:,:])\n",
        "  predictions.append(pred[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-7da9adc6b044>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mdata_to_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_predict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mdata_to_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_predict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_predict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1627\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       return self._concrete_stateful_fn._call_flat(\n\u001b[0;32m--> 895\u001b[0;31m           filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfn_with_cond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_kwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_filtered_flat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m:    Invalid input_h shape: [1,100,500] [1,1,500]\n\t [[{{node CudnnRNN}}]]\n\t [[model_4/lstm_5/PartitionedCall]] [Op:__inference_predict_function_1247336]\n\nFunction call stack:\npredict_function -> predict_function -> predict_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Zp30SLWZ7cG"
      },
      "source": [
        "np.asarray(predictions)[:,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OP2cvVBUW36R"
      },
      "source": [
        "serie_etude.index[taille_fenetre+1:taille_fenetre+2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7n75-EQyUfk8"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "tmax = len(np.asarray(predictions)[:,0])\n",
        "\n",
        "# Courbe originale\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index,y=serie_etude['diff'][0:taille_fenetre+1],line=dict(color='blue', width=1),name=\"diff_reel\"))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "#fig.add_trace(go.Scatter(x=serie_etude.index,y=data_to_predict[:,:,0][0],line=dict(color='green', width=1),name=\"data_to_predict\"))\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index[taille_fenetre+1:taille_fenetre+1+tmax],y=np.asarray(predictions)[:,0],line=dict(color='red', width=1),name=\"prediction\"))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=serie_etude.index[taille_fenetre+1:taille_fenetre+1+tmax],y=serie_etude['diff'][taille_fenetre+1:taille_fenetre+1+tmax],line=dict(color='black', width=1),name=\"true\"))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T72zWjfGrtQu"
      },
      "source": [
        "# Création du modèle LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr-AD-INrtQv"
      },
      "source": [
        "**1. Création du réseau**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipNxd-RNrtQv"
      },
      "source": [
        "Par défaut, la dimension des vecteurs cachés est de 10 et aucune régularisation n'est utilisée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52Iv3nK0rtQ9"
      },
      "source": [
        "dim_LSTM = 40\n",
        "l1_reg = 0.0\n",
        "l2_reg = 0.0\n",
        "\n",
        "entrees = tf.keras.layers.Input(shape=(taille_fenetre,1))\n",
        "\n",
        "# Encodeur\n",
        "s_encodeur = tf.keras.layers.LSTM(dim_LSTM, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg,l2=l2_reg))(entrees)\n",
        "  \n",
        "# Générateur\n",
        "sortie = tf.keras.layers.Dense(1,kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg,l2=l2_reg))(s_encodeur)\n",
        "\n",
        "# Construction du modèle\n",
        "model = tf.keras.Model(entrees,sortie)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qv4ZaPfrtQ-"
      },
      "source": [
        "**2. Optimisation de l'apprentissage**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlrGDGQ-rtQ-"
      },
      "source": [
        "Pour accélérer le traitement des données, nous n'allons pas utiliser l'intégralité des données pendant la mise à jour du gradient, comme cela a été fait jusqu'à présent (en utilisant le dataset).  \n",
        "Cette fois-ci, nous allons forcer les mises à jour du gradient à se produire de manière moins fréquente en attribuant la valeur du batch_size à prendre en compte lors de la regression du modèle.  \n",
        "Pour cela, on utilise l'argument \"batch_size\" dans la méthode fit. En précisant un batch_size=1000, cela signifie que :\n",
        " - Sur notre total de 56000 échantillons, 56 seront utilisés pour les calculs du gradient\n",
        " - Il y aura également 56 itérations à chaque période.\n",
        "  \n",
        "    \n",
        "    \n",
        "Si nous avions pris le dataset comme entrée, nous aurions eu :\n",
        "- Un total de 56000 échantillons également\n",
        "- Chaque période aurait également pris 56 itérations pour se compléter\n",
        "- Mais 1000 échantillons auraient été utilisés pour le calcul du gradient, au lieu de 56 avec la méthode utilisée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOnTaeMSrtQ_"
      },
      "source": [
        "batch_size = 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APgK7pSyrtQ_"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam()\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"logcosh\", optimizer=optimiseur, metrics=\"mse\")\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(x=x_train,y=y_train,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint],batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFX_PudkrtRA"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[30], taux[99], 0, 0.04])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8ptlq-JrtRA"
      },
      "source": [
        "# Chargement des poids sauvegardés\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ_TJVE_rtRB"
      },
      "source": [
        "max_periodes = 1000\n",
        "\n",
        "# Classe permettant d'arrêter l'entrainement si la variation\n",
        "# devient plus petite qu'une valeur à choisir sur un nombre\n",
        "# de périodes à choisir\n",
        "class StopTrain(keras.callbacks.Callback):\n",
        "    def __init__(self, delta=0.01,periodes=100, term=\"loss\", logs={}):\n",
        "      self.n_periodes = 0\n",
        "      self.periodes = periodes\n",
        "      self.loss_1 = 100\n",
        "      self.delta = delta\n",
        "      self.term = term\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "      diff_loss = abs(self.loss_1 - logs[self.term])\n",
        "      self.loss_1 = logs[self.term]\n",
        "      if (diff_loss < self.delta):\n",
        "        self.n_periodes = self.n_periodes + 1\n",
        "      else:\n",
        "        self.n_periodes = 0\n",
        "      if (self.n_periodes == self.periodes):\n",
        "        print(\"Arrêt de l'entrainement...\")\n",
        "        self.model.stop_training = True\n",
        "\n",
        "def  My_MSE(y_true,y_pred):\n",
        "  return(tf.keras.metrics.mse(y_true,y_pred)*std.numpy()+mean.numpy())\n",
        "  \n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.02,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0.01)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"logcosh\", optimizer=optimiseur, metrics=[\"mse\",My_MSE])\n",
        "\n",
        "# Entraine le modèle, avec une réduction des calculs du gradient\n",
        "historique = model.fit(x=x_train,y=y_train,validation_data=(x_val,y_val), epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=0.005,periodes = 10, term=\"My_MSE\")],batch_size=batch_size)\n",
        "\n",
        "# Entraine le modèle sans réduction de calculs\n",
        "#historique = model.fit(dataset,validation_data=dataset_val, epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=0.1,periodes = 10, term=\"val_My_MSE\")])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5ec3f0brtRC"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZFhBaeYrtRC"
      },
      "source": [
        "# Evaluation du modèle\n",
        "# Avec le modèle type encodeur/décodeur :\n",
        "# 56/56 [==============================] - 7s 113ms/step - loss: 1.1795e-04 - mse: 2.3664e-04 - My_MSE: 2712.2415\n",
        "# 14/14 [==============================] - 2s 113ms/step - loss: 0.3602 - mse: 1.9360 - My_MSE: 9650.2637\n",
        "model.evaluate(dataset)\n",
        "model.evaluate(dataset_val)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy8gdKf3rtRC"
      },
      "source": [
        "**3. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iodlvc6frtRD"
      },
      "source": [
        "# Création des instants d'entrainement et de validation\n",
        "y_train_timing = serie_entrainement.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_train)]\n",
        "y_val_timing = serie_test.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_val)]\n",
        "\n",
        "# Calcul des prédictions\n",
        "pred_ent = model.predict(x_train, verbose=1)\n",
        "pred_val = model.predict(x_val, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7URVy7qrtRD"
      },
      "source": [
        "**4.Affichage sur une période de 1 heure**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqONHLBbrtRD"
      },
      "source": [
        "Création d'une série contenant les valeurs originales et les prédictions, synchronisées dans le temps :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDgJ20qOrtRE"
      },
      "source": [
        "serie_btc_ent_ori = serie_entrainement*std+mean\n",
        "serie_btc_val_ori = serie_test*std+mean\n",
        "\n",
        "serie_btc_ent_pred = pd.Series(data=(pred_ent[:,0]*std+mean),index=y_train_timing)\n",
        "serie_btc_val_pred = pd.Series(data=(pred_val[:,0]*std+mean),index=y_val_timing)\n",
        "\n",
        "serie_btc_ori = pd.concat([serie_btc_ent_ori,serie_btc_val_ori])\n",
        "serie_btc_pred = pd.concat([serie_btc_ent_pred,serie_btc_val_pred])\n",
        "\n",
        "serie_btc_ori = serie_btc_ori.fillna(method=\"backfill\")\n",
        "serie_btc_pred = serie_btc_pred.fillna(method=\"backfill\")\n",
        "\n",
        "serie_btc_ent_ori = serie_btc_ent_ori.fillna(method=\"backfill\")\n",
        "serie_btc_val_ori = serie_btc_val_ori.fillna(method=\"backfill\")\n",
        "serie_btc_ent_pred = serie_btc_ent_pred.fillna(method=\"backfill\")\n",
        "serie_btc_val_pred = serie_btc_val_pred.fillna(method=\"backfill\")\n",
        "\n",
        "frame = {'BTC_ALL' : serie_btc_ori, 'BTC_ENT': serie_btc_ent_ori, 'BTC_VAL' : serie_btc_val_ori, 'BTC_PRED' : serie_btc_pred, 'BTC_PRED_ENT' : serie_btc_ent_pred, 'BTC_PRED_VAL':serie_btc_val_pred}\n",
        "df_resultats = pd.DataFrame(frame)\n",
        "df_resultats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7W2aoMPBrtRE"
      },
      "source": [
        "date_separation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfObUyVjrtRF"
      },
      "source": [
        "df_resultats.loc[date_separation-pd.Timedelta(hours=7):date_separation+pd.Timedelta(hours=7)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wv1MZoxPrtRF"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbe originale\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_ALL'],line=dict(color='blue', width=1),name=\"Prix BTC\"))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_ENT'],line=dict(color='green', width=1),name=\"Entrainement\"))\n",
        "\n",
        "# Courbe de validation\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_VAL'],line=dict(color='red', width=1),name=\"Validation\"))\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aq51_Kk6rtRK"
      },
      "source": [
        "**5. Détection de l'augmentation des erreurs dans la zone de validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2-WtViDrtRK"
      },
      "source": [
        "On peut imaginer devoir suivre en temps réel l'évolution des prédictions afin de détecter lorsque le modèle n'est plus valide."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6hb5UnZrtRK"
      },
      "source": [
        "# Détection de la date où commencent les anomalies\n",
        "# sur la période de validation\n",
        "\n",
        "# Construit les dataframe pour calculer les erreurs\n",
        "df_error_btc_ent = df_resultats[['BTC_ENT','BTC_PRED_ENT']].dropna()\n",
        "df_error_btc_val = df_resultats[['BTC_VAL','BTC_PRED_VAL']].dropna()\n",
        "\n",
        "# Calcul des erreurs relatives sur les entrainements et les validations\n",
        "erreur_ent = abs(np.asarray(df_error_btc_ent['BTC_ENT']) - np.asarray(df_error_btc_ent['BTC_PRED_ENT']))/np.asarray(df_error_btc_ent['BTC_ENT'])*100.0\n",
        "erreur_val = abs(np.asarray(df_error_btc_val['BTC_VAL']) - np.asarray(df_error_btc_val['BTC_PRED_VAL']))/np.asarray(df_error_btc_val['BTC_VAL'])*100.0\n",
        "\n",
        "# Erreur relative moyenne sur les entrainements\n",
        "erreur_ent_mape = tf.keras.metrics.mape(np.asarray(df_error_btc_ent['BTC_ENT']),np.asarray(df_error_btc_ent['BTC_PRED_ENT']))\n",
        "erreur_ent_mape_max = np.amax(erreur_ent)\n",
        "\n",
        "# Calcul le nombre d'anomalies sur les entrainements avec le ratio spécifié\n",
        "nbr_anomalies_ent = np.count_nonzero(erreur_ent>erreur_ent_mape)\n",
        "\n",
        "# Calcul du ratio d'anomalies sur la période d'entrainement\n",
        "ratio_ent = np.count_nonzero(erreur_ent>erreur_ent_mape)/erreur_ent.size\n",
        "\n",
        "# Recherche de la date à partir de laquelle on dépasse l\n",
        "n_erreurs = 0\n",
        "for i in range(5,erreur_val.size):\n",
        "  erreur_val_ = abs(np.asarray(df_error_btc_val['BTC_VAL'][i]) - np.asarray(df_error_btc_val['BTC_PRED_VAL'][i]))/np.asarray(df_error_btc_val['BTC_VAL'][i])*100.0\n",
        "  seuil = erreur_ent_mape_max*1\n",
        "  if erreur_val_ > seuil:\n",
        "    n_erreurs = n_erreurs + 1\n",
        "    if n_erreurs == 5:\n",
        "      index = df_error_btc_val.index[i]\n",
        "      break\n",
        "  else:\n",
        "    n_erreurs = 0\n",
        "print(index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBcqczT7rtRL"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbe originale\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_ALL'],line=dict(color='blue', width=1),name=\"Prix BTC\"))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_ENT'],line=dict(color='green', width=1),name=\"Entrainement\"))\n",
        "\n",
        "# Courbe de validation\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_VAL'],line=dict(color='red', width=1),name=\"Validation\"))\n",
        "\n",
        "# Délimite la zone d'erreur\n",
        "fig.add_shape(type=\"line\",x0=index,y0=0,x1=index,y1=np.amax(df_resultats['BTC_ALL']),name=\"Zone d'erreur\")\n",
        "\n",
        "# Affiche les courbes\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlsQSJqmrtRR"
      },
      "source": [
        "**5.Affichage sur une période de 1 jour**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P20ThOgSrtRR"
      },
      "source": [
        "# Echantillonage des données\n",
        "time = \"1D\"\n",
        "\n",
        "df_resultats = df_resultats.resample(time).asfreq()\n",
        "df_resultats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhUmbb8TrtRS"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbe originale\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_ALL'],line=dict(color='blue', width=1),name=\"Prix BTC\"))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_ENT'],line=dict(color='green', width=1),name=\"Entrainement\"))\n",
        "\n",
        "# Courbe de validation\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_VAL'],line=dict(color='red', width=1),name=\"Validation\"))\n",
        "\n",
        "# Délimite la zone d'erreur\n",
        "fig.add_shape(type=\"line\",x0=index,y0=0,x1=index,y1=np.amax(df_resultats['BTC_ALL']),name=\"Zone d'erreur\")\n",
        "\n",
        "# Affiche les courbes\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFgrzd6NrtRT"
      },
      "source": [
        "**6.Synthèse des erreurs sur les différentes zones**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIdnnWaYrtRU"
      },
      "source": [
        "# Echantillonage des données\n",
        "time = \"1H\"\n",
        "\n",
        "df_resultats = df_resultats.resample(time).asfreq()\n",
        "df_resultats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpdOSJcZrtRU"
      },
      "source": [
        "# Détection de la date où commencnte les anomalies\n",
        "# sur la période de validation\n",
        "\n",
        "# Construit les dataframe pour calculer les erreurs\n",
        "df_error_btc_ent = df_resultats[['BTC_ENT','BTC_PRED_ENT']].dropna()\n",
        "df_error_btc_val = df_resultats[['BTC_VAL','BTC_PRED_VAL']].dropna()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nou-vexFrtRU"
      },
      "source": [
        "# Erreurs d'entrainement\n",
        "mae_ent = tf.keras.metrics.mean_absolute_error(np.asarray(df_error_btc_ent['BTC_ENT']),np.asarray(df_error_btc_ent['BTC_PRED_ENT'])).numpy()\n",
        "mse_ent = tf.keras.metrics.mean_squared_error(np.asarray(df_error_btc_ent['BTC_ENT']),np.asarray(df_error_btc_ent['BTC_PRED_ENT'])).numpy()\n",
        "mape_ent = tf.keras.metrics.mape(np.asarray(df_error_btc_ent['BTC_ENT']),np.asarray(df_error_btc_ent['BTC_PRED_ENT'])).numpy()\n",
        "\n",
        "# Erreurs de validation\n",
        "mae_val = tf.keras.metrics.mean_absolute_error(np.asarray(df_error_btc_val['BTC_VAL']),np.asarray(df_error_btc_val['BTC_PRED_VAL'])).numpy()\n",
        "mse_val = tf.keras.metrics.mean_squared_error(np.asarray(df_error_btc_val['BTC_VAL']),np.asarray(df_error_btc_val['BTC_PRED_VAL'])).numpy()\n",
        "mape_val = tf.keras.metrics.mape(np.asarray(df_error_btc_val['BTC_VAL']),np.asarray(df_error_btc_val['BTC_PRED_VAL'])).numpy()\n",
        "\n",
        "# Erreurs sur la zone valide de validation\n",
        "mae_val_ok = tf.keras.metrics.mean_absolute_error(df_error_btc_val['BTC_VAL'][:index],np.asarray(df_error_btc_val['BTC_PRED_VAL'][:index])).numpy()\n",
        "mse_val_ok = tf.keras.metrics.mean_squared_error(df_error_btc_val['BTC_VAL'][:index],np.asarray(df_error_btc_val['BTC_PRED_VAL'][:index])).numpy()\n",
        "mape_val_ok = tf.keras.metrics.mape(df_error_btc_val['BTC_VAL'][:index],np.asarray(df_error_btc_val['BTC_PRED_VAL'][:index])).numpy()\n",
        "\n",
        "\n",
        "print(\"Erreur mae entrainement %s\" %mae_ent)\n",
        "print(\"Erreur mse entrainement : %s\" %mse_ent)\n",
        "print(\"Erreur mape entrainement : %s\\n\" %mape_ent)\n",
        "\n",
        "print(\"Erreur mae validation %s\" %mae_val)\n",
        "print(\"Erreur mse validation : %s\" %mse_val)\n",
        "print(\"Erreur mape entrainement : %s\\n\" %mape_val)\n",
        "\n",
        "print(\"Erreur mae validation zone OK %s\" %mae_val_ok)\n",
        "print(\"Erreur mse validation zone OK: %s\" %mse_val_ok)\n",
        "print(\"Erreur mape validation zone OK: %s\" %mape_val_ok)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PSUZ4SN2WRM"
      },
      "source": [
        "# Création du modèle LSTM avec auto-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC6DrlYT2WRS"
      },
      "source": [
        "**1. Création du réseau**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUF802Z22WRS"
      },
      "source": [
        "Par défaut, la dimension des vecteurs cachés est de 10 et aucune régularisation n'est utilisée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGtT8icQ7HBN"
      },
      "source": [
        "# Classe d'auto-attention\n",
        "# Applique les poids de la matrice d'attention sur les vecteurs de la couche récurrente\n",
        "\n",
        "# Importe le Backend de Keras\n",
        "from keras import backend as K\n",
        "\n",
        "# Définit une nouvelle classe Couche_Attention\n",
        "# Héritée de la classe Layer de Keras\n",
        "\n",
        "class Couche_Auto_Attention(tf.keras.layers.Layer):\n",
        "  # Fonction d'initialisation de la classe d'attention\n",
        "  def __init__(self,dim_att,nbr_hop):\n",
        "    self.dim_att = dim_att          # Dimension du vecteur d'attention\n",
        "    self.nbr_hop = nbr_hop\n",
        "    super().__init__()              # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.W = self.add_weight(shape=(self.dim_att,input_shape[2]),initializer='glorot_uniform',name=\"W\")\n",
        "    self.U = self.add_weight(shape=(self.nbr_hop,self.dim_att),initializer='glorot_uniform',name=\"U\")\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Définit la logique de la couche d'attention\n",
        "  # Arguments :   x : Tenseur d'entrée de dimension (None, nbr_v,dim)\n",
        "  def call(self,x):\n",
        "    # Calcul de la matrice XH contenant les\n",
        "    # représentations cachées des vecteurs\n",
        "    # issus de la couche GRU\n",
        "    xt = tf.transpose(x,perm=[0,2,1])           # (None,20,40) => (None,40,20)\n",
        "    Xh = tf.matmul(self.W,xt)                   # (#Att,40)x(None,40,20) = (None,#Att,20)\n",
        "    Xh = K.tanh(Xh)                             # Xh = (None,#Att,20)\n",
        "\n",
        "    # Calcul de la matrice des poids d'attention normalisés\n",
        "    A = tf.matmul(self.U,Xh)                    # (#hop,#Att)x(None,#Att,20) = (None,#Att,20)\n",
        "    A = tf.keras.activations.softmax(A,axis=2)  # (None,#Att,20)\n",
        "\n",
        "    # Calcul de la matrice des vecteur d'attentions\n",
        "    sortie = tf.matmul(A,x)                     # (None,#Att,20)x(None,20,40) = (None,#Att,40)\n",
        "    return tf.keras.layers.Flatten()(sortie)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uMyWVKe2WRT"
      },
      "source": [
        "dim_LSTM = 100\n",
        "l1_reg = 0.0\n",
        "l2_reg = 0.00\n",
        "nbr_hop = 20\n",
        "\n",
        "# Définition de l'entrée du modèle\n",
        "entrees = tf.keras.layers.Input(shape=(taille_fenetre,1))\n",
        "\n",
        "# Encodeur\n",
        "s_encodeur = tf.keras.layers.LSTM(dim_LSTM,return_sequences=True,recurrent_regularizer=tf.keras.regularizers.l2(1e-5))(entrees)\n",
        "s_attention = Couche_Auto_Attention(dim_att=dim_LSTM,nbr_hop=nbr_hop)(s_encodeur)\n",
        "\n",
        "# Décodeur\n",
        "s_decodeur = tf.keras.layers.Dense(dim_LSTM*nbr_hop,activation=\"tanh\")(s_attention)\n",
        "s_decodeur = tf.keras.layers.Concatenate()([s_decodeur,s_attention])\n",
        "\n",
        "# Générateur\n",
        "sortie = tf.keras.layers.Dense(1)(s_decodeur)\n",
        "\n",
        "# Construction du modèle\n",
        "model = tf.keras.Model(entrees,sortie)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwZnK5dl2WRV"
      },
      "source": [
        "**2. Optimisation de l'apprentissage**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwVSsLyv2WRV"
      },
      "source": [
        "Pour accélérer le traitement des données, nous n'allons pas utiliser l'intégralité des données pendant la mise à jour du gradient, comme cela a été fait jusqu'à présent (en utilisant le dataset).  \n",
        "Cette fois-ci, nous allons forcer les mises à jour du gradient à se produire de manière moins fréquente en attribuant la valeur du batch_size à prendre en compte lors de la regression du modèle.  \n",
        "Pour cela, on utilise l'argument \"batch_size\" dans la méthode fit. En précisant un batch_size=1000, cela signifie que :\n",
        " - Sur notre total de 56000 échantillons, 56 seront utilisés pour les calculs du gradient\n",
        " - Il y aura également 56 itérations à chaque période.\n",
        "  \n",
        "    \n",
        "    \n",
        "Si nous avions pris le dataset comme entrée, nous aurions eu :\n",
        "- Un total de 56000 échantillons également\n",
        "- Chaque période aurait également pris 56 itérations pour se compléter\n",
        "- Mais 1000 échantillons auraient été utilisés pour le calcul du gradient, au lieu de 56 avec la méthode utilisée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCl_AlQc2WRW"
      },
      "source": [
        "batch_size = 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ka2d6HB2WRW"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam()\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"mse\", optimizer=optimiseur, metrics=\"mse\")\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(x=x_train,y=y_train,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint],batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr6bQEZq2WRX"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 2])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryUpPIEF2WRX"
      },
      "source": [
        "# Chargement des poids sauvegardés\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhV93IpO2WRY"
      },
      "source": [
        "max_periodes = 1000\n",
        "\n",
        "# Classe permettant d'arrêter l'entrainement si la variation\n",
        "# devient plus petite qu'une valeur à choisir sur un nombre\n",
        "# de périodes à choisir\n",
        "class StopTrain(keras.callbacks.Callback):\n",
        "    def __init__(self, delta=0.01,periodes=100, term=\"loss\", logs={}):\n",
        "      self.n_periodes = 0\n",
        "      self.periodes = periodes\n",
        "      self.loss_1 = 100\n",
        "      self.delta = delta\n",
        "      self.term = term\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "      diff_loss = abs(self.loss_1 - logs[self.term])\n",
        "      self.loss_1 = logs[self.term]\n",
        "      if (diff_loss < self.delta):\n",
        "        self.n_periodes = self.n_periodes + 1\n",
        "      else:\n",
        "        self.n_periodes = 0\n",
        "      if (self.n_periodes == self.periodes):\n",
        "        print(\"Arrêt de l'entrainement...\")\n",
        "        self.model.stop_training = True\n",
        "\n",
        "def  My_MSE(y_true,y_pred):\n",
        "  return(tf.keras.metrics.mse(y_true,y_pred)*std.numpy()+mean.numpy())\n",
        "  \n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.001,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0.1)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"mse\", optimizer=optimiseur, metrics=[\"mse\",My_MSE])\n",
        "\n",
        "# Entraine le modèle, avec une réduction des calculs du gradient\n",
        "historique = model.fit(x=x_train,y=y_train,validation_data=(x_val,y_val), epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=0.0005,periodes = 10, term=\"My_MSE\")],batch_size=batch_size)\n",
        "\n",
        "# Entraine le modèle sans réduction de calculs\n",
        "#historique = model.fit(dataset,validation_data=dataset_val, epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=0.1,periodes = 10, term=\"val_My_MSE\")])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dgf1eJPPay1b"
      },
      "source": [
        "model.load_weights('poids_train.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JM-dtd32WRZ"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQj5FkF72WRa"
      },
      "source": [
        "# Evaluation du modèle\n",
        "\n",
        "model.evaluate(dataset)\n",
        "model.evaluate(dataset_val)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rExhkpy-2WRa"
      },
      "source": [
        "**3. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ii9KzNlZ2WRb"
      },
      "source": [
        "# Création des instants d'entrainement et de validation\n",
        "y_train_timing = serie_entrainement_x.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_train)]\n",
        "y_val_timing = serie_test_x.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_val)]\n",
        "\n",
        "# Calcul des prédictions\n",
        "pred_ent = model.predict(x_train, verbose=1)\n",
        "pred_val = model.predict(x_val, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEBjS2gG2WRb"
      },
      "source": [
        "**4.Affichage sur une période de 1 heure**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee3YihHo2WRb"
      },
      "source": [
        "Création d'une série contenant les valeurs originales et les prédictions, synchronisées dans le temps :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbyOs-He2WRc"
      },
      "source": [
        "serie_btc_ent_ori = serie_entrainement_y*std+mean\n",
        "serie_btc_val_ori = serie_test_y*std+mean\n",
        "\n",
        "serie_btc_ent_pred = pd.Series(data=(pred_ent[:,0]*std+mean),index=y_train_timing)\n",
        "serie_btc_val_pred = pd.Series(data=(pred_val[:,0]*std+mean),index=y_val_timing)\n",
        "\n",
        "serie_btc_ori = pd.concat([serie_btc_ent_ori,serie_btc_val_ori])\n",
        "serie_btc_pred = pd.concat([serie_btc_ent_pred,serie_btc_val_pred])\n",
        "\n",
        "serie_btc_ori = serie_btc_ori.fillna(method=\"backfill\")\n",
        "serie_btc_pred = serie_btc_pred.fillna(method=\"backfill\")\n",
        "\n",
        "serie_btc_ent_ori = serie_btc_ent_ori.fillna(method=\"backfill\")\n",
        "serie_btc_val_ori = serie_btc_val_ori.fillna(method=\"backfill\")\n",
        "serie_btc_ent_pred = serie_btc_ent_pred.fillna(method=\"backfill\")\n",
        "serie_btc_val_pred = serie_btc_val_pred.fillna(method=\"backfill\")\n",
        "\n",
        "frame = {'BTC_ALL' : serie_btc_ori, 'BTC_ENT': serie_btc_ent_ori, 'BTC_VAL' : serie_btc_val_ori, 'BTC_PRED' : serie_btc_pred, 'BTC_PRED_ENT' : serie_btc_ent_pred, 'BTC_PRED_VAL':serie_btc_val_pred}\n",
        "df_resultats = pd.DataFrame(frame)\n",
        "df_resultats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMvz38v02WRc"
      },
      "source": [
        "date_separation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3tE5FK-2WRd"
      },
      "source": [
        "df_resultats.loc[date_separation-pd.Timedelta(hours=7):date_separation+pd.Timedelta(hours=7)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvF19ok42WRd"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbe originale\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_ALL'],line=dict(color='blue', width=1),name=\"Prix BTC\"))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_ENT'],line=dict(color='green', width=1),name=\"Entrainement\"))\n",
        "\n",
        "# Courbe de validation\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_VAL'],line=dict(color='red', width=1),name=\"Validation\"))\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmyOVcdD2WRh"
      },
      "source": [
        "**5. Détection de l'augmentation des erreurs dans la zone de validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvS4fipz2WRi"
      },
      "source": [
        "On peut imaginer devoir suivre en temps réel l'évolution des prédictions afin de détecter lorsque le modèle n'est plus valide."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_aSn4Gg2WRi"
      },
      "source": [
        "# Détection de la date où commencent les anomalies\n",
        "# sur la période de validation\n",
        "\n",
        "# Construit les dataframe pour calculer les erreurs\n",
        "df_error_btc_ent = df_resultats[['BTC_ENT','BTC_PRED_ENT']].dropna()\n",
        "df_error_btc_val = df_resultats[['BTC_VAL','BTC_PRED_VAL']].dropna()\n",
        "\n",
        "# Calcul des erreurs relatives sur les entrainements et les validations\n",
        "erreur_ent = abs(np.asarray(df_error_btc_ent['BTC_ENT']) - np.asarray(df_error_btc_ent['BTC_PRED_ENT']))/np.asarray(df_error_btc_ent['BTC_ENT'])*100.0\n",
        "erreur_val = abs(np.asarray(df_error_btc_val['BTC_VAL']) - np.asarray(df_error_btc_val['BTC_PRED_VAL']))/np.asarray(df_error_btc_val['BTC_VAL'])*100.0\n",
        "\n",
        "# Erreur relative moyenne sur les entrainements\n",
        "erreur_ent_mape = tf.keras.metrics.mape(np.asarray(df_error_btc_ent['BTC_ENT']),np.asarray(df_error_btc_ent['BTC_PRED_ENT']))\n",
        "erreur_ent_mape_max = np.amax(erreur_ent)\n",
        "\n",
        "# Calcul le nombre d'anomalies sur les entrainements avec le ratio spécifié\n",
        "nbr_anomalies_ent = np.count_nonzero(erreur_ent>erreur_ent_mape)\n",
        "\n",
        "# Calcul du ratio d'anomalies sur la période d'entrainement\n",
        "ratio_ent = np.count_nonzero(erreur_ent>erreur_ent_mape)/erreur_ent.size\n",
        "\n",
        "# Recherche de la date à partir de laquelle on dépasse l\n",
        "n_erreurs = 0\n",
        "for i in range(5,erreur_val.size):\n",
        "  erreur_val_ = abs(np.asarray(df_error_btc_val['BTC_VAL'][i]) - np.asarray(df_error_btc_val['BTC_PRED_VAL'][i]))/np.asarray(df_error_btc_val['BTC_VAL'][i])*100.0\n",
        "  seuil = erreur_ent_mape_max*1\n",
        "  if erreur_val_ > seuil:\n",
        "    n_erreurs = n_erreurs + 1\n",
        "    if n_erreurs == 1:\n",
        "      index = df_error_btc_val.index[i]\n",
        "      break\n",
        "  else:\n",
        "    n_erreurs = 0\n",
        "print(index)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXx0vqSX2WRj"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbe originale\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_ALL'],line=dict(color='blue', width=1),name=\"Prix BTC\"))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_ENT'],line=dict(color='green', width=1),name=\"Entrainement\"))\n",
        "\n",
        "# Courbe de validation\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_VAL'],line=dict(color='red', width=1),name=\"Validation\"))\n",
        "\n",
        "# Délimite la zone d'erreur\n",
        "fig.add_shape(type=\"line\",x0=index,y0=0,x1=index,y1=np.amax(df_resultats['BTC_ALL']),name=\"Zone d'erreur\")\n",
        "\n",
        "# Affiche les courbes\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "st8Ynp3T2WRo"
      },
      "source": [
        "**5.Affichage sur une période de 1 jour**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZe5LHTL2WRo"
      },
      "source": [
        "# Echantillonage des données\n",
        "time = \"1D\"\n",
        "\n",
        "df_resultats = df_resultats.resample(time).asfreq()\n",
        "df_resultats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG6DOtBk2WRo"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbe originale\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_ALL'],line=dict(color='blue', width=1),name=\"Prix BTC\"))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_ENT'],line=dict(color='green', width=1),name=\"Entrainement\"))\n",
        "\n",
        "# Courbe de validation\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_VAL'],line=dict(color='red', width=1),name=\"Validation\"))\n",
        "\n",
        "# Délimite la zone d'erreur\n",
        "fig.add_shape(type=\"line\",x0=index,y0=0,x1=index,y1=np.amax(df_resultats['BTC_ALL']),name=\"Zone d'erreur\")\n",
        "\n",
        "# Affiche les courbes\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRKI61PK2WRp"
      },
      "source": [
        "**6.Synthèse des erreurs sur les différentes zones**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Hxnr0oT2WRq"
      },
      "source": [
        "# Echantillonage des données\n",
        "time = \"1H\"\n",
        "\n",
        "df_resultats = df_resultats.resample(time).asfreq()\n",
        "df_resultats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMW1t-kp2WRq"
      },
      "source": [
        "# Détection de la date où commencnte les anomalies\n",
        "# sur la période de validation\n",
        "\n",
        "# Construit les dataframe pour calculer les erreurs\n",
        "df_error_btc_ent = df_resultats[['BTC_ENT','BTC_PRED_ENT']].dropna()\n",
        "df_error_btc_val = df_resultats[['BTC_VAL','BTC_PRED_VAL']].dropna()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bxhjwQK2WRq"
      },
      "source": [
        "# Erreurs d'entrainement\n",
        "mae_ent = tf.keras.metrics.mean_absolute_error(np.asarray(df_error_btc_ent['BTC_ENT']),np.asarray(df_error_btc_ent['BTC_PRED_ENT'])).numpy()\n",
        "mse_ent = tf.keras.metrics.mean_squared_error(np.asarray(df_error_btc_ent['BTC_ENT']),np.asarray(df_error_btc_ent['BTC_PRED_ENT'])).numpy()\n",
        "mape_ent = tf.keras.metrics.mape(np.asarray(df_error_btc_ent['BTC_ENT']),np.asarray(df_error_btc_ent['BTC_PRED_ENT'])).numpy()\n",
        "\n",
        "# Erreurs de validation\n",
        "mae_val = tf.keras.metrics.mean_absolute_error(np.asarray(df_error_btc_val['BTC_VAL']),np.asarray(df_error_btc_val['BTC_PRED_VAL'])).numpy()\n",
        "mse_val = tf.keras.metrics.mean_squared_error(np.asarray(df_error_btc_val['BTC_VAL']),np.asarray(df_error_btc_val['BTC_PRED_VAL'])).numpy()\n",
        "mape_val = tf.keras.metrics.mape(np.asarray(df_error_btc_val['BTC_VAL']),np.asarray(df_error_btc_val['BTC_PRED_VAL'])).numpy()\n",
        "\n",
        "# Erreurs sur la zone valide de validation\n",
        "mae_val_ok = tf.keras.metrics.mean_absolute_error(df_error_btc_val['BTC_VAL'][:index],np.asarray(df_error_btc_val['BTC_PRED_VAL'][:index])).numpy()\n",
        "mse_val_ok = tf.keras.metrics.mean_squared_error(df_error_btc_val['BTC_VAL'][:index],np.asarray(df_error_btc_val['BTC_PRED_VAL'][:index])).numpy()\n",
        "mape_val_ok = tf.keras.metrics.mape(df_error_btc_val['BTC_VAL'][:index],np.asarray(df_error_btc_val['BTC_PRED_VAL'][:index])).numpy()\n",
        "\n",
        "\n",
        "print(\"Erreur mae entrainement %s\" %mae_ent)\n",
        "print(\"Erreur mse entrainement : %s\" %mse_ent)\n",
        "print(\"Erreur mape entrainement : %s\\n\" %mape_ent)\n",
        "\n",
        "print(\"Erreur mae validation %s\" %mae_val)\n",
        "print(\"Erreur mse validation : %s\" %mse_val)\n",
        "print(\"Erreur mape entrainement : %s\\n\" %mape_val)\n",
        "\n",
        "print(\"Erreur mae validation zone OK %s\" %mae_val_ok)\n",
        "print(\"Erreur mse validation zone OK: %s\" %mse_val_ok)\n",
        "print(\"Erreur mape validation zone OK: %s\" %mape_val_ok)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ntshp_muADWt"
      },
      "source": [
        "# Création du modèle End-To-End Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vddiuNuhAliy"
      },
      "source": [
        "def prepare_dataset_XY(serie, taille_fenetre, batch_size, nbr_sequences):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie)\n",
        "  dataset = dataset.window(taille_fenetre+1, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre + 1))\n",
        "  dataset = dataset.window(nbr_sequences+1, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(nbr_sequences+1,drop_remainder=True))\n",
        "  dataset = dataset.map(lambda x: [(tf.slice(x,[0,0],[nbr_sequences,taille_fenetre]),                           # (30;20)       [((30,20),(20)),(1)]\n",
        "                                   tf.squeeze(tf.slice(x,[nbr_sequences,0],[1,taille_fenetre]),axis=0)),        # (20)\n",
        "                                   tf.squeeze(tf.slice(x,[nbr_sequences,taille_fenetre],[1,1]),axis=0)])        # (1)\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WG2KIkJApxh"
      },
      "source": [
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "taille_fenetre = 15\n",
        "batch_size = 1000\n",
        "Nbr_Sequences = 10\n",
        "\n",
        "dataset = prepare_dataset_XY(serie_entrainement,taille_fenetre,batch_size,Nbr_Sequences)              # 56x((1000,10,15),(1000,15)),(1000,1)\n",
        "dataset_val = prepare_dataset_XY(serie_test,taille_fenetre,batch_size,Nbr_Sequences)              # 56x((1000,10,15),(1000,15)),(1000,1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQEziTjSCFDg"
      },
      "source": [
        "len(list(dataset.as_numpy_iterator()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4z2Y65dCJg4"
      },
      "source": [
        "for element in dataset.take(1):\n",
        "  print(element)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9JXgCGOCiiP"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "#56x((1000,4,1),(1000,1)) => (56*1000,4,1) ; (56*1000,1)\n",
        "\n",
        "x,y = tuple(zip(*dataset))\n",
        "\n",
        "# Recombine les données\n",
        "# (56,1000,4,1) => (56*128,4,1)\n",
        "# (56,1000,1) => (56*128,1)\n",
        "x_train = np.asarray(tf.reshape(np.asarray(x,dtype=np.float32),shape=(np.asarray(x).shape[0]*np.asarray(x).shape[1],taille_fenetre,1)))\n",
        "y_train = np.asarray(tf.reshape(np.asarray(y,dtype=np.float32),shape=(np.asarray(y).shape[0]*np.asarray(y).shape[1])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW2AKgnLADW9"
      },
      "source": [
        "**1. Création du réseau**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNx-iQ3bADW9"
      },
      "source": [
        "Par défaut, la dimension des vecteurs cachés est de 10 et aucune régularisation n'est utilisée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1P16yDVAM0B"
      },
      "source": [
        "# Définition du de la couche du modèle\n",
        "# End-to-End Memory Network\n",
        "# Epaquetage des données avec le dernier état caché d'une couche GRU\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "class Couche_End_to_End_MN(tf.keras.layers.Layer):\n",
        "  # Fonction d'initialisation de la classe d'attention\n",
        "  # dim_GRU : Dimension des vecteurs GRU\n",
        "  # x : Séquences à mémoriser (batch_size, Nbr_Sequence, taille_fenetre)\n",
        "  # Fonction de la couche lambda d'entrée\n",
        "  def __init__(self,dim_GRU,regul=0.0):\n",
        "    self.dim_GRU = dim_GRU\n",
        "    self.regul = regul\n",
        "    super().__init__()          # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    # Définition des couches GRU pour traiter les séquences d'entrée\n",
        "    self.couche_GRU_A = tf.keras.layers.GRU(self.dim_GRU,kernel_regularizer=tf.keras.regularizers.l2(self.regul))\n",
        "    self.couche_GRU_B = tf.keras.layers.GRU(self.dim_GRU,kernel_regularizer=tf.keras.regularizers.l2(self.regul))\n",
        "    self.couche_GRU_C = tf.keras.layers.GRU(self.dim_GRU,kernel_regularizer=tf.keras.regularizers.l2(self.regul))\n",
        "\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Définit la logique de la couche d'attention\n",
        "  # Arguments :     x : (batch_size, Nbr_Sequence, taille_fenetre)\n",
        "  #                 y : (batch_size, taille_fenetre)\n",
        "  # Exemple :   batch_size = 32\n",
        "  #             Nbr_Sequence =30\n",
        "  #             taille_fenetre = 20\n",
        "  #             dim_GRU = 40 \n",
        "  def call(self,x,y):\n",
        "    # Création des vecteurs mi dans le tenseur M\n",
        "    M = tf.expand_dims(x,axis=-1)                                   # (32,30,20) => (32,30,20,1)\n",
        "    M = tf.keras.layers.TimeDistributed(self.couche_GRU_A)(M)       # (32,30,20,1) => (32,30,40) : TimeStep = 30 : (32,20,1) envoyé\n",
        "    M = K.tanh(M)\n",
        "\n",
        "    # Création du vecteur d'état u\n",
        "    u = tf.expand_dims(y,axis=-1)                                   # (32,20) => (32,20,1)\n",
        "    u = self.couche_GRU_B(u)                                        # (32,20,1) => (32,40)\n",
        "    u = tf.expand_dims(u,axis=-1)                                   # (32,40) => (32,40,1)\n",
        "    u = K.tanh(u)                                                   # (32,40,1)\n",
        "\n",
        "    # Calcul des poids d'attention\n",
        "    p = tf.matmul(M,u)                                              # (32,30,40)x(32,40,1)=(32,30,1)\n",
        "    p = tf.keras.activations.softmax(p,axis=1)                      # (32,30,1)\n",
        "\n",
        "    # Création des vecteurs ci dans le tenseur C\n",
        "    C = tf.expand_dims(x,axis=-1)                                   # (32,30,20) => (32,30,20,1)\n",
        "    C = tf.keras.layers.TimeDistributed(self.couche_GRU_C)(C)       # (32,30,20,1) => (32,30,40) : TimeStep = 30 : (32,20,1) envoyé\n",
        "    C = K.tanh(C)\n",
        "\n",
        "    # Calcul du vecteur réponse issu de la mémoire\n",
        "    o = tf.multiply(C,p)                                            # (32,30,40)_x_(32,30,1) = (32,30,40)\n",
        "    o = K.sum(o, axis=1)                                            # (32,40)\n",
        "    o = K.tanh(o)                                                   # (32,40)\n",
        "    \n",
        "    # Retourne le vecteur d'attention\n",
        "    return (o+tf.squeeze(u,axis=2))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygj7fwujADW-"
      },
      "source": [
        "dim_LSTM = 40\n",
        "l1_reg = 0.0\n",
        "l2_reg = 0.0\n",
        "\n",
        "# Définition des entrées du modèle\n",
        "entrees_sequences = tf.keras.layers.Input(shape=(Nbr_Sequences,taille_fenetre))\n",
        "entrees_entrainement = tf.keras.layers.Input(shape=(taille_fenetre))\n",
        "\n",
        "# Encodeur\n",
        "s_encodeur = Couche_End_to_End_MN(dim_GRU=dim_LSTM,regul=0.0)(entrees_sequences,entrees_entrainement)\n",
        "\n",
        "# Décodeur\n",
        "s_decodeur = tf.keras.layers.Dense(dim_LSTM,activation=\"tanh\")(s_encodeur)\n",
        "s_decodeur = tf.keras.layers.Concatenate()([s_decodeur,s_encodeur])\n",
        "\n",
        "# Générateur\n",
        "sortie = tf.keras.layers.Dense(1)(s_decodeur)\n",
        "\n",
        "# Construction du modèle\n",
        "model = tf.keras.Model([entrees_sequences,entrees_entrainement],sortie)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFNCcD1BADW_"
      },
      "source": [
        "**2. Optimisation de l'apprentissage**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTQVpBLpADW_"
      },
      "source": [
        "Pour accélérer le traitement des données, nous n'allons pas utiliser l'intégralité des données pendant la mise à jour du gradient, comme cela a été fait jusqu'à présent (en utilisant le dataset).  \n",
        "Cette fois-ci, nous allons forcer les mises à jour du gradient à se produire de manière moins fréquente en attribuant la valeur du batch_size à prendre en compte lors de la regression du modèle.  \n",
        "Pour cela, on utilise l'argument \"batch_size\" dans la méthode fit. En précisant un batch_size=1000, cela signifie que :\n",
        " - Sur notre total de 56000 échantillons, 56 seront utilisés pour les calculs du gradient\n",
        " - Il y aura également 56 itérations à chaque période.\n",
        "  \n",
        "    \n",
        "    \n",
        "Si nous avions pris le dataset comme entrée, nous aurions eu :\n",
        "- Un total de 56000 échantillons également\n",
        "- Chaque période aurait également pris 56 itérations pour se compléter\n",
        "- Mais 1000 échantillons auraient été utilisés pour le calcul du gradient, au lieu de 56 avec la méthode utilisée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atvCPpf-ADXA"
      },
      "source": [
        "batch_size = 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZm9WrqVADXA"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam()\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"logcosh\", optimizer=optimiseur, metrics=\"mse\")\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint],batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2zHrpiAADXB"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[30], taux[99], 0, 0.04])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha5ZDQyOADXC"
      },
      "source": [
        "# Chargement des poids sauvegardés\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SzpxsrQADXC"
      },
      "source": [
        "max_periodes = 1000\n",
        "\n",
        "# Classe permettant d'arrêter l'entrainement si la variation\n",
        "# devient plus petite qu'une valeur à choisir sur un nombre\n",
        "# de périodes à choisir\n",
        "class StopTrain(keras.callbacks.Callback):\n",
        "    def __init__(self, delta=0.01,periodes=100, term=\"loss\", logs={}):\n",
        "      self.n_periodes = 0\n",
        "      self.periodes = periodes\n",
        "      self.loss_1 = 100\n",
        "      self.delta = delta\n",
        "      self.term = term\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "      diff_loss = abs(self.loss_1 - logs[self.term])\n",
        "      self.loss_1 = logs[self.term]\n",
        "      if (diff_loss < self.delta):\n",
        "        self.n_periodes = self.n_periodes + 1\n",
        "      else:\n",
        "        self.n_periodes = 0\n",
        "      if (self.n_periodes == self.periodes):\n",
        "        print(\"Arrêt de l'entrainement...\")\n",
        "        self.model.stop_training = True\n",
        "\n",
        "def  My_MSE(y_true,y_pred):\n",
        "  return(tf.keras.metrics.mse(y_true,y_pred)*std.numpy()+mean.numpy())\n",
        "  \n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.001,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0.01)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"logcosh\", optimizer=optimiseur, metrics=[\"mse\",My_MSE])\n",
        "\n",
        "# Entraine le modèle, avec une réduction des calculs du gradient\n",
        "#historique = model.fit(x=x_train,y=y_train,validation_data=(x_val,y_val), epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=0.005,periodes = 10, term=\"My_MSE\")],batch_size=batch_size)\n",
        "\n",
        "# Entraine le modèle sans réduction de calculs\n",
        "historique = model.fit(dataset,validation_data=dataset_val, epochs=max_periodes,verbose=1, callbacks=[CheckPoint,StopTrain(delta=0.1,periodes = 10, term=\"val_My_MSE\")])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xiy0xyuYJpC8"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('poids_train.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nehoz4YADXD"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7fBX8FzADXE"
      },
      "source": [
        "# Evaluation du modèle\n",
        "# Avec le modèle type encodeur/décodeur :\n",
        "# 56/56 [==============================] - 7s 113ms/step - loss: 1.1795e-04 - mse: 2.3664e-04 - My_MSE: 2712.2415\n",
        "# 14/14 [==============================] - 2s 113ms/step - loss: 0.3602 - mse: 1.9360 - My_MSE: 9650.2637\n",
        "model.evaluate(dataset)\n",
        "model.evaluate(dataset_val)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibjW2608ADXE"
      },
      "source": [
        "**3. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4XVlUVqADXF"
      },
      "source": [
        "horizon = 1\n",
        "# Création des instants d'entrainement et de validation\n",
        "y_train_timing = serie_entrainement.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_train)]\n",
        "y_val_timing = serie_test.index[taille_fenetre + horizon - 1:taille_fenetre + horizon - 1+len(y_val)]\n",
        "\n",
        "# Calcul des prédictions\n",
        "pred_ent = model.predict(x_train, verbose=1)\n",
        "pred_val = model.predict(x_val, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qssOW-x2ADXF"
      },
      "source": [
        "**4.Affichage sur une période de 1 heure**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLGHAwjIADXF"
      },
      "source": [
        "Création d'une série contenant les valeurs originales et les prédictions, synchronisées dans le temps :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoyojCloADXG"
      },
      "source": [
        "serie_btc_ent_ori = serie_entrainement*std+mean\n",
        "serie_btc_val_ori = serie_test*std+mean\n",
        "\n",
        "serie_btc_ent_pred = pd.Series(data=(pred_ent[:,0]*std+mean),index=y_train_timing)\n",
        "serie_btc_val_pred = pd.Series(data=(pred_val[:,0]*std+mean),index=y_val_timing)\n",
        "\n",
        "serie_btc_ori = pd.concat([serie_btc_ent_ori,serie_btc_val_ori])\n",
        "serie_btc_pred = pd.concat([serie_btc_ent_pred,serie_btc_val_pred])\n",
        "\n",
        "serie_btc_ori = serie_btc_ori.fillna(method=\"backfill\")\n",
        "serie_btc_pred = serie_btc_pred.fillna(method=\"backfill\")\n",
        "\n",
        "serie_btc_ent_ori = serie_btc_ent_ori.fillna(method=\"backfill\")\n",
        "serie_btc_val_ori = serie_btc_val_ori.fillna(method=\"backfill\")\n",
        "serie_btc_ent_pred = serie_btc_ent_pred.fillna(method=\"backfill\")\n",
        "serie_btc_val_pred = serie_btc_val_pred.fillna(method=\"backfill\")\n",
        "\n",
        "frame = {'BTC_ALL' : serie_btc_ori, 'BTC_ENT': serie_btc_ent_ori, 'BTC_VAL' : serie_btc_val_ori, 'BTC_PRED' : serie_btc_pred, 'BTC_PRED_ENT' : serie_btc_ent_pred, 'BTC_PRED_VAL':serie_btc_val_pred}\n",
        "df_resultats = pd.DataFrame(frame)\n",
        "df_resultats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SwwQkjiADXG"
      },
      "source": [
        "date_separation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqvOoeoYADXG"
      },
      "source": [
        "df_resultats.loc[date_separation-pd.Timedelta(hours=7):date_separation+pd.Timedelta(hours=7)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNqabyNzADXH"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbe originale\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_ALL'],line=dict(color='blue', width=1),name=\"Prix BTC\"))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_ENT'],line=dict(color='green', width=1),name=\"Entrainement\"))\n",
        "\n",
        "# Courbe de validation\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_VAL'],line=dict(color='red', width=1),name=\"Validation\"))\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCTa2-ZEADXM"
      },
      "source": [
        "**5. Détection de l'augmentation des erreurs dans la zone de validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3cjdfFLADXN"
      },
      "source": [
        "On peut imaginer devoir suivre en temps réel l'évolution des prédictions afin de détecter lorsque le modèle n'est plus valide."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snd-tMArADXN"
      },
      "source": [
        "# Détection de la date où commencent les anomalies\n",
        "# sur la période de validation\n",
        "\n",
        "# Construit les dataframe pour calculer les erreurs\n",
        "df_error_btc_ent = df_resultats[['BTC_ENT','BTC_PRED_ENT']].dropna()\n",
        "df_error_btc_val = df_resultats[['BTC_VAL','BTC_PRED_VAL']].dropna()\n",
        "\n",
        "# Calcul des erreurs relatives sur les entrainements et les validations\n",
        "erreur_ent = abs(np.asarray(df_error_btc_ent['BTC_ENT']) - np.asarray(df_error_btc_ent['BTC_PRED_ENT']))/np.asarray(df_error_btc_ent['BTC_ENT'])*100.0\n",
        "erreur_val = abs(np.asarray(df_error_btc_val['BTC_VAL']) - np.asarray(df_error_btc_val['BTC_PRED_VAL']))/np.asarray(df_error_btc_val['BTC_VAL'])*100.0\n",
        "\n",
        "# Erreur relative moyenne sur les entrainements\n",
        "erreur_ent_mape = tf.keras.metrics.mape(np.asarray(df_error_btc_ent['BTC_ENT']),np.asarray(df_error_btc_ent['BTC_PRED_ENT']))\n",
        "erreur_ent_mape_max = np.amax(erreur_ent)\n",
        "\n",
        "# Calcul le nombre d'anomalies sur les entrainements avec le ratio spécifié\n",
        "nbr_anomalies_ent = np.count_nonzero(erreur_ent>erreur_ent_mape)\n",
        "\n",
        "# Calcul du ratio d'anomalies sur la période d'entrainement\n",
        "ratio_ent = np.count_nonzero(erreur_ent>erreur_ent_mape)/erreur_ent.size\n",
        "\n",
        "# Recherche de la date à partir de laquelle on dépasse l\n",
        "n_erreurs = 0\n",
        "for i in range(5,erreur_val.size):\n",
        "  erreur_val_ = abs(np.asarray(df_error_btc_val['BTC_VAL'][i]) - np.asarray(df_error_btc_val['BTC_PRED_VAL'][i]))/np.asarray(df_error_btc_val['BTC_VAL'][i])*100.0\n",
        "  seuil = erreur_ent_mape_max*1\n",
        "  if erreur_val_ > seuil:\n",
        "    n_erreurs = n_erreurs + 1\n",
        "    if n_erreurs == 5:\n",
        "      index = df_error_btc_val.index[i]\n",
        "      break\n",
        "  else:\n",
        "    n_erreurs = 0\n",
        "print(index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubGN_i0fADXO"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbe originale\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_ALL'],line=dict(color='blue', width=1),name=\"Prix BTC\"))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_ENT'],line=dict(color='green', width=1),name=\"Entrainement\"))\n",
        "\n",
        "# Courbe de validation\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_VAL'],line=dict(color='red', width=1),name=\"Validation\"))\n",
        "\n",
        "# Délimite la zone d'erreur\n",
        "fig.add_shape(type=\"line\",x0=index,y0=0,x1=index,y1=np.amax(df_resultats['BTC_ALL']),name=\"Zone d'erreur\")\n",
        "\n",
        "# Affiche les courbes\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emkQ01bfADXT"
      },
      "source": [
        "**5.Affichage sur une période de 1 jour**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShhfXg8yADXU"
      },
      "source": [
        "# Echantillonage des données\n",
        "time = \"1D\"\n",
        "\n",
        "df_resultats = df_resultats.resample(time).asfreq()\n",
        "df_resultats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btEN2yyyADXU"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbe originale\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_ALL'],line=dict(color='blue', width=1),name=\"Prix BTC\"))\n",
        "\n",
        "# Courbes des prédictions d'entrainement\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_ENT'],line=dict(color='green', width=1),name=\"Entrainement\"))\n",
        "\n",
        "# Courbe de validation\n",
        "fig.add_trace(go.Scatter(x=df_resultats.index,y=df_resultats['BTC_PRED_VAL'],line=dict(color='red', width=1),name=\"Validation\"))\n",
        "\n",
        "# Délimite la zone d'erreur\n",
        "fig.add_shape(type=\"line\",x0=index,y0=0,x1=index,y1=np.amax(df_resultats['BTC_ALL']),name=\"Zone d'erreur\")\n",
        "\n",
        "# Affiche les courbes\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0eIXJrhADXV"
      },
      "source": [
        "**6.Synthèse des erreurs sur les différentes zones**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nP8wjWErADXV"
      },
      "source": [
        "# Echantillonage des données\n",
        "time = \"1H\"\n",
        "\n",
        "df_resultats = df_resultats.resample(time).asfreq()\n",
        "df_resultats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuofOZLJADXW"
      },
      "source": [
        "# Détection de la date où commencnte les anomalies\n",
        "# sur la période de validation\n",
        "\n",
        "# Construit les dataframe pour calculer les erreurs\n",
        "df_error_btc_ent = df_resultats[['BTC_ENT','BTC_PRED_ENT']].dropna()\n",
        "df_error_btc_val = df_resultats[['BTC_VAL','BTC_PRED_VAL']].dropna()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAyfDmMLADXW"
      },
      "source": [
        "# Erreurs d'entrainement\n",
        "mae_ent = tf.keras.metrics.mean_absolute_error(np.asarray(df_error_btc_ent['BTC_ENT']),np.asarray(df_error_btc_ent['BTC_PRED_ENT'])).numpy()\n",
        "mse_ent = tf.keras.metrics.mean_squared_error(np.asarray(df_error_btc_ent['BTC_ENT']),np.asarray(df_error_btc_ent['BTC_PRED_ENT'])).numpy()\n",
        "mape_ent = tf.keras.metrics.mape(np.asarray(df_error_btc_ent['BTC_ENT']),np.asarray(df_error_btc_ent['BTC_PRED_ENT'])).numpy()\n",
        "\n",
        "# Erreurs de validation\n",
        "mae_val = tf.keras.metrics.mean_absolute_error(np.asarray(df_error_btc_val['BTC_VAL']),np.asarray(df_error_btc_val['BTC_PRED_VAL'])).numpy()\n",
        "mse_val = tf.keras.metrics.mean_squared_error(np.asarray(df_error_btc_val['BTC_VAL']),np.asarray(df_error_btc_val['BTC_PRED_VAL'])).numpy()\n",
        "mape_val = tf.keras.metrics.mape(np.asarray(df_error_btc_val['BTC_VAL']),np.asarray(df_error_btc_val['BTC_PRED_VAL'])).numpy()\n",
        "\n",
        "# Erreurs sur la zone valide de validation\n",
        "mae_val_ok = tf.keras.metrics.mean_absolute_error(df_error_btc_val['BTC_VAL'][:index],np.asarray(df_error_btc_val['BTC_PRED_VAL'][:index])).numpy()\n",
        "mse_val_ok = tf.keras.metrics.mean_squared_error(df_error_btc_val['BTC_VAL'][:index],np.asarray(df_error_btc_val['BTC_PRED_VAL'][:index])).numpy()\n",
        "mape_val_ok = tf.keras.metrics.mape(df_error_btc_val['BTC_VAL'][:index],np.asarray(df_error_btc_val['BTC_PRED_VAL'][:index])).numpy()\n",
        "\n",
        "\n",
        "print(\"Erreur mae entrainement %s\" %mae_ent)\n",
        "print(\"Erreur mse entrainement : %s\" %mse_ent)\n",
        "print(\"Erreur mape entrainement : %s\\n\" %mape_ent)\n",
        "\n",
        "print(\"Erreur mae validation %s\" %mae_val)\n",
        "print(\"Erreur mse validation : %s\" %mse_val)\n",
        "print(\"Erreur mape entrainement : %s\\n\" %mape_val)\n",
        "\n",
        "print(\"Erreur mae validation zone OK %s\" %mae_val_ok)\n",
        "print(\"Erreur mse validation zone OK: %s\" %mse_val_ok)\n",
        "print(\"Erreur mape validation zone OK: %s\" %mape_val_ok)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}