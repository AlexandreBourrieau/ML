{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reseau_GRU_Avec_Auto_Attention.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOzMyPOUNnf3UZJKJvOAg2H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/Reseau_GRU_Avec_Auto_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubCeIvtF6R4W"
      },
      "source": [
        "Dans ce carnet nous allons mettre en place un modèle à réseau de neurones récurrent de type GRU associé à une **couche d'auto attention** comprenant une **matrice de contexte** pour réaliser des prédictions sur notre série temporelle.  \n",
        "Ce modèle est tiré du papier de recherche : [A Structured Self-attentive Sentence Embedding](https://arxiv.org/pdf/1703.03130)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRhtHsNn5fc3"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFeah3y_6kif"
      },
      "source": [
        "# Création de la série temporelle et du dataset pour l'entrainement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJfSLtub6sdc"
      },
      "source": [
        "# Fonction permettant d'afficher une série temporelle\n",
        "def affiche_serie(temps, serie, format=\"-\", debut=0, fin=None, label=None):\n",
        "    plt.plot(temps[debut:fin], serie[debut:fin], format, label=label)\n",
        "    plt.xlabel(\"Temps\")\n",
        "    plt.ylabel(\"Valeur\")\n",
        "    if label:\n",
        "        plt.legend(fontsize=14)\n",
        "    plt.grid(True)\n",
        "\n",
        "# Fonction permettant de créer une tendance\n",
        "def tendance(temps, pente=0):\n",
        "    return pente * temps\n",
        "\n",
        "# Fonction permettant de créer un motif\n",
        "def motif_periodique(instants):\n",
        "    return (np.where(instants < 0.4,                            # Si les instants sont < 0.4\n",
        "                    np.cos(instants * 2 * np.pi),               # Alors on retourne la fonction cos(2*pi*t)\n",
        "                    1 / np.exp(3 * instants)))                  # Sinon, on retourne la fonction exp(-3t)\n",
        "\n",
        "# Fonction permettant de créer une saisonnalité avec un motif\n",
        "def saisonnalite(temps, periode, amplitude=1, phase=0):\n",
        "    \"\"\"Répétition du motif sur la même période\"\"\"\n",
        "    instants = ((temps + phase) % periode) / periode            # Mapping du temps =[0 1 2 ... 1460] => instants = [0.0 ... 1.0]\n",
        "    return amplitude * motif_periodique(instants)\n",
        "\n",
        "# Fonction permettant de générer du bruit gaussien N(0,1)\n",
        "def bruit_blanc(temps, niveau_bruit=1, graine=None):\n",
        "    rnd = np.random.RandomState(graine)\n",
        "    return rnd.randn(len(temps)) * niveau_bruit\n",
        "\n",
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "# au format X(X1,X2,...Xn) / Y(Y1,Y2,...,Yn)\n",
        "# X sont les données d'entrées du réseau\n",
        "# Y sont les labels\n",
        "\n",
        "def prepare_dataset_XY(serie, taille_fenetre, batch_size, buffer_melange):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie)\n",
        "  dataset = dataset.window(taille_fenetre+1, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre + 1))\n",
        "  dataset = dataset.map(lambda x: (x[:-1], x[-1:]))\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "# Création de la série temporelle\n",
        "temps = np.arange(4 * 365)                # temps = [0 1 2 .... 4*365] = [0 1 2 .... 1460]\n",
        "amplitude = 40                            # Amplitude de la la saisonnalité\n",
        "niveau_bruit = 5                          # Niveau du bruit\n",
        "offset = 10                               # Offset de la série\n",
        "\n",
        "serie = offset + tendance(temps, 0.1) + saisonnalite(temps, periode=365, amplitude=amplitude) + bruit_blanc(temps,niveau_bruit,graine=40)\n",
        "\n",
        "temps_separation = 1000\n",
        "\n",
        "# Extraction des temps et des données d'entrainement\n",
        "temps_entrainement = temps[:temps_separation]\n",
        "x_entrainement = serie[:temps_separation]\n",
        "\n",
        "# Exctraction des temps et des données de valiadation\n",
        "temps_validation = temps[temps_separation:]\n",
        "x_validation = serie[temps_separation:]\n",
        "\n",
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "taille_fenetre = 20\n",
        "batch_size = 32\n",
        "buffer_melange = 1000\n",
        "\n",
        "# Création du dataset X,Y\n",
        "dataset = prepare_dataset_XY(x_entrainement,taille_fenetre,batch_size,buffer_melange)\n",
        "\n",
        "# Création du dataset X,Y de validation\n",
        "dataset_Val = prepare_dataset_XY(x_validation,taille_fenetre,batch_size,buffer_melange)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6WVzU_X3JxG"
      },
      "source": [
        "# Calcul de la moyenne et de l'écart type de la série\n",
        "mean = tf.math.reduce_mean(serie)\n",
        "std = tf.math.reduce_std(serie)\n",
        "\n",
        "# Normalise les données\n",
        "Serie_Normalisee = (serie-mean)/std\n",
        "min = tf.math.reduce_min(serie)\n",
        "max = tf.math.reduce_max(serie)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYtsc0Yk3LhT"
      },
      "source": [
        "# Création des données pour l'entrainement et le test\n",
        "x_entrainement_norm = Serie_Normalisee[:temps_separation]\n",
        "x_validation_norm = Serie_Normalisee[temps_separation:]\n",
        "\n",
        "# Création du dataset X,Y\n",
        "dataset_norm = prepare_dataset_XY(x_entrainement_norm,taille_fenetre,batch_size,buffer_melange)\n",
        "\n",
        "# Création du dataset X,Y de validation\n",
        "dataset_Val_norm = prepare_dataset_XY(x_validation_norm,taille_fenetre,batch_size,buffer_melange)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Yt-EgZ3sgPY"
      },
      "source": [
        "# Création du modèle GRU avec couche d'attention possédant un vecteur de contexte"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyrcfKcgCsZ7"
      },
      "source": [
        "**1. Création du réseau et adaptation des formats d'entrée et de sortie**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeJyix8HK7Kt"
      },
      "source": [
        "Sous forme de shéma, notre réseau est donc le suivant :\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OZkfsmnBNHY"
      },
      "source": [
        "<img src=\"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/images/AutoAttention.png?raw=true\" width=\"1200\"> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kgTrJOQ5DUo"
      },
      "source": [
        "# Remise à zéro de tous les états générés par Keras\n",
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLNIAGDlBizT"
      },
      "source": [
        "On créé une classe dérivée de la classe [Layer](https://keras.io/api/layers/base_layer/#layer-class) de Keras. Les méthodes utilisées sont les suivantes :  \n",
        " - [build](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#build) : Permet de créer les variables utilisées par la couche (commes les poids et les offsets)\n",
        " - [call](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#call) : Permet d'implanter la logique de la couche"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZK9V72Va9vg"
      },
      "source": [
        "<img src=\"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/images/AutoAttentionZoom.png?raw=true\" width=\"1200\"> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyoh5UpQXCqm"
      },
      "source": [
        "Parmi les nouvelles fonctions de Tensorflow et de Keras utilisées, on trouve :\n",
        "- [transpose](https://www.tensorflow.org/api_docs/python/tf/transpose) : Permet de transposer un tenseur et éventuellement de reconstituer l'ordre des axes avec l'argument `perm`\n",
        "- [add_weight](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#add_weight) : Méthode de la classe Layers de Keras, qui permet d'ajouter un paramètre (poids et offset ou autre) qui sera une variable mémoire pour la couche construite. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hhk0kmPSgqva"
      },
      "source": [
        "# Classe d'auto-attention\n",
        "# Applique les poids de la matrice d'attention sur les vecteurs de la couche récurrente\n",
        "\n",
        "# Importe le Backend de Keras\n",
        "from keras import backend as K\n",
        "\n",
        "# Définit une nouvelle classe Couche_Attention\n",
        "# Héritée de la classe Layer de Keras\n",
        "\n",
        "class Couche_Auto_Attention(tf.keras.layers.Layer):\n",
        "  # Fonction d'initialisation de la classe d'attention\n",
        "  def __init__(self,dim_att,nbr_hop):\n",
        "    self.dim_att = dim_att          # Dimension du vecteur d'attention\n",
        "    self.nbr_hop = nbr_hop\n",
        "    super().__init__()              # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.W = self.add_weight(shape=(self.dim_att,input_shape[2]),initializer=\"normal\",name=\"W\")\n",
        "    self.U = self.add_weight(shape=(self.nbr_hop,self.dim_att),initializer=\"normal\",name=\"U\")\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Définit la logique de la couche d'attention\n",
        "  # Arguments :   x : Tenseur d'entrée de dimension (None, nbr_v,dim)\n",
        "  def call(self,x):\n",
        "    # Calcul de la matrice XH contenant les\n",
        "    # représentations cachées des vecteurs\n",
        "    # issus de la couche GRU\n",
        "    xt = tf.transpose(x,perm=[0,2,1])         # x = (None, dim,20)\n",
        "    Xh = K.dot(self.W,xt)                     # Xh = (dim_att,None,20)\n",
        "    Xh = tf.transpose(Xh,perm=[1,0,2])        # Xh = (None, dim_att,20)\n",
        "    Xh = K.tanh(Xh)                           # Xh = (None, dim_att,20)\n",
        "\n",
        "    # Calcul de la matrice des poids d'attention normalisés\n",
        "    A = K.dot(self.U,Xh)                      # A = (nbr_hop,None,20)\n",
        "    A = tf.transpose(A,perm=[1,0,2])          # A = (None, nbr_hop,20)\n",
        "    A = tf.keras.activations.softmax(A,axis=1)\n",
        "\n",
        "    # Calcul de la matrice des vecteur d'attentions\n",
        "    sortie = tf.keras.layers.Dot(axes=(2,1))([A,x])         # sortie = (None,nbr_hop,dim_att)\n",
        "    return tf.keras.layers.Flatten()(sortie)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psWaWe7rHTup",
        "outputId": "18eadd25-da12-4754-a39a-98ccfb65b82b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dim_GRU = 40\n",
        "nbr_hop = 10\n",
        "\n",
        "# Fonction de la couche lambda d'entrée\n",
        "def Traitement_Entrees(x):\n",
        "  return tf.expand_dims(x,axis=-1)\n",
        "\n",
        "# Encodeur\n",
        "input = tf.keras.Input(shape=(taille_fenetre,))\n",
        "s = tf.keras.layers.Lambda(Traitement_Entrees)(input)\n",
        "#s = tf.keras.layers.Dropout(0.08)(s)\n",
        "encoder_output, encoder_state = tf.keras.layers.GRU(dim_GRU,return_sequences=True,return_state=True)(s)\n",
        "\n",
        "# Décodeur\n",
        "s = Couche_Auto_Attention(dim_GRU,nbr_hop)(encoder_output)\n",
        "output_att = tf.keras.layers.Lambda(Traitement_Entrees)(s)\n",
        "outputs = tf.keras.layers.GRU(dim_GRU)(output_att)\n",
        "out = tf.keras.layers.Dense(1)(outputs)\n",
        "\n",
        "model = tf.keras.Model(input,out)\n",
        "model.summary()\n",
        "\n",
        "model.save_weights('model_initial.h5')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 20)]              0         \n",
            "_________________________________________________________________\n",
            "lambda_2 (Lambda)            (None, 20, 1)             0         \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  [(None, 20, 40), (None, 4 5160      \n",
            "_________________________________________________________________\n",
            "couche__auto__attention_1 (C (None, 400)               2000      \n",
            "_________________________________________________________________\n",
            "lambda_3 (Lambda)            (None, 400, 1)            0         \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  (None, 40)                5160      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 41        \n",
            "=================================================================\n",
            "Total params: 12,361\n",
            "Trainable params: 12,361\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUM0-SSXGLIQ"
      },
      "source": [
        "**2. Optimisation du taux d'apprentissage**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jejCBhXVuNQ4",
        "outputId": "a39979ff-7af6-4b74-c588-ffcc49c1f523",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(lr=1e-8)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur, metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset_norm,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "30/30 [==============================] - 6s 97ms/step - loss: 0.6132 - mae: 1.0514\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.36688, saving model to poids.hdf5\n",
            "Epoch 2/100\n",
            "30/30 [==============================] - 3s 97ms/step - loss: 0.6132 - mae: 1.0514\n",
            "\n",
            "Epoch 00002: loss did not improve from 0.36688\n",
            "Epoch 3/100\n",
            "30/30 [==============================] - 3s 103ms/step - loss: 0.6132 - mae: 1.0514\n",
            "\n",
            "Epoch 00003: loss improved from 0.36688 to 0.36688, saving model to poids.hdf5\n",
            "Epoch 4/100\n",
            "30/30 [==============================] - 3s 104ms/step - loss: 0.6132 - mae: 1.0514\n",
            "\n",
            "Epoch 00004: loss did not improve from 0.36688\n",
            "Epoch 5/100\n",
            "30/30 [==============================] - 3s 105ms/step - loss: 0.6132 - mae: 1.0514\n",
            "\n",
            "Epoch 00005: loss did not improve from 0.36688\n",
            "Epoch 6/100\n",
            "30/30 [==============================] - 3s 103ms/step - loss: 0.6132 - mae: 1.0514\n",
            "\n",
            "Epoch 00006: loss did not improve from 0.36688\n",
            "Epoch 7/100\n",
            "30/30 [==============================] - 3s 104ms/step - loss: 0.6132 - mae: 1.0514\n",
            "\n",
            "Epoch 00007: loss did not improve from 0.36688\n",
            "Epoch 8/100\n",
            "30/30 [==============================] - 3s 105ms/step - loss: 0.6133 - mae: 1.0514\n",
            "\n",
            "Epoch 00008: loss did not improve from 0.36688\n",
            "Epoch 9/100\n",
            "30/30 [==============================] - 3s 106ms/step - loss: 0.6132 - mae: 1.0514\n",
            "\n",
            "Epoch 00009: loss did not improve from 0.36688\n",
            "Epoch 10/100\n",
            "30/30 [==============================] - 3s 108ms/step - loss: 0.6132 - mae: 1.0514\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.36688\n",
            "Epoch 11/100\n",
            "30/30 [==============================] - 3s 103ms/step - loss: 0.6132 - mae: 1.0514\n",
            "\n",
            "Epoch 00011: loss did not improve from 0.36688\n",
            "Epoch 12/100\n",
            "30/30 [==============================] - 3s 107ms/step - loss: 0.6132 - mae: 1.0514\n",
            "\n",
            "Epoch 00012: loss improved from 0.36688 to 0.36686, saving model to poids.hdf5\n",
            "Epoch 13/100\n",
            "30/30 [==============================] - 3s 108ms/step - loss: 0.6132 - mae: 1.0514\n",
            "\n",
            "Epoch 00013: loss did not improve from 0.36686\n",
            "Epoch 14/100\n",
            "30/30 [==============================] - 3s 105ms/step - loss: 0.6132 - mae: 1.0513\n",
            "\n",
            "Epoch 00014: loss improved from 0.36686 to 0.36686, saving model to poids.hdf5\n",
            "Epoch 15/100\n",
            "30/30 [==============================] - 3s 104ms/step - loss: 0.6132 - mae: 1.0513\n",
            "\n",
            "Epoch 00015: loss improved from 0.36686 to 0.36686, saving model to poids.hdf5\n",
            "Epoch 16/100\n",
            "30/30 [==============================] - 3s 104ms/step - loss: 0.6132 - mae: 1.0513\n",
            "\n",
            "Epoch 00016: loss improved from 0.36686 to 0.36685, saving model to poids.hdf5\n",
            "Epoch 17/100\n",
            "30/30 [==============================] - 3s 102ms/step - loss: 0.6132 - mae: 1.0513\n",
            "\n",
            "Epoch 00017: loss did not improve from 0.36685\n",
            "Epoch 18/100\n",
            "30/30 [==============================] - 3s 99ms/step - loss: 0.6132 - mae: 1.0513\n",
            "\n",
            "Epoch 00018: loss improved from 0.36685 to 0.36684, saving model to poids.hdf5\n",
            "Epoch 19/100\n",
            "30/30 [==============================] - 3s 107ms/step - loss: 0.6131 - mae: 1.0513\n",
            "\n",
            "Epoch 00019: loss improved from 0.36684 to 0.36682, saving model to poids.hdf5\n",
            "Epoch 20/100\n",
            "30/30 [==============================] - 3s 102ms/step - loss: 0.6131 - mae: 1.0512\n",
            "\n",
            "Epoch 00020: loss improved from 0.36682 to 0.36681, saving model to poids.hdf5\n",
            "Epoch 21/100\n",
            "30/30 [==============================] - 3s 104ms/step - loss: 0.6131 - mae: 1.0512\n",
            "\n",
            "Epoch 00021: loss improved from 0.36681 to 0.36678, saving model to poids.hdf5\n",
            "Epoch 22/100\n",
            "30/30 [==============================] - 3s 104ms/step - loss: 0.6130 - mae: 1.0511\n",
            "\n",
            "Epoch 00022: loss improved from 0.36678 to 0.36676, saving model to poids.hdf5\n",
            "Epoch 23/100\n",
            "30/30 [==============================] - 3s 105ms/step - loss: 0.6130 - mae: 1.0511\n",
            "\n",
            "Epoch 00023: loss improved from 0.36676 to 0.36673, saving model to poids.hdf5\n",
            "Epoch 24/100\n",
            "30/30 [==============================] - 3s 105ms/step - loss: 0.6129 - mae: 1.0510\n",
            "\n",
            "Epoch 00024: loss improved from 0.36673 to 0.36668, saving model to poids.hdf5\n",
            "Epoch 25/100\n",
            "30/30 [==============================] - 3s 107ms/step - loss: 0.6128 - mae: 1.0509\n",
            "\n",
            "Epoch 00025: loss improved from 0.36668 to 0.36665, saving model to poids.hdf5\n",
            "Epoch 26/100\n",
            "30/30 [==============================] - 3s 106ms/step - loss: 0.6127 - mae: 1.0507\n",
            "\n",
            "Epoch 00026: loss improved from 0.36665 to 0.36658, saving model to poids.hdf5\n",
            "Epoch 27/100\n",
            "30/30 [==============================] - 3s 106ms/step - loss: 0.6125 - mae: 1.0506\n",
            "\n",
            "Epoch 00027: loss improved from 0.36658 to 0.36650, saving model to poids.hdf5\n",
            "Epoch 28/100\n",
            "30/30 [==============================] - 3s 101ms/step - loss: 0.6124 - mae: 1.0504\n",
            "\n",
            "Epoch 00028: loss improved from 0.36650 to 0.36641, saving model to poids.hdf5\n",
            "Epoch 29/100\n",
            "30/30 [==============================] - 3s 102ms/step - loss: 0.6122 - mae: 1.0501\n",
            "\n",
            "Epoch 00029: loss improved from 0.36641 to 0.36629, saving model to poids.hdf5\n",
            "Epoch 30/100\n",
            "30/30 [==============================] - 3s 107ms/step - loss: 0.6119 - mae: 1.0498\n",
            "\n",
            "Epoch 00030: loss improved from 0.36629 to 0.36613, saving model to poids.hdf5\n",
            "Epoch 31/100\n",
            "30/30 [==============================] - 3s 107ms/step - loss: 0.6115 - mae: 1.0494\n",
            "\n",
            "Epoch 00031: loss improved from 0.36613 to 0.36594, saving model to poids.hdf5\n",
            "Epoch 32/100\n",
            "30/30 [==============================] - 3s 105ms/step - loss: 0.6111 - mae: 1.0489\n",
            "\n",
            "Epoch 00032: loss improved from 0.36594 to 0.36569, saving model to poids.hdf5\n",
            "Epoch 33/100\n",
            "30/30 [==============================] - 3s 106ms/step - loss: 0.6105 - mae: 1.0482\n",
            "\n",
            "Epoch 00033: loss improved from 0.36569 to 0.36540, saving model to poids.hdf5\n",
            "Epoch 34/100\n",
            "30/30 [==============================] - 3s 106ms/step - loss: 0.6098 - mae: 1.0474\n",
            "\n",
            "Epoch 00034: loss improved from 0.36540 to 0.36501, saving model to poids.hdf5\n",
            "Epoch 35/100\n",
            "30/30 [==============================] - 3s 104ms/step - loss: 0.6089 - mae: 1.0464\n",
            "\n",
            "Epoch 00035: loss improved from 0.36501 to 0.36454, saving model to poids.hdf5\n",
            "Epoch 36/100\n",
            "30/30 [==============================] - 3s 104ms/step - loss: 0.6078 - mae: 1.0450\n",
            "\n",
            "Epoch 00036: loss improved from 0.36454 to 0.36393, saving model to poids.hdf5\n",
            "Epoch 37/100\n",
            "30/30 [==============================] - 3s 108ms/step - loss: 0.6065 - mae: 1.0434\n",
            "\n",
            "Epoch 00037: loss improved from 0.36393 to 0.36319, saving model to poids.hdf5\n",
            "Epoch 38/100\n",
            "30/30 [==============================] - 3s 105ms/step - loss: 0.6047 - mae: 1.0414\n",
            "\n",
            "Epoch 00038: loss improved from 0.36319 to 0.36224, saving model to poids.hdf5\n",
            "Epoch 39/100\n",
            "30/30 [==============================] - 3s 102ms/step - loss: 0.6026 - mae: 1.0389\n",
            "\n",
            "Epoch 00039: loss improved from 0.36224 to 0.36108, saving model to poids.hdf5\n",
            "Epoch 40/100\n",
            "30/30 [==============================] - 3s 105ms/step - loss: 0.5998 - mae: 1.0356\n",
            "\n",
            "Epoch 00040: loss improved from 0.36108 to 0.35961, saving model to poids.hdf5\n",
            "Epoch 41/100\n",
            "30/30 [==============================] - 3s 105ms/step - loss: 0.5964 - mae: 1.0316\n",
            "\n",
            "Epoch 00041: loss improved from 0.35961 to 0.35780, saving model to poids.hdf5\n",
            "Epoch 42/100\n",
            "30/30 [==============================] - 3s 106ms/step - loss: 0.5922 - mae: 1.0266\n",
            "\n",
            "Epoch 00042: loss improved from 0.35780 to 0.35556, saving model to poids.hdf5\n",
            "Epoch 43/100\n",
            "30/30 [==============================] - 3s 105ms/step - loss: 0.5870 - mae: 1.0204\n",
            "\n",
            "Epoch 00043: loss improved from 0.35556 to 0.35281, saving model to poids.hdf5\n",
            "Epoch 44/100\n",
            "30/30 [==============================] - 3s 108ms/step - loss: 0.5804 - mae: 1.0127\n",
            "\n",
            "Epoch 00044: loss improved from 0.35281 to 0.34942, saving model to poids.hdf5\n",
            "Epoch 45/100\n",
            "30/30 [==============================] - 3s 107ms/step - loss: 0.5725 - mae: 1.0033\n",
            "\n",
            "Epoch 00045: loss improved from 0.34942 to 0.34534, saving model to poids.hdf5\n",
            "Epoch 46/100\n",
            "30/30 [==============================] - 3s 113ms/step - loss: 0.5627 - mae: 0.9917\n",
            "\n",
            "Epoch 00046: loss improved from 0.34534 to 0.34043, saving model to poids.hdf5\n",
            "Epoch 47/100\n",
            "30/30 [==============================] - 3s 113ms/step - loss: 0.5508 - mae: 0.9776\n",
            "\n",
            "Epoch 00047: loss improved from 0.34043 to 0.33460, saving model to poids.hdf5\n",
            "Epoch 48/100\n",
            "30/30 [==============================] - 3s 107ms/step - loss: 0.5366 - mae: 0.9608\n",
            "\n",
            "Epoch 00048: loss improved from 0.33460 to 0.32780, saving model to poids.hdf5\n",
            "Epoch 49/100\n",
            "30/30 [==============================] - 3s 107ms/step - loss: 0.5197 - mae: 0.9410\n",
            "\n",
            "Epoch 00049: loss improved from 0.32780 to 0.32006, saving model to poids.hdf5\n",
            "Epoch 50/100\n",
            "30/30 [==============================] - 3s 106ms/step - loss: 0.5000 - mae: 0.9180\n",
            "\n",
            "Epoch 00050: loss improved from 0.32006 to 0.31143, saving model to poids.hdf5\n",
            "Epoch 51/100\n",
            "30/30 [==============================] - 3s 104ms/step - loss: 0.4777 - mae: 0.8921\n",
            "\n",
            "Epoch 00051: loss improved from 0.31143 to 0.30223, saving model to poids.hdf5\n",
            "Epoch 52/100\n",
            "30/30 [==============================] - 3s 105ms/step - loss: 0.4530 - mae: 0.8641\n",
            "\n",
            "Epoch 00052: loss improved from 0.30223 to 0.29280, saving model to poids.hdf5\n",
            "Epoch 53/100\n",
            "30/30 [==============================] - 3s 105ms/step - loss: 0.4266 - mae: 0.8349\n",
            "\n",
            "Epoch 00053: loss improved from 0.29280 to 0.28366, saving model to poids.hdf5\n",
            "Epoch 54/100\n",
            "30/30 [==============================] - 3s 106ms/step - loss: 0.3997 - mae: 0.8056\n",
            "\n",
            "Epoch 00054: loss improved from 0.28366 to 0.27540, saving model to poids.hdf5\n",
            "Epoch 55/100\n",
            "30/30 [==============================] - 3s 106ms/step - loss: 0.3737 - mae: 0.7776\n",
            "\n",
            "Epoch 00055: loss improved from 0.27540 to 0.26842, saving model to poids.hdf5\n",
            "Epoch 56/100\n",
            "30/30 [==============================] - 3s 109ms/step - loss: 0.3503 - mae: 0.7520\n",
            "\n",
            "Epoch 00056: loss improved from 0.26842 to 0.26296, saving model to poids.hdf5\n",
            "Epoch 57/100\n",
            "30/30 [==============================] - 3s 102ms/step - loss: 0.3308 - mae: 0.7303\n",
            "\n",
            "Epoch 00057: loss improved from 0.26296 to 0.25874, saving model to poids.hdf5\n",
            "Epoch 58/100\n",
            "30/30 [==============================] - 3s 107ms/step - loss: 0.3165 - mae: 0.7137\n",
            "\n",
            "Epoch 00058: loss improved from 0.25874 to 0.25530, saving model to poids.hdf5\n",
            "Epoch 59/100\n",
            "30/30 [==============================] - 3s 107ms/step - loss: 0.3075 - mae: 0.7030\n",
            "\n",
            "Epoch 00059: loss improved from 0.25530 to 0.25187, saving model to poids.hdf5\n",
            "Epoch 60/100\n",
            "30/30 [==============================] - 3s 108ms/step - loss: 0.3037 - mae: 0.6979\n",
            "\n",
            "Epoch 00060: loss improved from 0.25187 to 0.24777, saving model to poids.hdf5\n",
            "Epoch 61/100\n",
            "30/30 [==============================] - 3s 107ms/step - loss: 0.3033 - mae: 0.6965\n",
            "\n",
            "Epoch 00061: loss improved from 0.24777 to 0.24197, saving model to poids.hdf5\n",
            "Epoch 62/100\n",
            "30/30 [==============================] - 3s 104ms/step - loss: 0.3037 - mae: 0.6944\n",
            "\n",
            "Epoch 00062: loss improved from 0.24197 to 0.23256, saving model to poids.hdf5\n",
            "Epoch 63/100\n",
            "30/30 [==============================] - 3s 107ms/step - loss: 0.2982 - mae: 0.6811\n",
            "\n",
            "Epoch 00063: loss improved from 0.23256 to 0.21480, saving model to poids.hdf5\n",
            "Epoch 64/100\n",
            "30/30 [==============================] - 3s 107ms/step - loss: 0.2754 - mae: 0.6365\n",
            "\n",
            "Epoch 00064: loss improved from 0.21480 to 0.18107, saving model to poids.hdf5\n",
            "Epoch 65/100\n",
            "30/30 [==============================] - 3s 105ms/step - loss: 0.2148 - mae: 0.5233\n",
            "\n",
            "Epoch 00065: loss improved from 0.18107 to 0.12512, saving model to poids.hdf5\n",
            "Epoch 66/100\n",
            "30/30 [==============================] - 3s 111ms/step - loss: 0.1254 - mae: 0.3878\n",
            "\n",
            "Epoch 00066: loss improved from 0.12512 to 0.07022, saving model to poids.hdf5\n",
            "Epoch 67/100\n",
            "30/30 [==============================] - 3s 108ms/step - loss: 0.0660 - mae: 0.2910\n",
            "\n",
            "Epoch 00067: loss improved from 0.07022 to 0.04421, saving model to poids.hdf5\n",
            "Epoch 68/100\n",
            "30/30 [==============================] - 3s 108ms/step - loss: 0.0433 - mae: 0.2293\n",
            "\n",
            "Epoch 00068: loss improved from 0.04421 to 0.03566, saving model to poids.hdf5\n",
            "Epoch 69/100\n",
            "30/30 [==============================] - 3s 110ms/step - loss: 0.0399 - mae: 0.2141\n",
            "\n",
            "Epoch 00069: loss improved from 0.03566 to 0.03486, saving model to poids.hdf5\n",
            "Epoch 70/100\n",
            "30/30 [==============================] - 3s 106ms/step - loss: 0.0419 - mae: 0.2180\n",
            "\n",
            "Epoch 00070: loss did not improve from 0.03486\n",
            "Epoch 71/100\n",
            "30/30 [==============================] - 3s 107ms/step - loss: 0.0441 - mae: 0.2281\n",
            "\n",
            "Epoch 00071: loss did not improve from 0.03486\n",
            "Epoch 72/100\n",
            "30/30 [==============================] - 3s 104ms/step - loss: 0.0513 - mae: 0.2430\n",
            "\n",
            "Epoch 00072: loss did not improve from 0.03486\n",
            "Epoch 73/100\n",
            "30/30 [==============================] - 3s 107ms/step - loss: 0.0584 - mae: 0.2584\n",
            "\n",
            "Epoch 00073: loss did not improve from 0.03486\n",
            "Epoch 74/100\n",
            "30/30 [==============================] - 3s 108ms/step - loss: 0.0642 - mae: 0.2668\n",
            "\n",
            "Epoch 00074: loss did not improve from 0.03486\n",
            "Epoch 75/100\n",
            "30/30 [==============================] - 3s 109ms/step - loss: 0.0677 - mae: 0.2744\n",
            "\n",
            "Epoch 00075: loss did not improve from 0.03486\n",
            "Epoch 76/100\n",
            "30/30 [==============================] - 3s 104ms/step - loss: 0.0748 - mae: 0.2766\n",
            "\n",
            "Epoch 00076: loss did not improve from 0.03486\n",
            "Epoch 77/100\n",
            "30/30 [==============================] - 3s 107ms/step - loss: 0.0814 - mae: 0.2933\n",
            "\n",
            "Epoch 00077: loss did not improve from 0.03486\n",
            "Epoch 78/100\n",
            "30/30 [==============================] - 3s 106ms/step - loss: 0.0904 - mae: 0.3144\n",
            "\n",
            "Epoch 00078: loss did not improve from 0.03486\n",
            "Epoch 79/100\n",
            "30/30 [==============================] - 3s 104ms/step - loss: 0.0962 - mae: 0.3322\n",
            "\n",
            "Epoch 00079: loss did not improve from 0.03486\n",
            "Epoch 80/100\n",
            "30/30 [==============================] - 3s 108ms/step - loss: 0.1321 - mae: 0.3956\n",
            "\n",
            "Epoch 00080: loss did not improve from 0.03486\n",
            "Epoch 81/100\n",
            "30/30 [==============================] - 3s 107ms/step - loss: 0.1262 - mae: 0.3517\n",
            "\n",
            "Epoch 00081: loss did not improve from 0.03486\n",
            "Epoch 82/100\n",
            "30/30 [==============================] - 3s 105ms/step - loss: 0.1678 - mae: 0.4305\n",
            "\n",
            "Epoch 00082: loss did not improve from 0.03486\n",
            "Epoch 83/100\n",
            "30/30 [==============================] - 3s 107ms/step - loss: 0.1851 - mae: 0.4427\n",
            "\n",
            "Epoch 00083: loss did not improve from 0.03486\n",
            "Epoch 84/100\n",
            "30/30 [==============================] - 3s 106ms/step - loss: 0.2296 - mae: 0.5363\n",
            "\n",
            "Epoch 00084: loss did not improve from 0.03486\n",
            "Epoch 85/100\n",
            "30/30 [==============================] - 3s 106ms/step - loss: nan - mae: nan\n",
            "\n",
            "Epoch 00085: loss did not improve from 0.03486\n",
            "Epoch 86/100\n",
            "30/30 [==============================] - 3s 108ms/step - loss: nan - mae: nan\n",
            "\n",
            "Epoch 00086: loss did not improve from 0.03486\n",
            "Epoch 87/100\n",
            "30/30 [==============================] - 3s 108ms/step - loss: nan - mae: nan\n",
            "\n",
            "Epoch 00087: loss did not improve from 0.03486\n",
            "Epoch 88/100\n",
            "30/30 [==============================] - 3s 104ms/step - loss: nan - mae: nan\n",
            "\n",
            "Epoch 00088: loss did not improve from 0.03486\n",
            "Epoch 89/100\n",
            "30/30 [==============================] - 3s 111ms/step - loss: nan - mae: nan\n",
            "\n",
            "Epoch 00089: loss did not improve from 0.03486\n",
            "Epoch 90/100\n",
            "30/30 [==============================] - 4s 119ms/step - loss: nan - mae: nan\n",
            "\n",
            "Epoch 00090: loss did not improve from 0.03486\n",
            "Epoch 91/100\n",
            "30/30 [==============================] - 4s 120ms/step - loss: nan - mae: nan\n",
            "\n",
            "Epoch 00091: loss did not improve from 0.03486\n",
            "Epoch 92/100\n",
            "30/30 [==============================] - 4s 136ms/step - loss: nan - mae: nan\n",
            "\n",
            "Epoch 00092: loss did not improve from 0.03486\n",
            "Epoch 93/100\n",
            "30/30 [==============================] - 3s 110ms/step - loss: nan - mae: nan\n",
            "\n",
            "Epoch 00093: loss did not improve from 0.03486\n",
            "Epoch 94/100\n",
            "30/30 [==============================] - 3s 106ms/step - loss: nan - mae: nan\n",
            "\n",
            "Epoch 00094: loss did not improve from 0.03486\n",
            "Epoch 95/100\n",
            "30/30 [==============================] - 3s 105ms/step - loss: nan - mae: nan\n",
            "\n",
            "Epoch 00095: loss did not improve from 0.03486\n",
            "Epoch 96/100\n",
            "30/30 [==============================] - 3s 106ms/step - loss: nan - mae: nan\n",
            "\n",
            "Epoch 00096: loss did not improve from 0.03486\n",
            "Epoch 97/100\n",
            "30/30 [==============================] - 3s 106ms/step - loss: nan - mae: nan\n",
            "\n",
            "Epoch 00097: loss did not improve from 0.03486\n",
            "Epoch 98/100\n",
            "30/30 [==============================] - 3s 109ms/step - loss: nan - mae: nan\n",
            "\n",
            "Epoch 00098: loss did not improve from 0.03486\n",
            "Epoch 99/100\n",
            "30/30 [==============================] - 3s 105ms/step - loss: nan - mae: nan\n",
            "\n",
            "Epoch 00099: loss did not improve from 0.03486\n",
            "Epoch 100/100\n",
            "30/30 [==============================] - 3s 106ms/step - loss: nan - mae: nan\n",
            "\n",
            "Epoch 00100: loss did not improve from 0.03486\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1_WMNlzu2B4",
        "outputId": "0e787c15-c8aa-453d-ec6f-97940a03e716",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 1])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, \"Evolution de l'erreur en fonction du taux d'apprentissage\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XdXh_b0GP_F"
      },
      "source": [
        "**3. Entrainement du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80pEtJ10wIfY"
      },
      "source": [
        "# Charge les meilleurs poids\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16ujUiELwR33",
        "outputId": "633f8bc1-0fce-4ad8-b797-783b23b5ff2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "class TimingCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, logs={}):\n",
        "        self.logs=[]\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.starttime = timer()\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.logs.append(timer()-self.starttime)\n",
        "\n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.1,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0.05\n",
        "    )\n",
        "\n",
        "cb = TimingCallback()\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(learning_rate=lr_schedule,momentum=0.9)\n",
        "#optimiseur=tf.keras.optimizers.SGD(lr=0.1,momentum=0.9)\n",
        "\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur,metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle\n",
        "historique = model.fit(dataset_norm,validation_data=dataset_Val_norm, epochs=500,verbose=1, callbacks=[CheckPoint,cb])\n",
        "\n",
        "print(cb.logs)\n",
        "print(sum(cb.logs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "30/30 [==============================] - 7s 131ms/step - loss: 0.0421 - mae: 0.2148 - val_loss: 0.1197 - val_mae: 0.4195\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.04281, saving model to poids_train.hdf5\n",
            "Epoch 2/500\n",
            "30/30 [==============================] - 4s 122ms/step - loss: 0.1262 - mae: 0.4399 - val_loss: 0.1287 - val_mae: 0.4324\n",
            "\n",
            "Epoch 00002: loss did not improve from 0.04281\n",
            "Epoch 3/500\n",
            "30/30 [==============================] - 4s 120ms/step - loss: 0.3287 - mae: 0.7335 - val_loss: 0.0438 - val_mae: 0.2399\n",
            "\n",
            "Epoch 00003: loss did not improve from 0.04281\n",
            "Epoch 4/500\n",
            "30/30 [==============================] - 4s 120ms/step - loss: 0.3696 - mae: 0.7675 - val_loss: 0.2246 - val_mae: 0.5768\n",
            "\n",
            "Epoch 00004: loss did not improve from 0.04281\n",
            "Epoch 5/500\n",
            "30/30 [==============================] - 4s 119ms/step - loss: 0.7625 - mae: 1.1671 - val_loss: 0.2855 - val_mae: 0.6601\n",
            "\n",
            "Epoch 00005: loss did not improve from 0.04281\n",
            "Epoch 6/500\n",
            "30/30 [==============================] - 3s 117ms/step - loss: 0.5789 - mae: 0.9891 - val_loss: 0.2472 - val_mae: 0.6093\n",
            "\n",
            "Epoch 00006: loss did not improve from 0.04281\n",
            "Epoch 7/500\n",
            "30/30 [==============================] - 3s 117ms/step - loss: 0.7886 - mae: 1.2073 - val_loss: 0.7225 - val_mae: 1.2009\n",
            "\n",
            "Epoch 00007: loss did not improve from 0.04281\n",
            "Epoch 8/500\n",
            "30/30 [==============================] - 4s 119ms/step - loss: 0.3733 - mae: 0.7803 - val_loss: 0.2349 - val_mae: 0.5915\n",
            "\n",
            "Epoch 00008: loss did not improve from 0.04281\n",
            "Epoch 9/500\n",
            "30/30 [==============================] - 4s 128ms/step - loss: 0.6922 - mae: 1.0970 - val_loss: 0.4185 - val_mae: 0.8386\n",
            "\n",
            "Epoch 00009: loss did not improve from 0.04281\n",
            "Epoch 10/500\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.5505 - mae: 0.9411 - val_loss: 0.4685 - val_mae: 0.9015\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.04281\n",
            "Epoch 11/500\n",
            "30/30 [==============================] - 4s 122ms/step - loss: 0.4612 - mae: 0.8477 - val_loss: 0.3496 - val_mae: 0.7483\n",
            "\n",
            "Epoch 00011: loss did not improve from 0.04281\n",
            "Epoch 12/500\n",
            "30/30 [==============================] - 4s 121ms/step - loss: 0.6023 - mae: 0.9947 - val_loss: 0.3432 - val_mae: 0.7385\n",
            "\n",
            "Epoch 00012: loss did not improve from 0.04281\n",
            "Epoch 13/500\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.5957 - mae: 0.9920 - val_loss: 0.3105 - val_mae: 0.6962\n",
            "\n",
            "Epoch 00013: loss did not improve from 0.04281\n",
            "Epoch 14/500\n",
            "30/30 [==============================] - 4s 122ms/step - loss: 0.5769 - mae: 0.9699 - val_loss: 0.2501 - val_mae: 0.6129\n",
            "\n",
            "Epoch 00014: loss did not improve from 0.04281\n",
            "Epoch 15/500\n",
            "30/30 [==============================] - 4s 121ms/step - loss: 0.6517 - mae: 1.0528 - val_loss: 0.2798 - val_mae: 0.6529\n",
            "\n",
            "Epoch 00015: loss did not improve from 0.04281\n",
            "Epoch 16/500\n",
            "30/30 [==============================] - 3s 117ms/step - loss: 0.6301 - mae: 1.0365 - val_loss: 0.1864 - val_mae: 0.5255\n",
            "\n",
            "Epoch 00016: loss did not improve from 0.04281\n",
            "Epoch 17/500\n",
            "30/30 [==============================] - 4s 119ms/step - loss: 0.6767 - mae: 1.0874 - val_loss: 0.1604 - val_mae: 0.4925\n",
            "\n",
            "Epoch 00017: loss did not improve from 0.04281\n",
            "Epoch 18/500\n",
            "30/30 [==============================] - 4s 122ms/step - loss: 0.7429 - mae: 1.1613 - val_loss: 0.1542 - val_mae: 0.4840\n",
            "\n",
            "Epoch 00018: loss did not improve from 0.04281\n",
            "Epoch 19/500\n",
            "30/30 [==============================] - 4s 120ms/step - loss: 0.7552 - mae: 1.1729 - val_loss: 0.1529 - val_mae: 0.4817\n",
            "\n",
            "Epoch 00019: loss did not improve from 0.04281\n",
            "Epoch 20/500\n",
            "30/30 [==============================] - 4s 119ms/step - loss: 0.7920 - mae: 1.2211 - val_loss: 0.1952 - val_mae: 0.5376\n",
            "\n",
            "Epoch 00020: loss did not improve from 0.04281\n",
            "Epoch 21/500\n",
            "30/30 [==============================] - 4s 122ms/step - loss: 0.8958 - mae: 1.3503 - val_loss: 0.3341 - val_mae: 0.7270\n",
            "\n",
            "Epoch 00021: loss did not improve from 0.04281\n",
            "Epoch 22/500\n",
            "30/30 [==============================] - 4s 124ms/step - loss: 0.8416 - mae: 1.2958 - val_loss: 0.4750 - val_mae: 0.9107\n",
            "\n",
            "Epoch 00022: loss did not improve from 0.04281\n",
            "Epoch 23/500\n",
            "30/30 [==============================] - 3s 115ms/step - loss: 0.7600 - mae: 1.2111 - val_loss: 0.6474 - val_mae: 1.1171\n",
            "\n",
            "Epoch 00023: loss did not improve from 0.04281\n",
            "Epoch 24/500\n",
            "30/30 [==============================] - 4s 121ms/step - loss: 0.6568 - mae: 1.1059 - val_loss: 0.7556 - val_mae: 1.2391\n",
            "\n",
            "Epoch 00024: loss did not improve from 0.04281\n",
            "Epoch 25/500\n",
            "30/30 [==============================] - 4s 124ms/step - loss: 0.5838 - mae: 1.0295 - val_loss: 0.8149 - val_mae: 1.3035\n",
            "\n",
            "Epoch 00025: loss did not improve from 0.04281\n",
            "Epoch 26/500\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.5440 - mae: 0.9870 - val_loss: 0.8334 - val_mae: 1.3235\n",
            "\n",
            "Epoch 00026: loss did not improve from 0.04281\n",
            "Epoch 27/500\n",
            "30/30 [==============================] - 4s 121ms/step - loss: 0.5265 - mae: 0.9682 - val_loss: 0.8453 - val_mae: 1.3362\n",
            "\n",
            "Epoch 00027: loss did not improve from 0.04281\n",
            "Epoch 28/500\n",
            "30/30 [==============================] - 4s 119ms/step - loss: 0.5155 - mae: 0.9560 - val_loss: 0.8533 - val_mae: 1.3448\n",
            "\n",
            "Epoch 00028: loss did not improve from 0.04281\n",
            "Epoch 29/500\n",
            "30/30 [==============================] - 4s 119ms/step - loss: 0.5055 - mae: 0.9450 - val_loss: 0.8574 - val_mae: 1.3493\n",
            "\n",
            "Epoch 00029: loss did not improve from 0.04281\n",
            "Epoch 30/500\n",
            "30/30 [==============================] - 4s 120ms/step - loss: 0.4964 - mae: 0.9348 - val_loss: 0.8551 - val_mae: 1.3471\n",
            "\n",
            "Epoch 00030: loss did not improve from 0.04281\n",
            "Epoch 31/500\n",
            "30/30 [==============================] - 4s 122ms/step - loss: 0.4875 - mae: 0.9248 - val_loss: 0.8380 - val_mae: 1.3294\n",
            "\n",
            "Epoch 00031: loss did not improve from 0.04281\n",
            "Epoch 32/500\n",
            "30/30 [==============================] - 4s 121ms/step - loss: 0.4781 - mae: 0.9141 - val_loss: 0.7742 - val_mae: 1.2626\n",
            "\n",
            "Epoch 00032: loss did not improve from 0.04281\n",
            "Epoch 33/500\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.4633 - mae: 0.8965 - val_loss: 0.4551 - val_mae: 0.9075\n",
            "\n",
            "Epoch 00033: loss did not improve from 0.04281\n",
            "Epoch 34/500\n",
            "30/30 [==============================] - 4s 121ms/step - loss: 0.4049 - mae: 0.8111 - val_loss: 0.1008 - val_mae: 0.3879\n",
            "\n",
            "Epoch 00034: loss did not improve from 0.04281\n",
            "Epoch 35/500\n",
            "30/30 [==============================] - 4s 124ms/step - loss: 0.1183 - mae: 0.3610 - val_loss: 0.1290 - val_mae: 0.4340\n",
            "\n",
            "Epoch 00035: loss did not improve from 0.04281\n",
            "Epoch 36/500\n",
            "30/30 [==============================] - 4s 122ms/step - loss: 0.0493 - mae: 0.2252 - val_loss: 0.0617 - val_mae: 0.2876\n",
            "\n",
            "Epoch 00036: loss improved from 0.04281 to 0.04167, saving model to poids_train.hdf5\n",
            "Epoch 37/500\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0491 - mae: 0.2204 - val_loss: 0.0742 - val_mae: 0.3169\n",
            "\n",
            "Epoch 00037: loss improved from 0.04167 to 0.03881, saving model to poids_train.hdf5\n",
            "Epoch 38/500\n",
            "30/30 [==============================] - 4s 121ms/step - loss: 0.0431 - mae: 0.2082 - val_loss: 0.0525 - val_mae: 0.2579\n",
            "\n",
            "Epoch 00038: loss improved from 0.03881 to 0.03556, saving model to poids_train.hdf5\n",
            "Epoch 39/500\n",
            "30/30 [==============================] - 3s 116ms/step - loss: 0.0406 - mae: 0.2052 - val_loss: 0.0432 - val_mae: 0.2293\n",
            "\n",
            "Epoch 00039: loss improved from 0.03556 to 0.03440, saving model to poids_train.hdf5\n",
            "Epoch 40/500\n",
            "30/30 [==============================] - 4s 120ms/step - loss: 0.0380 - mae: 0.2019 - val_loss: 0.0367 - val_mae: 0.2078\n",
            "\n",
            "Epoch 00040: loss improved from 0.03440 to 0.03324, saving model to poids_train.hdf5\n",
            "Epoch 41/500\n",
            "30/30 [==============================] - 4s 119ms/step - loss: 0.0361 - mae: 0.1998 - val_loss: 0.0332 - val_mae: 0.1957\n",
            "\n",
            "Epoch 00041: loss improved from 0.03324 to 0.03233, saving model to poids_train.hdf5\n",
            "Epoch 42/500\n",
            "30/30 [==============================] - 4s 120ms/step - loss: 0.0346 - mae: 0.1981 - val_loss: 0.0316 - val_mae: 0.1904\n",
            "\n",
            "Epoch 00042: loss improved from 0.03233 to 0.03156, saving model to poids_train.hdf5\n",
            "Epoch 43/500\n",
            "30/30 [==============================] - 4s 119ms/step - loss: 0.0333 - mae: 0.1964 - val_loss: 0.0311 - val_mae: 0.1895\n",
            "\n",
            "Epoch 00043: loss improved from 0.03156 to 0.03086, saving model to poids_train.hdf5\n",
            "Epoch 44/500\n",
            "30/30 [==============================] - 4s 122ms/step - loss: 0.0321 - mae: 0.1944 - val_loss: 0.0313 - val_mae: 0.1912\n",
            "\n",
            "Epoch 00044: loss improved from 0.03086 to 0.03023, saving model to poids_train.hdf5\n",
            "Epoch 45/500\n",
            "30/30 [==============================] - 4s 119ms/step - loss: 0.0310 - mae: 0.1922 - val_loss: 0.0319 - val_mae: 0.1941\n",
            "\n",
            "Epoch 00045: loss improved from 0.03023 to 0.02969, saving model to poids_train.hdf5\n",
            "Epoch 46/500\n",
            "30/30 [==============================] - 4s 121ms/step - loss: 0.0300 - mae: 0.1898 - val_loss: 0.0327 - val_mae: 0.1976\n",
            "\n",
            "Epoch 00046: loss improved from 0.02969 to 0.02924, saving model to poids_train.hdf5\n",
            "Epoch 47/500\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0291 - mae: 0.1872 - val_loss: 0.0335 - val_mae: 0.2011\n",
            "\n",
            "Epoch 00047: loss improved from 0.02924 to 0.02885, saving model to poids_train.hdf5\n",
            "Epoch 48/500\n",
            "30/30 [==============================] - 4s 120ms/step - loss: 0.0283 - mae: 0.1846 - val_loss: 0.0344 - val_mae: 0.2046\n",
            "\n",
            "Epoch 00048: loss improved from 0.02885 to 0.02849, saving model to poids_train.hdf5\n",
            "Epoch 49/500\n",
            "30/30 [==============================] - 4s 125ms/step - loss: 0.0276 - mae: 0.1822 - val_loss: 0.0352 - val_mae: 0.2081\n",
            "\n",
            "Epoch 00049: loss improved from 0.02849 to 0.02815, saving model to poids_train.hdf5\n",
            "Epoch 50/500\n",
            "30/30 [==============================] - 4s 120ms/step - loss: 0.0271 - mae: 0.1803 - val_loss: 0.0361 - val_mae: 0.2117\n",
            "\n",
            "Epoch 00050: loss improved from 0.02815 to 0.02785, saving model to poids_train.hdf5\n",
            "Epoch 51/500\n",
            "30/30 [==============================] - 4s 121ms/step - loss: 0.0266 - mae: 0.1789 - val_loss: 0.0370 - val_mae: 0.2153\n",
            "\n",
            "Epoch 00051: loss improved from 0.02785 to 0.02758, saving model to poids_train.hdf5\n",
            "Epoch 52/500\n",
            "23/30 [======================>.......] - ETA: 0s - loss: 0.0258 - mae: 0.1792"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COP9u4yitvmw"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o2zh6N9t1b7"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement[400:500])),erreur_entrainement[400:500], label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement[400:500])),erreur_validation[400:500], label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6Gq2CkeGR_1"
      },
      "source": [
        "**4. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRxXtHRXGXfF"
      },
      "source": [
        "taille_fenetre = 20\n",
        "\n",
        "# Création d'une liste vide pour recevoir les prédictions\n",
        "predictions = []\n",
        "\n",
        "# Calcul des prédiction pour chaque groupe de 20 valeurs consécutives de la série\n",
        "# dans l'intervalle de validation\n",
        "for t in temps[temps_separation:-taille_fenetre]:\n",
        "    X = np.reshape(Serie_Normalisee[t:t+taille_fenetre],(1,taille_fenetre))\n",
        "    predictions.append(model.predict(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd2nfDcgG8EO"
      },
      "source": [
        "# Affiche la série et les prédictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps,serie,label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title('Prédictions avec le modèle GRU + Attention avec vecteur contexte')\n",
        "plt.show()\n",
        "\n",
        "# Zoom sur l'intervalle de validation\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps[temps_separation:],serie[temps_separation:],label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title(\"Prédictions avec le modèle GRU + Attention avec vecteur contexte (zoom sur l'intervalle de validation)\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDS7BJvZG_e1"
      },
      "source": [
        "# Calcule de l'erreur quadratique moyenne et de l'erreur absolue moyenne \n",
        "\n",
        "mae = tf.keras.metrics.mean_absolute_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "mse = tf.keras.metrics.mean_squared_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "\n",
        "print(mae)\n",
        "print(mse)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}