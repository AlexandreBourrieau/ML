{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reseau_LSTMN.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/Reseau_LSTMN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tefj_HcCgH6c"
      },
      "source": [
        "# Long Short-Term Memory-Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubCeIvtF6R4W"
      },
      "source": [
        "Dans ce carnet nous allons mettre en place un modèle à réseau de neurones récurrent de type LSTMN pour réaliser des prédictions sur notre série temporelle.  \n",
        "Ce modèle est tiré du papier de recherche : [Long Short-Term Memory-Networks for Machine Reading](https://arxiv.org/pdf/1601.06733)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRhtHsNn5fc3"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFeah3y_6kif"
      },
      "source": [
        "# Création de la série temporelle et du dataset pour l'entrainement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJfSLtub6sdc"
      },
      "source": [
        "# Fonction permettant d'afficher une série temporelle\n",
        "def affiche_serie(temps, serie, format=\"-\", debut=0, fin=None, label=None):\n",
        "    plt.plot(temps[debut:fin], serie[debut:fin], format, label=label)\n",
        "    plt.xlabel(\"Temps\")\n",
        "    plt.ylabel(\"Valeur\")\n",
        "    if label:\n",
        "        plt.legend(fontsize=14)\n",
        "    plt.grid(True)\n",
        "\n",
        "# Fonction permettant de créer une tendance\n",
        "def tendance(temps, pente=0):\n",
        "    return pente * temps\n",
        "\n",
        "# Fonction permettant de créer un motif\n",
        "def motif_periodique(instants):\n",
        "    return (np.where(instants < 0.4,                            # Si les instants sont < 0.4\n",
        "                    np.cos(instants * 2 * np.pi),               # Alors on retourne la fonction cos(2*pi*t)\n",
        "                    1 / np.exp(3 * instants)))                  # Sinon, on retourne la fonction exp(-3t)\n",
        "\n",
        "# Fonction permettant de créer une saisonnalité avec un motif\n",
        "def saisonnalite(temps, periode, amplitude=1, phase=0):\n",
        "    \"\"\"Répétition du motif sur la même période\"\"\"\n",
        "    instants = ((temps + phase) % periode) / periode            # Mapping du temps =[0 1 2 ... 1460] => instants = [0.0 ... 1.0]\n",
        "    return amplitude * motif_periodique(instants)\n",
        "\n",
        "# Fonction permettant de générer du bruit gaussien N(0,1)\n",
        "def bruit_blanc(temps, niveau_bruit=1, graine=None):\n",
        "    rnd = np.random.RandomState(graine)\n",
        "    return rnd.randn(len(temps)) * niveau_bruit\n",
        "\n",
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "# au format X(X1,X2,...Xn) / Y(Y1,Y2,...,Yn)\n",
        "# X sont les données d'entrées du réseau\n",
        "# Y sont les labels\n",
        "\n",
        "def prepare_dataset_XY(serie, taille_fenetre, batch_size, buffer_melange):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie)\n",
        "  dataset = dataset.window(taille_fenetre+1, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre + 1))\n",
        "  dataset = dataset.map(lambda x: (x[:-1], x[-1:]))\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "# Création de la série temporelle\n",
        "temps = np.arange(4 * 365)                # temps = [0 1 2 .... 4*365] = [0 1 2 .... 1460]\n",
        "amplitude = 40                            # Amplitude de la la saisonnalité\n",
        "niveau_bruit = 5                          # Niveau du bruit\n",
        "offset = 10                               # Offset de la série\n",
        "\n",
        "serie = offset + tendance(temps, 0.1) + saisonnalite(temps, periode=365, amplitude=amplitude) + bruit_blanc(temps,niveau_bruit,graine=40)\n",
        "\n",
        "temps_separation = 1000\n",
        "\n",
        "# Extraction des temps et des données d'entrainement\n",
        "temps_entrainement = temps[:temps_separation]\n",
        "x_entrainement = serie[:temps_separation]\n",
        "\n",
        "# Exctraction des temps et des données de valiadation\n",
        "temps_validation = temps[temps_separation:]\n",
        "x_validation = serie[temps_separation:]\n",
        "\n",
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "taille_fenetre = 20\n",
        "batch_size = 32\n",
        "buffer_melange = 1000\n",
        "\n",
        "# Création du dataset X,Y\n",
        "dataset = prepare_dataset_XY(x_entrainement,taille_fenetre,batch_size,buffer_melange)\n",
        "\n",
        "# Création du dataset X,Y de validation\n",
        "dataset_Val = prepare_dataset_XY(x_validation,taille_fenetre,batch_size,buffer_melange)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6WVzU_X3JxG"
      },
      "source": [
        "# Calcul de la moyenne et de l'écart type de la série\n",
        "mean = tf.math.reduce_mean(serie)\n",
        "std = tf.math.reduce_std(serie)\n",
        "\n",
        "# Normalise les données\n",
        "Serie_Normalisee = (serie-mean)/std\n",
        "min = tf.math.reduce_min(serie)\n",
        "max = tf.math.reduce_max(serie)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYtsc0Yk3LhT"
      },
      "source": [
        "# Création des données pour l'entrainement et le test\n",
        "x_entrainement_norm = Serie_Normalisee[:temps_separation]\n",
        "x_validation_norm = Serie_Normalisee[temps_separation:]\n",
        "\n",
        "# Création du dataset X,Y\n",
        "dataset_norm = prepare_dataset_XY(x_entrainement_norm,taille_fenetre,batch_size,buffer_melange)\n",
        "\n",
        "# Création du dataset X,Y de validation\n",
        "dataset_Val_norm = prepare_dataset_XY(x_validation_norm,taille_fenetre,batch_size,buffer_melange)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Yt-EgZ3sgPY"
      },
      "source": [
        "# Création d'une couche LSTM personnelle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-C3-DsSAjtf"
      },
      "source": [
        "Pour commencer, regradons tout d'abord comment créer une couche LSTM en dérivant une classe Layers de Keras.  \n",
        "On utilise ici les ressources de wikipédia : [Algorithme d'une couche LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZoCohMaBJU4"
      },
      "source": [
        "<img src=\"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/images/LSTM.png?raw=true\" width=\"800\"> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxEXQ14sAAHw"
      },
      "source": [
        "**1. Création de la classe LSTM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_GJYIDRAlhu"
      },
      "source": [
        "Ce qu'il y a de nouveau lors de la création de la couche personnelle est qu'il faut maintenant avoir deux variables internes à la couches à mémoriser (les deux vecteurs internes - cell vector & hidden vector):\n",
        " - Ces vecteurs doivent être initialisés dans la fonction `build()`\n",
        " - Puis ils doivent être sauvegardés pour le batch courant à la fin de la fonction `call()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hhk0kmPSgqva"
      },
      "source": [
        "# Classe LSTM simple\n",
        "\n",
        "# Importe le Backend de Keras\n",
        "from keras import backend as K\n",
        "\n",
        "# Définit une nouvelle classe personnelle LSTM\n",
        "# https://en.wikipedia.org/wiki/Long_short-term_memory\n",
        "\n",
        "# Dans les dimensions ci-dessous :\n",
        "# dimension de la série : dim_serie = 1\n",
        "# dimension des vecteurs internes LSTM : dim_LSTM = 40\n",
        "\n",
        "class Couche_LSTM(tf.keras.layers.Layer):\n",
        "  # Fonction d'initialisation de la classe d'attention\n",
        "  def __init__(self,dim_LSTM):\n",
        "    self.dim_LSTM = dim_LSTM        # Dimension du vecteur d'attention\n",
        "    super().__init__()              # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    dim_serie = input_shape[2]\n",
        "\n",
        "    # Paramètres de la forget gate :\n",
        "    # ##############################\n",
        "    # Matrices de poids Wf(40,1), Uf(40,40) et offset bf(40)\n",
        "    self.Wf = self.add_weight(shape=(self.dim_LSTM,dim_serie),initializer=\"normal\",name=\"Wf\",trainable=True)\n",
        "    self.Uf = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Uf\",trainable=True)\n",
        "    self.bf = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"normal\",name=\"bf\",trainable=True)\n",
        "\n",
        "    # Paramètres de l'input gate :\n",
        "    # ##############################\n",
        "    # Matrices de poids Wi(40,1), Ui(40,40) et offset bi(40)\n",
        "    self.Wi = self.add_weight(shape=(self.dim_LSTM,dim_serie),initializer=\"normal\",name=\"Wi\",trainable=True)\n",
        "    self.Ui = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Ui\",trainable=True)\n",
        "    self.bi = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"zeros\",name=\"bi\",trainable=True)\n",
        "\n",
        "    # Paramètres de l'output gate :\n",
        "    # ##############################\n",
        "    # Matrices de poids Wo(40,1), Uo(40,40) et offset bo(40)\n",
        "    self.Wo = self.add_weight(shape=(self.dim_LSTM,dim_serie),initializer=\"normal\",name=\"Wo\",trainable=True)\n",
        "    self.Uo = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Uo\",trainable=True)\n",
        "    self.bo = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"normal\",name=\"bo\",trainable=True)\n",
        "\n",
        "    # Paramètres du cell vector :\n",
        "    # ##############################\n",
        "    # Matrices de poids Wo(40,1), Uo(40,40) et offset bo(40)\n",
        "    self.Wc = self.add_weight(shape=(self.dim_LSTM,dim_serie),initializer=\"normal\",name=\"Wc\",trainable=True)\n",
        "    self.Uc = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Uc\",trainable=True)\n",
        "    self.bc = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"normal\",name=\"bc\",trainable=True)\n",
        "\n",
        "    # Vecteurs Cell states :\n",
        "    # ######################\n",
        "    cell_states = getattr(self, 'cell_states', None)\n",
        "    if cell_states is None:\n",
        "      cell_states = []\n",
        "      cell_states.append(tf.zeros(shape=(self.dim_LSTM,1)))\n",
        "    self.cell_states = cell_states\n",
        "\n",
        "    # Vecteurs Hidden states :\n",
        "    # ########################\n",
        "    hidden_states = getattr(self, 'hidden_states', None)\n",
        "    if hidden_states is None:\n",
        "      hidden_states = []\n",
        "      hidden_states.append(tf.zeros(shape=(self.dim_LSTM,1)))\n",
        "    self.hidden_states = hidden_states\n",
        "\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Définit la logique de la couche LSTM\n",
        "  # Arguments :   x : Tenseur d'entrée de dimension (None, taille_fenetre,dim_serie)\n",
        "  def call(self,x):\n",
        "    # Charge les vecteurs internes\n",
        "    cell_states = self.cell_states.copy()\n",
        "    hidden_states = self.hidden_states.copy()\n",
        "    ct = cell_states[0]\n",
        "    ht = hidden_states[0]\n",
        "\n",
        "    for t in range(0,x.shape[1]):\n",
        "      xt = x[:,t,:]                             # (32,dim_serie)\n",
        "      xt = tf.expand_dims(xt,axis=-1)           # (32,dim_serie,1)\n",
        "\n",
        "      # Calcul du vecteur d'activation de la forget gate à l'instant t\n",
        "      ft = tf.matmul(self.Wf,xt)                # (40,1)x(32,1,1) = (32,40,1)\n",
        "      ft = ft + tf.matmul(self.Uf,ht)      # (32,40,1)\n",
        "      ft = ft + self.bf                         # (32,40,1)\n",
        "      ft = tf.keras.activations.sigmoid(ft)\n",
        "\n",
        "      # Calcul du vecteur d'activation de l'input gate à l'instant t\n",
        "      it = tf.matmul(self.Wi,xt)                # (32,40,1)\n",
        "      it = it + tf.matmul(self.Ui,ht)      # (32,40,1)\n",
        "      it = it + self.bi                         # (32,40,1)\n",
        "      it = tf.keras.activations.sigmoid(it)\n",
        "\n",
        "      # Calcul du vecteur d'activation de l'output gate à l'instant t\n",
        "      ot = tf.matmul(self.Wo,xt)                # (32,40,1)\n",
        "      ot = ot + tf.matmul(self.Uo,ht)      # (32,40,1)\n",
        "      ot = ot + self.bo                         # (32,40,1)\n",
        "      ot = tf.keras.activations.sigmoid(ot)\n",
        "\n",
        "      # Calcul du cell input activation vector à l'instant t\n",
        "      ct_t = tf.matmul(self.Wc,xt)                 # (32,40,1)\n",
        "      ct_t = ct_t + tf.matmul(self.Uc,ht)          # (32,40,1)\n",
        "      ct_t = ct_t + self.bc                        # (32,40,1)\n",
        "      ct_t = tf.keras.activations.tanh(ct_t)\n",
        "\n",
        "      # Calcul du cell state à l'instant t\n",
        "      ct = tf.multiply(ft,ct)                     # (32,40,1)\n",
        "      ct = ct + tf.multiply(it,ct_t)              # (32,40,1)\n",
        "      ht = tf.multiply(ot,tf.keras.activations.tanh(ct))\n",
        "\n",
        "      # Enregistre les vecteurs internes au temps courant\n",
        "      if (t==0):\n",
        "        hidden_states[t] = ht\n",
        "        cell_states[t] = ct\n",
        "      else:\n",
        "        hidden_states.append(ht)\n",
        "        cell_states.append(ct)\n",
        "\n",
        "    # Enregistre les vecteurs internes du batch courant\n",
        "    for hidden_state_, hidden_state in zip(tf.nest.flatten(self.hidden_states),tf.nest.flatten(hidden_states)):\n",
        "      hidden_states_ = hidden_state\n",
        "\n",
        "    for cell_state_, cell_state in zip(tf.nest.flatten(self.cell_states),tf.nest.flatten(cell_states)):\n",
        "      cell_state_ = cell_state\n",
        "\n",
        "    # Retourne le dernier hidden state\n",
        "    return tf.squeeze(ht,axis=-1)               # return (32,40)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4Rok0lzsron"
      },
      "source": [
        "dim_LSTM = 40\n",
        "\n",
        "# Fonction de la couche lambda d'entrée\n",
        "def Traitement_Entrees(x):\n",
        "  return tf.expand_dims(x,axis=-1)\n",
        "\n",
        "# Encodeur\n",
        "\n",
        "input = tf.keras.Input(shape=(taille_fenetre,),batch_size=batch_size)\n",
        "s = tf.keras.layers.Lambda(Traitement_Entrees)(input)\n",
        "encoder_state = Couche_LSTM(dim_LSTM=dim_LSTM)(s)\n",
        "\n",
        "# Décodeur\n",
        "input_dec = tf.keras.layers.Lambda(Traitement_Entrees)(encoder_state)\n",
        "outputs = Couche_LSTM(dim_LSTM=dim_LSTM)(input_dec)\n",
        "out = tf.keras.layers.Dense(1)(outputs)\n",
        "\n",
        "model = tf.keras.Model(input,out)\n",
        "model.summary()\n",
        "\n",
        "model.save_weights('model_initial.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTqdYAsF_ici"
      },
      "source": [
        "dim_LSTM = 40\n",
        "\n",
        "# Fonction de la couche lambda d'entrée\n",
        "def Traitement_Entrees(x):\n",
        "  return tf.expand_dims(x,axis=-1)\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.Input(shape=(taille_fenetre,)))\n",
        "model.add(tf.keras.layers.Lambda(Traitement_Entrees))\n",
        "model.add(Couche_LSTM(dim_LSTM=dim_LSTM))\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "model.save_weights('model_initial.h5')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUM0-SSXGLIQ"
      },
      "source": [
        "**2. Optimisation du taux d'apprentissage**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jejCBhXVuNQ4"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur, metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset_norm,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1_WMNlzu2B4"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 1])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XdXh_b0GP_F"
      },
      "source": [
        "**3. Entrainement du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80pEtJ10wIfY"
      },
      "source": [
        "# Charge les meilleurs poids\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16ujUiELwR33"
      },
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "class TimingCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, logs={}):\n",
        "        self.logs=[]\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.starttime = timer()\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.logs.append(timer()-self.starttime)\n",
        "\n",
        "\n",
        "cb = TimingCallback()\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(lr=1e-2,momentum=0.9)\n",
        "\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur,metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle\n",
        "historique = model.fit(dataset_norm,validation_data=dataset_Val_norm, epochs=500,verbose=1, callbacks=[CheckPoint,cb])\n",
        "\n",
        "print(cb.logs)\n",
        "print(sum(cb.logs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COP9u4yitvmw"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o2zh6N9t1b7"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement[400:500])),erreur_entrainement[400:500], label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement[400:500])),erreur_validation[400:500], label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6Gq2CkeGR_1"
      },
      "source": [
        "**4. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRxXtHRXGXfF"
      },
      "source": [
        "taille_fenetre = 20\n",
        "\n",
        "# Création d'une liste vide pour recevoir les prédictions\n",
        "predictions = []\n",
        "\n",
        "# Calcul des prédiction pour chaque groupe de 20 valeurs consécutives de la série\n",
        "# dans l'intervalle de validation\n",
        "for t in temps[temps_separation:-taille_fenetre]:\n",
        "    X = np.reshape(Serie_Normalisee[t:t+taille_fenetre],(1,taille_fenetre))\n",
        "    predictions.append(model.predict(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd2nfDcgG8EO"
      },
      "source": [
        "# Affiche la série et les prédictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps,serie,label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title('Prédictions avec le modèle GRU + Attention avec vecteur contexte')\n",
        "plt.show()\n",
        "\n",
        "# Zoom sur l'intervalle de validation\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps[temps_separation:],serie[temps_separation:],label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title(\"Prédictions avec le modèle GRU + Attention avec vecteur contexte (zoom sur l'intervalle de validation)\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDS7BJvZG_e1"
      },
      "source": [
        "# Calcule de l'erreur quadratique moyenne et de l'erreur absolue moyenne \n",
        "\n",
        "mae = tf.keras.metrics.mean_absolute_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "mse = tf.keras.metrics.mean_squared_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "\n",
        "print(mae)\n",
        "print(mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2cMi-70AJ5-"
      },
      "source": [
        "# Création du modèle LSTMN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gXNei98G3J9"
      },
      "source": [
        "<img src=\"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/images/LSTMN_Calcul2.png?raw=true\" width=\"1200\"> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNuSRvEoqEhq"
      },
      "source": [
        "**1. Création de la couche LSTMN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-LL0N5_qCQe"
      },
      "source": [
        "# Classe LSTMN\n",
        "\n",
        "# Importe le Backend de Keras\n",
        "from keras import backend as K\n",
        "\n",
        "# Définit une nouvelle classe personnelle LSTMN\n",
        "# https://arxiv.org/pdf/1601.06733.pdf\n",
        "\n",
        "\n",
        "# Dans les dimensions ci-dessous :\n",
        "# dimension de la série : dim_serie = 1\n",
        "# dimension des vecteurs internes LSTM : dim_LSTM = 40\n",
        "# batch_size = 32\n",
        "\n",
        "class Couche_LSTMN(tf.keras.layers.Layer):\n",
        "  # Fonction d'initialisation de la classe d'attention\n",
        "  def __init__(self,dim_LSTM,return_sequence=False):\n",
        "    self.dim_LSTM = dim_LSTM                # Dimension du vecteur d'attention\n",
        "    self.return_sequence = return_sequence  # Retourne l'ensemble des états cachés ?\n",
        "    super().__init__()                      # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    dim_serie = input_shape[2]\n",
        "    nbr_instants = input_shape[1]\n",
        "\n",
        "    # Paramètres de la forget gate :\n",
        "    # ##############################\n",
        "    # Matrices de poids Wf(40,1), Uf(40,40) et offset bf(40)\n",
        "    self.Wf = self.add_weight(shape=(self.dim_LSTM,dim_serie),initializer=\"normal\",name=\"Wf\",trainable=True)\n",
        "    self.Uf = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Uf\",trainable=True)\n",
        "    self.bf = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"zeros\",name=\"bf\",trainable=True)\n",
        "\n",
        "    # Paramètres de l'input gate :\n",
        "    # ##############################\n",
        "    # Matrices de poids Wi(40,1), Ui(40,40) et offset bi(40)\n",
        "    self.Wi = self.add_weight(shape=(self.dim_LSTM,dim_serie),initializer=\"normal\",name=\"Wi\",trainable=True)\n",
        "    self.Ui = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Ui\",trainable=True)\n",
        "    self.bi = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"zeros\",name=\"bi\",trainable=True)\n",
        "\n",
        "    # Paramètres de l'output gate :\n",
        "    # ##############################\n",
        "    # Matrices de poids Wo(40,1), Uo(40,40) et offset bo(40)\n",
        "    self.Wo = self.add_weight(shape=(self.dim_LSTM,dim_serie),initializer=\"normal\",name=\"Wo\",trainable=True)\n",
        "    self.Uo = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Uo\",trainable=True)\n",
        "    self.bo = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"zeros\",name=\"bo\",trainable=True)\n",
        "\n",
        "    # Paramètres du cell vector :\n",
        "    # ##############################\n",
        "    # Matrices de poids Wc(40,1), Uc(40,40) et offset bc(40)\n",
        "    self.Wc = self.add_weight(shape=(self.dim_LSTM,dim_serie),initializer=\"normal\",name=\"Wc\",trainable=True)\n",
        "    self.Uc = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Uc\",trainable=True)\n",
        "    self.bc = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"zeros\",name=\"bc\",trainable=True)\n",
        "\n",
        "    # Paramètres de l'attention :\n",
        "    # ###########################\n",
        "    # Matrices de poids Wh(40,40); Wx(40,1) et Wh_t(40,40)\n",
        "    self.Wh = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Wh\",trainable=True)\n",
        "    self.Wx = self.add_weight(shape=(self.dim_LSTM,dim_serie),initializer=\"normal\",name=\"Wx\",trainable=True)\n",
        "    self.Wh_t = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Wh_t\",trainable=True)\n",
        "\n",
        "    # Vecteur contexte :  v(1,40)\n",
        "    #############################\n",
        "    self.v = self.add_weight(shape=(dim_serie,self.dim_LSTM),initializer=\"normal\",name=\"v\",trainable=True)\n",
        "\n",
        "    # Memory tape : Ct    Liste[(40,1)]\n",
        "    # #################################\n",
        "    memory_tape = getattr(self, 'memory_tape', None)\n",
        "    if memory_tape is None:\n",
        "      memory_tape = []\n",
        "      memory_tape.append(tf.zeros(shape=(self.dim_LSTM,1)))         # Ct[0] = c0 = 0\n",
        "    self.memory_tape = memory_tape\n",
        "\n",
        "    # Hidden tape : Ht    Liste[(40,1)]\n",
        "    # #################################\n",
        "    hidden_tape = getattr(self, 'hidden_tape', None)\n",
        "    if hidden_tape is None:\n",
        "      hidden_tape = []\n",
        "      hidden_tape.append(tf.zeros(shape=(self.dim_LSTM,1)))         # Ht[0] = h0 = 0\n",
        "    self.hidden_tape = hidden_tape\n",
        "\n",
        "    # Vecteurs d'adaptation du Hidden state : Ht_t    Liste[(40,1)]\n",
        "    # #############################################################\n",
        "    hidden_t_tape = getattr(self, 'hidden_t_tape', None)\n",
        "    if hidden_t_tape is None:\n",
        "      hidden_t_tape = []\n",
        "      hidden_t_tape.append(tf.zeros(shape=(self.dim_LSTM,1)))       # Ht_t[0] = h0_t = 0\n",
        "    self.hidden_t_tape = hidden_t_tape\n",
        "\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Définit la logique de la couche LSTM\n",
        "  # Arguments :   x : Tenseur d'entrée de dimension (None, taille_fenetre,dim_serie)\n",
        "  def call(self,x):\n",
        "\n",
        "    # Charge les tables mémoires\n",
        "    Ct = self.memory_tape.copy()\n",
        "    Ht = self.hidden_tape.copy()\n",
        "    Ht_t = self.hidden_t_tape.copy()\n",
        "\n",
        "    for t in range(1,x.shape[1]+1):\n",
        "      xt = x[:,t-1,:]                           # (32,1)\n",
        "      xt = tf.expand_dims(xt,axis=-1)           # (32,1,1)\n",
        "\n",
        "      # Calcul des poids d'attention\n",
        "      at_i = tf.zeros(shape=(x.shape[0],1,1))   # a0_0 = 0 (32,1,1)\n",
        "      for i in range(1,t):\n",
        "        at_ = tf.matmul(self.v,tf.keras.activations.tanh(tf.matmul(self.Wh,Ht[i]) + tf.matmul(self.Wx,xt) + tf.matmul(self.Wh_t,Ht_t[t-1])))\n",
        "        at_i = tf.concat([at_,at_i],axis=1)\n",
        "      if t > 1:\n",
        "        at_i = tf.slice(at_i,[0,0,0],[at_i.shape[0],at_i.shape[1]-1,at_i.shape[2]])\n",
        "\n",
        "      # Calcul des poids d'attention normalisés\n",
        "      st_i = tf.keras.activations.softmax(at_i,axis=1)        # (32,t,1)\n",
        "\n",
        "      # Calcul du vecteur d'adaptation ht_t\n",
        "      ht_t = tf.zeros(shape=(x.shape[0],self.dim_LSTM,1))                                       # h0_t = 0\n",
        "      for i in range(1,t):\n",
        "        ht_t = ht_t + tf.multiply(tf.expand_dims(st_i[:,i-1,:],axis=-1),Ht[i])      # (32,40,1)\n",
        "\n",
        "      # Calcul du vecteur d'adaptation ct_t\n",
        "      ct_t = tf.zeros(shape=(x.shape[0],self.dim_LSTM,1))                           # c0_t = 0\n",
        "      for i in range(1,t):\n",
        "        ct_t = ct_t + tf.multiply(tf.expand_dims(st_i[:,i-1,:],axis=-1),Ct[i])      # (32,40,1)\n",
        "      \n",
        "      # Calcul du vecteur d'activation de la forget gate à l'instant t\n",
        "      ft = tf.matmul(self.Wf,xt)                    # (40,1)x(32,1,1) = (32,40,1)\n",
        "      ft = ft + tf.matmul(self.Uf,ht_t)             # (40,40)x(32,40,1) = (32,40,1)\n",
        "      ft = ft + self.bf                             # (32,40,1) + (40,1) = (32,40,1)\n",
        "      ft = tf.keras.activations.sigmoid(ft) \n",
        "\n",
        "      # Calcul du vecteur d'activation de l'input gate à l'instant t\n",
        "      it = tf.matmul(self.Wi,xt)                    # (40,1)x(32,1,1) = (32,40,1)\n",
        "      it = it + tf.matmul(self.Ui,ht_t)             # (40,40)x(32,40,1) = (32,40,1)\n",
        "      it = it + self.bi                             # (32,40,1) + (40,1) = (32,40,1)\n",
        "      it = tf.keras.activations.sigmoid(it)\n",
        "\n",
        "      # Calcul du vecteur d'activation de l'output gate à l'instant t\n",
        "      ot = tf.matmul(self.Wo,xt)                    # (40,1)x(32,1,1) = (32,40,1)\n",
        "      ot = ot + tf.matmul(self.Uo,ht_t)             # (40,40)x(32,40,1) = (32,40,1)\n",
        "      ot = ot + self.bo                             # (32,40,1) + (40,1) = (32,40,1)\n",
        "      ot = tf.keras.activations.sigmoid(ot)\n",
        "\n",
        "      # Calcul du cell input activation vector à l'instant t\n",
        "      ct_h = tf.matmul(self.Wc,xt)                 # (40,1)x(32,1,1) = (32,40,1)\n",
        "      ct_h = ct_h + tf.matmul(self.Uc,ht_t)        # (40,40)x(32,40,1) = (32,40,1)\n",
        "      ct_h = ct_h + self.bc                        # (32,40,1) + (40,1) = (32,40,1)\n",
        "      ct_h = tf.keras.activations.tanh(ct_h)\n",
        "\n",
        "      # Calcul du cell state à l'instant t\n",
        "      ct = tf.multiply(ft,ct_t)                    # (32,40,1)\n",
        "      ct = ct + tf.multiply(it,ct_h)               # (32,40,1)\n",
        "      ht = tf.multiply(ot,tf.keras.activations.tanh(ct))\n",
        "\n",
        "      # Enregistre les vecteurs internes du temps courant\n",
        "      Ht.append(ht)                                 # Enregistre Ht[t] = ht\n",
        "      Ct.append(ct)                                 # Enregistre Ct[t] = ct\n",
        "      Ht_t.append(ht_t)                             # Enregistre Ht_t[t] = ht_t\n",
        "\n",
        "    # Enregistre les vecteurs internes du batch courant\n",
        "    for memory_tape_, memory_tape in zip(tf.nest.flatten(self.memory_tape),tf.nest.flatten(Ct)):\n",
        "      memory_tape_ = memory_tape\n",
        "    for hidden_tape_, hidden_tape in zip(tf.nest.flatten(self.hidden_tape),tf.nest.flatten(Ht)):\n",
        "      hidden_tape_ = hidden_tape\n",
        "    for hidden_t_tape_, hidden_t_tape in zip(tf.nest.flatten(self.hidden_t_tape),tf.nest.flatten(Ht_t)):\n",
        "      hidden_t_tape_ = hidden_t_tape\n",
        "\n",
        "    # Retourne le dernier hidden state ou l'ensemble des vecteurs\n",
        "    if self.return_sequence == False:\n",
        "      return tf.squeeze(ht,axis=-1)               # return (32,40)\n",
        "    else:\n",
        "      hidden_states = tf.zeros(shape=(x.shape[0],1,self.dim_LSTM,1))   # (32,1,40,1)\n",
        "      for i in range(0,x.shape[1]):\n",
        "        hidden_states = tf.concat([hidden_states,tf.expand_dims(Ht[i+1],axis=1)],axis=1)\n",
        "      hidden_states = tf.slice(hidden_states,[0,0,0,0],[hidden_states.shape[0],hidden_states.shape[1]-1,hidden_states.shape[2],hidden_states.shape[3]])\n",
        "      return hidden_states                        # return (32,20,40,1)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l95xZ4dChGsm"
      },
      "source": [
        "**2. Création du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayLr0z26rQpH",
        "outputId": "2848e873-e653-46ce-b07a-3a2819b3aef6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dim_LSTM = 40\n",
        "\n",
        "# Fonction de la couche lambda d'entrée\n",
        "def Traitement_Entrees(x):\n",
        "  return tf.expand_dims(x,axis=-1)\n",
        "\n",
        "# Encodeur\n",
        "input = tf.keras.Input(shape=(taille_fenetre,),batch_size=batch_size)\n",
        "s = tf.keras.layers.Dropout(0.1)(input)\n",
        "s = tf.keras.layers.Lambda(Traitement_Entrees)(s)\n",
        "encoder_state = Couche_LSTMN(dim_LSTM=dim_LSTM)(s)\n",
        "\n",
        "# Décodeur\n",
        "input_dec = tf.keras.layers.Lambda(Traitement_Entrees)(encoder_state)\n",
        "outputs = Couche_LSTMN(dim_LSTM=dim_LSTM)(input_dec)\n",
        "out = tf.keras.layers.Dense(1)(outputs)\n",
        "\n",
        "model = tf.keras.Model(input,out)\n",
        "model.summary()\n",
        "\n",
        "model.save_weights('model_initial.h5')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(32, 20)]                0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (32, 20)                  0         \n",
            "_________________________________________________________________\n",
            "lambda_2 (Lambda)            (32, 20, 1)               0         \n",
            "_________________________________________________________________\n",
            "couche_lstmn_2 (Couche_LSTMN (32, 40)                  10000     \n",
            "_________________________________________________________________\n",
            "lambda_3 (Lambda)            (32, 40, 1)               0         \n",
            "_________________________________________________________________\n",
            "couche_lstmn_3 (Couche_LSTMN (32, 40)                  10000     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (32, 1)                   41        \n",
            "=================================================================\n",
            "Total params: 20,041\n",
            "Trainable params: 20,041\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjwtBSMhgNpO"
      },
      "source": [
        "dim_LSTM = 40\n",
        "\n",
        "# Fonction de la couche lambda d'entrée\n",
        "def Traitement_Entrees(x):\n",
        "  return tf.expand_dims(x,axis=-1)\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.Input(shape=(taille_fenetre,), batch_size=batch_size))\n",
        "model.add(tf.keras.layers.Lambda(Traitement_Entrees))\n",
        "model.add(Couche_LSTMN(dim_LSTM=dim_LSTM,return_sequence=False))\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "model.save_weights('model_initial.h5')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZGh7uKYhJHF"
      },
      "source": [
        "**3. Entrainement du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REdXsfkohNTw",
        "outputId": "75c9f107-46f4-460a-d19c-4988df8bbbdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur, metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset_norm,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "30/30 [==============================] - 268s 246ms/step - loss: 0.6147 - mae: 1.0529\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.36774, saving model to poids.hdf5\n",
            "Epoch 2/100\n",
            "30/30 [==============================] - 7s 245ms/step - loss: 0.6147 - mae: 1.0529\n",
            "\n",
            "Epoch 00002: loss improved from 0.36774 to 0.36774, saving model to poids.hdf5\n",
            "Epoch 3/100\n",
            "30/30 [==============================] - 7s 243ms/step - loss: 0.6147 - mae: 1.0529\n",
            "\n",
            "Epoch 00003: loss improved from 0.36774 to 0.36774, saving model to poids.hdf5\n",
            "Epoch 4/100\n",
            "30/30 [==============================] - 7s 243ms/step - loss: 0.6147 - mae: 1.0529\n",
            "\n",
            "Epoch 00004: loss improved from 0.36774 to 0.36774, saving model to poids.hdf5\n",
            "Epoch 5/100\n",
            "30/30 [==============================] - 7s 244ms/step - loss: 0.6147 - mae: 1.0529\n",
            "\n",
            "Epoch 00005: loss improved from 0.36774 to 0.36774, saving model to poids.hdf5\n",
            "Epoch 6/100\n",
            "30/30 [==============================] - 7s 242ms/step - loss: 0.6147 - mae: 1.0529\n",
            "\n",
            "Epoch 00006: loss improved from 0.36774 to 0.36774, saving model to poids.hdf5\n",
            "Epoch 7/100\n",
            "30/30 [==============================] - 7s 242ms/step - loss: 0.6147 - mae: 1.0529\n",
            "\n",
            "Epoch 00007: loss improved from 0.36774 to 0.36773, saving model to poids.hdf5\n",
            "Epoch 8/100\n",
            "30/30 [==============================] - 7s 244ms/step - loss: 0.6147 - mae: 1.0529\n",
            "\n",
            "Epoch 00008: loss improved from 0.36773 to 0.36773, saving model to poids.hdf5\n",
            "Epoch 9/100\n",
            "30/30 [==============================] - 7s 244ms/step - loss: 0.6147 - mae: 1.0529\n",
            "\n",
            "Epoch 00009: loss improved from 0.36773 to 0.36773, saving model to poids.hdf5\n",
            "Epoch 10/100\n",
            "30/30 [==============================] - 7s 243ms/step - loss: 0.6147 - mae: 1.0529\n",
            "\n",
            "Epoch 00010: loss improved from 0.36773 to 0.36772, saving model to poids.hdf5\n",
            "Epoch 11/100\n",
            "30/30 [==============================] - 7s 244ms/step - loss: 0.6146 - mae: 1.0529\n",
            "\n",
            "Epoch 00011: loss improved from 0.36772 to 0.36771, saving model to poids.hdf5\n",
            "Epoch 12/100\n",
            "30/30 [==============================] - 7s 244ms/step - loss: 0.6146 - mae: 1.0529\n",
            "\n",
            "Epoch 00012: loss improved from 0.36771 to 0.36771, saving model to poids.hdf5\n",
            "Epoch 13/100\n",
            "30/30 [==============================] - 7s 244ms/step - loss: 0.6146 - mae: 1.0528\n",
            "\n",
            "Epoch 00013: loss improved from 0.36771 to 0.36770, saving model to poids.hdf5\n",
            "Epoch 14/100\n",
            "30/30 [==============================] - 7s 243ms/step - loss: 0.6146 - mae: 1.0528\n",
            "\n",
            "Epoch 00014: loss improved from 0.36770 to 0.36768, saving model to poids.hdf5\n",
            "Epoch 15/100\n",
            "30/30 [==============================] - 7s 243ms/step - loss: 0.6146 - mae: 1.0528\n",
            "\n",
            "Epoch 00015: loss improved from 0.36768 to 0.36767, saving model to poids.hdf5\n",
            "Epoch 16/100\n",
            "30/30 [==============================] - 7s 244ms/step - loss: 0.6145 - mae: 1.0527\n",
            "\n",
            "Epoch 00016: loss improved from 0.36767 to 0.36765, saving model to poids.hdf5\n",
            "Epoch 17/100\n",
            "30/30 [==============================] - 7s 243ms/step - loss: 0.6145 - mae: 1.0527\n",
            "\n",
            "Epoch 00017: loss improved from 0.36765 to 0.36762, saving model to poids.hdf5\n",
            "Epoch 18/100\n",
            "30/30 [==============================] - 7s 242ms/step - loss: 0.6144 - mae: 1.0526\n",
            "\n",
            "Epoch 00018: loss improved from 0.36762 to 0.36759, saving model to poids.hdf5\n",
            "Epoch 19/100\n",
            "30/30 [==============================] - 7s 244ms/step - loss: 0.6143 - mae: 1.0525\n",
            "\n",
            "Epoch 00019: loss improved from 0.36759 to 0.36755, saving model to poids.hdf5\n",
            "Epoch 20/100\n",
            "30/30 [==============================] - 7s 245ms/step - loss: 0.6142 - mae: 1.0524\n",
            "\n",
            "Epoch 00020: loss improved from 0.36755 to 0.36749, saving model to poids.hdf5\n",
            "Epoch 21/100\n",
            "30/30 [==============================] - 7s 245ms/step - loss: 0.6141 - mae: 1.0523\n",
            "\n",
            "Epoch 00021: loss improved from 0.36749 to 0.36743, saving model to poids.hdf5\n",
            "Epoch 22/100\n",
            "30/30 [==============================] - 7s 244ms/step - loss: 0.6140 - mae: 1.0521\n",
            "\n",
            "Epoch 00022: loss improved from 0.36743 to 0.36735, saving model to poids.hdf5\n",
            "Epoch 23/100\n",
            "30/30 [==============================] - 7s 242ms/step - loss: 0.6138 - mae: 1.0519\n",
            "\n",
            "Epoch 00023: loss improved from 0.36735 to 0.36724, saving model to poids.hdf5\n",
            "Epoch 24/100\n",
            "30/30 [==============================] - 7s 243ms/step - loss: 0.6135 - mae: 1.0516\n",
            "\n",
            "Epoch 00024: loss improved from 0.36724 to 0.36711, saving model to poids.hdf5\n",
            "Epoch 25/100\n",
            "30/30 [==============================] - 7s 243ms/step - loss: 0.6132 - mae: 1.0512\n",
            "\n",
            "Epoch 00025: loss improved from 0.36711 to 0.36695, saving model to poids.hdf5\n",
            "Epoch 26/100\n",
            "30/30 [==============================] - 7s 244ms/step - loss: 0.6128 - mae: 1.0508\n",
            "\n",
            "Epoch 00026: loss improved from 0.36695 to 0.36675, saving model to poids.hdf5\n",
            "Epoch 27/100\n",
            "30/30 [==============================] - 7s 245ms/step - loss: 0.6124 - mae: 1.0502\n",
            "\n",
            "Epoch 00027: loss improved from 0.36675 to 0.36649, saving model to poids.hdf5\n",
            "Epoch 28/100\n",
            "30/30 [==============================] - 7s 244ms/step - loss: 0.6117 - mae: 1.0495\n",
            "\n",
            "Epoch 00028: loss improved from 0.36649 to 0.36617, saving model to poids.hdf5\n",
            "Epoch 29/100\n",
            "30/30 [==============================] - 7s 246ms/step - loss: 0.6110 - mae: 1.0486\n",
            "\n",
            "Epoch 00029: loss improved from 0.36617 to 0.36576, saving model to poids.hdf5\n",
            "Epoch 30/100\n",
            "30/30 [==============================] - 7s 246ms/step - loss: 0.6100 - mae: 1.0475\n",
            "\n",
            "Epoch 00030: loss improved from 0.36576 to 0.36525, saving model to poids.hdf5\n",
            "Epoch 31/100\n",
            "30/30 [==============================] - 7s 245ms/step - loss: 0.6088 - mae: 1.0461\n",
            "\n",
            "Epoch 00031: loss improved from 0.36525 to 0.36461, saving model to poids.hdf5\n",
            "Epoch 32/100\n",
            "30/30 [==============================] - 7s 244ms/step - loss: 0.6073 - mae: 1.0443\n",
            "\n",
            "Epoch 00032: loss improved from 0.36461 to 0.36381, saving model to poids.hdf5\n",
            "Epoch 33/100\n",
            "30/30 [==============================] - 7s 246ms/step - loss: 0.6054 - mae: 1.0421\n",
            "\n",
            "Epoch 00033: loss improved from 0.36381 to 0.36282, saving model to poids.hdf5\n",
            "Epoch 34/100\n",
            "30/30 [==============================] - 7s 246ms/step - loss: 0.6030 - mae: 1.0393\n",
            "\n",
            "Epoch 00034: loss improved from 0.36282 to 0.36157, saving model to poids.hdf5\n",
            "Epoch 35/100\n",
            "30/30 [==============================] - 7s 247ms/step - loss: 0.6001 - mae: 1.0358\n",
            "\n",
            "Epoch 00035: loss improved from 0.36157 to 0.36003, saving model to poids.hdf5\n",
            "Epoch 36/100\n",
            "30/30 [==============================] - 7s 245ms/step - loss: 0.5964 - mae: 1.0315\n",
            "\n",
            "Epoch 00036: loss improved from 0.36003 to 0.35811, saving model to poids.hdf5\n",
            "Epoch 37/100\n",
            "30/30 [==============================] - 7s 247ms/step - loss: 0.5918 - mae: 1.0261\n",
            "\n",
            "Epoch 00037: loss improved from 0.35811 to 0.35574, saving model to poids.hdf5\n",
            "Epoch 38/100\n",
            "30/30 [==============================] - 7s 247ms/step - loss: 0.5861 - mae: 1.0193\n",
            "\n",
            "Epoch 00038: loss improved from 0.35574 to 0.35284, saving model to poids.hdf5\n",
            "Epoch 39/100\n",
            "30/30 [==============================] - 7s 246ms/step - loss: 0.5790 - mae: 1.0110\n",
            "\n",
            "Epoch 00039: loss improved from 0.35284 to 0.34930, saving model to poids.hdf5\n",
            "Epoch 40/100\n",
            "30/30 [==============================] - 7s 246ms/step - loss: 0.5703 - mae: 1.0008\n",
            "\n",
            "Epoch 00040: loss improved from 0.34930 to 0.34502, saving model to poids.hdf5\n",
            "Epoch 41/100\n",
            "30/30 [==============================] - 7s 245ms/step - loss: 0.5597 - mae: 0.9883\n",
            "\n",
            "Epoch 00041: loss improved from 0.34502 to 0.33991, saving model to poids.hdf5\n",
            "Epoch 42/100\n",
            "30/30 [==============================] - 7s 246ms/step - loss: 0.5469 - mae: 0.9731\n",
            "\n",
            "Epoch 00042: loss improved from 0.33991 to 0.33389, saving model to poids.hdf5\n",
            "Epoch 43/100\n",
            "30/30 [==============================] - 7s 245ms/step - loss: 0.5315 - mae: 0.9550\n",
            "\n",
            "Epoch 00043: loss improved from 0.33389 to 0.32694, saving model to poids.hdf5\n",
            "Epoch 44/100\n",
            "30/30 [==============================] - 7s 245ms/step - loss: 0.5133 - mae: 0.9337\n",
            "\n",
            "Epoch 00044: loss improved from 0.32694 to 0.31911, saving model to poids.hdf5\n",
            "Epoch 45/100\n",
            "30/30 [==============================] - 7s 246ms/step - loss: 0.4922 - mae: 0.9093\n",
            "\n",
            "Epoch 00045: loss improved from 0.31911 to 0.31058, saving model to poids.hdf5\n",
            "Epoch 46/100\n",
            "30/30 [==============================] - 7s 247ms/step - loss: 0.4684 - mae: 0.8818\n",
            "\n",
            "Epoch 00046: loss improved from 0.31058 to 0.30168, saving model to poids.hdf5\n",
            "Epoch 47/100\n",
            "30/30 [==============================] - 7s 246ms/step - loss: 0.4423 - mae: 0.8523\n",
            "\n",
            "Epoch 00047: loss improved from 0.30168 to 0.29290, saving model to poids.hdf5\n",
            "Epoch 48/100\n",
            "30/30 [==============================] - 7s 245ms/step - loss: 0.4147 - mae: 0.8218\n",
            "\n",
            "Epoch 00048: loss improved from 0.29290 to 0.28485, saving model to poids.hdf5\n",
            "Epoch 49/100\n",
            "30/30 [==============================] - 7s 246ms/step - loss: 0.3869 - mae: 0.7914\n",
            "\n",
            "Epoch 00049: loss improved from 0.28485 to 0.27816, saving model to poids.hdf5\n",
            "Epoch 50/100\n",
            "30/30 [==============================] - 7s 244ms/step - loss: 0.3606 - mae: 0.7626\n",
            "\n",
            "Epoch 00050: loss improved from 0.27816 to 0.27329, saving model to poids.hdf5\n",
            "Epoch 51/100\n",
            "30/30 [==============================] - 7s 246ms/step - loss: 0.3374 - mae: 0.7368\n",
            "\n",
            "Epoch 00051: loss improved from 0.27329 to 0.27040, saving model to poids.hdf5\n",
            "Epoch 52/100\n",
            "30/30 [==============================] - 7s 246ms/step - loss: 0.3189 - mae: 0.7155\n",
            "\n",
            "Epoch 00052: loss improved from 0.27040 to 0.26927, saving model to poids.hdf5\n",
            "Epoch 53/100\n",
            "30/30 [==============================] - 7s 247ms/step - loss: 0.3059 - mae: 0.7003\n",
            "\n",
            "Epoch 00053: loss did not improve from 0.26927\n",
            "Epoch 54/100\n",
            "30/30 [==============================] - 7s 245ms/step - loss: 0.2990 - mae: 0.6923\n",
            "\n",
            "Epoch 00054: loss did not improve from 0.26927\n",
            "Epoch 55/100\n",
            "30/30 [==============================] - 7s 246ms/step - loss: 0.2980 - mae: 0.6919\n",
            "\n",
            "Epoch 00055: loss did not improve from 0.26927\n",
            "Epoch 56/100\n",
            "30/30 [==============================] - 7s 245ms/step - loss: 0.3024 - mae: 0.6984\n",
            "\n",
            "Epoch 00056: loss did not improve from 0.26927\n",
            "Epoch 57/100\n",
            "30/30 [==============================] - 7s 244ms/step - loss: 0.3111 - mae: 0.7107\n",
            "\n",
            "Epoch 00057: loss did not improve from 0.26927\n",
            "Epoch 58/100\n",
            "30/30 [==============================] - 7s 246ms/step - loss: 0.3236 - mae: 0.7281\n",
            "\n",
            "Epoch 00058: loss did not improve from 0.26927\n",
            "Epoch 59/100\n",
            "30/30 [==============================] - 7s 246ms/step - loss: 0.3404 - mae: 0.7507\n",
            "\n",
            "Epoch 00059: loss did not improve from 0.26927\n",
            "Epoch 60/100\n",
            "30/30 [==============================] - 7s 246ms/step - loss: 0.3625 - mae: 0.7797\n",
            "\n",
            "Epoch 00060: loss did not improve from 0.26927\n",
            "Epoch 61/100\n",
            "30/30 [==============================] - 7s 246ms/step - loss: 0.3914 - mae: 0.8162\n",
            "\n",
            "Epoch 00061: loss did not improve from 0.26927\n",
            "Epoch 62/100\n",
            "30/30 [==============================] - 7s 247ms/step - loss: 0.4286 - mae: 0.8612\n",
            "\n",
            "Epoch 00062: loss did not improve from 0.26927\n",
            "Epoch 63/100\n",
            "30/30 [==============================] - 7s 248ms/step - loss: 0.4748 - mae: 0.9146\n",
            "\n",
            "Epoch 00063: loss did not improve from 0.26927\n",
            "Epoch 64/100\n",
            "30/30 [==============================] - 7s 246ms/step - loss: 0.5292 - mae: 0.9749\n",
            "\n",
            "Epoch 00064: loss did not improve from 0.26927\n",
            "Epoch 65/100\n",
            "30/30 [==============================] - 7s 244ms/step - loss: 0.5926 - mae: 1.0425\n",
            "\n",
            "Epoch 00065: loss did not improve from 0.26927\n",
            "Epoch 66/100\n",
            "30/30 [==============================] - 7s 246ms/step - loss: 0.6700 - mae: 1.1228\n",
            "\n",
            "Epoch 00066: loss did not improve from 0.26927\n",
            "Epoch 67/100\n",
            "30/30 [==============================] - 7s 247ms/step - loss: 0.7665 - mae: 1.2215\n",
            "\n",
            "Epoch 00067: loss did not improve from 0.26927\n",
            "Epoch 68/100\n",
            "30/30 [==============================] - 7s 245ms/step - loss: 0.8677 - mae: 1.3239\n",
            "\n",
            "Epoch 00068: loss did not improve from 0.26927\n",
            "Epoch 69/100\n",
            "30/30 [==============================] - 7s 246ms/step - loss: 0.9116 - mae: 1.3568\n",
            "\n",
            "Epoch 00069: loss did not improve from 0.26927\n",
            "Epoch 70/100\n",
            "30/30 [==============================] - 7s 248ms/step - loss: 0.8154 - mae: 1.2426\n",
            "\n",
            "Epoch 00070: loss did not improve from 0.26927\n",
            "Epoch 71/100\n",
            "30/30 [==============================] - 7s 247ms/step - loss: 0.6514 - mae: 1.0575\n",
            "\n",
            "Epoch 00071: loss did not improve from 0.26927\n",
            "Epoch 72/100\n",
            "30/30 [==============================] - 7s 245ms/step - loss: 0.4852 - mae: 0.8981\n",
            "\n",
            "Epoch 00072: loss did not improve from 0.26927\n",
            "Epoch 73/100\n",
            "30/30 [==============================] - 7s 245ms/step - loss: 0.6375 - mae: 1.0589\n",
            "\n",
            "Epoch 00073: loss did not improve from 0.26927\n",
            "Epoch 74/100\n",
            "30/30 [==============================] - 7s 246ms/step - loss: 0.7558 - mae: 1.1962\n",
            "\n",
            "Epoch 00074: loss did not improve from 0.26927\n",
            "Epoch 75/100\n",
            "30/30 [==============================] - 7s 247ms/step - loss: 0.8318 - mae: 1.3001\n",
            "\n",
            "Epoch 00075: loss did not improve from 0.26927\n",
            "Epoch 76/100\n",
            "30/30 [==============================] - 7s 245ms/step - loss: 0.3323 - mae: 0.6896\n",
            "\n",
            "Epoch 00076: loss improved from 0.26927 to 0.25774, saving model to poids.hdf5\n",
            "Epoch 77/100\n",
            "30/30 [==============================] - 7s 247ms/step - loss: 0.7096 - mae: 1.1699\n",
            "\n",
            "Epoch 00077: loss did not improve from 0.25774\n",
            "Epoch 78/100\n",
            "30/30 [==============================] - 7s 247ms/step - loss: 0.6398 - mae: 1.0923\n",
            "\n",
            "Epoch 00078: loss did not improve from 0.25774\n",
            "Epoch 79/100\n",
            "30/30 [==============================] - 7s 245ms/step - loss: 0.8952 - mae: 1.3150\n",
            "\n",
            "Epoch 00079: loss did not improve from 0.25774\n",
            "Epoch 80/100\n",
            "30/30 [==============================] - 7s 246ms/step - loss: 0.4726 - mae: 0.8716\n",
            "\n",
            "Epoch 00080: loss did not improve from 0.25774\n",
            "Epoch 81/100\n",
            "30/30 [==============================] - 7s 246ms/step - loss: 1.1961 - mae: 1.6289\n",
            "\n",
            "Epoch 00081: loss did not improve from 0.25774\n",
            "Epoch 82/100\n",
            "30/30 [==============================] - 7s 245ms/step - loss: 1.8960 - mae: 2.3847\n",
            "\n",
            "Epoch 00082: loss did not improve from 0.25774\n",
            "Epoch 83/100\n",
            "30/30 [==============================] - 7s 247ms/step - loss: 1.0108 - mae: 1.4928\n",
            "\n",
            "Epoch 00083: loss did not improve from 0.25774\n",
            "Epoch 84/100\n",
            "30/30 [==============================] - 7s 247ms/step - loss: 2.3143 - mae: 2.8121\n",
            "\n",
            "Epoch 00084: loss did not improve from 0.25774\n",
            "Epoch 85/100\n",
            "30/30 [==============================] - 7s 246ms/step - loss: 2.3818 - mae: 2.8339\n",
            "\n",
            "Epoch 00085: loss did not improve from 0.25774\n",
            "Epoch 86/100\n",
            "30/30 [==============================] - 7s 247ms/step - loss: 2.4231 - mae: 2.8904\n",
            "\n",
            "Epoch 00086: loss did not improve from 0.25774\n",
            "Epoch 87/100\n",
            "30/30 [==============================] - 7s 247ms/step - loss: 3.2255 - mae: 3.6541\n",
            "\n",
            "Epoch 00087: loss did not improve from 0.25774\n",
            "Epoch 88/100\n",
            "30/30 [==============================] - 7s 246ms/step - loss: 4.3734 - mae: 4.8721\n",
            "\n",
            "Epoch 00088: loss did not improve from 0.25774\n",
            "Epoch 89/100\n",
            "30/30 [==============================] - 7s 246ms/step - loss: 4.2080 - mae: 4.6719\n",
            "\n",
            "Epoch 00089: loss did not improve from 0.25774\n",
            "Epoch 90/100\n",
            "30/30 [==============================] - 7s 245ms/step - loss: 4.0799 - mae: 4.5799\n",
            "\n",
            "Epoch 00090: loss did not improve from 0.25774\n",
            "Epoch 91/100\n",
            "30/30 [==============================] - 7s 246ms/step - loss: 9.2175 - mae: 9.7152\n",
            "\n",
            "Epoch 00091: loss did not improve from 0.25774\n",
            "Epoch 92/100\n",
            "30/30 [==============================] - 7s 246ms/step - loss: 5.8023 - mae: 6.2929\n",
            "\n",
            "Epoch 00092: loss did not improve from 0.25774\n",
            "Epoch 93/100\n",
            "30/30 [==============================] - 7s 245ms/step - loss: 7.6607 - mae: 8.1606\n",
            "\n",
            "Epoch 00093: loss did not improve from 0.25774\n",
            "Epoch 94/100\n",
            "30/30 [==============================] - 7s 248ms/step - loss: 9.9058 - mae: 10.4058\n",
            "\n",
            "Epoch 00094: loss did not improve from 0.25774\n",
            "Epoch 95/100\n",
            "30/30 [==============================] - 7s 247ms/step - loss: 12.5133 - mae: 13.0133\n",
            "\n",
            "Epoch 00095: loss did not improve from 0.25774\n",
            "Epoch 96/100\n",
            "30/30 [==============================] - 7s 246ms/step - loss: 15.9677 - mae: 16.4677\n",
            "\n",
            "Epoch 00096: loss did not improve from 0.25774\n",
            "Epoch 97/100\n",
            "30/30 [==============================] - 7s 247ms/step - loss: 20.1469 - mae: 20.6469\n",
            "\n",
            "Epoch 00097: loss did not improve from 0.25774\n",
            "Epoch 98/100\n",
            "30/30 [==============================] - 7s 245ms/step - loss: 25.5774 - mae: 26.0774\n",
            "\n",
            "Epoch 00098: loss did not improve from 0.25774\n",
            "Epoch 99/100\n",
            "30/30 [==============================] - 7s 246ms/step - loss: 32.2445 - mae: 32.7445\n",
            "\n",
            "Epoch 00099: loss did not improve from 0.25774\n",
            "Epoch 100/100\n",
            "30/30 [==============================] - 7s 246ms/step - loss: 40.8078 - mae: 41.3078\n",
            "\n",
            "Epoch 00100: loss did not improve from 0.25774\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6UF44BnpVtc",
        "outputId": "7da473fa-e049-4712-fb13-b53b3b214287",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 1])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, \"Evolution de l'erreur en fonction du taux d'apprentissage\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLGfH1jGpbkT"
      },
      "source": [
        "# Charge les meilleurs poids\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VsRDR-8pery",
        "outputId": "18b00bcc-954d-4a22-c955-c2fa58952040",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "class TimingCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, logs={}):\n",
        "        self.logs=[]\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.starttime = timer()\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.logs.append(timer()-self.starttime)\n",
        "\n",
        "\n",
        "cb = TimingCallback()\n",
        "\n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.001,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0.05\n",
        "    )\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(learning_rate=lr_schedule,momentum=0.9)\n",
        "\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur,metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle\n",
        "historique = model.fit(dataset_norm,validation_data=dataset_Val_norm, epochs=500,verbose=1, callbacks=[CheckPoint,cb])\n",
        "\n",
        "print(cb.logs)\n",
        "print(sum(cb.logs))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "30/30 [==============================] - 333s 2s/step - loss: 0.5994 - mae: 1.0270 - val_loss: 0.7995 - val_mae: 1.2868\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.36373, saving model to poids_train.hdf5\n",
            "Epoch 2/500\n",
            "30/30 [==============================] - 8s 283ms/step - loss: 0.4922 - mae: 0.9072 - val_loss: 0.8972 - val_mae: 1.3909\n",
            "\n",
            "Epoch 00002: loss improved from 0.36373 to 0.31636, saving model to poids_train.hdf5\n",
            "Epoch 3/500\n",
            "30/30 [==============================] - 8s 280ms/step - loss: 0.4272 - mae: 0.8358 - val_loss: 0.9620 - val_mae: 1.4583\n",
            "\n",
            "Epoch 00003: loss improved from 0.31636 to 0.29149, saving model to poids_train.hdf5\n",
            "Epoch 4/500\n",
            "30/30 [==============================] - 8s 279ms/step - loss: 0.3881 - mae: 0.7926 - val_loss: 1.0059 - val_mae: 1.5034\n",
            "\n",
            "Epoch 00004: loss improved from 0.29149 to 0.27843, saving model to poids_train.hdf5\n",
            "Epoch 5/500\n",
            "30/30 [==============================] - 8s 278ms/step - loss: 0.3632 - mae: 0.7648 - val_loss: 1.0367 - val_mae: 1.5348\n",
            "\n",
            "Epoch 00005: loss improved from 0.27843 to 0.27114, saving model to poids_train.hdf5\n",
            "Epoch 6/500\n",
            "30/30 [==============================] - 8s 279ms/step - loss: 0.3466 - mae: 0.7457 - val_loss: 1.0590 - val_mae: 1.5574\n",
            "\n",
            "Epoch 00006: loss improved from 0.27114 to 0.26680, saving model to poids_train.hdf5\n",
            "Epoch 7/500\n",
            "30/30 [==============================] - 8s 279ms/step - loss: 0.3349 - mae: 0.7321 - val_loss: 1.0756 - val_mae: 1.5743\n",
            "\n",
            "Epoch 00007: loss improved from 0.26680 to 0.26406, saving model to poids_train.hdf5\n",
            "Epoch 8/500\n",
            "30/30 [==============================] - 8s 279ms/step - loss: 0.3265 - mae: 0.7221 - val_loss: 1.0882 - val_mae: 1.5871\n",
            "\n",
            "Epoch 00008: loss improved from 0.26406 to 0.26223, saving model to poids_train.hdf5\n",
            "Epoch 9/500\n",
            "30/30 [==============================] - 8s 280ms/step - loss: 0.3201 - mae: 0.7145 - val_loss: 1.0981 - val_mae: 1.5971\n",
            "\n",
            "Epoch 00009: loss improved from 0.26223 to 0.26096, saving model to poids_train.hdf5\n",
            "Epoch 10/500\n",
            "30/30 [==============================] - 8s 278ms/step - loss: 0.3152 - mae: 0.7086 - val_loss: 1.1060 - val_mae: 1.6051\n",
            "\n",
            "Epoch 00010: loss improved from 0.26096 to 0.26004, saving model to poids_train.hdf5\n",
            "Epoch 11/500\n",
            "30/30 [==============================] - 8s 279ms/step - loss: 0.3114 - mae: 0.7039 - val_loss: 1.1123 - val_mae: 1.6115\n",
            "\n",
            "Epoch 00011: loss improved from 0.26004 to 0.25935, saving model to poids_train.hdf5\n",
            "Epoch 12/500\n",
            "30/30 [==============================] - 8s 277ms/step - loss: 0.3083 - mae: 0.7001 - val_loss: 1.1175 - val_mae: 1.6167\n",
            "\n",
            "Epoch 00012: loss improved from 0.25935 to 0.25882, saving model to poids_train.hdf5\n",
            "Epoch 13/500\n",
            "30/30 [==============================] - 8s 281ms/step - loss: 0.3058 - mae: 0.6971 - val_loss: 1.1218 - val_mae: 1.6210\n",
            "\n",
            "Epoch 00013: loss improved from 0.25882 to 0.25841, saving model to poids_train.hdf5\n",
            "Epoch 14/500\n",
            "30/30 [==============================] - 8s 278ms/step - loss: 0.3037 - mae: 0.6945 - val_loss: 1.1254 - val_mae: 1.6246\n",
            "\n",
            "Epoch 00014: loss improved from 0.25841 to 0.25808, saving model to poids_train.hdf5\n",
            "Epoch 15/500\n",
            "30/30 [==============================] - 8s 279ms/step - loss: 0.3020 - mae: 0.6923 - val_loss: 1.1284 - val_mae: 1.6277\n",
            "\n",
            "Epoch 00015: loss improved from 0.25808 to 0.25780, saving model to poids_train.hdf5\n",
            "Epoch 16/500\n",
            "30/30 [==============================] - 8s 279ms/step - loss: 0.3005 - mae: 0.6905 - val_loss: 1.1310 - val_mae: 1.6303\n",
            "\n",
            "Epoch 00016: loss improved from 0.25780 to 0.25757, saving model to poids_train.hdf5\n",
            "Epoch 17/500\n",
            "30/30 [==============================] - 8s 279ms/step - loss: 0.2993 - mae: 0.6889 - val_loss: 1.1332 - val_mae: 1.6325\n",
            "\n",
            "Epoch 00017: loss improved from 0.25757 to 0.25738, saving model to poids_train.hdf5\n",
            "Epoch 18/500\n",
            "30/30 [==============================] - 8s 276ms/step - loss: 0.2982 - mae: 0.6876 - val_loss: 1.1351 - val_mae: 1.6344\n",
            "\n",
            "Epoch 00018: loss improved from 0.25738 to 0.25721, saving model to poids_train.hdf5\n",
            "Epoch 19/500\n",
            "30/30 [==============================] - 8s 278ms/step - loss: 0.2973 - mae: 0.6864 - val_loss: 1.1367 - val_mae: 1.6361\n",
            "\n",
            "Epoch 00019: loss improved from 0.25721 to 0.25707, saving model to poids_train.hdf5\n",
            "Epoch 20/500\n",
            "30/30 [==============================] - 8s 278ms/step - loss: 0.2965 - mae: 0.6854 - val_loss: 1.1382 - val_mae: 1.6376\n",
            "\n",
            "Epoch 00020: loss improved from 0.25707 to 0.25695, saving model to poids_train.hdf5\n",
            "Epoch 21/500\n",
            "30/30 [==============================] - 8s 279ms/step - loss: 0.2958 - mae: 0.6845 - val_loss: 1.1395 - val_mae: 1.6389\n",
            "\n",
            "Epoch 00021: loss improved from 0.25695 to 0.25684, saving model to poids_train.hdf5\n",
            "Epoch 22/500\n",
            "30/30 [==============================] - 8s 279ms/step - loss: 0.2952 - mae: 0.6837 - val_loss: 1.1406 - val_mae: 1.6400\n",
            "\n",
            "Epoch 00022: loss improved from 0.25684 to 0.25674, saving model to poids_train.hdf5\n",
            "Epoch 23/500\n",
            "30/30 [==============================] - 8s 277ms/step - loss: 0.2947 - mae: 0.6830 - val_loss: 1.1416 - val_mae: 1.6410\n",
            "\n",
            "Epoch 00023: loss improved from 0.25674 to 0.25665, saving model to poids_train.hdf5\n",
            "Epoch 24/500\n",
            "30/30 [==============================] - 8s 277ms/step - loss: 0.2942 - mae: 0.6824 - val_loss: 1.1425 - val_mae: 1.6419\n",
            "\n",
            "Epoch 00024: loss improved from 0.25665 to 0.25657, saving model to poids_train.hdf5\n",
            "Epoch 25/500\n",
            "30/30 [==============================] - 8s 279ms/step - loss: 0.2937 - mae: 0.6819 - val_loss: 1.1433 - val_mae: 1.6427\n",
            "\n",
            "Epoch 00025: loss improved from 0.25657 to 0.25650, saving model to poids_train.hdf5\n",
            "Epoch 26/500\n",
            "30/30 [==============================] - 8s 279ms/step - loss: 0.2933 - mae: 0.6814 - val_loss: 1.1440 - val_mae: 1.6435\n",
            "\n",
            "Epoch 00026: loss improved from 0.25650 to 0.25643, saving model to poids_train.hdf5\n",
            "Epoch 27/500\n",
            "30/30 [==============================] - 8s 276ms/step - loss: 0.2930 - mae: 0.6809 - val_loss: 1.1447 - val_mae: 1.6441\n",
            "\n",
            "Epoch 00027: loss improved from 0.25643 to 0.25637, saving model to poids_train.hdf5\n",
            "Epoch 28/500\n",
            "30/30 [==============================] - 8s 277ms/step - loss: 0.2927 - mae: 0.6805 - val_loss: 1.1453 - val_mae: 1.6447\n",
            "\n",
            "Epoch 00028: loss improved from 0.25637 to 0.25631, saving model to poids_train.hdf5\n",
            "Epoch 29/500\n",
            "30/30 [==============================] - 8s 278ms/step - loss: 0.2924 - mae: 0.6801 - val_loss: 1.1458 - val_mae: 1.6453\n",
            "\n",
            "Epoch 00029: loss improved from 0.25631 to 0.25626, saving model to poids_train.hdf5\n",
            "Epoch 30/500\n",
            "30/30 [==============================] - 8s 277ms/step - loss: 0.2921 - mae: 0.6798 - val_loss: 1.1463 - val_mae: 1.6457\n",
            "\n",
            "Epoch 00030: loss improved from 0.25626 to 0.25622, saving model to poids_train.hdf5\n",
            "Epoch 31/500\n",
            "30/30 [==============================] - 8s 277ms/step - loss: 0.2919 - mae: 0.6795 - val_loss: 1.1467 - val_mae: 1.6462\n",
            "\n",
            "Epoch 00031: loss improved from 0.25622 to 0.25617, saving model to poids_train.hdf5\n",
            "Epoch 32/500\n",
            "30/30 [==============================] - 8s 278ms/step - loss: 0.2917 - mae: 0.6792 - val_loss: 1.1471 - val_mae: 1.6466\n",
            "\n",
            "Epoch 00032: loss improved from 0.25617 to 0.25613, saving model to poids_train.hdf5\n",
            "Epoch 33/500\n",
            "30/30 [==============================] - 8s 278ms/step - loss: 0.2915 - mae: 0.6789 - val_loss: 1.1475 - val_mae: 1.6470\n",
            "\n",
            "Epoch 00033: loss improved from 0.25613 to 0.25609, saving model to poids_train.hdf5\n",
            "Epoch 34/500\n",
            "30/30 [==============================] - 8s 283ms/step - loss: 0.2913 - mae: 0.6787 - val_loss: 1.1479 - val_mae: 1.6473\n",
            "\n",
            "Epoch 00034: loss improved from 0.25609 to 0.25605, saving model to poids_train.hdf5\n",
            "Epoch 35/500\n",
            "30/30 [==============================] - 8s 279ms/step - loss: 0.2911 - mae: 0.6784 - val_loss: 1.1482 - val_mae: 1.6477\n",
            "\n",
            "Epoch 00035: loss improved from 0.25605 to 0.25602, saving model to poids_train.hdf5\n",
            "Epoch 36/500\n",
            "30/30 [==============================] - 8s 279ms/step - loss: 0.2909 - mae: 0.6782 - val_loss: 1.1485 - val_mae: 1.6479\n",
            "\n",
            "Epoch 00036: loss improved from 0.25602 to 0.25599, saving model to poids_train.hdf5\n",
            "Epoch 37/500\n",
            "30/30 [==============================] - 8s 278ms/step - loss: 0.2908 - mae: 0.6780 - val_loss: 1.1488 - val_mae: 1.6482\n",
            "\n",
            "Epoch 00037: loss improved from 0.25599 to 0.25596, saving model to poids_train.hdf5\n",
            "Epoch 38/500\n",
            " 9/30 [========>.....................] - ETA: 5s - loss: 0.3891 - mae: 0.8272"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-c9cfd5e201fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Entraine le modèle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mhistorique\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_Val_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCheckPoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZsv4CMjucWS",
        "outputId": "9394615e-cd54-46fc-e0a3-99e347d25903",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, \"Evolution de l'erreur en fonction de la période\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrveY5aRsnRe"
      },
      "source": [
        "**4. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPTggVTss0rt"
      },
      "source": [
        "taille_fenetre = 20\n",
        "\n",
        "# Création d'une liste vide pour recevoir les prédictions\n",
        "predictions = []\n",
        "\n",
        "# Calcul des prédiction pour chaque groupe de 20 valeurs consécutives de la série\n",
        "# dans l'intervalle de validation\n",
        "for t in temps[temps_separation:-taille_fenetre]:\n",
        "    X = np.reshape(Serie_Normalisee[t:t+taille_fenetre],(1,taille_fenetre))\n",
        "    predictions.append(model.predict(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYzyz6EMs72l"
      },
      "source": [
        "# Affiche la série et les prédictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps,serie,label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title('Prédictions avec le modèle LSTMN')\n",
        "plt.show()\n",
        "\n",
        "# Zoom sur l'intervalle de validation\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps[temps_separation:],serie[temps_separation:],label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title(\"Prédictions avec le modèle LSTMN (zoom sur l'intervalle de validation)\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZYKVb2Us-b_"
      },
      "source": [
        "# Calcule de l'erreur quadratique moyenne et de l'erreur absolue moyenne \n",
        "\n",
        "mae = tf.keras.metrics.mean_absolute_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "mse = tf.keras.metrics.mean_squared_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "\n",
        "print(mae)\n",
        "print(mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GP93ANo1XU3"
      },
      "source": [
        "# Création du modèle LSTMN multi-couches - Sans \"skip connections\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yFPByTxT_QW"
      },
      "source": [
        "La structure reste sensiblement la même qu'avant sauf que maintenant :\n",
        " - Les états cachés sont remontés en sortie vers la prochaine couche\n",
        " - L'attention dans les couches supérieures est calculée non plus à partir des valeurs d'entrées mais des anciens états cachés.\n",
        " - Il y a donc une nouvelle matrice de poids qui intervient dans le calcul de l'attention : Wl\n",
        " - Les matrices Wf, Wi, Wo et Wc changent également de dimension pour s'adapter à leurs vecteurs d'entrées qui sont maintenant les vecteurs cachés.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_bvPbrY1XU4"
      },
      "source": [
        "<img src=\"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/images/LSTMN_Calcul5.png?raw=true\" width=\"1200\"> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUMDw7lR1XU4"
      },
      "source": [
        "**1. Création de la multi-couche LSTMN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_tVN9sG1XU5"
      },
      "source": [
        "# Classe Multi couche LSTMN\n",
        "\n",
        "# Importe le Backend de Keras\n",
        "from keras import backend as K\n",
        "\n",
        "# Définit une nouvelle classe personnelle LSTMN\n",
        "# https://arxiv.org/pdf/1601.06733.pdf\n",
        "\n",
        "\n",
        "# Dans les dimensions ci-dessous :\n",
        "# dimension de la série : dim_serie = 1\n",
        "# dimension des vecteurs internes LSTM : dim_LSTM = 40\n",
        "# batch_size = 32\n",
        "\n",
        "class Couche_LSTMN_Layers(tf.keras.layers.Layer):\n",
        "  # Fonction d'initialisation de la classe d'attention\n",
        "  def __init__(self,dim_LSTM,return_sequence = False):\n",
        "    self.dim_LSTM = dim_LSTM                # Dimension du vecteur d'attention\n",
        "    self.return_sequence = return_sequence  # Retourne l'ensemble des états cachés ?\n",
        "    super().__init__()                      # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    dim_serie = input_shape[2]\n",
        "    nbr_instants = input_shape[1]\n",
        "\n",
        "    # Paramètres de la forget gate :\n",
        "    # ##############################\n",
        "    # Matrices de poids Wf(40,40), Uf(40,40) et offset bf(40)\n",
        "    self.Wf = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Wf\",trainable=True)\n",
        "    self.Uf = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Uf\",trainable=True)\n",
        "    self.bf = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"zeros\",name=\"bf\",trainable=True)\n",
        "\n",
        "    # Paramètres de l'input gate :\n",
        "    # ##############################\n",
        "    # Matrices de poids Wi(40,40), Ui(40,40) et offset bi(40)\n",
        "    self.Wi = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Wi\",trainable=True)\n",
        "    self.Ui = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Ui\",trainable=True)\n",
        "    self.bi = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"zeros\",name=\"bi\",trainable=True)\n",
        "\n",
        "    # Paramètres de l'output gate :\n",
        "    # ##############################\n",
        "    # Matrices de poids Wo(40,40), Uo(40,40) et offset bo(40)\n",
        "    self.Wo = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Wo\",trainable=True)\n",
        "    self.Uo = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Uo\",trainable=True)\n",
        "    self.bo = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"zeros\",name=\"bo\",trainable=True)\n",
        "\n",
        "    # Paramètres du cell vector :\n",
        "    # ##############################\n",
        "    # Matrices de poids Wc(40,40), Uc(40,40) et offset bc(40)\n",
        "    self.Wc = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Wc\",trainable=True)\n",
        "    self.Uc = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Uc\",trainable=True)\n",
        "    self.bc = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"zeros\",name=\"bc\",trainable=True)\n",
        "\n",
        "    # Paramètres de l'attention :\n",
        "    # ###########################\n",
        "    # Matrices de poids Wh(40,40); Wl(40,40) et Wh_t(40,40)\n",
        "    self.Wh = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Wh\",trainable=True)\n",
        "    self.Wl = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Wl\",trainable=True)\n",
        "    self.Wh_t = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Wh_t\",trainable=True)\n",
        "\n",
        "    # Vecteur contexte :  v(1,40)\n",
        "    #############################\n",
        "    self.v = self.add_weight(shape=(dim_serie,self.dim_LSTM),initializer=\"normal\",name=\"v\",trainable=True)\n",
        "\n",
        "    # Memory tape : Ct    Liste[(40,1)]\n",
        "    # #################################\n",
        "    memory_tape = getattr(self, 'memory_tape', None)\n",
        "    if memory_tape is None:\n",
        "      memory_tape = []\n",
        "      memory_tape.append(tf.zeros(shape=(self.dim_LSTM,1)))         # Ct[0] = c0 = 0\n",
        "    self.memory_tape = memory_tape\n",
        "\n",
        "    # Hidden tape : Ht    Liste[(40,1)]\n",
        "    # #################################\n",
        "    hidden_tape = getattr(self, 'hidden_tape', None)\n",
        "    if hidden_tape is None:\n",
        "      hidden_tape = []\n",
        "      hidden_tape.append(tf.zeros(shape=(self.dim_LSTM,1)))         # Ht[0] = h0 = 0\n",
        "    self.hidden_tape = hidden_tape\n",
        "\n",
        "    # Vecteurs d'adaptation du Hidden state : Ht_t    Liste[(40,1)]\n",
        "    # #############################################################\n",
        "    hidden_t_tape = getattr(self, 'hidden_t_tape', None)\n",
        "    if hidden_t_tape is None:\n",
        "      hidden_t_tape = []\n",
        "      hidden_t_tape.append(tf.zeros(shape=(self.dim_LSTM,1)))       # Ht_t[0] = h0_t = 0\n",
        "    self.hidden_t_tape = hidden_t_tape\n",
        "\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Définit la logique de la couche LSTM\n",
        "  # Arguments :   x : Tenseur d'entrée de dimension (None, taille_fenetre,dim_LSTM,1)\n",
        "  def call(self,x):\n",
        "    # Charge les tables mémoires\n",
        "    Ct = self.memory_tape.copy()\n",
        "    Ht = self.hidden_tape.copy()\n",
        "    Ht_t = self.hidden_t_tape.copy()\n",
        "\n",
        "    for t in range(1,x.shape[1]+1):\n",
        "      xt = x[:,t-1,:,:]                         # (32,20,40,1)\n",
        "\n",
        "      # Calcul des poids d'attention\n",
        "      at_i = tf.zeros(shape=(x.shape[0],1,1))   # a0_0 = 0 (32,1,1)\n",
        "      for i in range(1,t):\n",
        "        at_ = tf.matmul(self.v,tf.keras.activations.tanh(tf.matmul(self.Wh,Ht[i]) + tf.matmul(self.Wl,xt) + tf.matmul(self.Wh_t,Ht_t[t-1])))\n",
        "        at_i = tf.concat([at_,at_i],axis=1)\n",
        "      if t > 1:\n",
        "        at_i = tf.slice(at_i,[0,0,0],[at_i.shape[0],at_i.shape[1]-1,at_i.shape[2]])\n",
        "\n",
        "      # Calcul des poids d'attention normalisés\n",
        "      st_i = tf.keras.activations.softmax(at_i,axis=1)        # (32,t,1)\n",
        "\n",
        "      # Calcul du vecteur d'adaptation ht_t\n",
        "      ht_t = tf.zeros(shape=(x.shape[0],self.dim_LSTM,1))                                       # h0_t = 0\n",
        "      for i in range(1,t):\n",
        "        ht_t = ht_t + tf.multiply(tf.expand_dims(st_i[:,i-1,:],axis=-1),Ht[i])      # (32,40,1)\n",
        "\n",
        "      # Calcul du vecteur d'adaptation ct_t\n",
        "      ct_t = tf.zeros(shape=(x.shape[0],self.dim_LSTM,1))                           # c0_t = 0\n",
        "      for i in range(1,t):\n",
        "        ct_t = ct_t + tf.multiply(tf.expand_dims(st_i[:,i-1,:],axis=-1),Ct[i])      # (32,40,1)\n",
        "      \n",
        "      # Calcul du vecteur d'activation de la forget gate à l'instant t\n",
        "      ft = tf.matmul(self.Wf,xt)                    # (40,1)x(32,1,1) = (32,40,1)\n",
        "      ft = ft + tf.matmul(self.Uf,ht_t)             # (40,40)x(32,40,1) = (32,40,1)\n",
        "      ft = ft + self.bf                             # (32,40,1) + (40,1) = (32,40,1)\n",
        "      ft = tf.keras.activations.sigmoid(ft) \n",
        "\n",
        "      # Calcul du vecteur d'activation de l'input gate à l'instant t\n",
        "      it = tf.matmul(self.Wi,xt)                    # (40,1)x(32,1,1) = (32,40,1)\n",
        "      it = it + tf.matmul(self.Ui,ht_t)             # (40,40)x(32,40,1) = (32,40,1)\n",
        "      it = it + self.bi                             # (32,40,1) + (40,1) = (32,40,1)\n",
        "      it = tf.keras.activations.sigmoid(it)\n",
        "\n",
        "      # Calcul du vecteur d'activation de l'output gate à l'instant t\n",
        "      ot = tf.matmul(self.Wo,xt)                    # (40,1)x(32,1,1) = (32,40,1)\n",
        "      ot = ot + tf.matmul(self.Uo,ht_t)             # (40,40)x(32,40,1) = (32,40,1)\n",
        "      ot = ot + self.bo                             # (32,40,1) + (40,1) = (32,40,1)\n",
        "      ot = tf.keras.activations.sigmoid(ot)\n",
        "\n",
        "      # Calcul du cell input activation vector à l'instant t\n",
        "      ct_h = tf.matmul(self.Wc,xt)                 # (40,1)x(32,1,1) = (32,40,1)\n",
        "      ct_h = ct_h + tf.matmul(self.Uc,ht_t)        # (40,40)x(32,40,1) = (32,40,1)\n",
        "      ct_h = ct_h + self.bc                        # (32,40,1) + (40,1) = (32,40,1)\n",
        "      ct_h = tf.keras.activations.tanh(ct_h)\n",
        "\n",
        "      # Calcul du cell state à l'instant t\n",
        "      ct = tf.multiply(ft,ct_t)                    # (32,40,1)\n",
        "      ct = ct + tf.multiply(it,ct_h)               # (32,40,1)\n",
        "      ht = tf.multiply(ot,tf.keras.activations.tanh(ct))\n",
        "\n",
        "      # Enregistre les vecteurs internes du temps courant\n",
        "      Ht.append(ht)                                 # Enregistre Ht[t] = ht\n",
        "      Ct.append(ct)                                 # Enregistre Ct[t] = ct\n",
        "      Ht_t.append(ht_t)                             # Enregistre Ht_t[t] = ht_t\n",
        "\n",
        "    # Enregistre les vecteurs internes du batch courant\n",
        "    for memory_tape_, memory_tape in zip(tf.nest.flatten(self.memory_tape),tf.nest.flatten(Ct)):\n",
        "      memory_tape_ = memory_tape\n",
        "    for hidden_tape_, hidden_tape in zip(tf.nest.flatten(self.hidden_tape),tf.nest.flatten(Ht)):\n",
        "      hidden_tape_ = hidden_tape\n",
        "    for hidden_t_tape_, hidden_t_tape in zip(tf.nest.flatten(self.hidden_t_tape),tf.nest.flatten(Ht_t)):\n",
        "      hidden_t_tape_ = hidden_t_tape\n",
        "\n",
        "    # Retourne le dernier hidden state ou l'ensemble des vecteurs\n",
        "    if self.return_sequence == False:\n",
        "      return tf.squeeze(ht,axis=-1)               # return (32,40)\n",
        "    else:\n",
        "      hidden_states = tf.zeros(shape=(x.shape[0],1,self.dim_LSTM,1))   # (32,1,40,1)\n",
        "      for i in range(0,x.shape[1]):\n",
        "        hidden_states = tf.concat([hidden_states,tf.expand_dims(Ht[i+1],axis=1)],axis=1)\n",
        "      hidden_states = tf.slice(hidden_states,[0,0,0,0],[hidden_states.shape[0],hidden_states.shape[1]-1,hidden_states.shape[2],hidden_states.shape[3]])\n",
        "      return hidden_states                        # return (32,20,40,1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIVnvnym1XU6"
      },
      "source": [
        "**2. Création du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ieny0G8b1XU6"
      },
      "source": [
        "dim_LSTM = 40\n",
        "\n",
        "# Fonction de la couche lambda d'entrée\n",
        "def Traitement_Entrees(x):\n",
        "  return tf.expand_dims(x,axis=-1)\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.Input(shape=(taille_fenetre,), batch_size=batch_size))\n",
        "model.add(tf.keras.layers.Lambda(Traitement_Entrees))\n",
        "model.add(Couche_LSTMN(dim_LSTM=dim_LSTM,return_sequence=True))\n",
        "model.add(Couche_LSTMN_Layers(dim_LSTM=dim_LSTM,return_sequence=False))\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "model.save_weights('model_initial.h5')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koUv2Kyq1XU6"
      },
      "source": [
        "**3. Entrainement du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4Vny6yr1XU7"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur, metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset_norm,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aaLeG6n1XU7"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 1])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqaT3i5y1XU7"
      },
      "source": [
        "# Charge les meilleurs poids\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiXDVmvC1XU8"
      },
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "class TimingCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, logs={}):\n",
        "        self.logs=[]\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.starttime = timer()\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.logs.append(timer()-self.starttime)\n",
        "\n",
        "\n",
        "cb = TimingCallback()\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(lr=7e-2,momentum=0.9)\n",
        "\n",
        "\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur,metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle\n",
        "historique = model.fit(dataset_norm,validation_data=dataset_Val_norm, epochs=500,verbose=1, callbacks=[CheckPoint,cb])\n",
        "\n",
        "print(cb.logs)\n",
        "print(sum(cb.logs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X38GhBRL1XU8"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHjPHr3O1XU8"
      },
      "source": [
        "**4. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IegVdqzG1XU8"
      },
      "source": [
        "# Charge les meilleurs poids\n",
        "model.load_weights(\"poids_train.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnEm_JTa1XU9"
      },
      "source": [
        "taille_fenetre = 20\n",
        "\n",
        "# Création d'une liste vide pour recevoir les prédictions\n",
        "predictions = []\n",
        "\n",
        "# Calcul des prédiction pour chaque groupe de 20 valeurs consécutives de la série\n",
        "# dans l'intervalle de validation\n",
        "for t in temps[temps_separation:-taille_fenetre]:\n",
        "    X = np.reshape(Serie_Normalisee[t:t+taille_fenetre],(1,taille_fenetre))\n",
        "    predictions.append(model.predict(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZ0qtxwp1XU9"
      },
      "source": [
        "# Affiche la série et les prédictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps,serie,label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title('Prédictions avec le modèle LSTMN')\n",
        "plt.show()\n",
        "\n",
        "# Zoom sur l'intervalle de validation\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps[temps_separation:],serie[temps_separation:],label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title(\"Prédictions avec le modèle LSTMN (zoom sur l'intervalle de validation)\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHu47KqE1XU9"
      },
      "source": [
        "# Calcule de l'erreur quadratique moyenne et de l'erreur absolue moyenne \n",
        "\n",
        "mae = tf.keras.metrics.mean_absolute_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "mse = tf.keras.metrics.mean_squared_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "\n",
        "print(mae)\n",
        "print(mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilDkJ-4iKeFb"
      },
      "source": [
        "# Création du modèle LSTMN multi-couches - Avec \"skip connections\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIeIOTc5KeFl"
      },
      "source": [
        "La structure reste sensiblement la même qu'avant sauf que maintenant :\n",
        " - Les états cachés sont remontés en sortie vers la prochaine couche\n",
        " - L'attention dans les couches supérieures est calculée non plus à partir des valeurs d'entrées mais des anciens états cachés.\n",
        " - Il y a donc une nouvelle matrice de poids qui intervient dans le calcul de l'attention : Wl\n",
        " - Les vecteurs internes des couches intermédiaires sont calculés à partir des entrées X, qui sont donc remontées également sur ces couches. Les couches intermédiaires sont donc à deux entrées."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DegQ3xpKeFm"
      },
      "source": [
        "<img src=\"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/images/LSTMN_Calcul6.png?raw=true\" width=\"1200\"> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5rH9LfLKeFm"
      },
      "source": [
        "**1. Création de la multi-couche LSTMN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUEKGBnYKeFn"
      },
      "source": [
        "# Classe Multi couche LSTMN\n",
        "\n",
        "# Importe le Backend de Keras\n",
        "from keras import backend as K\n",
        "\n",
        "# Définit une nouvelle classe personnelle LSTMN\n",
        "# https://arxiv.org/pdf/1601.06733.pdf\n",
        "\n",
        "\n",
        "# Dans les dimensions ci-dessous :\n",
        "# dimension de la série : dim_serie = 1\n",
        "# dimension des vecteurs internes LSTM : dim_LSTM = 40\n",
        "# batch_size = 32\n",
        "\n",
        "class Couche_LSTMN_Layers(tf.keras.layers.Layer):\n",
        "  # Fonction d'initialisation de la classe d'attention\n",
        "  def __init__(self,dim_LSTM,return_sequence = False):\n",
        "    self.dim_LSTM = dim_LSTM                # Dimension du vecteur d'attention\n",
        "    self.return_sequence = return_sequence  # Retourne l'ensemble des états cachés ?\n",
        "    super().__init__()                      # Appel du __init__() de la classe Layer\n",
        "  \n",
        "\n",
        "  # input_shape :   x : Tenseur d'entrée de dimension (None, taille_fenetre,dim_serie)\n",
        "  #                              \n",
        "  def build(self,input_shape):\n",
        "    dim_serie = input_shape[2]\n",
        "    nbr_instants = input_shape[1]\n",
        "\n",
        "    # Paramètres de la forget gate :\n",
        "    # ##############################\n",
        "    # Matrices de poids Wf(40,1), Uf(40,40) et offset bf(40)\n",
        "    self.Wf = self.add_weight(shape=(self.dim_LSTM,dim_serie),initializer=\"normal\",name=\"Wf\",trainable=True)\n",
        "    self.Uf = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Uf\",trainable=True)\n",
        "    self.bf = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"zeros\",name=\"bf\",trainable=True)\n",
        "\n",
        "    # Paramètres de l'input gate :\n",
        "    # ##############################\n",
        "    # Matrices de poids Wi(40,1), Ui(40,40) et offset bi(40)\n",
        "    self.Wi = self.add_weight(shape=(self.dim_LSTM,dim_serie),initializer=\"normal\",name=\"Wi\",trainable=True)\n",
        "    self.Ui = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Ui\",trainable=True)\n",
        "    self.bi = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"zeros\",name=\"bi\",trainable=True)\n",
        "\n",
        "    # Paramètres de l'output gate :\n",
        "    # ##############################\n",
        "    # Matrices de poids Wo(40,1), Uo(40,40) et offset bo(40)\n",
        "    self.Wo = self.add_weight(shape=(self.dim_LSTM,dim_serie),initializer=\"normal\",name=\"Wo\",trainable=True)\n",
        "    self.Uo = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Uo\",trainable=True)\n",
        "    self.bo = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"zeros\",name=\"bo\",trainable=True)\n",
        "\n",
        "    # Paramètres du cell vector :\n",
        "    # ##############################\n",
        "    # Matrices de poids Wc(40,1), Uc(40,40) et offset bc(40)\n",
        "    self.Wc = self.add_weight(shape=(self.dim_LSTM,dim_serie),initializer=\"normal\",name=\"Wc\",trainable=True)\n",
        "    self.Uc = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Uc\",trainable=True)\n",
        "    self.bc = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"zeros\",name=\"bc\",trainable=True)\n",
        "\n",
        "    # Paramètres de l'attention :\n",
        "    # ###########################\n",
        "    # Matrices de poids Wh(40,40); Wl(40,40) et Wh_t(40,40)\n",
        "    self.Wh = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Wh\",trainable=True)\n",
        "    self.Wl = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Wl\",trainable=True)\n",
        "    self.Wh_t = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Wh_t\",trainable=True)\n",
        "\n",
        "    # Vecteur contexte :  v(1,40)\n",
        "    #############################\n",
        "    self.v = self.add_weight(shape=(dim_serie,self.dim_LSTM),initializer=\"normal\",name=\"v\",trainable=True)\n",
        "\n",
        "    # Memory tape : Ct    Liste[(40,1)]\n",
        "    # #################################\n",
        "    memory_tape = getattr(self, 'memory_tape', None)\n",
        "    if memory_tape is None:\n",
        "      memory_tape = []\n",
        "      memory_tape.append(tf.zeros(shape=(self.dim_LSTM,1)))         # Ct[0] = c0 = 0\n",
        "    self.memory_tape = memory_tape\n",
        "\n",
        "    # Hidden tape : Ht    Liste[(40,1)]\n",
        "    # #################################\n",
        "    hidden_tape = getattr(self, 'hidden_tape', None)\n",
        "    if hidden_tape is None:\n",
        "      hidden_tape = []\n",
        "      hidden_tape.append(tf.zeros(shape=(self.dim_LSTM,1)))         # Ht[0] = h0 = 0\n",
        "    self.hidden_tape = hidden_tape\n",
        "\n",
        "    # Vecteurs d'adaptation du Hidden state : Ht_t    Liste[(40,1)]\n",
        "    # #############################################################\n",
        "    hidden_t_tape = getattr(self, 'hidden_t_tape', None)\n",
        "    if hidden_t_tape is None:\n",
        "      hidden_t_tape = []\n",
        "      hidden_t_tape.append(tf.zeros(shape=(self.dim_LSTM,1)))       # Ht_t[0] = h0_t = 0\n",
        "    self.hidden_t_tape = hidden_t_tape\n",
        "\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Définit la logique de la couche LSTM\n",
        "  # Arguments :   X : Tenseur d'entrée de dimension (None, taille_fenetre,dim_serie)\n",
        "  #               H : Tenseur des vecteurs cachés de la couche précédente (None,taille_fenetre,dim_LSTM,1)\n",
        "  def call(self,X,H):\n",
        "    # Charge les tables mémoires\n",
        "    Ct = self.memory_tape.copy()\n",
        "    Ht = self.hidden_tape.copy()\n",
        "    Ht_t = self.hidden_t_tape.copy()\n",
        "\n",
        "    for t in range(1,x.shape[1]+1):\n",
        "      xt = X[:,t-1,:]                      # (32,1)\n",
        "      xt = tf.expand_dims(xt,axis=-1)      # (32,1,1)\n",
        "      h = H[:,t-1,:,:]                     # (32,40,1)\n",
        "\n",
        "      # Calcul des poids d'attention avec les vecteurs cachés de la couche précédente\n",
        "      at_i = tf.zeros(shape=(x.shape[0],1,1))   # a0_0 = 0 (32,1,1)\n",
        "      for i in range(1,t):\n",
        "        at_ = tf.matmul(self.v,tf.keras.activations.tanh(tf.matmul(self.Wh,Ht[i]) + tf.matmul(self.Wl,h) + tf.matmul(self.Wh_t,Ht_t[t-1])))\n",
        "        at_i = tf.concat([at_,at_i],axis=1)\n",
        "      if t > 1:\n",
        "        at_i = tf.slice(at_i,[0,0,0],[at_i.shape[0],at_i.shape[1]-1,at_i.shape[2]])\n",
        "\n",
        "      # Calcul des poids d'attention normalisés\n",
        "      st_i = tf.keras.activations.softmax(at_i,axis=1)        # (32,t,1)\n",
        "\n",
        "      # Calcul du vecteur d'adaptation ht_t\n",
        "      ht_t = tf.zeros(shape=(x.shape[0],self.dim_LSTM,1))                                       # h0_t = 0\n",
        "      for i in range(1,t):\n",
        "        ht_t = ht_t + tf.multiply(tf.expand_dims(st_i[:,i-1,:],axis=-1),Ht[i])      # (32,40,1)\n",
        "\n",
        "      # Calcul du vecteur d'adaptation ct_t\n",
        "      ct_t = tf.zeros(shape=(x.shape[0],self.dim_LSTM,1))                           # c0_t = 0\n",
        "      for i in range(1,t):\n",
        "        ct_t = ct_t + tf.multiply(tf.expand_dims(st_i[:,i-1,:],axis=-1),Ct[i])      # (32,40,1)\n",
        "      \n",
        "      # Calcul du vecteur d'activation de la forget gate à l'instant t avec les entrées X\n",
        "      ft = tf.matmul(self.Wf,xt)                    # (40,1)x(32,1,1) = (32,40,1)\n",
        "      ft = ft + tf.matmul(self.Uf,ht_t)             # (40,40)x(32,40,1) = (32,40,1)\n",
        "      ft = ft + self.bf                             # (32,40,1) + (40,1) = (32,40,1)\n",
        "      ft = tf.keras.activations.sigmoid(ft) \n",
        "\n",
        "      # Calcul du vecteur d'activation de l'input gate à l'instant t avec les entrées X\n",
        "      it = tf.matmul(self.Wi,xt)                    # (40,1)x(32,1,1) = (32,40,1)\n",
        "      it = it + tf.matmul(self.Ui,ht_t)             # (40,40)x(32,40,1) = (32,40,1)\n",
        "      it = it + self.bi                             # (32,40,1) + (40,1) = (32,40,1)\n",
        "      it = tf.keras.activations.sigmoid(it)\n",
        "\n",
        "      # Calcul du vecteur d'activation de l'output gate à l'instant t avec les entrées X\n",
        "      ot = tf.matmul(self.Wo,xt)                    # (40,1)x(32,1,1) = (32,40,1)\n",
        "      ot = ot + tf.matmul(self.Uo,ht_t)             # (40,40)x(32,40,1) = (32,40,1)\n",
        "      ot = ot + self.bo                             # (32,40,1) + (40,1) = (32,40,1)\n",
        "      ot = tf.keras.activations.sigmoid(ot)\n",
        "\n",
        "      # Calcul du cell input activation vector à l'instant t avec les entrées X\n",
        "      ct_h = tf.matmul(self.Wc,xt)                 # (40,1)x(32,1,1) = (32,40,1)\n",
        "      ct_h = ct_h + tf.matmul(self.Uc,ht_t)        # (40,40)x(32,40,1) = (32,40,1)\n",
        "      ct_h = ct_h + self.bc                        # (32,40,1) + (40,1) = (32,40,1)\n",
        "      ct_h = tf.keras.activations.tanh(ct_h)\n",
        "\n",
        "      # Calcul du cell state à l'instant t avec les entrées X\n",
        "      ct = tf.multiply(ft,ct_t)                    # (32,40,1)\n",
        "      ct = ct + tf.multiply(it,ct_h)               # (32,40,1)\n",
        "      ht = tf.multiply(ot,tf.keras.activations.tanh(ct))\n",
        "\n",
        "      # Enregistre les vecteurs internes du temps courant\n",
        "      Ht.append(ht)                                 # Enregistre Ht[t] = ht\n",
        "      Ct.append(ct)                                 # Enregistre Ct[t] = ct\n",
        "      Ht_t.append(ht_t)                             # Enregistre Ht_t[t] = ht_t\n",
        "\n",
        "    # Enregistre les vecteurs internes du batch courant\n",
        "    for memory_tape_, memory_tape in zip(tf.nest.flatten(self.memory_tape),tf.nest.flatten(Ct)):\n",
        "      memory_tape_ = memory_tape\n",
        "    for hidden_tape_, hidden_tape in zip(tf.nest.flatten(self.hidden_tape),tf.nest.flatten(Ht)):\n",
        "      hidden_tape_ = hidden_tape\n",
        "    for hidden_t_tape_, hidden_t_tape in zip(tf.nest.flatten(self.hidden_t_tape),tf.nest.flatten(Ht_t)):\n",
        "      hidden_t_tape_ = hidden_t_tape\n",
        "\n",
        "    # Retourne le dernier hidden state ou l'ensemble des vecteurs\n",
        "    if self.return_sequence == False:\n",
        "      return tf.squeeze(ht,axis=-1)               # return (32,40)\n",
        "    else:\n",
        "      hidden_states = tf.zeros(shape=(x.shape[0],1,self.dim_LSTM,1))   # (32,1,40,1)\n",
        "      for i in range(0,x.shape[1]):\n",
        "        hidden_states = tf.concat([hidden_states,tf.expand_dims(Ht[i+1],axis=1)],axis=1)\n",
        "      hidden_states = tf.slice(hidden_states,[0,0,0,0],[hidden_states.shape[0],hidden_states.shape[1]-1,hidden_states.shape[2],hidden_states.shape[3]])\n",
        "      return hidden_states                        # return (32,20,40,1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCgn0V_PKeFq"
      },
      "source": [
        "**2. Création du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnU27YHpKeFs"
      },
      "source": [
        "dim_LSTM = 40\n",
        "\n",
        "# Fonction de la couche lambda d'entrée\n",
        "def Traitement_Entrees(x):\n",
        "  return tf.expand_dims(x,axis=-1)\n",
        "\n",
        "entrees_x = tf.keras.Input(shape=(taille_fenetre,), batch_size=batch_size)\n",
        "entrees_h = tf.keras.Input(shape=(taille_fenetre,dim_LSTM,1), batch_size=batch_size)\n",
        "\n",
        "x = tf.keras.layers.Lambda(Traitement_Entrees)(entrees_x)\n",
        "h = Couche_LSTMN(dim_LSTM=dim_LSTM,return_sequence=True)(x)\n",
        "\n",
        "sorties = Couche_LSTMN_Layers(dim_LSTM=dim_LSTM,return_sequence=False)(x,h)\n",
        "sorties = tf.keras.layers.Dense(1)(sorties)\n",
        "model = tf.keras.Model(entrees_x,sorties)\n",
        "\n",
        "model.save_weights('model_initial.h5')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQZrbpZJKeFu"
      },
      "source": [
        "**3. Entrainement du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmhIb0JUKeFv"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur, metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset_norm,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2s85jr4xKeFv"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 1])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLvbZdTBKeFw"
      },
      "source": [
        "# Charge les meilleurs poids\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuZvPbowKeFx"
      },
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "class TimingCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, logs={}):\n",
        "        self.logs=[]\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.starttime = timer()\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.logs.append(timer()-self.starttime)\n",
        "\n",
        "\n",
        "cb = TimingCallback()\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(lr=3e-2,momentum=0.9)\n",
        "optimiseur=tf.keras.optimizers.Adam()\n",
        "\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur,metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle\n",
        "historique = model.fit(dataset_norm,validation_data=dataset_Val_norm, epochs=500,verbose=1, callbacks=[CheckPoint,cb])\n",
        "\n",
        "print(cb.logs)\n",
        "print(sum(cb.logs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CB5qu-UKeFx"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ovY-xTgKeFz"
      },
      "source": [
        "**4. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iAsUciuKeFz"
      },
      "source": [
        "# Charge les meilleurs poids\n",
        "model.load_weights(\"poids_train.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9JaY8bdKeF0"
      },
      "source": [
        "taille_fenetre = 20\n",
        "\n",
        "# Création d'une liste vide pour recevoir les prédictions\n",
        "predictions = []\n",
        "\n",
        "# Calcul des prédiction pour chaque groupe de 20 valeurs consécutives de la série\n",
        "# dans l'intervalle de validation\n",
        "for t in temps[temps_separation:-taille_fenetre]:\n",
        "    X = np.reshape(Serie_Normalisee[t:t+taille_fenetre],(1,taille_fenetre))\n",
        "    predictions.append(model.predict(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDC0v2xTKeF0"
      },
      "source": [
        "# Affiche la série et les prédictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps,serie,label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title('Prédictions avec le modèle LSTMN')\n",
        "plt.show()\n",
        "\n",
        "# Zoom sur l'intervalle de validation\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps[temps_separation:],serie[temps_separation:],label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title(\"Prédictions avec le modèle LSTMN (zoom sur l'intervalle de validation)\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v8QjYSFKeF1"
      },
      "source": [
        "# Calcule de l'erreur quadratique moyenne et de l'erreur absolue moyenne \n",
        "\n",
        "mae = tf.keras.metrics.mean_absolute_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "mse = tf.keras.metrics.mean_squared_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "\n",
        "print(mae)\n",
        "print(mse)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}