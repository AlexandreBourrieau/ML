{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reseau_LSTMN.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/Reseau_LSTMN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tefj_HcCgH6c"
      },
      "source": [
        "# Long Short-Term Memory-Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubCeIvtF6R4W"
      },
      "source": [
        "Dans ce carnet nous allons mettre en place un modèle à réseau de neurones récurrent de type LSTMN pour réaliser des prédictions sur notre série temporelle.  \n",
        "Ce modèle est tiré du papier de recherche : [Long Short-Term Memory-Networks for Machine Reading](https://arxiv.org/pdf/1601.06733)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRhtHsNn5fc3"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFeah3y_6kif"
      },
      "source": [
        "# Création de la série temporelle et du dataset pour l'entrainement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJfSLtub6sdc"
      },
      "source": [
        "# Fonction permettant d'afficher une série temporelle\n",
        "def affiche_serie(temps, serie, format=\"-\", debut=0, fin=None, label=None):\n",
        "    plt.plot(temps[debut:fin], serie[debut:fin], format, label=label)\n",
        "    plt.xlabel(\"Temps\")\n",
        "    plt.ylabel(\"Valeur\")\n",
        "    if label:\n",
        "        plt.legend(fontsize=14)\n",
        "    plt.grid(True)\n",
        "\n",
        "# Fonction permettant de créer une tendance\n",
        "def tendance(temps, pente=0):\n",
        "    return pente * temps\n",
        "\n",
        "# Fonction permettant de créer un motif\n",
        "def motif_periodique(instants):\n",
        "    return (np.where(instants < 0.4,                            # Si les instants sont < 0.4\n",
        "                    np.cos(instants * 2 * np.pi),               # Alors on retourne la fonction cos(2*pi*t)\n",
        "                    1 / np.exp(3 * instants)))                  # Sinon, on retourne la fonction exp(-3t)\n",
        "\n",
        "# Fonction permettant de créer une saisonnalité avec un motif\n",
        "def saisonnalite(temps, periode, amplitude=1, phase=0):\n",
        "    \"\"\"Répétition du motif sur la même période\"\"\"\n",
        "    instants = ((temps + phase) % periode) / periode            # Mapping du temps =[0 1 2 ... 1460] => instants = [0.0 ... 1.0]\n",
        "    return amplitude * motif_periodique(instants)\n",
        "\n",
        "# Fonction permettant de générer du bruit gaussien N(0,1)\n",
        "def bruit_blanc(temps, niveau_bruit=1, graine=None):\n",
        "    rnd = np.random.RandomState(graine)\n",
        "    return rnd.randn(len(temps)) * niveau_bruit\n",
        "\n",
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "# au format X(X1,X2,...Xn) / Y(Y1,Y2,...,Yn)\n",
        "# X sont les données d'entrées du réseau\n",
        "# Y sont les labels\n",
        "\n",
        "def prepare_dataset_XY(serie, taille_fenetre, batch_size, buffer_melange):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie)\n",
        "  dataset = dataset.window(taille_fenetre+1, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre + 1))\n",
        "  dataset = dataset.map(lambda x: (x[:-1], x[-1:]))\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "# Création de la série temporelle\n",
        "temps = np.arange(4 * 365)                # temps = [0 1 2 .... 4*365] = [0 1 2 .... 1460]\n",
        "amplitude = 40                            # Amplitude de la la saisonnalité\n",
        "niveau_bruit = 5                          # Niveau du bruit\n",
        "offset = 10                               # Offset de la série\n",
        "\n",
        "serie = offset + tendance(temps, 0.1) + saisonnalite(temps, periode=365, amplitude=amplitude) + bruit_blanc(temps,niveau_bruit,graine=40)\n",
        "\n",
        "temps_separation = 1000\n",
        "\n",
        "# Extraction des temps et des données d'entrainement\n",
        "temps_entrainement = temps[:temps_separation]\n",
        "x_entrainement = serie[:temps_separation]\n",
        "\n",
        "# Exctraction des temps et des données de valiadation\n",
        "temps_validation = temps[temps_separation:]\n",
        "x_validation = serie[temps_separation:]\n",
        "\n",
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "taille_fenetre = 20\n",
        "batch_size = 32\n",
        "buffer_melange = 1000\n",
        "\n",
        "# Création du dataset X,Y\n",
        "dataset = prepare_dataset_XY(x_entrainement,taille_fenetre,batch_size,buffer_melange)\n",
        "\n",
        "# Création du dataset X,Y de validation\n",
        "dataset_Val = prepare_dataset_XY(x_validation,taille_fenetre,batch_size,buffer_melange)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6WVzU_X3JxG"
      },
      "source": [
        "# Calcul de la moyenne et de l'écart type de la série\n",
        "mean = tf.math.reduce_mean(serie)\n",
        "std = tf.math.reduce_std(serie)\n",
        "\n",
        "# Normalise les données\n",
        "Serie_Normalisee = (serie-mean)/std\n",
        "min = tf.math.reduce_min(serie)\n",
        "max = tf.math.reduce_max(serie)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYtsc0Yk3LhT"
      },
      "source": [
        "# Création des données pour l'entrainement et le test\n",
        "x_entrainement_norm = Serie_Normalisee[:temps_separation]\n",
        "x_validation_norm = Serie_Normalisee[temps_separation:]\n",
        "\n",
        "# Création du dataset X,Y\n",
        "dataset_norm = prepare_dataset_XY(x_entrainement_norm,taille_fenetre,batch_size,buffer_melange)\n",
        "\n",
        "# Création du dataset X,Y de validation\n",
        "dataset_Val_norm = prepare_dataset_XY(x_validation_norm,taille_fenetre,batch_size,buffer_melange)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Yt-EgZ3sgPY"
      },
      "source": [
        "# Création d'une couche LSTM personnelle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-C3-DsSAjtf"
      },
      "source": [
        "Pour commencer, regradons tout d'abord comment créer une couche LSTM en dérivant une classe Layers de Keras.  \n",
        "On utilise ici les ressources de wikipédia : [Algorithme d'une couche LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZoCohMaBJU4"
      },
      "source": [
        "<img src=\"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/images/LSTM.png?raw=true\" width=\"800\"> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxEXQ14sAAHw"
      },
      "source": [
        "**1. Création de la classe LSTM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_GJYIDRAlhu"
      },
      "source": [
        "Ce qu'il y a de nouveau lors de la création de la couche personnelle est qu'il faut maintenant avoir deux variables internes à la couches à mémoriser (les deux vecteurs internes - cell vector & hidden vector):\n",
        " - Ces vecteurs doivent être initialisés dans la fonction `build()`\n",
        " - Puis ils doivent être sauvegardés pour le batch courant à la fin de la fonction `call()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hhk0kmPSgqva"
      },
      "source": [
        "# Classe LSTM simple\n",
        "\n",
        "# Importe le Backend de Keras\n",
        "from keras import backend as K\n",
        "\n",
        "# Définit une nouvelle classe personnelle LSTM\n",
        "# https://en.wikipedia.org/wiki/Long_short-term_memory\n",
        "\n",
        "# Dans les dimensions ci-dessous :\n",
        "# dimension de la série : dim_serie = 1\n",
        "# dimension des vecteurs internes LSTM : dim_LSTM = 40\n",
        "\n",
        "class Couche_LSTM(tf.keras.layers.Layer):\n",
        "  # Fonction d'initialisation de la classe d'attention\n",
        "  def __init__(self,dim_LSTM, regulariser = 0.0):\n",
        "    self.dim_LSTM = dim_LSTM        # Dimension du vecteur d'attention\n",
        "    self.regulariser = regulariser\n",
        "    super().__init__()              # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    dim_serie = input_shape[2]\n",
        "\n",
        "    # Paramètres de la forget gate :\n",
        "    # ##############################\n",
        "    # Matrices de poids Wf(40,1), Uf(40,40) et offset bf(40)\n",
        "    self.Wf = self.add_weight(shape=(self.dim_LSTM,dim_serie),initializer=\"normal\",name=\"Wf\",trainable=True,regularizer=tf.keras.regularizers.l2(self.regulariser))\n",
        "    self.Uf = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Uf\",trainable=True)\n",
        "    self.bf = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"zeros\",name=\"bf\",trainable=True)\n",
        "\n",
        "    # Paramètres de l'input gate :\n",
        "    # ##############################\n",
        "    # Matrices de poids Wi(40,1), Ui(40,40) et offset bi(40)\n",
        "    self.Wi = self.add_weight(shape=(self.dim_LSTM,dim_serie),initializer=\"normal\",name=\"Wi\",trainable=True, regularizer=tf.keras.regularizers.l2(self.regulariser))\n",
        "    self.Ui = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Ui\",trainable=True)\n",
        "    self.bi = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"zeros\",name=\"bi\",trainable=True)\n",
        "\n",
        "    # Paramètres de l'output gate :\n",
        "    # ##############################\n",
        "    # Matrices de poids Wo(40,1), Uo(40,40) et offset bo(40)\n",
        "    self.Wo = self.add_weight(shape=(self.dim_LSTM,dim_serie),initializer=\"normal\",name=\"Wo\",trainable=True, regularizer=tf.keras.regularizers.l2(self.regulariser))\n",
        "    self.Uo = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Uo\",trainable=True)\n",
        "    self.bo = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"normal\",name=\"bo\",trainable=True)\n",
        "\n",
        "    # Paramètres du cell vector :\n",
        "    # ##############################\n",
        "    # Matrices de poids Wo(40,1), Uo(40,40) et offset bo(40)\n",
        "    self.Wc = self.add_weight(shape=(self.dim_LSTM,dim_serie),initializer=\"normal\",name=\"Wc\",trainable=True, regularizer=tf.keras.regularizers.l2(self.regulariser))\n",
        "    self.Uc = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Uc\",trainable=True)\n",
        "    self.bc = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"zeros\",name=\"bc\",trainable=True)\n",
        "\n",
        "    # Vecteurs Cell states :\n",
        "    # ######################\n",
        "    cell_states = getattr(self, 'cell_states', None)\n",
        "    if cell_states is None:\n",
        "      cell_states = []\n",
        "      cell_states.append(tf.zeros(shape=(self.dim_LSTM,1)))\n",
        "    self.cell_states = cell_states\n",
        "\n",
        "    # Vecteurs Hidden states :\n",
        "    # ########################\n",
        "    hidden_states = getattr(self, 'hidden_states', None)\n",
        "    if hidden_states is None:\n",
        "      hidden_states = []\n",
        "      hidden_states.append(tf.zeros(shape=(self.dim_LSTM,1)))\n",
        "    self.hidden_states = hidden_states\n",
        "\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Définit la logique de la couche LSTM\n",
        "  # Arguments :   x : Tenseur d'entrée de dimension (None, taille_fenetre,dim_serie)\n",
        "  def call(self,x):\n",
        "    # Charge les vecteurs internes\n",
        "    cell_states = self.cell_states.copy()\n",
        "    hidden_states = self.hidden_states.copy()\n",
        "    ct = cell_states[0]\n",
        "    ht = hidden_states[0]\n",
        "\n",
        "    for t in range(0,x.shape[1]):\n",
        "      xt = x[:,t,:]                             # (32,dim_serie)\n",
        "      xt = tf.expand_dims(xt,axis=-1)           # (32,dim_serie,1)\n",
        "\n",
        "      # Calcul du vecteur d'activation de la forget gate à l'instant t\n",
        "      ft = tf.matmul(self.Wf,xt)                # (40,1)x(32,1,1) = (32,40,1)\n",
        "      ft = ft + tf.matmul(self.Uf,ht)      # (32,40,1)\n",
        "      ft = ft + self.bf                         # (32,40,1)\n",
        "      ft = tf.keras.activations.sigmoid(ft)\n",
        "\n",
        "      # Calcul du vecteur d'activation de l'input gate à l'instant t\n",
        "      it = tf.matmul(self.Wi,xt)                # (32,40,1)\n",
        "      it = it + tf.matmul(self.Ui,ht)      # (32,40,1)\n",
        "      it = it + self.bi                         # (32,40,1)\n",
        "      it = tf.keras.activations.sigmoid(it)\n",
        "\n",
        "      # Calcul du vecteur d'activation de l'output gate à l'instant t\n",
        "      ot = tf.matmul(self.Wo,xt)                # (32,40,1)\n",
        "      ot = ot + tf.matmul(self.Uo,ht)      # (32,40,1)\n",
        "      ot = ot + self.bo                         # (32,40,1)\n",
        "      ot = tf.keras.activations.sigmoid(ot)\n",
        "\n",
        "      # Calcul du cell input activation vector à l'instant t\n",
        "      ct_t = tf.matmul(self.Wc,xt)                 # (32,40,1)\n",
        "      ct_t = ct_t + tf.matmul(self.Uc,ht)          # (32,40,1)\n",
        "      ct_t = ct_t + self.bc                        # (32,40,1)\n",
        "      ct_t = tf.keras.activations.tanh(ct_t)\n",
        "\n",
        "      # Calcul du cell state à l'instant t\n",
        "      ct = tf.multiply(ft,ct)                     # (32,40,1)\n",
        "      ct = ct + tf.multiply(it,ct_t)              # (32,40,1)\n",
        "      ht = tf.multiply(ot,tf.keras.activations.tanh(ct))\n",
        "\n",
        "      # Enregistre les vecteurs internes au temps courant\n",
        "      if (t==0):\n",
        "        hidden_states[t] = ht\n",
        "        cell_states[t] = ct\n",
        "      else:\n",
        "        hidden_states.append(ht)\n",
        "        cell_states.append(ct)\n",
        "\n",
        "    # Enregistre les vecteurs internes du batch courant\n",
        "    for hidden_state_, hidden_state in zip(tf.nest.flatten(self.hidden_states),tf.nest.flatten(hidden_states)):\n",
        "      hidden_states_ = hidden_state\n",
        "\n",
        "    for cell_state_, cell_state in zip(tf.nest.flatten(self.cell_states),tf.nest.flatten(cell_states)):\n",
        "      cell_state_ = cell_state\n",
        "\n",
        "    # Retourne le dernier hidden state\n",
        "    return tf.squeeze(ht,axis=-1)               # return (32,40)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4Rok0lzsron"
      },
      "source": [
        "dim_LSTM = 40\n",
        "\n",
        "# Fonction de la couche lambda d'entrée\n",
        "def Traitement_Entrees(x):\n",
        "  return tf.expand_dims(x,axis=-1)\n",
        "\n",
        "# Définition de l'entrée du modèle\n",
        "entrees = tf.keras.layers.Input(shape=(taille_fenetre),batch_size=batch_size)\n",
        "\n",
        "# Encodeur\n",
        "e_adapt = tf.keras.layers.Lambda(Traitement_Entrees)(entrees)\n",
        "s_encodeur = Couche_LSTM(dim_LSTM,regulariser=1e-5)(e_adapt)\n",
        "\n",
        "# Décodeur\n",
        "s_decodeur = tf.keras.layers.Dense(dim_LSTM,activation=\"tanh\")(s_encodeur)\n",
        "s_decodeur = tf.keras.layers.Concatenate()([s_decodeur,s_encodeur])\n",
        "\n",
        "# Générateur\n",
        "sortie = tf.keras.layers.Dense(1)(s_decodeur)\n",
        "\n",
        "# Construction du modèle\n",
        "model = tf.keras.Model(entrees,sortie)\n",
        "\n",
        "model.save_weights(\"model_initial.hdf5\")\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUM0-SSXGLIQ"
      },
      "source": [
        "**2. Optimisation du taux d'apprentissage**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jejCBhXVuNQ4"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur, metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset_norm,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1_WMNlzu2B4"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 1])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XdXh_b0GP_F"
      },
      "source": [
        "**3. Entrainement du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80pEtJ10wIfY"
      },
      "source": [
        "# Charge les meilleurs poids\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16ujUiELwR33"
      },
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "class TimingCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, logs={}):\n",
        "        self.n_steps = 0\n",
        "        self.t_step = 0\n",
        "        self.n_batch = 0\n",
        "        self.total_batch = 0\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.starttime = timer()\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.t_step = self.t_step  + timer()-self.starttime\n",
        "        self.n_steps = self.n_steps + 1\n",
        "        if (self.total_batch == 0):\n",
        "          self.total_batch=self.n_batch - 1\n",
        "    def on_train_batch_begin(self,batch,logs=None):\n",
        "      self.n_batch= self.n_batch + 1\n",
        "    def GetInfos(self):\n",
        "      return([self.t_step/(self.n_steps*self.total_batch), self.t_step, self.total_batch])\n",
        "\n",
        "cb = TimingCallback()\n",
        "\n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.01,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0.01)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(learning_rate=lr_schedule,momentum=0.9)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_entrainement.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur,metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle\n",
        "historique = model.fit(dataset_norm,validation_data=dataset_Val_norm, epochs=500,verbose=1, callbacks=[CheckPoint,cb])\n",
        "\n",
        "# Affiche quelques informations sur les timings\n",
        "infos = cb.GetInfos()\n",
        "print(\"Step time : %.3f\" %infos[0])\n",
        "print(\"Total time : %.3f\" %infos[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COP9u4yitvmw"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o2zh6N9t1b7"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement[400:500])),erreur_entrainement[400:500], label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement[400:500])),erreur_validation[400:500], label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6Gq2CkeGR_1"
      },
      "source": [
        "**4. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRxXtHRXGXfF"
      },
      "source": [
        "taille_fenetre = 20\n",
        "\n",
        "# Création d'une liste vide pour recevoir les prédictions\n",
        "predictions = []\n",
        "\n",
        "# Calcul des prédiction pour chaque groupe de 20 valeurs consécutives de la série\n",
        "# dans l'intervalle de validation\n",
        "for t in temps[temps_separation:-taille_fenetre]:\n",
        "    X = np.reshape(Serie_Normalisee[t:t+taille_fenetre],(1,taille_fenetre))\n",
        "    predictions.append(model.predict(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd2nfDcgG8EO"
      },
      "source": [
        "# Affiche la série et les prédictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps,serie,label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title('Prédictions avec le modèle GRU + Attention avec vecteur contexte')\n",
        "plt.show()\n",
        "\n",
        "# Zoom sur l'intervalle de validation\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps[temps_separation:],serie[temps_separation:],label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title(\"Prédictions avec le modèle GRU + Attention avec vecteur contexte (zoom sur l'intervalle de validation)\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDS7BJvZG_e1"
      },
      "source": [
        "# Calcule de l'erreur quadratique moyenne et de l'erreur absolue moyenne \n",
        "\n",
        "mae = tf.keras.metrics.mean_absolute_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "mse = tf.keras.metrics.mean_squared_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "\n",
        "print(mae)\n",
        "print(mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2cMi-70AJ5-"
      },
      "source": [
        "# Création de la couche LSTMN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gXNei98G3J9"
      },
      "source": [
        "<img src=\"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/images/LSTMN_Calcul2.png?raw=true\" width=\"1200\"> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNuSRvEoqEhq"
      },
      "source": [
        "**1. Création de la couche LSTMN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-LL0N5_qCQe"
      },
      "source": [
        "# Classe LSTMN\n",
        "\n",
        "# Importe le Backend de Keras\n",
        "from keras import backend as K\n",
        "\n",
        "# Définit une nouvelle classe personnelle LSTMN\n",
        "# https://arxiv.org/pdf/1601.06733.pdf\n",
        "\n",
        "\n",
        "# Dans les dimensions ci-dessous :\n",
        "# dimension de la série : dim_serie = 1\n",
        "# dimension des vecteurs internes LSTM : dim_LSTM = 40\n",
        "# batch_size = 32\n",
        "\n",
        "class Couche_LSTMN(tf.keras.layers.Layer):\n",
        "  # Fonction d'initialisation de la classe d'attention\n",
        "  def __init__(self,dim_LSTM,return_sequence=False,regulariser=0.0):\n",
        "    self.dim_LSTM = dim_LSTM                # Dimension du vecteur d'attention\n",
        "    self.return_sequence = return_sequence  # Retourne l'ensemble des états cachés ?\n",
        "    self.regulariser = regulariser\n",
        "    super().__init__()                      # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    dim_serie = input_shape[2]\n",
        "    nbr_instants = input_shape[1]\n",
        "\n",
        "    # Paramètres de la forget gate :\n",
        "    # ##############################\n",
        "    # Matrices de poids Wf(40,1), Uf(40,40) et offset bf(40)\n",
        "    self.Wf = self.add_weight(shape=(self.dim_LSTM,dim_serie),initializer=\"normal\",name=\"Wf\",trainable=True,regularizer=tf.keras.regularizers.l2(self.regulariser))\n",
        "    self.Uf = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Uf\",trainable=True)\n",
        "    self.bf = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"zeros\",name=\"bf\",trainable=True)\n",
        "\n",
        "    # Paramètres de l'input gate :\n",
        "    # ##############################\n",
        "    # Matrices de poids Wi(40,1), Ui(40,40) et offset bi(40)\n",
        "    self.Wi = self.add_weight(shape=(self.dim_LSTM,dim_serie),initializer=\"normal\",name=\"Wi\",trainable=True,regularizer=tf.keras.regularizers.l2(self.regulariser))\n",
        "    self.Ui = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Ui\",trainable=True)\n",
        "    self.bi = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"zeros\",name=\"bi\",trainable=True)\n",
        "\n",
        "    # Paramètres de l'output gate :\n",
        "    # ##############################\n",
        "    # Matrices de poids Wo(40,1), Uo(40,40) et offset bo(40)\n",
        "    self.Wo = self.add_weight(shape=(self.dim_LSTM,dim_serie),initializer=\"normal\",name=\"Wo\",trainable=True,regularizer=tf.keras.regularizers.l2(self.regulariser))\n",
        "    self.Uo = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Uo\",trainable=True)\n",
        "    self.bo = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"zeros\",name=\"bo\",trainable=True)\n",
        "\n",
        "    # Paramètres du cell vector :\n",
        "    # ##############################\n",
        "    # Matrices de poids Wc(40,1), Uc(40,40) et offset bc(40)\n",
        "    self.Wc = self.add_weight(shape=(self.dim_LSTM,dim_serie),initializer=\"normal\",name=\"Wc\",trainable=True,regularizer=tf.keras.regularizers.l2(self.regulariser))\n",
        "    self.Uc = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Uc\",trainable=True)\n",
        "    self.bc = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"zeros\",name=\"bc\",trainable=True)\n",
        "\n",
        "    # Paramètres de l'attention :\n",
        "    # ###########################\n",
        "    # Matrices de poids Wh(40,40); Wx(40,1) et Wh_t(40,40)\n",
        "    self.Wh = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Wh\",trainable=True)\n",
        "    self.Wx = self.add_weight(shape=(self.dim_LSTM,dim_serie),initializer=\"normal\",name=\"Wx\",trainable=True)\n",
        "    self.Wh_t = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Wh_t\",trainable=True)\n",
        "\n",
        "    # Vecteur contexte :  v(1,40)\n",
        "    #############################\n",
        "    self.v = self.add_weight(shape=(dim_serie,self.dim_LSTM),initializer=\"normal\",name=\"v\",trainable=True)\n",
        "\n",
        "    # Memory tape : Ct    Liste[(40,1)]\n",
        "    # #################################\n",
        "    memory_tape = getattr(self, 'memory_tape', None)\n",
        "    if memory_tape is None:\n",
        "      memory_tape = []\n",
        "      memory_tape.append(tf.zeros(shape=(self.dim_LSTM,1)))         # Ct[0] = c0 = 0\n",
        "    self.memory_tape = memory_tape\n",
        "\n",
        "    # Hidden tape : Ht    Liste[(40,1)]\n",
        "    # #################################\n",
        "    hidden_tape = getattr(self, 'hidden_tape', None)\n",
        "    if hidden_tape is None:\n",
        "      hidden_tape = []\n",
        "      hidden_tape.append(tf.zeros(shape=(self.dim_LSTM,1)))         # Ht[0] = h0 = 0\n",
        "    self.hidden_tape = hidden_tape\n",
        "\n",
        "    # Vecteurs d'adaptation du Hidden state : Ht_t    Liste[(40,1)]\n",
        "    # #############################################################\n",
        "    hidden_t_tape = getattr(self, 'hidden_t_tape', None)\n",
        "    if hidden_t_tape is None:\n",
        "      hidden_t_tape = []\n",
        "      hidden_t_tape.append(tf.zeros(shape=(self.dim_LSTM,1)))       # Ht_t[0] = h0_t = 0\n",
        "    self.hidden_t_tape = hidden_t_tape\n",
        "\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Définit la logique de la couche LSTM\n",
        "  # Arguments :   x : Tenseur d'entrée de dimension (None, taille_fenetre,dim_serie)\n",
        "  def call(self,x):\n",
        "\n",
        "    # Charge les tables mémoires\n",
        "    Ct = self.memory_tape.copy()\n",
        "    Ht = self.hidden_tape.copy()\n",
        "    Ht_t = self.hidden_t_tape.copy()\n",
        "\n",
        "    for t in range(1,x.shape[1]+1):\n",
        "      xt = x[:,t-1,:]                           # (32,1)\n",
        "      xt = tf.expand_dims(xt,axis=-1)           # (32,1,1)\n",
        "\n",
        "      # Calcul des poids d'attention\n",
        "      at_i = tf.zeros(shape=(x.shape[0],1,1))   # a0_0 = 0 (32,1,1)\n",
        "      for i in range(1,t):\n",
        "        at_ = tf.matmul(self.v,tf.keras.activations.tanh(tf.matmul(self.Wh,Ht[i]) + tf.matmul(self.Wx,xt) + tf.matmul(self.Wh_t,Ht_t[t-1])))\n",
        "        at_i = tf.concat([at_,at_i],axis=1)\n",
        "      if t > 1:\n",
        "        at_i = tf.slice(at_i,[0,0,0],[at_i.shape[0],at_i.shape[1]-1,at_i.shape[2]])\n",
        "\n",
        "      # Calcul des poids d'attention normalisés\n",
        "      st_i = tf.keras.activations.softmax(at_i,axis=1)        # (32,t,1)\n",
        "\n",
        "      # Calcul du vecteur d'adaptation ht_t\n",
        "      ht_t = tf.zeros(shape=(x.shape[0],self.dim_LSTM,1))                                       # h0_t = 0\n",
        "      for i in range(1,t):\n",
        "        ht_t = ht_t + tf.multiply(tf.expand_dims(st_i[:,i-1,:],axis=-1),Ht[i])      # (32,40,1)\n",
        "\n",
        "      # Calcul du vecteur d'adaptation ct_t\n",
        "      ct_t = tf.zeros(shape=(x.shape[0],self.dim_LSTM,1))                           # c0_t = 0\n",
        "      for i in range(1,t):\n",
        "        ct_t = ct_t + tf.multiply(tf.expand_dims(st_i[:,i-1,:],axis=-1),Ct[i])      # (32,40,1)\n",
        "      \n",
        "      # Calcul du vecteur d'activation de la forget gate à l'instant t\n",
        "      ft = tf.matmul(self.Wf,xt)                    # (40,1)x(32,1,1) = (32,40,1)\n",
        "      ft = ft + tf.matmul(self.Uf,ht_t)             # (40,40)x(32,40,1) = (32,40,1)\n",
        "      ft = ft + self.bf                             # (32,40,1) + (40,1) = (32,40,1)\n",
        "      ft = tf.keras.activations.sigmoid(ft) \n",
        "\n",
        "      # Calcul du vecteur d'activation de l'input gate à l'instant t\n",
        "      it = tf.matmul(self.Wi,xt)                    # (40,1)x(32,1,1) = (32,40,1)\n",
        "      it = it + tf.matmul(self.Ui,ht_t)             # (40,40)x(32,40,1) = (32,40,1)\n",
        "      it = it + self.bi                             # (32,40,1) + (40,1) = (32,40,1)\n",
        "      it = tf.keras.activations.sigmoid(it)\n",
        "\n",
        "      # Calcul du vecteur d'activation de l'output gate à l'instant t\n",
        "      ot = tf.matmul(self.Wo,xt)                    # (40,1)x(32,1,1) = (32,40,1)\n",
        "      ot = ot + tf.matmul(self.Uo,ht_t)             # (40,40)x(32,40,1) = (32,40,1)\n",
        "      ot = ot + self.bo                             # (32,40,1) + (40,1) = (32,40,1)\n",
        "      ot = tf.keras.activations.sigmoid(ot)\n",
        "\n",
        "      # Calcul du cell input activation vector à l'instant t\n",
        "      ct_h = tf.matmul(self.Wc,xt)                 # (40,1)x(32,1,1) = (32,40,1)\n",
        "      ct_h = ct_h + tf.matmul(self.Uc,ht_t)        # (40,40)x(32,40,1) = (32,40,1)\n",
        "      ct_h = ct_h + self.bc                        # (32,40,1) + (40,1) = (32,40,1)\n",
        "      ct_h = tf.keras.activations.tanh(ct_h)\n",
        "\n",
        "      # Calcul du cell state à l'instant t\n",
        "      ct = tf.multiply(ft,ct_t)                    # (32,40,1)\n",
        "      ct = ct + tf.multiply(it,ct_h)               # (32,40,1)\n",
        "      ht = tf.multiply(ot,tf.keras.activations.tanh(ct))\n",
        "\n",
        "      # Enregistre les vecteurs internes du temps courant\n",
        "      Ht.append(ht)                                 # Enregistre Ht[t] = ht\n",
        "      Ct.append(ct)                                 # Enregistre Ct[t] = ct\n",
        "      Ht_t.append(ht_t)                             # Enregistre Ht_t[t] = ht_t\n",
        "\n",
        "    # Enregistre les vecteurs internes du batch courant\n",
        "    for memory_tape_, memory_tape in zip(tf.nest.flatten(self.memory_tape),tf.nest.flatten(Ct)):\n",
        "      memory_tape_ = memory_tape\n",
        "    for hidden_tape_, hidden_tape in zip(tf.nest.flatten(self.hidden_tape),tf.nest.flatten(Ht)):\n",
        "      hidden_tape_ = hidden_tape\n",
        "    for hidden_t_tape_, hidden_t_tape in zip(tf.nest.flatten(self.hidden_t_tape),tf.nest.flatten(Ht_t)):\n",
        "      hidden_t_tape_ = hidden_t_tape\n",
        "\n",
        "    # Retourne le dernier hidden state ou l'ensemble des vecteurs\n",
        "    if self.return_sequence == False:\n",
        "      return tf.squeeze(ht,axis=-1)               # return (32,40)\n",
        "    else:\n",
        "      hidden_states = tf.zeros(shape=(x.shape[0],1,self.dim_LSTM,1))   # (32,1,40,1)\n",
        "      for i in range(0,x.shape[1]):\n",
        "        hidden_states = tf.concat([hidden_states,tf.expand_dims(Ht[i+1],axis=1)],axis=1)\n",
        "      hidden_states = tf.slice(hidden_states,[0,0,0,0],[hidden_states.shape[0],hidden_states.shape[1]-1,hidden_states.shape[2],hidden_states.shape[3]])\n",
        "      return (hidden_states)                        # return (32,20,40,1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k38UWQnZ6lv7"
      },
      "source": [
        "# Prédicitons avec une couche LSTMN sans attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l95xZ4dChGsm"
      },
      "source": [
        "**1. Création du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHWmpyaOUC8U"
      },
      "source": [
        "dim_LSTM = 40\n",
        "\n",
        "# Fonction de la couche lambda d'entrée\n",
        "def Traitement_Entrees(x):\n",
        "  return tf.expand_dims(x,axis=-1)\n",
        "\n",
        "# Définition de l'entrée du modèle\n",
        "entrees = tf.keras.layers.Input(shape=(taille_fenetre),batch_size=batch_size)\n",
        "\n",
        "# Encodeur\n",
        "e_adapt = tf.keras.layers.Lambda(Traitement_Entrees)(entrees)\n",
        "s_encodeur = Couche_LSTMN(dim_LSTM,return_sequence=False,regulariser=1e-5)(e_adapt)\n",
        "\n",
        "# Décodeur\n",
        "s_decodeur = tf.keras.layers.Dense(dim_LSTM,activation=\"tanh\")(s_encodeur)\n",
        "s_decodeur = tf.keras.layers.Concatenate()([s_decodeur,s_encodeur])\n",
        "\n",
        "# Générateur\n",
        "sortie = tf.keras.layers.Dense(1)(s_decodeur)\n",
        "\n",
        "# Construction du modèle\n",
        "model = tf.keras.Model(entrees,sortie)\n",
        "\n",
        "model.save_weights(\"model_initial.hdf5\")\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZGh7uKYhJHF"
      },
      "source": [
        "**2. Entrainement du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REdXsfkohNTw"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "#optimiseur=tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n",
        "optimiseur=tf.keras.optimizers.Adam()\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur, metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset_norm,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6UF44BnpVtc"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 1])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLGfH1jGpbkT"
      },
      "source": [
        "# Charge les meilleurs poids\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VsRDR-8pery"
      },
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "class TimingCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, logs={}):\n",
        "        self.logs=[]\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.starttime = timer()\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.logs.append(timer()-self.starttime)\n",
        "\n",
        "\n",
        "cb = TimingCallback()\n",
        "\n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.01,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0.01\n",
        "    )\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(learning_rate=lr_schedule,momentum=0.9)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur,metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle\n",
        "historique = model.fit(dataset_norm,validation_data=dataset_Val_norm, epochs=30,verbose=1, callbacks=[CheckPoint,cb])\n",
        "\n",
        "print(cb.logs)\n",
        "print(sum(cb.logs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZsv4CMjucWS"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrveY5aRsnRe"
      },
      "source": [
        "**3. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPTggVTss0rt"
      },
      "source": [
        "taille_fenetre = 20\n",
        "\n",
        "# Création d'une liste vide pour recevoir les prédictions\n",
        "predictions = []\n",
        "\n",
        "# Calcul des prédiction pour chaque groupe de 20 valeurs consécutives de la série\n",
        "# dans l'intervalle de validation\n",
        "for t in temps[temps_separation:-taille_fenetre]:\n",
        "    X = np.reshape(Serie_Normalisee[t:t+taille_fenetre],(1,taille_fenetre))\n",
        "    predictions.append(model.predict(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYzyz6EMs72l"
      },
      "source": [
        "# Affiche la série et les prédictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps,serie,label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title('Prédictions avec le modèle LSTMN sans attention')\n",
        "plt.show()\n",
        "\n",
        "# Zoom sur l'intervalle de validation\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps[temps_separation:],serie[temps_separation:],label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title(\"Prédictions avec le modèle LSTMN sans attention (zoom sur l'intervalle de validation)\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZYKVb2Us-b_"
      },
      "source": [
        "# Calcule de l'erreur quadratique moyenne et de l'erreur absolue moyenne \n",
        "\n",
        "mae = tf.keras.metrics.mean_absolute_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "mse = tf.keras.metrics.mean_squared_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "\n",
        "print(mae)\n",
        "print(mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF-4sUY061Gz"
      },
      "source": [
        "# Prédicitons avec une couche LSTMN et attention simple"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro8XCyRN61HE"
      },
      "source": [
        "# Classe d'attention simple\n",
        "# Applique les poids d'attention sur les vecteurs de la couche récurrente\n",
        "\n",
        "# Importe le Backend de Keras\n",
        "from keras import backend as K\n",
        "\n",
        "# Définit une nouvelle classe Couche_Attention\n",
        "# Héritée de la classe Layer de Keras\n",
        "\n",
        "class Couche_Attention(tf.keras.layers.Layer):\n",
        "  # Fonction d'initialisation de la classe d'attention\n",
        "  def __init__(self):\n",
        "    super().__init__()          # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.w = self.add_weight(shape=(input_shape[2],1),initializer=\"normal\",name=\"w\")\n",
        "    self.b = self.add_weight(shape=(input_shape[1],1),initializer=\"zeros\",name=\"b\")\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Définit la logique de la couche d'attention\n",
        "  # Arguments :   x : Tenseur d'entrée de dimension (None, nbr_v,dim)\n",
        "  def call(self,x):\n",
        "    e = tf.matmul(x,self.w) + self.b                # (32,20,40)x(40,1) + (20,1) = (32,20,1)\n",
        "    e = K.tanh(e)                                   # (32,20,1)\n",
        "    a = tf.keras.activations.softmax(e,axis=1)      # (32,20,1)\n",
        "    xa = tf.multiply(x,a)                           # (32,20,40)\n",
        "    sortie = K.sum(xa,axis=1)                       # (32,40)\n",
        "    return sortie"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5PLnJic61HF"
      },
      "source": [
        "**2. Création du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxdHyxB861HG"
      },
      "source": [
        "dim_LSTM = 40\n",
        "\n",
        "# Fonction de la couche lambda d'entrée\n",
        "def Traitement_Entrees(x):\n",
        "  return tf.expand_dims(x,axis=-1)\n",
        "\n",
        "# Définition de l'entrée du modèle\n",
        "entrees = tf.keras.layers.Input(shape=(taille_fenetre),batch_size=batch_size)\n",
        "\n",
        "# Encodeur\n",
        "e_adapt = tf.keras.layers.Lambda(Traitement_Entrees)(entrees)\n",
        "s_encodeur = Couche_LSTMN(dim_LSTM,return_sequence=True,regulariser=1e-5)(e_adapt)\n",
        "s_attention = Couche_Attention()(tf.squeeze(s_encodeur,axis=-1))\n",
        "\n",
        "# Décodeur\n",
        "s_decodeur = tf.keras.layers.Dense(dim_LSTM,activation=\"tanh\")(s_attention)\n",
        "s_decodeur = tf.keras.layers.Concatenate()([s_decodeur,s_attention])\n",
        "\n",
        "# Générateur\n",
        "sortie = tf.keras.layers.Dense(1)(s_decodeur)\n",
        "\n",
        "# Construction du modèle\n",
        "model = tf.keras.Model(entrees,sortie)\n",
        "\n",
        "model.save_weights(\"model_initial.hdf5\")\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ibh4iHkO61HH"
      },
      "source": [
        "**3. Entrainement du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJZ0Yy8h61HI"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur, metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset_norm,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TD-GSPd761HK"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 1])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TV-egeu61HL"
      },
      "source": [
        "# Charge les meilleurs poids\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Pu8ACXT61HL"
      },
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "class TimingCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, logs={}):\n",
        "        self.logs=[]\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.starttime = timer()\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.logs.append(timer()-self.starttime)\n",
        "\n",
        "\n",
        "cb = TimingCallback()\n",
        "\n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.006,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0.01\n",
        "    )\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(learning_rate=lr_schedule,momentum=0.9)\n",
        "\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur,metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle\n",
        "historique = model.fit(dataset_norm,validation_data=dataset_Val_norm, epochs=30,verbose=1, callbacks=[CheckPoint,cb])\n",
        "\n",
        "print(cb.logs)\n",
        "print(sum(cb.logs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVaRZVK761HM"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBTC1eEu61HN"
      },
      "source": [
        "**4. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3He_TROd61HN"
      },
      "source": [
        "taille_fenetre = 20\n",
        "\n",
        "# Création d'une liste vide pour recevoir les prédictions\n",
        "predictions = []\n",
        "\n",
        "# Calcul des prédiction pour chaque groupe de 20 valeurs consécutives de la série\n",
        "# dans l'intervalle de validation\n",
        "for t in temps[temps_separation:-taille_fenetre]:\n",
        "    X = np.reshape(Serie_Normalisee[t:t+taille_fenetre],(1,taille_fenetre))\n",
        "    predictions.append(model.predict(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CV2R04xZ61HO"
      },
      "source": [
        "# Affiche la série et les prédictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps,serie,label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title('Prédictions avec le modèle LSTMN')\n",
        "plt.show()\n",
        "\n",
        "# Zoom sur l'intervalle de validation\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps[temps_separation:],serie[temps_separation:],label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title(\"Prédictions avec le modèle LSTMN (zoom sur l'intervalle de validation)\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3RNxAo461HO"
      },
      "source": [
        "# Calcule de l'erreur quadratique moyenne et de l'erreur absolue moyenne \n",
        "\n",
        "mae = tf.keras.metrics.mean_absolute_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "mse = tf.keras.metrics.mean_squared_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "\n",
        "print(mae)\n",
        "print(mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GP93ANo1XU3"
      },
      "source": [
        "# Création du modèle LSTMN multi-couches - Sans \"skip connections\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yFPByTxT_QW"
      },
      "source": [
        "La structure reste sensiblement la même qu'avant sauf que maintenant :\n",
        " - Les états cachés sont remontés en sortie vers la prochaine couche\n",
        " - L'attention dans les couches supérieures est calculée non plus à partir des valeurs d'entrées mais des anciens états cachés.\n",
        " - Il y a donc une nouvelle matrice de poids qui intervient dans le calcul de l'attention : Wl\n",
        " - Les matrices Wf, Wi, Wo et Wc changent également de dimension pour s'adapter à leurs vecteurs d'entrées qui sont maintenant les vecteurs cachés.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_bvPbrY1XU4"
      },
      "source": [
        "<img src=\"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/images/LSTMN_Calcul5.png?raw=true\" width=\"1200\"> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUMDw7lR1XU4"
      },
      "source": [
        "**1. Création de la multi-couche LSTMN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_tVN9sG1XU5"
      },
      "source": [
        "# Classe Multi couche LSTMN\n",
        "\n",
        "# Importe le Backend de Keras\n",
        "from keras import backend as K\n",
        "\n",
        "# Définit une nouvelle classe personnelle LSTMN\n",
        "# https://arxiv.org/pdf/1601.06733.pdf\n",
        "\n",
        "\n",
        "# Dans les dimensions ci-dessous :\n",
        "# dimension de la série : dim_serie = 1\n",
        "# dimension des vecteurs internes LSTM : dim_LSTM = 40\n",
        "# batch_size = 32\n",
        "\n",
        "class Couche_LSTMN_Layers(tf.keras.layers.Layer):\n",
        "  # Fonction d'initialisation de la classe d'attention\n",
        "  def __init__(self,dim_LSTM,return_sequence = False, regulariser = 0.0):\n",
        "    self.dim_LSTM = dim_LSTM                # Dimension du vecteur d'attention\n",
        "    self.return_sequence = return_sequence  # Retourne l'ensemble des états cachés ?\n",
        "    self.regulariser = regulariser\n",
        "    super().__init__()                      # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    dim_serie = input_shape[2]\n",
        "    nbr_instants = input_shape[1]\n",
        "\n",
        "    # Paramètres de la forget gate :\n",
        "    # ##############################\n",
        "    # Matrices de poids Wf(40,40), Uf(40,40) et offset bf(40)\n",
        "    self.Wf = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Wf\",trainable=True,regularizer=tf.keras.regularizers.l2(self.regulariser))\n",
        "    self.Uf = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Uf\",trainable=True)\n",
        "    self.bf = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"zeros\",name=\"bf\",trainable=True)\n",
        "\n",
        "    # Paramètres de l'input gate :\n",
        "    # ##############################\n",
        "    # Matrices de poids Wi(40,40), Ui(40,40) et offset bi(40)\n",
        "    self.Wi = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Wi\",trainable=True,regularizer=tf.keras.regularizers.l2(self.regulariser))\n",
        "    self.Ui = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Ui\",trainable=True)\n",
        "    self.bi = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"zeros\",name=\"bi\",trainable=True)\n",
        "\n",
        "    # Paramètres de l'output gate :\n",
        "    # ##############################\n",
        "    # Matrices de poids Wo(40,40), Uo(40,40) et offset bo(40)\n",
        "    self.Wo = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Wo\",trainable=True,regularizer=tf.keras.regularizers.l2(self.regulariser))\n",
        "    self.Uo = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Uo\",trainable=True)\n",
        "    self.bo = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"zeros\",name=\"bo\",trainable=True)\n",
        "\n",
        "    # Paramètres du cell vector :\n",
        "    # ##############################\n",
        "    # Matrices de poids Wc(40,40), Uc(40,40) et offset bc(40)\n",
        "    self.Wc = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Wc\",trainable=True,regularizer=tf.keras.regularizers.l2(self.regulariser))\n",
        "    self.Uc = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Uc\",trainable=True)\n",
        "    self.bc = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"zeros\",name=\"bc\",trainable=True)\n",
        "\n",
        "    # Paramètres de l'attention :\n",
        "    # ###########################\n",
        "    # Matrices de poids Wh(40,40); Wl(40,40) et Wh_t(40,40)\n",
        "    self.Wh = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Wh\",trainable=True)\n",
        "    self.Wl = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Wl\",trainable=True)\n",
        "    self.Wh_t = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Wh_t\",trainable=True)\n",
        "\n",
        "    # Vecteur contexte :  v(1,40)\n",
        "    #############################\n",
        "    self.v = self.add_weight(shape=(dim_serie,self.dim_LSTM),initializer=\"normal\",name=\"v\",trainable=True)\n",
        "\n",
        "    # Memory tape : Ct    Liste[(40,1)]\n",
        "    # #################################\n",
        "    memory_tape = getattr(self, 'memory_tape', None)\n",
        "    if memory_tape is None:\n",
        "      memory_tape = []\n",
        "      memory_tape.append(tf.zeros(shape=(self.dim_LSTM,1)))         # Ct[0] = c0 = 0\n",
        "    self.memory_tape = memory_tape\n",
        "\n",
        "    # Hidden tape : Ht    Liste[(40,1)]\n",
        "    # #################################\n",
        "    hidden_tape = getattr(self, 'hidden_tape', None)\n",
        "    if hidden_tape is None:\n",
        "      hidden_tape = []\n",
        "      hidden_tape.append(tf.zeros(shape=(self.dim_LSTM,1)))         # Ht[0] = h0 = 0\n",
        "    self.hidden_tape = hidden_tape\n",
        "\n",
        "    # Vecteurs d'adaptation du Hidden state : Ht_t    Liste[(40,1)]\n",
        "    # #############################################################\n",
        "    hidden_t_tape = getattr(self, 'hidden_t_tape', None)\n",
        "    if hidden_t_tape is None:\n",
        "      hidden_t_tape = []\n",
        "      hidden_t_tape.append(tf.zeros(shape=(self.dim_LSTM,1)))       # Ht_t[0] = h0_t = 0\n",
        "    self.hidden_t_tape = hidden_t_tape\n",
        "\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Définit la logique de la couche LSTM\n",
        "  # Arguments :   x : Tenseur d'entrée de dimension (None, taille_fenetre,dim_LSTM,1)\n",
        "  def call(self,x):\n",
        "    # Charge les tables mémoires\n",
        "    Ct = self.memory_tape.copy()\n",
        "    Ht = self.hidden_tape.copy()\n",
        "    Ht_t = self.hidden_t_tape.copy()\n",
        "\n",
        "    for t in range(1,x.shape[1]+1):\n",
        "      xt = x[:,t-1,:,:]                         # (32,20,40,1)\n",
        "\n",
        "      # Calcul des poids d'attention\n",
        "      at_i = tf.zeros(shape=(x.shape[0],1,1))   # a0_0 = 0 (32,1,1)\n",
        "      for i in range(1,t):\n",
        "        at_ = tf.matmul(self.v,tf.keras.activations.tanh(tf.matmul(self.Wh,Ht[i]) + tf.matmul(self.Wl,xt) + tf.matmul(self.Wh_t,Ht_t[t-1])))\n",
        "        at_i = tf.concat([at_,at_i],axis=1)\n",
        "      if t > 1:\n",
        "        at_i = tf.slice(at_i,[0,0,0],[at_i.shape[0],at_i.shape[1]-1,at_i.shape[2]])\n",
        "\n",
        "      # Calcul des poids d'attention normalisés\n",
        "      st_i = tf.keras.activations.softmax(at_i,axis=1)        # (32,t,1)\n",
        "\n",
        "      # Calcul du vecteur d'adaptation ht_t\n",
        "      ht_t = tf.zeros(shape=(x.shape[0],self.dim_LSTM,1))                                       # h0_t = 0\n",
        "      for i in range(1,t):\n",
        "        ht_t = ht_t + tf.multiply(tf.expand_dims(st_i[:,i-1,:],axis=-1),Ht[i])      # (32,40,1)\n",
        "\n",
        "      # Calcul du vecteur d'adaptation ct_t\n",
        "      ct_t = tf.zeros(shape=(x.shape[0],self.dim_LSTM,1))                           # c0_t = 0\n",
        "      for i in range(1,t):\n",
        "        ct_t = ct_t + tf.multiply(tf.expand_dims(st_i[:,i-1,:],axis=-1),Ct[i])      # (32,40,1)\n",
        "      \n",
        "      # Calcul du vecteur d'activation de la forget gate à l'instant t\n",
        "      ft = tf.matmul(self.Wf,xt)                    # (40,1)x(32,1,1) = (32,40,1)\n",
        "      ft = ft + tf.matmul(self.Uf,ht_t)             # (40,40)x(32,40,1) = (32,40,1)\n",
        "      ft = ft + self.bf                             # (32,40,1) + (40,1) = (32,40,1)\n",
        "      ft = tf.keras.activations.sigmoid(ft) \n",
        "\n",
        "      # Calcul du vecteur d'activation de l'input gate à l'instant t\n",
        "      it = tf.matmul(self.Wi,xt)                    # (40,1)x(32,1,1) = (32,40,1)\n",
        "      it = it + tf.matmul(self.Ui,ht_t)             # (40,40)x(32,40,1) = (32,40,1)\n",
        "      it = it + self.bi                             # (32,40,1) + (40,1) = (32,40,1)\n",
        "      it = tf.keras.activations.sigmoid(it)\n",
        "\n",
        "      # Calcul du vecteur d'activation de l'output gate à l'instant t\n",
        "      ot = tf.matmul(self.Wo,xt)                    # (40,1)x(32,1,1) = (32,40,1)\n",
        "      ot = ot + tf.matmul(self.Uo,ht_t)             # (40,40)x(32,40,1) = (32,40,1)\n",
        "      ot = ot + self.bo                             # (32,40,1) + (40,1) = (32,40,1)\n",
        "      ot = tf.keras.activations.sigmoid(ot)\n",
        "\n",
        "      # Calcul du cell input activation vector à l'instant t\n",
        "      ct_h = tf.matmul(self.Wc,xt)                 # (40,1)x(32,1,1) = (32,40,1)\n",
        "      ct_h = ct_h + tf.matmul(self.Uc,ht_t)        # (40,40)x(32,40,1) = (32,40,1)\n",
        "      ct_h = ct_h + self.bc                        # (32,40,1) + (40,1) = (32,40,1)\n",
        "      ct_h = tf.keras.activations.tanh(ct_h)\n",
        "\n",
        "      # Calcul du cell state à l'instant t\n",
        "      ct = tf.multiply(ft,ct_t)                    # (32,40,1)\n",
        "      ct = ct + tf.multiply(it,ct_h)               # (32,40,1)\n",
        "      ht = tf.multiply(ot,tf.keras.activations.tanh(ct))\n",
        "\n",
        "      # Enregistre les vecteurs internes du temps courant\n",
        "      Ht.append(ht)                                 # Enregistre Ht[t] = ht\n",
        "      Ct.append(ct)                                 # Enregistre Ct[t] = ct\n",
        "      Ht_t.append(ht_t)                             # Enregistre Ht_t[t] = ht_t\n",
        "\n",
        "    # Enregistre les vecteurs internes du batch courant\n",
        "    for memory_tape_, memory_tape in zip(tf.nest.flatten(self.memory_tape),tf.nest.flatten(Ct)):\n",
        "      memory_tape_ = memory_tape\n",
        "    for hidden_tape_, hidden_tape in zip(tf.nest.flatten(self.hidden_tape),tf.nest.flatten(Ht)):\n",
        "      hidden_tape_ = hidden_tape\n",
        "    for hidden_t_tape_, hidden_t_tape in zip(tf.nest.flatten(self.hidden_t_tape),tf.nest.flatten(Ht_t)):\n",
        "      hidden_t_tape_ = hidden_t_tape\n",
        "\n",
        "    # Retourne le dernier hidden state ou l'ensemble des vecteurs\n",
        "    if self.return_sequence == False:\n",
        "      return tf.squeeze(ht,axis=-1)               # return (32,40)\n",
        "    else:\n",
        "      hidden_states = tf.zeros(shape=(x.shape[0],1,self.dim_LSTM,1))   # (32,1,40,1)\n",
        "      for i in range(0,x.shape[1]):\n",
        "        hidden_states = tf.concat([hidden_states,tf.expand_dims(Ht[i+1],axis=1)],axis=1)\n",
        "      hidden_states = tf.slice(hidden_states,[0,0,0,0],[hidden_states.shape[0],hidden_states.shape[1]-1,hidden_states.shape[2],hidden_states.shape[3]])\n",
        "      return (tf.squeeze(hidden_states,axis=-1))                   # return (32,20,40)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIVnvnym1XU6"
      },
      "source": [
        "**2. Création du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu6n686zyImL"
      },
      "source": [
        "dim_LSTM = 40\n",
        "dim_Att = 100\n",
        "nbr_hop = 5\n",
        "\n",
        "# Fonction de la couche lambda d'entrée\n",
        "def Traitement_Entrees(x):\n",
        "  return tf.expand_dims(x,axis=-1)\n",
        "\n",
        "# Définition de l'entrée du modèle\n",
        "entrees = tf.keras.layers.Input(shape=(taille_fenetre),batch_size=batch_size)\n",
        "\n",
        "# Encodeur\n",
        "e_adapt = tf.keras.layers.Lambda(Traitement_Entrees)(entrees)\n",
        "s_encodeur = Couche_LSTMN(dim_LSTM,return_sequence=True,regulariser=1e-5)(e_adapt)\n",
        "s_encodeur = Couche_LSTMN_Layers(dim_LSTM=dim_LSTM,return_sequence=True,regulariser=1e-5)(s_encodeur)\n",
        "s_attention = Couche_Auto_Attention(dim_att=dim_LSTM,nbr_hop=nbr_hop)(s_encodeur)\n",
        "\n",
        "# Décodeur\n",
        "s_decodeur = tf.keras.layers.Dense(dim_LSTM*nbr_hop,activation=\"tanh\")(s_attention)\n",
        "s_decodeur = tf.keras.layers.Concatenate()([s_decodeur,s_attention])\n",
        "\n",
        "# Générateur\n",
        "sortie = tf.keras.layers.Dense(1)(s_decodeur)\n",
        "\n",
        "# Construction du modèle\n",
        "model = tf.keras.Model(entrees,sortie)\n",
        "\n",
        "model.save_weights(\"model_initial.hdf5\")\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koUv2Kyq1XU6"
      },
      "source": [
        "**3. Entrainement du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4Vny6yr1XU7"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam()\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur, metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset_norm,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aaLeG6n1XU7"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 1])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqaT3i5y1XU7"
      },
      "source": [
        "# Charge les meilleurs poids\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiXDVmvC1XU8"
      },
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "class TimingCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, logs={}):\n",
        "        self.logs=[]\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.starttime = timer()\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.logs.append(timer()-self.starttime)\n",
        "\n",
        "\n",
        "cb = TimingCallback()\n",
        "\n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.001,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0.01\n",
        "    )\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "#optimiseur=tf.keras.optimizers.SGD(learning_rate=lr_schedule,momentum=0.9)\n",
        "optimiseur=tf.keras.optimizers.Adam(0.001)\n",
        "\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur,metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle\n",
        "historique = model.fit(dataset_norm,validation_data=dataset_Val_norm, epochs=20,verbose=1, callbacks=[CheckPoint,cb])\n",
        "\n",
        "print(cb.logs)\n",
        "print(sum(cb.logs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X38GhBRL1XU8"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHjPHr3O1XU8"
      },
      "source": [
        "**4. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IegVdqzG1XU8"
      },
      "source": [
        "# Charge les meilleurs poids\n",
        "model.load_weights(\"poids_train.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnEm_JTa1XU9"
      },
      "source": [
        "taille_fenetre = 20\n",
        "\n",
        "# Création d'une liste vide pour recevoir les prédictions\n",
        "predictions = []\n",
        "\n",
        "# Calcul des prédiction pour chaque groupe de 20 valeurs consécutives de la série\n",
        "# dans l'intervalle de validation\n",
        "for t in temps[temps_separation:-taille_fenetre]:\n",
        "    X = np.reshape(Serie_Normalisee[t:t+taille_fenetre],(1,taille_fenetre))\n",
        "    predictions.append(model.predict(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZ0qtxwp1XU9"
      },
      "source": [
        "# Affiche la série et les prédictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps,serie,label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title('Prédictions avec le modèle LSTMN')\n",
        "plt.show()\n",
        "\n",
        "# Zoom sur l'intervalle de validation\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps[temps_separation:],serie[temps_separation:],label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title(\"Prédictions avec le modèle LSTMN (zoom sur l'intervalle de validation)\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHu47KqE1XU9"
      },
      "source": [
        "# Calcule de l'erreur quadratique moyenne et de l'erreur absolue moyenne \n",
        "\n",
        "mae = tf.keras.metrics.mean_absolute_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "mse = tf.keras.metrics.mean_squared_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "\n",
        "print(mae)\n",
        "print(mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilDkJ-4iKeFb"
      },
      "source": [
        "# Création de la coucheLSTMN multi-couches - Avec \"skip connections\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIeIOTc5KeFl"
      },
      "source": [
        "La structure reste sensiblement la même qu'avant sauf que maintenant :\n",
        " - Les états cachés sont remontés en sortie vers la prochaine couche\n",
        " - L'attention dans les couches supérieures est calculée non plus à partir des valeurs d'entrées mais des anciens états cachés.\n",
        " - Il y a donc une nouvelle matrice de poids qui intervient dans le calcul de l'attention : Wl\n",
        " - Les vecteurs internes des couches intermédiaires sont calculés à partir des entrées X, qui sont donc remontées également sur ces couches. Les couches intermédiaires sont donc à deux entrées."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DegQ3xpKeFm"
      },
      "source": [
        "<img src=\"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/images/LSTMN_Calcul6.png?raw=true\" width=\"1200\"> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5rH9LfLKeFm"
      },
      "source": [
        "**1. Création de la multi-couche LSTMN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUEKGBnYKeFn"
      },
      "source": [
        "# Classe Multi couche LSTMN\n",
        "\n",
        "# Importe le Backend de Keras\n",
        "from keras import backend as K\n",
        "\n",
        "# Définit une nouvelle classe personnelle LSTMN\n",
        "# https://arxiv.org/pdf/1601.06733.pdf\n",
        "\n",
        "\n",
        "# Dans les dimensions ci-dessous :\n",
        "# dimension de la série : dim_serie = 1\n",
        "# dimension des vecteurs internes LSTM : dim_LSTM = 40\n",
        "# batch_size = 32\n",
        "\n",
        "class Couche_LSTMN_Layers(tf.keras.layers.Layer):\n",
        "  # Fonction d'initialisation de la classe d'attention\n",
        "  def __init__(self,dim_LSTM,return_sequence = False, regulariser=0.0):\n",
        "    self.dim_LSTM = dim_LSTM                # Dimension du vecteur d'attention\n",
        "    self.return_sequence = return_sequence  # Retourne l'ensemble des états cachés ?\n",
        "    self.regulariser = regulariser\n",
        "    super().__init__()                      # Appel du __init__() de la classe Layer\n",
        "  \n",
        "\n",
        "  # input_shape :   x : Tenseur d'entrée de dimension (None, taille_fenetre,dim_serie)\n",
        "  #                              \n",
        "  def build(self,input_shape):\n",
        "    dim_serie = input_shape[2]\n",
        "    nbr_instants = input_shape[1]\n",
        "\n",
        "    # Paramètres de la forget gate :\n",
        "    # ##############################\n",
        "    # Matrices de poids Wf(40,1), Uf(40,40) et offset bf(40)\n",
        "    self.Wf = self.add_weight(shape=(self.dim_LSTM,dim_serie),initializer=\"normal\",name=\"Wf\",trainable=True,regularizer=tf.keras.regularizers.l2(self.regulariser))\n",
        "    self.Uf = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Uf\",trainable=True)\n",
        "    self.bf = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"zeros\",name=\"bf\",trainable=True)\n",
        "\n",
        "    # Paramètres de l'input gate :\n",
        "    # ##############################\n",
        "    # Matrices de poids Wi(40,1), Ui(40,40) et offset bi(40)\n",
        "    self.Wi = self.add_weight(shape=(self.dim_LSTM,dim_serie),initializer=\"normal\",name=\"Wi\",trainable=True,regularizer=tf.keras.regularizers.l2(self.regulariser))\n",
        "    self.Ui = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Ui\",trainable=True)\n",
        "    self.bi = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"zeros\",name=\"bi\",trainable=True)\n",
        "\n",
        "    # Paramètres de l'output gate :\n",
        "    # ##############################\n",
        "    # Matrices de poids Wo(40,1), Uo(40,40) et offset bo(40)\n",
        "    self.Wo = self.add_weight(shape=(self.dim_LSTM,dim_serie),initializer=\"normal\",name=\"Wo\",trainable=True,regularizer=tf.keras.regularizers.l2(self.regulariser))\n",
        "    self.Uo = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Uo\",trainable=True)\n",
        "    self.bo = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"zeros\",name=\"bo\",trainable=True)\n",
        "\n",
        "    # Paramètres du cell vector :\n",
        "    # ##############################\n",
        "    # Matrices de poids Wc(40,1), Uc(40,40) et offset bc(40)\n",
        "    self.Wc = self.add_weight(shape=(self.dim_LSTM,dim_serie),initializer=\"normal\",name=\"Wc\",trainable=True,regularizer=tf.keras.regularizers.l2(self.regulariser))\n",
        "    self.Uc = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Uc\",trainable=True)\n",
        "    self.bc = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"zeros\",name=\"bc\",trainable=True)\n",
        "\n",
        "    # Paramètres de l'attention :\n",
        "    # ###########################\n",
        "    # Matrices de poids Wh(40,40); Wl(40,40) et Wh_t(40,40)\n",
        "    self.Wh = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Wh\",trainable=True)\n",
        "    self.Wl = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Wl\",trainable=True)\n",
        "    self.Wh_t = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Wh_t\",trainable=True)\n",
        "\n",
        "    # Vecteur contexte :  v(1,40)\n",
        "    #############################\n",
        "    self.v = self.add_weight(shape=(dim_serie,self.dim_LSTM),initializer=\"normal\",name=\"v\",trainable=True)\n",
        "\n",
        "    # Memory tape : Ct    Liste[(40,1)]\n",
        "    # #################################\n",
        "    memory_tape = getattr(self, 'memory_tape', None)\n",
        "    if memory_tape is None:\n",
        "      memory_tape = []\n",
        "      memory_tape.append(tf.zeros(shape=(self.dim_LSTM,1)))         # Ct[0] = c0 = 0\n",
        "    self.memory_tape = memory_tape\n",
        "\n",
        "    # Hidden tape : Ht    Liste[(40,1)]\n",
        "    # #################################\n",
        "    hidden_tape = getattr(self, 'hidden_tape', None)\n",
        "    if hidden_tape is None:\n",
        "      hidden_tape = []\n",
        "      hidden_tape.append(tf.zeros(shape=(self.dim_LSTM,1)))         # Ht[0] = h0 = 0\n",
        "    self.hidden_tape = hidden_tape\n",
        "\n",
        "    # Vecteurs d'adaptation du Hidden state : Ht_t    Liste[(40,1)]\n",
        "    # #############################################################\n",
        "    hidden_t_tape = getattr(self, 'hidden_t_tape', None)\n",
        "    if hidden_t_tape is None:\n",
        "      hidden_t_tape = []\n",
        "      hidden_t_tape.append(tf.zeros(shape=(self.dim_LSTM,1)))       # Ht_t[0] = h0_t = 0\n",
        "    self.hidden_t_tape = hidden_t_tape\n",
        "\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Définit la logique de la couche LSTM\n",
        "  # Arguments :   X : Tenseur d'entrée de dimension (None, taille_fenetre,dim_serie)\n",
        "  #               H : Tenseur des vecteurs cachés de la couche précédente (None,taille_fenetre,dim_LSTM,1)\n",
        "  def call(self,X,H):\n",
        "    # Charge les tables mémoires\n",
        "    Ct = self.memory_tape.copy()\n",
        "    Ht = self.hidden_tape.copy()\n",
        "    Ht_t = self.hidden_t_tape.copy()\n",
        "\n",
        "    for t in range(1,X.shape[1]+1):\n",
        "      xt = X[:,t-1,:]                      # (32,1)\n",
        "      xt = tf.expand_dims(xt,axis=-1)      # (32,1,1)\n",
        "      h = H[:,t-1,:,:]                     # (32,40,1)\n",
        "\n",
        "      # Calcul des poids d'attention avec les vecteurs cachés de la couche précédente\n",
        "      at_i = tf.zeros(shape=(X.shape[0],1,1))   # a0_0 = 0 (32,1,1)\n",
        "      for i in range(1,t):\n",
        "        at_ = tf.matmul(self.v,tf.keras.activations.tanh(tf.matmul(self.Wh,Ht[i]) + tf.matmul(self.Wl,h) + tf.matmul(self.Wh_t,Ht_t[t-1])))\n",
        "        at_i = tf.concat([at_,at_i],axis=1)\n",
        "      if t > 1:\n",
        "        at_i = tf.slice(at_i,[0,0,0],[at_i.shape[0],at_i.shape[1]-1,at_i.shape[2]])\n",
        "\n",
        "      # Calcul des poids d'attention normalisés\n",
        "      st_i = tf.keras.activations.softmax(at_i,axis=1)        # (32,t,1)\n",
        "\n",
        "      # Calcul du vecteur d'adaptation ht_t\n",
        "      ht_t = tf.zeros(shape=(X.shape[0],self.dim_LSTM,1))                                       # h0_t = 0\n",
        "      for i in range(1,t):\n",
        "        ht_t = ht_t + tf.multiply(tf.expand_dims(st_i[:,i-1,:],axis=-1),Ht[i])      # (32,40,1)\n",
        "\n",
        "      # Calcul du vecteur d'adaptation ct_t\n",
        "      ct_t = tf.zeros(shape=(X.shape[0],self.dim_LSTM,1))                           # c0_t = 0\n",
        "      for i in range(1,t):\n",
        "        ct_t = ct_t + tf.multiply(tf.expand_dims(st_i[:,i-1,:],axis=-1),Ct[i])      # (32,40,1)\n",
        "      \n",
        "      # Calcul du vecteur d'activation de la forget gate à l'instant t avec les entrées X\n",
        "      ft = tf.matmul(self.Wf,xt)                    # (40,1)x(32,1,1) = (32,40,1)\n",
        "      ft = ft + tf.matmul(self.Uf,ht_t)             # (40,40)x(32,40,1) = (32,40,1)\n",
        "      ft = ft + self.bf                             # (32,40,1) + (40,1) = (32,40,1)\n",
        "      ft = tf.keras.activations.sigmoid(ft) \n",
        "\n",
        "      # Calcul du vecteur d'activation de l'input gate à l'instant t avec les entrées X\n",
        "      it = tf.matmul(self.Wi,xt)                    # (40,1)x(32,1,1) = (32,40,1)\n",
        "      it = it + tf.matmul(self.Ui,ht_t)             # (40,40)x(32,40,1) = (32,40,1)\n",
        "      it = it + self.bi                             # (32,40,1) + (40,1) = (32,40,1)\n",
        "      it = tf.keras.activations.sigmoid(it)\n",
        "\n",
        "      # Calcul du vecteur d'activation de l'output gate à l'instant t avec les entrées X\n",
        "      ot = tf.matmul(self.Wo,xt)                    # (40,1)x(32,1,1) = (32,40,1)\n",
        "      ot = ot + tf.matmul(self.Uo,ht_t)             # (40,40)x(32,40,1) = (32,40,1)\n",
        "      ot = ot + self.bo                             # (32,40,1) + (40,1) = (32,40,1)\n",
        "      ot = tf.keras.activations.sigmoid(ot)\n",
        "\n",
        "      # Calcul du cell input activation vector à l'instant t avec les entrées X\n",
        "      ct_h = tf.matmul(self.Wc,xt)                 # (40,1)x(32,1,1) = (32,40,1)\n",
        "      ct_h = ct_h + tf.matmul(self.Uc,ht_t)        # (40,40)x(32,40,1) = (32,40,1)\n",
        "      ct_h = ct_h + self.bc                        # (32,40,1) + (40,1) = (32,40,1)\n",
        "      ct_h = tf.keras.activations.tanh(ct_h)\n",
        "\n",
        "      # Calcul du cell state à l'instant t avec les entrées X\n",
        "      ct = tf.multiply(ft,ct_t)                    # (32,40,1)\n",
        "      ct = ct + tf.multiply(it,ct_h)               # (32,40,1)\n",
        "      ht = tf.multiply(ot,tf.keras.activations.tanh(ct))\n",
        "\n",
        "      # Enregistre les vecteurs internes du temps courant\n",
        "      Ht.append(ht)                                 # Enregistre Ht[t] = ht\n",
        "      Ct.append(ct)                                 # Enregistre Ct[t] = ct\n",
        "      Ht_t.append(ht_t)                             # Enregistre Ht_t[t] = ht_t\n",
        "\n",
        "    # Enregistre les vecteurs internes du batch courant\n",
        "    for memory_tape_, memory_tape in zip(tf.nest.flatten(self.memory_tape),tf.nest.flatten(Ct)):\n",
        "      memory_tape_ = memory_tape\n",
        "    for hidden_tape_, hidden_tape in zip(tf.nest.flatten(self.hidden_tape),tf.nest.flatten(Ht)):\n",
        "      hidden_tape_ = hidden_tape\n",
        "    for hidden_t_tape_, hidden_t_tape in zip(tf.nest.flatten(self.hidden_t_tape),tf.nest.flatten(Ht_t)):\n",
        "      hidden_t_tape_ = hidden_t_tape\n",
        "\n",
        "    # Retourne le dernier hidden state ou l'ensemble des vecteurs\n",
        "    if self.return_sequence == False:\n",
        "      return tf.squeeze(ht,axis=-1)               # return (32,40)\n",
        "    else:\n",
        "      hidden_states = tf.zeros(shape=(X.shape[0],1,self.dim_LSTM,1))   # (32,1,40,1)\n",
        "      for i in range(0,X.shape[1]):\n",
        "        hidden_states = tf.concat([hidden_states,tf.expand_dims(Ht[i+1],axis=1)],axis=1)\n",
        "      hidden_states = tf.slice(hidden_states,[0,0,0,0],[hidden_states.shape[0],hidden_states.shape[1]-1,hidden_states.shape[2],hidden_states.shape[3]])\n",
        "      return (hidden_states)      # return (32,20,40,1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1DNXI2B7HIT"
      },
      "source": [
        "# Prédicitons avec deux couches LSTMN sans attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCgn0V_PKeFq"
      },
      "source": [
        "**1. Création du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGzpSpaw1Hez"
      },
      "source": [
        "# Sans intention\n",
        "# 2 couches LSTMN\n",
        "\n",
        "dim_LSTM = 40\n",
        "\n",
        "# Fonction de la couche lambda d'entrée\n",
        "def Traitement_Entrees(x):\n",
        "  return tf.expand_dims(x,axis=-1)\n",
        "\n",
        "# Définition de l'entrée du modèle\n",
        "entrees_x = tf.keras.layers.Input(shape=(taille_fenetre),batch_size=batch_size)\n",
        "\n",
        "# Encodeur\n",
        "e_adapt = tf.keras.layers.Lambda(Traitement_Entrees)(entrees_x)\n",
        "s_encodeur = Couche_LSTMN(dim_LSTM,return_sequence=True,regulariser=1e-5)(e_adapt)\n",
        "s_encodeur = Couche_LSTMN_Layers(dim_LSTM=dim_LSTM,return_sequence=False,regulariser=1e-5)(e_adapt,s_encodeur)\n",
        "\n",
        "# Décodeur\n",
        "s_decodeur = tf.keras.layers.Dense(dim_LSTM,activation=\"tanh\")(s_encodeur)\n",
        "s_decodeur = tf.keras.layers.Concatenate()([s_decodeur,s_encodeur])\n",
        "\n",
        "# Générateur\n",
        "sortie = tf.keras.layers.Dense(1)(s_decodeur)\n",
        "\n",
        "# Construction du modèle\n",
        "model = tf.keras.Model(entrees_x,sortie)\n",
        "\n",
        "model.save_weights(\"model_initial.hdf5\")\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQZrbpZJKeFu"
      },
      "source": [
        "**2. Entrainement du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmhIb0JUKeFv"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur, metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset_norm,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2s85jr4xKeFv"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 1])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLvbZdTBKeFw"
      },
      "source": [
        "# Charge les meilleurs poids\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuZvPbowKeFx"
      },
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "class TimingCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, logs={}):\n",
        "        self.logs=[]\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.starttime = timer()\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.logs.append(timer()-self.starttime)\n",
        "\n",
        "\n",
        "cb = TimingCallback()\n",
        "\n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.001,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0.1\n",
        "    )\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "#optimiseur=tf.keras.optimizers.SGD(learning_rate=lr_schedule,momentum=0.9)\n",
        "optimiseur=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur,metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle\n",
        "historique = model.fit(dataset_norm,validation_data=dataset_Val_norm, epochs=30,verbose=1, callbacks=[CheckPoint,cb])\n",
        "\n",
        "print(cb.logs)\n",
        "print(sum(cb.logs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CB5qu-UKeFx"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ovY-xTgKeFz"
      },
      "source": [
        "**3. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9JaY8bdKeF0"
      },
      "source": [
        "taille_fenetre = 20\n",
        "\n",
        "# Création d'une liste vide pour recevoir les prédictions\n",
        "predictions = []\n",
        "\n",
        "# Calcul des prédiction pour chaque groupe de 20 valeurs consécutives de la série\n",
        "# dans l'intervalle de validation\n",
        "for t in temps[temps_separation:-taille_fenetre]:\n",
        "    X = np.reshape(Serie_Normalisee[t:t+taille_fenetre],(1,taille_fenetre))\n",
        "    predictions.append(model.predict(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDC0v2xTKeF0"
      },
      "source": [
        "# Affiche la série et les prédictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps,serie,label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title('Prédictions avec le modèle LSTMN à deux couches')\n",
        "plt.show()\n",
        "\n",
        "# Zoom sur l'intervalle de validation\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps[temps_separation:],serie[temps_separation:],label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title(\"Prédictions avec le modèle LSTMN à deux couches(zoom sur l'intervalle de validation)\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v8QjYSFKeF1"
      },
      "source": [
        "# Calcule de l'erreur quadratique moyenne et de l'erreur absolue moyenne \n",
        "\n",
        "mae = tf.keras.metrics.mean_absolute_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "mse = tf.keras.metrics.mean_squared_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "\n",
        "print(mae)\n",
        "print(mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1-rz47bK0LY"
      },
      "source": [
        "# Prédicitons avec trois couches LSTMN sans attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FJwhq-YK0Lz"
      },
      "source": [
        "**1. Création du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdfxBEs4K0L0"
      },
      "source": [
        "# Sans intention\n",
        "# 2 couches LSTMN\n",
        "\n",
        "dim_LSTM = 40\n",
        "\n",
        "# Fonction de la couche lambda d'entrée\n",
        "def Traitement_Entrees(x):\n",
        "  return tf.expand_dims(x,axis=-1)\n",
        "\n",
        "# Définition de l'entrée du modèle\n",
        "entrees_x = tf.keras.layers.Input(shape=(taille_fenetre),batch_size=batch_size)\n",
        "\n",
        "# Encodeur\n",
        "e_adapt = tf.keras.layers.Lambda(Traitement_Entrees)(entrees_x)\n",
        "s_encodeur = Couche_LSTMN(dim_LSTM,return_sequence=True,regulariser=1e-5)(e_adapt)\n",
        "s_encodeur = Couche_LSTMN_Layers(dim_LSTM=dim_LSTM,return_sequence=True,regulariser=1e-5)(e_adapt,s_encodeur)\n",
        "s_encodeur = Couche_LSTMN_Layers(dim_LSTM=dim_LSTM,return_sequence=False,regulariser=1e-5)(e_adapt,s_encodeur)\n",
        "\n",
        "# Décodeur\n",
        "s_decodeur = tf.keras.layers.Dense(dim_LSTM,activation=\"tanh\")(s_encodeur)\n",
        "s_decodeur = tf.keras.layers.Concatenate()([s_decodeur,s_encodeur])\n",
        "\n",
        "# Générateur\n",
        "sortie = tf.keras.layers.Dense(1)(s_decodeur)\n",
        "\n",
        "# Construction du modèle\n",
        "model = tf.keras.Model(entrees_x,sortie)\n",
        "\n",
        "model.save_weights(\"model_initial.hdf5\")\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msfRt91VK0L1"
      },
      "source": [
        "**2. Entrainement du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzYUKF-zK0L2"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam()\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur, metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset_norm,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKhqMEdgK0L2"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 1])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYVkz6LBK0L3"
      },
      "source": [
        "# Charge les meilleurs poids\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hb1epY8K0L3"
      },
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "class TimingCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, logs={}):\n",
        "        self.logs=[]\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.starttime = timer()\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.logs.append(timer()-self.starttime)\n",
        "\n",
        "\n",
        "cb = TimingCallback()\n",
        "\n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.01,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0.01\n",
        "    )\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur,metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle\n",
        "historique = model.fit(dataset_norm,validation_data=dataset_Val_norm, epochs=30,verbose=1, callbacks=[CheckPoint,cb])\n",
        "\n",
        "print(cb.logs)\n",
        "print(sum(cb.logs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhRskTeLK0L4"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEzOAUWMK0L4"
      },
      "source": [
        "**3. Prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-j2SbCNK0L5"
      },
      "source": [
        "taille_fenetre = 20\n",
        "\n",
        "# Création d'une liste vide pour recevoir les prédictions\n",
        "predictions = []\n",
        "\n",
        "# Calcul des prédiction pour chaque groupe de 20 valeurs consécutives de la série\n",
        "# dans l'intervalle de validation\n",
        "for t in temps[temps_separation:-taille_fenetre]:\n",
        "    X = np.reshape(Serie_Normalisee[t:t+taille_fenetre],(1,taille_fenetre))\n",
        "    predictions.append(model.predict(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3kYRo2GK0L5"
      },
      "source": [
        "# Affiche la série et les prédictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps,serie,label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title('Prédictions avec le modèle LSTMN')\n",
        "plt.show()\n",
        "\n",
        "# Zoom sur l'intervalle de validation\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps[temps_separation:],serie[temps_separation:],label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title(\"Prédictions avec le modèle LSTMN (zoom sur l'intervalle de validation)\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIldKrdGK0L5"
      },
      "source": [
        "# Calcule de l'erreur quadratique moyenne et de l'erreur absolue moyenne \n",
        "\n",
        "mae = tf.keras.metrics.mean_absolute_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "mse = tf.keras.metrics.mean_squared_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "\n",
        "print(mae)\n",
        "print(mse)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}