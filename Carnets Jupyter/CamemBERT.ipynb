{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment-BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzTINeLp1aBD",
        "colab_type": "text"
      },
      "source": [
        "# **CamemBERT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CevacBEycWG",
        "colab_type": "text"
      },
      "source": [
        "[CamemBERT](https://camembert-model.fr/) est un modèle de traitement automatique de la langue Française, basé sur l'architecture [RoBERTa](https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/) développée par Facebook AI en 2019, dédiée à l'Anglais. RoBERTa est lui même basé sur [BERT](https://fr.wikipedia.org/wiki/BERT_(mod%C3%A8le_de_langage)) qui a été développé par Google en 2018.  \n",
        "CamemBERT est donc un cousin Français de BERT, qui a pu voir le jour lorsque les équipes de Facebook associés aux chercheurs de [l'INRIA](https://www.inria.fr/fr) ont rendu public ce modèle pré-entrainé sur 138GB de texte Français.  \n",
        "CamemBERT a été pré-entraîné sur un corpus francophone et avec des hyper-paramètres différents découverts et testés pour la première fois par l’équipe de Facebook. Le choix de ces hyper-paramètres était tellement réussi que l’entreprise a annoncé le sortie d’un “nouveau” modèle baptisé RoBERTa. Pourtant, il n’y a rien de nouveau dans RoBERTA qui comme CamemBERT reste une copie de BERT. Voici ces hyper-paramètres:  \n",
        "*  CamemBERT choisit les mots à prédire de manière dynamique, c’est-à-dire, non pas lors du pré-processing des données en entrée, mais lors de forward pass, en masquant au hasard certains mots d’une séquence.  \n",
        "*  Il utilise un batch size différent: ~8 000 contre 256 dans le cas de BERT.  \n",
        "*  CamemBERT a un seul objectif de pré-entrainement: prédiction des “mots masqués” d’une séquence. BERT en avait deux : prédiction des “mots masqués” et de la phrase suivante d’une séquence. Ce dernier objectif s’est avéré improductif pour l’entrainement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aeHO-8w11jm",
        "colab_type": "text"
      },
      "source": [
        "# **Exemple d'utlisation de CamemBERT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdqHiJ0_1-yr",
        "colab_type": "text"
      },
      "source": [
        "CamemBERT a été entrainé dans le but de prédire des \"mots masqués\" dans un texte. Nous allons voir un exemple de ce que peut faire ce modèle pré-entrainé."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDlpYuxU2v6B",
        "colab_type": "text"
      },
      "source": [
        "Commençons par installer sur la machine les modules python dont nous aurons besoin et qui ne sont pas pré-installés. Ensuite nous importons les bibliothèques.  \n",
        "En particulier, nous utiliserons la bibliothèque [Transformers](https://huggingface.co/transformers/#) créée par [Hugging Face](https://huggingface.co/). Cette bibliothèque contient des centaines de modèles pré-entrainés pour réaliser des opérations sur les données textuelles, comme la classification, l'extraction d'informations, le \"questions-réponses\", la traduction, ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdwgQ7MfNoj2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJzvJaBONyHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from transformers import TFCamembertForMaskedLM\n",
        "from transformers import AutoTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV2cj3KN4rDm",
        "colab_type": "text"
      },
      "source": [
        "Définisson la variable tokenizer qui permettra d'instancier le tokenizer pour CamemBERT fourni par la librairie \"transformers\" :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "772aq9lI41KX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('jplu/tf-camembert-base',)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fI43XfZ4JuL",
        "colab_type": "text"
      },
      "source": [
        "Définissons maintenant la phrase que nopus souhaitons compléter puis utlisons le tokenizer de la bibliothèque \"transformers\" pour préparer le texte :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9mWdtCE9NAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "phrase = \"L'intelligence artificielle va mener à la <mask> du monde !\"\n",
        "output_tokenizer = tokenizer.encode_plus(phrase, max_length=100, padding=\"longest\", truncation=True, return_tensors='tf')\n",
        "output_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENYtpJbd5qRd",
        "colab_type": "text"
      },
      "source": [
        "Récupérons la position du masque (mask) dans la séquence retournée par le tokenizer :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWEF1kRt5-EW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mask_index = (output_tokenizer['input_ids'][0].numpy() == tokenizer.mask_token_id).nonzero()\n",
        "mask_index = np.reshape(mask_index,(1))[0]\n",
        "mask_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3luydxK483g",
        "colab_type": "text"
      },
      "source": [
        "Instancions maintenant le modèle CamemBERT pré-entrainé avec le jeu de données de base pour Tensorflow :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtZniccuxmi2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = TFCamembertForMaskedLM.from_pretrained(\"jplu/tf-camembert-base\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8upQ4QXi9Hbw",
        "colab_type": "text"
      },
      "source": [
        "Puis lançons le modèle sur la phrase :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXVhMcTk25xQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output = model(output_tokenizer['input_ids'])[0]\n",
        "output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS_IKc1CHb8U",
        "colab_type": "text"
      },
      "source": [
        "On récupère les (32005 !) valeurs en sortie du modèle correspondants à l'emplacement du masque :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HbqsyMHQQT8Y",
        "colab": {}
      },
      "source": [
        "output  = output[0, mask_index, :]\n",
        "output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7Gdzo6TH4-A",
        "colab_type": "text"
      },
      "source": [
        "Puis on applique une fonctioon d'activation Soft-Max afin de normaliser les probabilités sur chaque valeurs :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpgh6OwtQ2FO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "proba = tf.nn.softmax(output)\n",
        "proba"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuOhpSQFIVIY",
        "colab_type": "text"
      },
      "source": [
        "On récupère ensuite les valeurs et les indices des 5 plus grandes probabilités parmi ces 32005 probabilités :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8phoc1vLR9ej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top_proba, top_indices = tf.math.top_k(proba,k=5)\n",
        "print(top_proba)\n",
        "print(top_indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsz7jaXjIjhr",
        "colab_type": "text"
      },
      "source": [
        "On va chercher ensuite les valeurs numériques des mots correspondants aux emplacements de ces probibilités les plus fortes, puis on reconvertit ces valeurs en mots réels (fonction inverse du tokenizer) :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5z_jPAvTktU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "topk_predicted_token_bpe2 = \" \".join([tokenizer.convert_ids_to_tokens(int(tf.keras.backend.get_value(top_indices[i]))) for i in range(len(top_indices))])\n",
        "topk_predicted_token_bpe2 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWJm87fFI9UB",
        "colab_type": "text"
      },
      "source": [
        "On place dans la variable \"mask\" le mot clé utlisé dans la phrase pour le masque :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50TaMa6noF6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mask = tokenizer.mask_token\n",
        "mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc_1FBxkJJsT",
        "colab_type": "text"
      },
      "source": [
        "On sépare les résultats obtenus :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NScwVqbvqvK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "topk_predicted_token_bpe2.split(\" \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9gHN2YJJPoI",
        "colab_type": "text"
      },
      "source": [
        "Puis on remplace le masque dans la phrase initiale, en utlisant toutes les possibilités trouvées :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jD_3OKp8vfkz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "topk_filled_outputs_ = []\n",
        "for index2, predicted_token_bpe2 in enumerate(topk_predicted_token_bpe2.split(\" \")):\n",
        "  predicted_token_ = predicted_token_bpe2.replace(\"\\u2581\", \" \")\n",
        "  if \" {0}\".format(tokenizer.mask_token) in phrase:\n",
        "    topk_filled_outputs_.append((phrase.replace(\" {0}\".format(tokenizer.mask_token), predicted_token_)))\n",
        "  else:\n",
        "    topk_filled_outputs_.append((phrase.replace(tokenizer.mask_token, predicted_token_)))\n",
        "\n",
        "topk_filled_outputs_"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}