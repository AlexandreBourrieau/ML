{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment-BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzTINeLp1aBD"
      },
      "source": [
        "# **CamemBERT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CevacBEycWG"
      },
      "source": [
        "[CamemBERT](https://camembert-model.fr/) est un modèle de traitement automatique de la langue Française, basé sur l'architecture [RoBERTa](https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/) développée par Facebook AI en 2019, dédiée à l'Anglais. RoBERTa est lui même basé sur [BERT](https://fr.wikipedia.org/wiki/BERT_(mod%C3%A8le_de_langage)) qui a été développé par Google en 2018.  \n",
        "CamemBERT est donc un cousin Français de BERT, qui a pu voir le jour lorsque les équipes de Facebook associés aux chercheurs de [l'INRIA](https://www.inria.fr/fr) ont rendu public ce modèle pré-entrainé sur 138GB de texte Français.  \n",
        "CamemBERT a été pré-entraîné sur un corpus francophone et avec des hyper-paramètres différents découverts et testés pour la première fois par l’équipe de Facebook. Le choix de ces hyper-paramètres était tellement réussi que l’entreprise a annoncé le sortie d’un “nouveau” modèle baptisé RoBERTa. Pourtant, il n’y a rien de nouveau dans RoBERTA qui comme CamemBERT reste une copie de BERT. Voici ces hyper-paramètres:  \n",
        "*  CamemBERT choisit les mots à prédire de manière dynamique, c’est-à-dire, non pas lors du pré-processing des données en entrée, mais lors de forward pass, en masquant au hasard certains mots d’une séquence.  \n",
        "*  Il utilise un batch size différent: ~8 000 contre 256 dans le cas de BERT.  \n",
        "*  CamemBERT a un seul objectif de pré-entrainement: prédiction des “mots masqués” d’une séquence. BERT en avait deux : prédiction des “mots masqués” et de la phrase suivante d’une séquence. Ce dernier objectif s’est avéré improductif pour l’entrainement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aeHO-8w11jm"
      },
      "source": [
        "# **Exemple d'utlisation de CamemBERT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdqHiJ0_1-yr"
      },
      "source": [
        "CamemBERT a été entrainé dans le but de prédire des \"mots masqués\" dans un texte. Nous allons voir un exemple de ce que peut faire ce modèle pré-entrainé."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDlpYuxU2v6B"
      },
      "source": [
        "Commençons par installer sur la machine les modules python dont nous aurons besoin et qui ne sont pas pré-installés. Ensuite nous importons les bibliothèques.  \n",
        "En particulier, nous utiliserons la bibliothèque [Transformers](https://huggingface.co/transformers/#) créée par [Hugging Face](https://huggingface.co/). Cette bibliothèque contient des centaines de modèles pré-entrainés pour réaliser des opérations sur les données textuelles, comme la classification, l'extraction d'informations, le \"questions-réponses\", la traduction, ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdwgQ7MfNoj2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a852c00f-9f98-43d2-bac1-86c5d56ac047"
      },
      "source": [
        "!pip3 install transformers --quiet\r\n",
        "!pip3 install transformers sentencepiece"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.3.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.91)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJzvJaBONyHe"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from transformers import TFCamembertForMaskedLM\n",
        "from transformers import AutoTokenizer"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV2cj3KN4rDm"
      },
      "source": [
        "Définisson la variable tokenizer qui permettra d'instancier le tokenizer pour CamemBERT fourni par la librairie \"transformers\" :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "772aq9lI41KX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "704b27c9-6549-4750-fd94-5d077da53263"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"jplu/tf-camembert-base\",use_fast=False)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-64e4ab9eaff7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"jplu/tf-camembert-base\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muse_fast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0mtokenizer_class_py\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTOKENIZER_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muse_fast\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         return cls._from_pretrained(\n\u001b[0;32m-> 1789\u001b[0;31m             \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_configuration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1790\u001b[0m         )\n\u001b[1;32m   1791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         \u001b[0;31m# Instantiate tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1859\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m             \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1861\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m             raise OSError(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/models/camembert/tokenization_camembert_fast.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, tokenizer_file, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, additional_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mmask_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0madditional_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madditional_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         )\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             raise ValueError(\n\u001b[0;32m--> 103\u001b[0;31m                 \u001b[0;34m\"Couldn't instantiate the backend tokenizer from one of: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m                 \u001b[0;34m\"(1) a `tokenizers` library serialization file, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;34m\"(2) a slow tokenizer instance to convert or \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Couldn't instantiate the backend tokenizer from one of: (1) a `tokenizers` library serialization file, (2) a slow tokenizer instance to convert or (3) an equivalent slow tokenizer class to instantiate and convert. You need to have sentencepiece installed to convert a slow tokenizer to a fast one."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fI43XfZ4JuL"
      },
      "source": [
        "Définissons maintenant la phrase que nopus souhaitons compléter puis utlisons le tokenizer de la bibliothèque \"transformers\" pour préparer le texte :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9mWdtCE9NAP"
      },
      "source": [
        "phrase = \"L'intelligence artificielle va mener à la <mask> du monde !\"\n",
        "output_tokenizer = tokenizer.encode_plus(phrase, max_length=100, padding=\"longest\", truncation=True, return_tensors='tf')\n",
        "output_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENYtpJbd5qRd"
      },
      "source": [
        "Récupérons la position du masque (mask) dans la séquence retournée par le tokenizer :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWEF1kRt5-EW"
      },
      "source": [
        "mask_index = (output_tokenizer['input_ids'][0].numpy() == tokenizer.mask_token_id).nonzero()\n",
        "mask_index = np.reshape(mask_index,(1))[0]\n",
        "mask_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3luydxK483g"
      },
      "source": [
        "Instancions maintenant le modèle CamemBERT pré-entrainé avec le jeu de données de base pour Tensorflow :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtZniccuxmi2"
      },
      "source": [
        "model = TFCamembertForMaskedLM.from_pretrained(\"jplu/tf-camembert-base\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8upQ4QXi9Hbw"
      },
      "source": [
        "Puis lançons le modèle sur la phrase :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXVhMcTk25xQ"
      },
      "source": [
        "output = model(output_tokenizer['input_ids'])[0]\n",
        "output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS_IKc1CHb8U"
      },
      "source": [
        "On récupère les (32005 !) valeurs en sortie du modèle correspondants à l'emplacement du masque :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbqsyMHQQT8Y"
      },
      "source": [
        "output  = output[0, mask_index, :]\n",
        "output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7Gdzo6TH4-A"
      },
      "source": [
        "Puis on applique une fonctioon d'activation Soft-Max afin de normaliser les probabilités sur chaque valeurs :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpgh6OwtQ2FO"
      },
      "source": [
        "proba = tf.nn.softmax(output)\n",
        "proba"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuOhpSQFIVIY"
      },
      "source": [
        "On récupère ensuite les valeurs et les indices des 5 plus grandes probabilités parmi ces 32005 probabilités :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8phoc1vLR9ej"
      },
      "source": [
        "top_proba, top_indices = tf.math.top_k(proba,k=5)\n",
        "print(top_proba)\n",
        "print(top_indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsz7jaXjIjhr"
      },
      "source": [
        "On va chercher ensuite les valeurs numériques des mots correspondants aux emplacements de ces probibilités les plus fortes, puis on reconvertit ces valeurs en mots réels (fonction inverse du tokenizer) :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5z_jPAvTktU"
      },
      "source": [
        "topk_predicted_token_bpe2 = \" \".join([tokenizer.convert_ids_to_tokens(int(tf.keras.backend.get_value(top_indices[i]))) for i in range(len(top_indices))])\n",
        "topk_predicted_token_bpe2 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWJm87fFI9UB"
      },
      "source": [
        "On place dans la variable \"mask\" le mot clé utlisé dans la phrase pour le masque :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50TaMa6noF6r"
      },
      "source": [
        "mask = tokenizer.mask_token\n",
        "mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc_1FBxkJJsT"
      },
      "source": [
        "On sépare les résultats obtenus :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NScwVqbvqvK"
      },
      "source": [
        "topk_predicted_token_bpe2.split(\" \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9gHN2YJJPoI"
      },
      "source": [
        "Puis on remplace le masque dans la phrase initiale, en utlisant toutes les possibilités trouvées :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jD_3OKp8vfkz"
      },
      "source": [
        "topk_filled_outputs_ = []\n",
        "for index2, predicted_token_bpe2 in enumerate(topk_predicted_token_bpe2.split(\" \")):\n",
        "  predicted_token_ = predicted_token_bpe2.replace(\"\\u2581\", \" \")\n",
        "  if \" {0}\".format(tokenizer.mask_token) in phrase:\n",
        "    topk_filled_outputs_.append((phrase.replace(\" {0}\".format(tokenizer.mask_token), predicted_token_)))\n",
        "  else:\n",
        "    topk_filled_outputs_.append((phrase.replace(tokenizer.mask_token, predicted_token_)))\n",
        "\n",
        "topk_filled_outputs_"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}